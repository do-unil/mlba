{
  "hash": "c4323be14263a76d85983aad46a90a3e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Web scraping & APIs\"\nexecute:\n  freeze: auto\n  eval: false\n---\n\n\n\n\n# Motivation\n\nNavigating the digital age means unlocking the treasure trove of data available online. Web scraping and APIs aren't just technical skills; they're your keys to a universe of data for any project you can imagine. Think about the ease of analyzing trends, financial markets, or even sports‚Äîall through data you gather and analyze yourself.\n\nIn this section, Ilia walks us through the essentials of harnessing web data, offering a powerful alternative for those looking to source unique datasets for their projects. Knowing these techniques empowers you to find and utilize data that sparks your curiosity and fuels your research. Let's dive in and discover how these tools can transform your approach to data collection.\n\n# Video lecture\n\nFirst, we start with a video lecture given by Ilia on web scraping and the use of APIs during the [Text Mining course](https://hec-si.unil.ch/gide-api/web/syllabus/2840?base_url=https://www.unil.ch/hec/fr/home/menuinst/masters/management/cours-et-horaires.html?url=) of 2022. The rest of this page contains a set of practice exercises that were shared during this lecture.\n\n<!-- {{< video https://drive.google.com/file/d/13hz208TzcE0WgnZiS48ZKafAEpnqp_cZ/preview >}} -->\n\n<style>\n.video-container {\n  position: relative;\n  padding-bottom: 56.25%; /* 16:9 aspect ratio */\n  padding-top: 25px;\n  height: 0;\n}\n\n.video-container iframe {\n  position: absolute;\n  top: 0;\n  left: 0;\n  width: 100%;\n  height: 100%;\n}\n</style>\n\n<!-- Embed video -->\n<div class=\"video-container\">\n<iframe src=\"https://drive.google.com/file/d/13hz208TzcE0WgnZiS48ZKafAEpnqp_cZ/preview\" allow=\"autoplay\" allowfullscreen></iframe>\n</div>\n\n<br>\n\n[In-class R script shown in the video above üìÑ](SeleniumRDemo.R)\n\n[In-class and practice datasets üìÅ](data.zip)\n\n[Selector gadget üîó](https://selectorgadget.com/)\n\n# Practice web scraping in R\n\n::: callout-note\nUnlike the lab sessions, we do not provide the Python code, but the principles behind web scraping in R and Python remain the same.\n:::\n\n## Using CSS\n\nIn this pratice, we learn how to use the `rvest` package to extract  information from the famous IMDB (Internet Movie Database) site of the 50 most popular movies (<https://www.imdb.com/search/title/?groups=top_250&sort=user_rating>). The page was \nsaved (downloaded) and is also available in the `data/` folder. Alternatively, you can directly work on the link. However, bear in mind that thr structure of online websites can change in time, therefore, the code below might need adjustments (i.e., change in tags).\n\nFirst, we load the page. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\nlibrary(magick)\nlibrary(tidyverse)\nlibrary(flextable)\nlibrary(pdftools)\nlibrary(tesseract)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# local file (html)\nimdb.html <- read_html(\"data/IMDb _Top 250.html\") \n# or alternatively use the link\n# imdb.html <- read_html(\"https://www.imdb.com/search/title/?groups=top_250&sort=user_rating\") # webpage\n```\n:::\n\n\n\n\nNow, we identify *the positions of the titles*. On the web page (opened preferably with Chrome) right-click on a title and select \"Inspect\". The tag corresponding to the titles appears on the developer window (partially reproduced below).\n\n\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n<div class=\"lister-item-content\">\n<h3 class=\"lister-item-header\">\n    <span class=\"lister-item-index unbold text-primary\">1.</span>\n    <a href=\"https://www.imdb.com/title/tt0111161/?ref_=adv_li_tt\">The Shawshank Redemption</a>\n    <span class=\"lister-item-year text-muted unbold\">(1994)</span>\n</h3>\n[...]\n</div>\n```\n:::\n\n\n\n\nLooking above, the title (\"The Shawshank Redemption\") is under the `div` tag with `class=\"lister-item-content\"`, then the sub-tag `h3` within it then the tag `a` within it. The `html_nodes` function can target this tag. The \"dot\" after `div` indicates the class value. It actually targets all such tags.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitles <- imdb.html %>%\n  html_nodes(\"div.lister-item-content h3 a\") \nhead(titles) \n```\n:::\n\n\n\n\nThe results are cleaned from the html code (i.e., only the texts remain) using `html_text2` function.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitles <- html_text2(titles)\nhead(titles)\n```\n:::\n\n\n\n\nAnother way would have been to use the fact that the targeted `h3` tags have a class value. Modify the previous code to extract tags a within `h3` with class value \"lister-item-header\". \n\n<details><summary>**Answer**</summary>\n<p>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitles <- imdb.html %>% \n  html_nodes(\"h3.lister-item-header a\") %>% \n  html_text2()\ntitles\n```\n:::\n\n\n\n</p></details>\n\nNow, repeat that approach for the year and the run time. You may use the function `substr` to extract the year from the text.\n\n<details><summary>**Answer**</summary>\n<p>\nFor the years:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Extract the years\nyears <- imdb.html %>% \n  html_nodes(\"div.lister-item-content h3 .lister-item-year\") %>%\n  html_text2()\nyears <- as.numeric(substr(years,\n                           start = 2,\n                           stop = 5))\n# take only characters 2 to 5 corresponding to the year\n##years <- as.numeric(gsub(\"[^0-9.-]\", \"\", years)) # an alternative: keep only the numbers in a string\n```\n:::\n\n\n\n\nFor the run times, first they are extracted in the format \"120 min\". Then, the run time is split by space which gives \"120\" and \"min\". The `unlist` command casts this to a vector. Then we take one element every two (corresponding to the minutes).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nruntimes <- imdb.html %>%\n  html_nodes(\"span.runtime\") %>%\n  html_text2()\nruntimes <- as.numeric(\n  unlist(\n    strsplit(runtimes, \" \"))[seq(from = 1, by = 2, len = 50)]) \n# by space, \n```\n:::\n\n\n\n</p></details>\n\nSuppose that now we want to extract the description. In this case, there is no unique class value identifying the field (see the html code). However, one can note that it is the 4^th^ paragraph (element) within a `div` tag with a useful class value. To access the k-th paragraph you can use `p.nth-child(k)` starting from the correct hierarchical position. For example, `p:nth-child(2)` extract the 2-nd paragraph. \n\nFor the 4-th paragraph (i.e., the wanted description), a possible code is thus\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndesc <- imdb.html %>% \n  html_nodes(\"div.lister-item-content p:nth-child(4)\") %>% \n  html_text2()\nhead(desc)\n```\n:::\n\n\n\n\nTo finish, we build a data frame containing this information (tibble format below).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimdb.top.50 <- tibble(data.frame(\n  Titles = titles,\n  Years = years,\n  RunTimes = runtimes,\n Desc = desc))\nimdb.top.50 %>% \n  head() %>% \n  flextable() %>% \n  autofit()\n```\n:::\n\n\n\n\n## XPath \n\nIn the previous part, we used the CSS to identify the tags. We now use an alternative: the XPath. The Xpath is preferably used when we want to extract a specific text. For example, we want to extract the description of the first description: right-click and select inspect. Then right-click the corresponding code line, and select \"Copy xpath\". Pass this, to the xpath parameter of `html_nodes` like below:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndesc1 <- imdb.html %>% \n  html_nodes(xpath=\"//*[@id='main']/div/div[3]/div/div[1]/div[3]/p[2]/text()\") %>% \n  html_text2()\ndesc1\n```\n:::\n\n\n\n\n::: callout-note\nIn the `xpath`, you must turn the quotes around `main` to simple quotes.\n:::\n\nThis is convenient when you want to extract a particular text. You can also use the [Selector Gadget](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=en) from Chrome to extract multiple Xpath.\n\n# Parsing PDF files\n\nIn this part, we practice the text extraction from a PDF file. First, we use the `pdf_text` function to read the text on the file \"cs.pdf\".\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncs.text <- pdf_text(\"data/cs.pdf\")\ncs.text[[1]]\n```\n:::\n\n\n\n\nThe resulting object is a vector of strings (one element per page). By inspecting the first one, you see that there are lots of EOL characters (\\r\\n). Suppose now that we want to extract separately all the lines, we can use the function `readr::read_lines` that will split them accordingly.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncs.text <- cs.text %>% \n  read_lines()\nhead(cs.text)\n```\n:::\n\n\n\n\n::: callout-warning\n### The package `tabulizer` may not work\nPlease note that the package `tabulizer` is not updated as often anymore. Therefore, even when trying to get it from its github page (<https://github.com/ropensci/tabulizer>), it may not work. Nevertheless, we have kept the details here, and you can use it from your side or alternatively look for similar packages online.\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nif (!require(\"remotes\")) {\n    install.packages(\"remotes\")\n}\n# on 64-bit Windows\nremotes::install_github(c(\"ropensci/tabulizerjars\", \"ropensci/tabulizer\"), INSTALL_opts = \"--no-multiarch\")\n# elsewhere\nremotes::install_github(c(\"ropensci/tabulizerjars\", \"ropensci/tabulizer\"))\n```\n:::\n\n\n\n\nSuppose now that we want to extract the Table in Appendix p.10. The library `tabulizer` and its function `extract_tables` allows to make a step in this direction. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cs.table <- tabulizer::extract_tables(\"data/cs.pdf\",\n#   output = \"data.frame\",\n#   pages = 10, \n#   guess = TRUE\n# )\n# cs.table[[1]][1:10,] ## 10 first rows of the table (which is stored in the first element of a list)\n```\n:::\n\n\n\n\nWe can note that special characters (here special dashes \"-\") could not be recognized. But the encoding can now be work out and the table be cleaned for further usage. The technique is far from perfect and is unlikly to be of any use for automatic extraction on a large number of documents, except if they are all very well structured.\n\n# Optical Character Recognition (OCR)\n\nIn this part, we use the Optical Character Recognition (OCR) functionalities accessible via R to read a text from an image. First, we read the image using `image_read` in the package `magick`, the result is then passed to the `image_ocr` function in the package `tesseract`. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext <- image_read(\"data/TextFR.png\") %>%\n  image_ocr()\ntext\n```\n:::\n\n\n\n\nDepending on your OS, you may find several mistakes in the final result. An improvement can be obtain by indicating to `image_ocr()` the language, here French. You may need to download the language first:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntesseract::tesseract_download(\"fra\")\ntext <- image_read(\"data/TextFR.png\") %>%\n  image_ocr(language = \"fra\")\ntext\n```\n:::\n\n\n\n\nIt is still not perfect.  Further improvement can be obtained by cleaning the image before to apply the OCR. This can be done with several tools in package `magick`. Below some proposition:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntext <- image_read(\"data/TextFR.png\") %>%\n  image_resize(\"4000x\") %>% ## resize the picture width -> 4000 keeping the ratio\n  image_convert(colorspace = 'gray') %>% ## change the background color\n  image_trim() %>% ## removes edges that are the background color from the image\n  image_ocr(language = \"fra\") \ntext\ncat(text) ## a more friendly version\n```\n:::\n\n\n\n\nThe result should be close to perfect.\n\n# API\n\nIn this final part, you are invited to create your API key for [The Guardian Open Plateform](https://open-platform.theguardian.com/) and to use it to extract papers using the `guardianapi` package.\n\nFirst, register to the open platform and save your key character. Then we request articles about \"Arsenal Football Club\" between for parts of August of 2022.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(guardianapi)\ngu_api_key()\n#My Key: \"fa9a4ddf-1e70-404f-889c-70ef31414cc5\"\n#Enter your key\nArsenal <- gu_content(\"Arsenal Football Club\",\n                      from_date = \"2022-08-01\",\n                      to_date = \"2022-08-22\")\n```\n:::\n\n\n\n\nAs we see on the first article, the text hence read is an HTML code. We now turn it into a text using `rvest`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\nread_html(Arsenal$body[1]) %>% html_text()\n```\n:::\n\n\n\n\nFrom this point onwards, you can tokenize and analyze the data on your own.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}