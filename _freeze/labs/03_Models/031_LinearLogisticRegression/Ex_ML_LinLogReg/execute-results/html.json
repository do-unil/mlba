{
  "hash": "c9fbb091c277272603105ac7805f9add",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Models: Linear and logistic regressions\"\noutput-file: Ex_ML_LinLogReg.html\n---\n\n\n\n# Linear regression: real estate application\n\nThe dataset we'll be using for the first part of the exercise is real estate transaction prices in Taiwan, which can be accessed from this link [this link](https://archive.ics.uci.edu/ml/datasets/Real+estate+valuation+data+set). This dataset was modified for this exercise. The modified file `real_estate_data.csv` is in the exercise folder under `/data/`.\n\nThe aim is to predict the house prices from available features: *No*, *Month*, *Year*, *TransDate*, *HouseAge*, *Dist*, *NumStores*, *Lat*, *Long*, *Price*. *No* is the transaction number and will not be used.\n\n## EDA\n\nFirst, an EDA of the data is needed. After exploring the structure, the *Price* is shown with the year and month.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreal_estate_data <- read.csv(here::here(\"labs/data/real_estate_data.csv\"))\n\n## adapt the path to the data\n# if you encountered any error with the encoding of the data (`Error in gregexpr...`), just re-run the code again\nstr(real_estate_data)\nlibrary(summarytools)\ndfSummary(real_estate_data)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nreal_estate_data %>% ggplot(aes(x=Month, y=Price, fill=as.factor(Year))) + \n  geom_boxplot()+ facet_wrap(~as.factor(Year))\n```\n\n::: {.cell-output-display}\n![](Ex_ML_LinLogReg_files/figure-html/unnamed-chunk-1-1.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n'data.frame':\t414 obs. of  10 variables:\n $ No       : int  1 2 3 4 5 6 7 8 9 10 ...\n $ TransDate: num  2013 2013 2014 2014 2013 ...\n $ HouseAge : num  32 19.5 13.3 13.3 5 7.1 34.5 20.3 31.7 17.9 ...\n $ Dist     : num  84.9 306.6 562 562 390.6 ...\n $ NumStores: int  10 9 5 5 5 3 7 6 1 3 ...\n $ Lat      : num  25 25 25 25 25 ...\n $ Long     : num  122 122 122 122 122 ...\n $ Price    : num  37.9 42.2 47.3 54.8 43.1 32.1 40.3 46.7 18.8 22.1 ...\n $ Month    : chr  \"Dec\" \"Dec\" \"Jul\" \"Jul\" ...\n $ Year     : int  2012 2012 2013 2013 2012 2012 2012 2013 2013 2013 ...\nData Frame Summary  \nreal_estate_data  \nDimensions: 414 x 10  \nDuplicates: 0  \n\n-----------------------------------------------------------------------------------------------------------------\nNo   Variable      Stats / Values                Freqs (% of Valid)    Graph                 Valid      Missing  \n---- ------------- ----------------------------- --------------------- --------------------- ---------- ---------\n1    No            Mean (sd) : 207.5 (119.7)     414 distinct values   : : : : : : : :       414        0        \n     [integer]     min < med < max:              (Integer sequence)    : : : : : : : :       (100.0%)   (0.0%)   \n                   1 < 207.5 < 414                                     : : : : : : : :                           \n                   IQR (CV) : 206.5 (0.6)                              : : : : : : : : .                         \n                                                                       : : : : : : : : :                         \n\n2    TransDate     Mean (sd) : 2013.1 (0.3)      12 distinct values                      :   414        0        \n     [numeric]     min < med < max:                                    :       .       : :   (100.0%)   (0.0%)   \n                   2012.7 < 2013.2 < 2013.6                            :   .   :   .   : :                       \n                   IQR (CV) : 0.5 (0)                                  : : : : : : : : : :                       \n                                                                       : : : : : : : : : :                       \n\n3    HouseAge      Mean (sd) : 17.7 (11.4)       236 distinct values       : :               414        0        \n     [numeric]     min < med < max:                                    :   : :     .         (100.0%)   (0.0%)   \n                   0 < 16.1 < 43.8                                     : . : :     :                             \n                   IQR (CV) : 19.1 (0.6)                               : : : :   . : .                           \n                                                                       : : : : : : : : .                         \n\n4    Dist          Mean (sd) : 1083.9 (1262.1)   259 distinct values   :                     414        0        \n     [numeric]     min < med < max:                                    :                     (100.0%)   (0.0%)   \n                   23.4 < 492.2 < 6488                                 :                                         \n                   IQR (CV) : 1165 (1.2)                               :                                         \n                                                                       : : : .     .                             \n\n5    NumStores     Mean (sd) : 4.1 (2.9)         11 distinct values    :                     414        0        \n     [integer]     min < med < max:                                    :                     (100.0%)   (0.0%)   \n                   0 < 4 < 10                                          :       :                                 \n                   IQR (CV) : 5 (0.7)                                  :   : . : . . .                           \n                                                                       : : : : : : : : : .                       \n\n6    Lat           Mean (sd) : 25 (0)            234 distinct values           :             414        0        \n     [numeric]     min < med < max:                                          . :             (100.0%)   (0.0%)   \n                   24.9 < 25 < 25                                            : : .                               \n                   IQR (CV) : 0 (0)                                        . : : :                               \n                                                                         : : : : :                               \n\n7    Long          Mean (sd) : 121.5 (0)         232 distinct values                 :       414        0        \n     [numeric]     min < med < max:                                                  :       (100.0%)   (0.0%)   \n                   121.5 < 121.5 < 121.6                                           : :                           \n                   IQR (CV) : 0 (0)                                            .   : :                           \n                                                                           . . : . : : :                         \n\n8    Price         Mean (sd) : 38 (13.6)         270 distinct values       : :               414        0        \n     [numeric]     min < med < max:                                      : : :               (100.0%)   (0.0%)   \n                   7.6 < 38.5 < 117.5                                    : : : .                                 \n                   IQR (CV) : 18.9 (0.4)                                 : : : :                                 \n                                                                       : : : : : .                               \n\n9    Month         1. Apr                        61 (14.7%)            II                    414        0        \n     [character]   2. Dec                        38 ( 9.2%)            I                     (100.0%)   (0.0%)   \n                   3. Jan                        74 (17.9%)            III                                       \n                   4. Jul                        70 (16.9%)            III                                       \n                   5. Jun                        58 (14.0%)            II                                        \n                   6. Mar                        25 ( 6.0%)            I                                         \n                   7. Oct                        58 (14.0%)            II                                        \n                   8. Sep                        30 ( 7.2%)            I                                         \n\n10   Year          Min  : 2012                   2012 : 126 (30.4%)    IIIIII                414        0        \n     [integer]     Mean : 2012.7                 2013 : 288 (69.6%)    IIIIIIIIIIIII         (100.0%)   (0.0%)   \n                   Max  : 2013                                                                                   \n-----------------------------------------------------------------------------------------------------------------\n```\n\n\n:::\n:::\n\n\nThe results show how important it is to make an EDA! It appears that the data does not contain transactions for all the months of 2012 and 2013, but just some months by the end of 2012 and the first half of 2013. This shows that it is pointless to use month and year here. This is why we prefer *TransDate*, a value indicating the transaction time on a linear scale (e.g., 2013.250 is March 2013).\n\nNow we focus on the link between *Price* and the other features.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(GGally)\nreal_estate_data %>% \n  select(Price, HouseAge, Dist, Lat, Long, TransDate) %>% \n  ggpairs()\n```\n\n::: {.cell-output-display}\n![](Ex_ML_LinLogReg_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nNo clear link appears. The linear regression will help to discover if a combination of the features can predict the price.\n\n## Modelling\n\nFirst, we split the data into training/test set (75/25).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(234)\nindex <- sample(x=c(1,2), size=nrow(real_estate_data), replace=TRUE, prob=c(0.75,0.25)) # 1==training set, 2==test set\ndat_tr_restate <- real_estate_data[index==1,]\ndat_te_restate <- real_estate_data[index==2,]\n```\n:::\n\n\nThen, we fit the linear regression to the training set.\n\n::: panel-tabset\n\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod_lm <- lm(Price~TransDate+\n               HouseAge+\n               Dist+\n               NumStores+\n               Lat+\n               Long, data=dat_tr_restate)\nsummary(mod_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Price ~ TransDate + HouseAge + Dist + NumStores + \n    Lat + Long, data = dat_tr_restate)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.031  -5.167  -1.115   3.771  75.027 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -9.413e+03  7.826e+03  -1.203   0.2299    \nTransDate    3.720e+00  1.760e+00   2.113   0.0353 *  \nHouseAge    -2.433e-01  4.434e-02  -5.487 8.22e-08 ***\nDist        -4.981e-03  8.287e-04  -6.010 4.97e-09 ***\nNumStores    1.021e+00  2.129e-01   4.796 2.47e-06 ***\nLat          2.350e+02  5.045e+01   4.659 4.64e-06 ***\nLong        -3.210e+01  5.783e+01  -0.555   0.5791    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.991 on 325 degrees of freedom\nMultiple R-squared:  0.564,\tAdjusted R-squared:  0.5559 \nF-statistic: 70.07 on 6 and 325 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# In R, we load the conda environment as usual\nlibrary(reticulate)\nreticulate::use_condaenv(\"MLBA\", required = TRUE)\ngc(full = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          used  (Mb) gc trigger  (Mb) max used  (Mb)\nNcells 2663127 142.3    5024474 268.4  3525680 188.3\nVcells 4810159  36.7   10146329  77.5  8346827  63.7\n```\n\n\n:::\n:::\n\n\nIn python, we then use the `statsmodels` library to fit a linear regression model to the training data and perform feature elimination. We use the `.fit()` method to fit the model with the formula for the variable names. Note that python's `summary()` function is unique to the `statsmodels` libraries and produces similar information to its R counterpart.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"\n\nimport mkl\n# %env OMP_NUM_THREADS=1\n# set the number of threads. Here we set it to 1 to avoid parallelization when rendering quarto, but you can set it to higher values.\nmkl.set_num_threads(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n1\n```\n\n\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# Import necessary library\nimport statsmodels.formula.api as smf\n\n# Fit a linear regression model to the training data & print the summary\nmod_lm_py = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long', data=r.dat_tr_restate).fit()\nprint(mod_lm_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                             OLS Regression Results                            \n===============================================================================\nDep. Variable:                   Price   R-squared:                       0.564\nModel:                             OLS   Adj. R-squared:                  0.556\nMethod:                  Least Squares   F-statistic:                     70.07\nDate:              sam., 28 févr. 2026   Prob (F-statistic):           1.13e-55\nTime:                         23:01:11   Log-Likelihood:                -1196.7\nNo. Observations:                  332   AIC:                             2407.\nDf Residuals:                      325   BIC:                             2434.\nDf Model:                            6                                         \nCovariance Type:             nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -9413.4672   7825.675     -1.203      0.230   -2.48e+04    5981.905\nTransDate      3.7205      1.760      2.113      0.035       0.257       7.184\nHouseAge      -0.2433      0.044     -5.487      0.000      -0.331      -0.156\nDist          -0.0050      0.001     -6.010      0.000      -0.007      -0.003\nNumStores      1.0211      0.213      4.796      0.000       0.602       1.440\nLat          235.0369     50.448      4.659      0.000     135.791     334.283\nLong         -32.1049     57.826     -0.555      0.579    -145.865      81.656\n==============================================================================\nOmnibus:                      207.286   Durbin-Watson:                   2.246\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             3757.196\nSkew:                           2.219   Prob(JB):                         0.00\nKurtosis:                      18.871   Cond. No.                     3.76e+07\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 3.76e+07. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n\n\n:::\n:::\n\n\nIt's not a suprise that the results are same as the ones obtained in R.\n\n:::\n\n\n## Variable selection & interpretation\n\nThe stepwise variable selection can be performed using the function **step**. By default, it is a backward selection; see `?step` for details (parameter **direction** is **backward** when **scope** is empty).\n\n::: panel-tabset\n\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstep(mod_lm) # see the result\nmod_lm_sel <- step(mod_lm) # store the final model into mod_lm_sel\nsummary(mod_lm_sel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStart:  AIC=1465.21\nPrice ~ TransDate + HouseAge + Dist + NumStores + Lat + Long\n\n            Df Sum of Sq   RSS    AIC\n- Long       1     24.92 26297 1463.5\n<none>                   26272 1465.2\n- TransDate  1    361.09 26633 1467.8\n- Lat        1   1754.67 28027 1484.7\n- NumStores  1   1859.23 28131 1485.9\n- HouseAge   1   2434.13 28706 1492.6\n- Dist       1   2920.26 29192 1498.2\n\nStep:  AIC=1463.53\nPrice ~ TransDate + HouseAge + Dist + NumStores + Lat\n\n            Df Sum of Sq   RSS    AIC\n<none>                   26297 1463.5\n- TransDate  1     350.5 26647 1465.9\n- Lat        1    1813.8 28111 1483.7\n- NumStores  1    1885.2 28182 1484.5\n- HouseAge   1    2431.5 28728 1490.9\n- Dist       1    5821.3 32118 1527.9\n\nCall:\nlm(formula = Price ~ TransDate + HouseAge + Dist + NumStores + \n    Lat, data = dat_tr_restate)\n\nCoefficients:\n(Intercept)    TransDate     HouseAge         Dist    NumStores          Lat  \n -1.326e+04    3.658e+00   -2.432e-01   -4.635e-03    1.027e+00    2.378e+02  \n\nStart:  AIC=1465.21\nPrice ~ TransDate + HouseAge + Dist + NumStores + Lat + Long\n\n            Df Sum of Sq   RSS    AIC\n- Long       1     24.92 26297 1463.5\n<none>                   26272 1465.2\n- TransDate  1    361.09 26633 1467.8\n- Lat        1   1754.67 28027 1484.7\n- NumStores  1   1859.23 28131 1485.9\n- HouseAge   1   2434.13 28706 1492.6\n- Dist       1   2920.26 29192 1498.2\n\nStep:  AIC=1463.53\nPrice ~ TransDate + HouseAge + Dist + NumStores + Lat\n\n            Df Sum of Sq   RSS    AIC\n<none>                   26297 1463.5\n- TransDate  1     350.5 26647 1465.9\n- Lat        1    1813.8 28111 1483.7\n- NumStores  1    1885.2 28182 1484.5\n- HouseAge   1    2431.5 28728 1490.9\n- Dist       1    5821.3 32118 1527.9\n\nCall:\nlm(formula = Price ~ TransDate + HouseAge + Dist + NumStores + \n    Lat, data = dat_tr_restate)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-34.934  -5.236  -1.201   3.825  75.413 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.326e+04  3.639e+03  -3.643 0.000313 ***\nTransDate    3.658e+00  1.755e+00   2.084 0.037898 *  \nHouseAge    -2.432e-01  4.429e-02  -5.490 8.08e-08 ***\nDist        -4.635e-03  5.456e-04  -8.495 7.15e-16 ***\nNumStores    1.027e+00  2.124e-01   4.834 2.06e-06 ***\nLat          2.378e+02  5.015e+01   4.742 3.17e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.981 on 326 degrees of freedom\nMultiple R-squared:  0.5636,\tAdjusted R-squared:  0.5569 \nF-statistic:  84.2 on 5 and 326 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n### Python\n\nAs python does not have an exact equivalent of `stats::step()` function, which performs both forward and backward selection based on AIC, we have to implement it manually. We start with the full model and iteratively remove the feature with the highest p-value and add the feature with the lowest AIC until we can no longer improve the AIC. The final model is stored in `mod_lm_sel`. For an extensive explanation of what this while loop is doing and how the backward+forward is computed, check the code below:\n\n<details>\n\n<summary>Explaining feature elimination in python (while loop) </summary>\n\nWe start by setting `mod_lm_sel_py` to the full model `mod_lm_py`. Then, we enter a while loop that continues until we break out of it. In each iteration of the loop, we store the current model in prev_model for later comparison. We start by dropping the feature with the highest p-value from the current model using `idxmax()`, which returns the label of the maximum value in the pvalues attribute of the `mod_lm_sel_py` object. We exclude the intercept term from the list of labels by specifying `labels=['Intercept']`. We then create a new model using `smf.ols()` with the feature removed and fit it to the training data using `fit()`. We store this new model in `mod_lm_sel_py`.\n\nNext, we check whether the AIC of the new model is larger than the previous model's. If it is, we break out of the while loop and use the previous model (prev_model) as the final model. If not, we continue to the next step of the loop. Here, we look for the feature with the lowest AIC among the remaining features using `idxmin()` on the pvalues attribute, again excluding the intercept term. We create a new model by adding this feature to the current model using `smf.ols()`, fit it to the training data using `fit()`, and store it in `mod_lm_sel_new`.\n\nWe then check whether the AIC of the new model is larger than that of the current model. If it is, we break out of the while loop and use the current model (`mod_lm_sel_py`) as the final model. If not, we update `mod_lm_sel_py` with the new model and continue to the next iteration of the loop. This way, we iteratively remove the feature with the highest p-value and add the feature with the lowest AIC until we can no longer improve the AIC. The final model is stored in `mod_lm_sel_py`.\n\n</details>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# perform both forward and backward selection using AIC\nmod_lm_sel_py = mod_lm_py\n\nwhile True:\n    prev_model = mod_lm_sel_py\n    # drop the feature with the highest p-value\n    feature_to_drop = mod_lm_sel_py.pvalues.drop(labels=['Intercept']).idxmax()\n    mod_lm_sel_py = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long - ' + feature_to_drop, data=r.dat_tr_restate).fit()\n    # check if AIC has increased, if yes, break the loop and use the previous model\n    if mod_lm_sel_py.aic > prev_model.aic:\n        mod_lm_sel_py = prev_model\n        break\n    \n    # add the feature with the lowest AIC\n    feature_to_add = mod_lm_sel_py.pvalues.drop(labels=['Intercept']).idxmin()\n    mod_lm_sel_py_new = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long + ' + feature_to_add, data=r.dat_tr_restate).fit()\n    # check if AIC has increased, if yes, break the loop and use the previous model\n    if mod_lm_sel_py_new.aic > mod_lm_sel_py.aic:\n        break\n    mod_lm_sel_py = mod_lm_sel_py_new\n    \nprint(mod_lm_sel_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                             OLS Regression Results                            \n===============================================================================\nDep. Variable:                   Price   R-squared:                       0.564\nModel:                             OLS   Adj. R-squared:                  0.557\nMethod:                  Least Squares   F-statistic:                     84.20\nDate:              sam., 28 févr. 2026   Prob (F-statistic):           1.36e-56\nTime:                         23:01:12   Log-Likelihood:                -1196.9\nNo. Observations:                  332   AIC:                             2406.\nDf Residuals:                      326   BIC:                             2429.\nDf Model:                            5                                         \nCovariance Type:             nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept  -1.326e+04   3639.370     -3.643      0.000   -2.04e+04   -6099.096\nTransDate      3.6579      1.755      2.084      0.038       0.206       7.110\nHouseAge      -0.2432      0.044     -5.490      0.000      -0.330      -0.156\nDist          -0.0046      0.001     -8.495      0.000      -0.006      -0.004\nNumStores      1.0270      0.212      4.834      0.000       0.609       1.445\nLat          237.7985     50.149      4.742      0.000     139.142     336.455\n==============================================================================\nOmnibus:                      209.843   Durbin-Watson:                   2.249\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             3876.569\nSkew:                           2.252   Prob(JB):                         0.00\nKurtosis:                      19.123   Cond. No.                     1.75e+07\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.75e+07. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n\n\n:::\n:::\n\n\n:::\n\nAfter identifying the most important features, you can fit a new model using only those features and evaluate its performance using the test set.\n\nThe final model does not contain *Long*. In terms of interpretations, for example:\n\n-   The price increased on average by 3.7 per year (*TransDate*)\n-   It diminishes in average by (-2)2.4 per year (*HouseAge*)\n-   etc.\n\n## Inference\n\nWe now predict the prices in the test set. We can make a scatter plot of the predictions versus the observed prices to inspect that. We already know by looking at the $R^2$ in the summary that the prediction quality is not good.\n\n::: panel-tabset\n\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod_lm_pred <- predict(mod_lm_sel, newdata=dat_te_restate)\nplot(dat_te_restate$Price ~ mod_lm_pred, xlab=\"Prediction\", ylab=\"Observed prices\")\nabline(0,1) # line showing the obs -- pred agreement\n```\n\n::: {.cell-output-display}\n![](Ex_ML_LinLogReg_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n### Python\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nmod_lm_sel_pred = mod_lm_sel_py.predict(r.dat_te_restate)\nfig, ax = plt.subplots()\nax.scatter(x=mod_lm_sel_pred, y=r.dat_te_restate['Price']);\nax.set_xlabel('Prediction');\nax.set_ylabel('Observed prices');\nax.plot(ax.get_xlim(), ax.get_ylim(), ls=\"--\", c=\".3\");\nplt.show()\n```\n:::\n\n\n:::\n\nIt appears that the lowest and the highest prices are underestimated. At the center (around 30), the prices are slightly overestimated.\n\nAs an exercise, write down the prediction equation of the selected model. Use this equation to explain how instances 1 and 2 (test set) are predicted and calculate the predictions manually. Verify your results using the *predict* function from the previous R code.\n\n<details>\n\n<summary>Answer</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmod_lm_pred[c(1,2)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       2        4 \n47.77995 47.82163 \n```\n\n\n:::\n:::\n\n\n$$\ny = -0.000133 + 3.66\\times TransDate -0.243\\times HouseAge \\\\-0.00464\\times Dist + 1.027\\times NumStores + 237.8\\times Lat\n$$\n\n</details>\n\n# Logistic regression: visit data\n\nTo illustrate a logistic regression, we use the data set **DocVis** extracted (modified for the exercise) the library **AER**. The data set reports a 1977--1978 Australian Health Survey. The aim is to predict the outcome **visits**, a binary variable indicating if the individual had at least one visit to a doctor in the past two weeks, using all the other features. To learn more about these features, look at the data described below.\n\n<details>\n\n<summary>Data Description</summary>\n\n\nThe predictors are as followed:\n\n-   gender: M/F\n-   age: Age in years divided by 100.\n-   income: Annual income in tens of thousands of dollars.\n-   illness: Number of illnesses in past 2 weeks.\n-   reduced: Number of days of reduced activity in past 2 weeks due to illness or injury.\n-   health: General health questionnaire score using Goldberg's method.\n-   private: Factor. Does the individual have private health insurance?\n-   freepoor: Factor. Does the individual have free government health insurance due to low income?\n-   freerepat: Factor. Does the individual have free government health insurance due to old age, disability or veteran status?\n-   nchronic: Factor. Is there a chronic condition not limiting activity?\n-   lchronic: Factor. Is there a chronic condition limiting activity?\n\n</details>\n\n\nWe can now load the dataset.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nDocVis <- read.csv(here::here(\"labs/data/DocVis.csv\")) ## found in the same data folder\n```\n:::\n\n\nTo facilitate the use of logistic regression in **R**, it is **strongly recommended** to have a 0/1 outcome rather than a categorical one. This makes much easier the recognition of the positive label (the \"1\") and the negative one (the \"0\"). Since we want to predict **visits**, we transform it accordingly.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nDocVis$visits <- ifelse(DocVis$visits==\"Yes\",1,0)\n```\n:::\n\n\n## Modelling\n\nWe can split our data and fit the logistic regression. The function for this is **glm**. This function encompasses a larger class of models (namely, the generalized linear models) which includes the logistic regression, accessible with **family=\"binomial\"**.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(234)\nindex <- sample(x=c(1,2), size=nrow(DocVis), replace=TRUE, prob=c(0.75,0.25)) # 1==training set, 2==test set\ndat_tr_visit <- DocVis[index==1,]\ndat_te_visit <- DocVis[index==2,]\n```\n:::\n\n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvis_logr <- glm(visits~., data=dat_tr_visit, family=\"binomial\")\nsummary(vis_logr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = visits ~ ., family = \"binomial\", data = dat_tr_visit)\n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -2.333303   0.164588 -14.177  < 2e-16 ***\ngendermale   -0.318588   0.093701  -3.400 0.000674 ***\nage           0.344459   0.285299   1.207 0.227293    \nincome        0.009721   0.137314   0.071 0.943562    \nillness       0.273529   0.033011   8.286  < 2e-16 ***\nreduced       0.161265   0.013531  11.918  < 2e-16 ***\nhealth        0.054113   0.019843   2.727 0.006392 ** \nprivateyes    0.263077   0.113554   2.317 0.020518 *  \nfreepooryes  -0.665795   0.288494  -2.308 0.021009 *  \nfreerepatyes  0.364835   0.160273   2.276 0.022826 *  \nnchronicyes   0.089645   0.103798   0.864 0.387782    \nlchronicyes   0.176745   0.143272   1.234 0.217340    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3985.6  on 3912  degrees of freedom\nResidual deviance: 3494.8  on 3901  degrees of freedom\nAIC: 3518.8\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# a hack around this technique to not type all the variable names\nvis_formula = 'visits ~ ' + ' + '.join(r.dat_tr_visit.columns.difference(['visits']))\n\n# create a logistic regression model\nvis_logr_py = sm.formula.logit(formula= vis_formula, data=r.dat_tr_visit).fit()\n\nprint(vis_logr_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOptimization terminated successfully.\n         Current function value: 0.446559\n         Iterations 6\n                            Logit Regression Results                           \n===============================================================================\nDep. Variable:                  visits   No. Observations:                 3913\nModel:                           Logit   Df Residuals:                     3901\nMethod:                            MLE   Df Model:                           11\nDate:              sam., 28 févr. 2026   Pseudo R-squ.:                  0.1231\nTime:                         23:01:12   Log-Likelihood:                -1747.4\nconverged:                        True   LL-Null:                       -1992.8\nCovariance Type:             nonrobust   LLR p-value:                 2.963e-98\n====================================================================================\n                       coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept           -2.3333      0.165    -14.176      0.000      -2.656      -2.011\nfreepoor[T.yes]     -0.6658      0.289     -2.307      0.021      -1.231      -0.100\nfreerepat[T.yes]     0.3648      0.160      2.276      0.023       0.051       0.679\ngender[T.male]      -0.3186      0.094     -3.400      0.001      -0.502      -0.135\nlchronic[T.yes]      0.1767      0.143      1.234      0.217      -0.104       0.458\nnchronic[T.yes]      0.0896      0.104      0.864      0.388      -0.114       0.293\nprivate[T.yes]       0.2631      0.114      2.317      0.021       0.041       0.486\nage                  0.3445      0.285      1.207      0.227      -0.215       0.904\nhealth               0.0541      0.020      2.727      0.006       0.015       0.093\nillness              0.2735      0.033      8.286      0.000       0.209       0.338\nincome               0.0097      0.137      0.071      0.944      -0.259       0.279\nreduced              0.1613      0.014     11.918      0.000       0.135       0.188\n====================================================================================\n```\n\n\n:::\n:::\n\n\n::: callout-warning\n# Using `.` for formulas in R vs Python \nIn R, the dot `.` is used as shorthand to indicate that we want to include all other variables in the formula as predictors except for the outcome variable. So, if our outcome variable is y and we want to include all other variables in our data frame as predictors, we can write `y ~ .` in the formula. \n\nIn Python, however, the dot `.` is not used in the same way in formulas. Instead, to include all other variables as predictors except for `y`, we would write `y ~ x1 + x2 + ...` where `x1, x2`, etc. represent the names of the predictor variables. Also, `statsmodels` has a similar syntax to R base regressions. In most other typical ML libraries in Python, you must provide the column values instead of using the column names.\n\n:::\n\nNote that the `family=\"binomial\"` argument in R is not needed in Python since `sm.formula.logit()` assumes the logistic regression model is fitted using a binomial distribution by default.\n\n:::\n\n## Variable selection & interpretation\n\nNow, we can apply the variable selection:\n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvis_logr_sel <- step(vis_logr)\nsummary(vis_logr_sel)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStart:  AIC=3518.77\nvisits ~ gender + age + income + illness + reduced + health + \n    private + freepoor + freerepat + nchronic + lchronic\n\n            Df Deviance    AIC\n- income     1   3494.8 3516.8\n- nchronic   1   3495.5 3517.5\n- age        1   3496.2 3518.2\n- lchronic   1   3496.3 3518.3\n<none>           3494.8 3518.8\n- freerepat  1   3500.0 3522.0\n- private    1   3500.2 3522.2\n- freepoor   1   3500.7 3522.7\n- health     1   3502.1 3524.1\n- gender     1   3506.4 3528.4\n- illness    1   3562.9 3584.9\n- reduced    1   3650.9 3672.9\n\nStep:  AIC=3516.77\nvisits ~ gender + age + illness + reduced + health + private + \n    freepoor + freerepat + nchronic + lchronic\n\n            Df Deviance    AIC\n- nchronic   1   3495.5 3515.5\n- age        1   3496.2 3516.2\n- lchronic   1   3496.3 3516.3\n<none>           3494.8 3516.8\n- freerepat  1   3500.1 3520.1\n- private    1   3500.3 3520.3\n- freepoor   1   3501.0 3521.0\n- health     1   3502.1 3522.1\n- gender     1   3506.9 3526.9\n- illness    1   3563.0 3583.0\n- reduced    1   3651.0 3671.0\n\nStep:  AIC=3515.52\nvisits ~ gender + age + illness + reduced + health + private + \n    freepoor + freerepat + lchronic\n\n            Df Deviance    AIC\n- lchronic   1   3496.4 3514.4\n<none>           3495.5 3515.5\n- age        1   3497.6 3515.6\n- freerepat  1   3501.0 3519.0\n- private    1   3501.3 3519.3\n- freepoor   1   3501.6 3519.6\n- health     1   3502.7 3520.7\n- gender     1   3508.0 3526.0\n- illness    1   3573.5 3591.5\n- reduced    1   3651.9 3669.9\n\nStep:  AIC=3514.4\nvisits ~ gender + age + illness + reduced + health + private + \n    freepoor + freerepat\n\n            Df Deviance    AIC\n<none>           3496.4 3514.4\n- age        1   3498.5 3514.5\n- private    1   3502.3 3518.3\n- freerepat  1   3502.3 3518.3\n- freepoor   1   3502.3 3518.3\n- health     1   3504.3 3520.3\n- gender     1   3508.6 3524.6\n- illness    1   3577.0 3593.0\n- reduced    1   3661.4 3677.4\n\nCall:\nglm(formula = visits ~ gender + age + illness + reduced + health + \n    private + freepoor + freerepat, family = \"binomial\", data = dat_tr_visit)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -2.31795    0.13920 -16.652  < 2e-16 ***\ngendermale   -0.31838    0.09136  -3.485 0.000492 ***\nage           0.39762    0.27656   1.438 0.150503    \nillness       0.28431    0.03152   9.019  < 2e-16 ***\nreduced       0.16340    0.01337  12.217  < 2e-16 ***\nhealth        0.05589    0.01966   2.843 0.004470 ** \nprivateyes    0.27249    0.11258   2.420 0.015503 *  \nfreepooryes  -0.65344    0.28444  -2.297 0.021601 *  \nfreerepatyes  0.38038    0.15674   2.427 0.015231 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 3985.6  on 3912  degrees of freedom\nResidual deviance: 3496.4  on 3904  degrees of freedom\nAIC: 3514.4\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n### Python\n\nAs already seen in the linear regression part, in python, we don't have the same implementation of the step function, hence why we designed the while loop earlier. It is good practice to create a single function with this step while loop to handle all cases (linear, logistic etc); however, we only implement it here for logistic regression. Therefore, to tackle this, we will create a function that does step-wise elimination for us. We define a function called `forward_selected` that performs forward selection on a given dataset to select the best predictors for a response variable based on AIC. The function takes two arguments: `data`, a pandas DataFrame containing the predictors and `response` variable, and response, a string specifying the name of the response variable.\n\n<details>\n\n<summary>For more explanation of the code, click on me</summary>\n\nThe function first initializes two sets: `remaining` and `selected`. `remaining` contains the names of all columns in the `data` DataFrame except for the `response` variable, while `selected` is initially empty. The function then initializes `current_aic` and `best_new_aic` to infinity. The main loop of the function continues as long as `remaining` is not empty and `current_aic` is equal to `best_new_aic`. At each iteration, the function iterates over all columns in `remaining` and computes the AIC for a logistic regression model that includes the `response` variable and the currently selected predictors, as well as the current candidate predictor. The function then adds the candidate predictor and its AIC to a list of `(aic, candidate)` tuples, and sorts the list by increasing AIC. The function then selects the candidate with the lowest AIC and adds it to the `selected` set, removes it from the `remaining` set, and updates `current_aic` to the new lowest AIC. The function continues this process until no candidate can improve the AIC. Finally, the function fits a logistic regression model using the selected predictors and returns the resulting model.\n\n\n</details>\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# code taken from the link below and adjusted for logistic regression with AIC criteria\n# https://planspace.org/20150423-forward_selection_with_statsmodels/\n\ndef forward_selected(data, response):\n    \"\"\"Linear model designed by forward selection.\n\n    Parameters:\n    -----------\n    data : pandas DataFrame with all possible predictors and response\n\n    response: string, name of response column in data\n\n    Returns:\n    --------\n    model: an \"optimal\" fitted statsmodels linear model\n           with an intercept\n           selected by forward selection\n           evaluated by AIC\n    \"\"\"\n    remaining = set(data.columns)\n    remaining.remove(response)\n    selected = []\n    current_aic, best_new_aic = float(\"inf\"), float(\"inf\")\n    while remaining and current_aic == best_new_aic:\n        aics_with_candidates = []\n        for candidate in remaining:\n            formula = \"{} ~ {} + 1\".format(response,\n                                           ' + '.join(selected + [candidate]))\n            model = smf.logit(formula, data).fit(disp=0)\n            aic = model.aic\n            aics_with_candidates.append((aic, candidate))\n        aics_with_candidates.sort()\n        best_new_aic, best_candidate = aics_with_candidates.pop(0)\n        if current_aic > best_new_aic:\n            remaining.remove(best_candidate)\n            selected.append(best_candidate)\n            current_aic = best_new_aic\n    formula = \"{} ~ {} + 1\".format(response,\n                                   ' + '.join(selected))\n    model = smf.logit(formula, data).fit(disp=0)\n    return model\n\n\nmod_logit_sel_py = forward_selected(r.dat_tr_visit, 'visits')\n\nprint(mod_logit_sel_py.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            Logit Regression Results                           \n===============================================================================\nDep. Variable:                  visits   No. Observations:                 3913\nModel:                           Logit   Df Residuals:                     3906\nMethod:                            MLE   Df Model:                            6\nDate:              sam., 28 févr. 2026   Pseudo R-squ.:                  0.1209\nTime:                         23:01:15   Log-Likelihood:                -1751.9\nconverged:                        True   LL-Null:                       -1992.8\nCovariance Type:             nonrobust   LLR p-value:                6.902e-101\n===================================================================================\n                      coef    std err          z      P>|z|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nIntercept          -2.2457      0.125    -17.989      0.000      -2.490      -2.001\ngender[T.male]     -0.3542      0.090     -3.919      0.000      -0.531      -0.177\nfreepoor[T.yes]    -0.8098      0.278     -2.917      0.004      -1.354      -0.266\nreduced             0.1620      0.013     12.159      0.000       0.136       0.188\nillness             0.2864      0.031      9.123      0.000       0.225       0.348\nage                 0.7754      0.216      3.591      0.000       0.352       1.199\nhealth              0.0565      0.020      2.880      0.004       0.018       0.095\n===================================================================================\n```\n\n\n:::\n:::\n\n\nWe can see that the results of `mod_logit_sel_py` model are slightly different from the R version, but nevertheless, we have reduced the features and the interpretations (see below) with both R and python versions remain the same.\n\n:::\n\nWe can see that the probability of a visit is\n\n-   smaller for males\n-   increasing with age\n-   larger with illness\n-   etc.\n\n## Inference\n\n::: panel-tabset\n\n### R\nThe **predict** function with **type=\"response\"** will predict the probability of the positive class (\"1\"). If it is set to **\"link\"** it produces the linear predictor (i.e., the $z$). To make the prediction, we thus have to identify if the predicted probability is larger or lower than 0.5.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprob_te_visit <- predict(vis_logr_sel, newdata = dat_te_visit, type=\"response\")\npred_te_visit <- ifelse(prob_te_visit >= 0.5, 1, 0)\ntable(Pred=pred_te_visit, Obs=dat_te_visit$visits)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    Obs\nPred    0    1\n   0 1011  204\n   1   25   37\n```\n\n\n:::\n:::\n\n\n### Python\n\nThe explanation is similar to that of R, with a slight different that here we use `pandas.crosstab` to make our confusion matrix.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport pandas as pd\n\nprob_te_visit = mod_logit_sel_py.predict(r.dat_te_visit)\npred_te_visit = [1 if p >= 0.5 else 0 for p in prob_te_visit]\nconf_mat = pd.crosstab(pred_te_visit, r.dat_te_visit['visits'], rownames=['Pred'], colnames=['Obs'])\nprint(conf_mat)\n```\n:::\n\n\nThe results are extremely close to the R version.\n\n:::\n\nThe predictions are not really good. It is in fact a difficult data set. Indeed, the number of 0 is so large compare to the 1, that predicting a 0 always provides a good model overall. That issue will be addressed further later on in the course.\n\nFor now, this can be further inspected by looking at the predicted probabilities per observed label.\n\n::: panel-tabset\n\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nboxplot(prob_te_visit~dat_te_visit$visits)\n```\n\n::: {.cell-output-display}\n![](Ex_ML_LinLogReg_files/figure-html/unnamed-chunk-19-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfig, ax = plt.subplots()\nax.boxplot([prob_te_visit[r.dat_te_visit['visits']==0], prob_te_visit[r.dat_te_visit['visits']==1]]);\nax.set_xticklabels(['No Visit', 'Visit']);\nax.set_ylabel('Predicted Probability');\nax.set_title('Predicted Probabilities by Visit Status');\nplt.show()\n```\n:::\n\n\n:::\n\nWe see that if the lowest predicted probabilities are usually assigned to 0-observations, most of the probabilities remain below 0.5 (even for the 1-observations). A good model would have two well separated boxplots, well away from 0.5.\n\nNow, as an exercise, write down the prediction equation of the selected model, like you did for linear regression. Use this equation to explain how instance 1 and 2 (test set) are predicted, and calculate the predictions manually. Verify your results using the function *predict* used before.\n\n<details>\n\n<summary>Answer</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nprob_te_visit[c(1,2)]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         2          4 \n0.21367628 0.09309046 \n```\n\n\n:::\n:::\n\n\n$$\nz(x) = -2.31795-0.31838\\times gender\\_male+0.39762\\times age+\\\\0.28431\\times illness+0.16340\\times reduced+0.05589\\times health+\\\\0.27249\\times private\\_eyes -0.65344\\times freepoor\\_yes+\\\\0.38038\\times freerepat\\_yes  \n$$ Then $$\nP(Y=1 | X=x) = \\frac{e^{z(x)}}{1+e^{z(x)}}\n$$\n\n</details>\n\n# LASSO & Ridge regressions\n\nYou have been introduced to lasso and ridge regression during the *Variable selection with penalization* part of the lecture.\n\n\nLasso and Ridge Regression are two regularization techniques used in regression models to prevent overfitting by adding a penalty term to the loss function. Lasso regression (aka $L_1$) adds a penalty term equal to the absolute value of the coefficients. In contrast, Ridge regression (aka $L_2$) adds a penalty term equal to the squared value of the coefficients. The effect of the penalty term is to shrink the coefficients towards zero, which can help reduce model complexity and improve generalization performance. In this case, we apply lasso and ridge to the real estate data and do not cover logistic regression (example already seen during the class).\n\n\nFirst, we need to turn our predictors into matrices, as this is required by the `glmnet` package in R and works with the python implementation.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# glmnet can only work with matrix objects, columns 2-7 correspond to the same ones used by `lm`\ndat_tr_re_mat_x <- select(dat_tr_restate, c(2:7)) %>% as.matrix()\ndat_tr_re_mat_y <- pull(dat_tr_restate,'Price')\ndat_te_re_mat_x <- select(dat_te_restate, c(2:7)) %>% as.matrix()\ndat_te_re_mat_y <- pull(dat_te_restate,'Price')\n```\n:::\n\n\n::: panel-tabset\n\n## R\n\nOn the newly created matrices, we run cross-validated lasso and ridge with the `cv.glmnet()`, function where setting the `alpha` (penalty) parameter as 1 produces lasso regression and 0 produces ridge regression. The default value of alpha is 1, which corresponds to lasso regression. For 0<alpha<1, it performs Elastic Net regression (a combination of $L_1$ and $L_2$ regularization).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load appropriate library and set a seed\nlibrary(glmnet)\nset.seed(123)\n\n# Fit Ridge regression model\nridge_fit <- cv.glmnet(x = dat_tr_re_mat_x, y = dat_tr_re_mat_y, alpha = 0)\n\n# Fit Lasso regression model\nlasso_fit <- cv.glmnet(x = dat_tr_re_mat_x, y = dat_tr_re_mat_y, alpha = 1) #if you change the `family` argument to `bionomial`, you can get also logistic regression\n```\n:::\n\n\nWe can then fit the final models with the best parameters:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nridge_fit_best <- glmnet(x=dat_tr_re_mat_x, y = dat_tr_re_mat_y, \n                         lambda = ridge_fit$lambda.min)\n\nlasso_fit_best <- glmnet(x=dat_tr_re_mat_x, y=dat_tr_re_mat_y, \n                         lambda = lasso_fit$lambda.min) #can also use lasso_fit$lambda.1se\n```\n:::\n\n\nWe can compare different performances for this task using `caret::postResample()`. We will learn more this function and it's metrics the upcoming courses & lab sessions.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# lasso & ridge performance on the training set\ncaret::postResample(predict(ridge_fit_best, newx = dat_tr_re_mat_x), dat_tr_re_mat_y)\ncaret::postResample(predict(lasso_fit_best, newx = dat_tr_re_mat_x), dat_tr_re_mat_y)\n\n# lasso & ridge performance on the test set\ncaret::postResample(predict(ridge_fit_best, newx = dat_te_re_mat_x), dat_te_re_mat_y)\ncaret::postResample(predict(lasso_fit_best, newx = dat_te_re_mat_x), dat_te_re_mat_y)\n\n# Step-wise lm performance on training and test sets\ncaret::postResample(predict(mod_lm_sel,dat_tr_restate), dat_tr_re_mat_y)\ncaret::postResample(predict(mod_lm_sel,dat_te_restate), dat_te_re_mat_y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     RMSE  Rsquared       MAE \n9.0656535 0.5558078 6.2141456 \n     RMSE  Rsquared       MAE \n8.9037222 0.5634319 6.1024607 \n     RMSE  Rsquared       MAE \n8.9111448 0.6089043 6.6326836 \n     RMSE  Rsquared       MAE \n8.4792450 0.6434067 6.3485929 \n     RMSE  Rsquared       MAE \n8.8998617 0.5635734 6.1053064 \n     RMSE  Rsquared       MAE \n8.4307702 0.6479616 6.3107488 \n```\n\n\n:::\n:::\n\n\nIn this case, the lasso is better than the ridge on the test set, and if you have many features, this could be a useful technique. However, they are both outperformed by step-wise linear regression. Lasso and ridge are more useful when you have many more variables. You can try this already by taking more variables for your `dat_tr_re_mat_x` and `dat_te_re_mat_x` such `select(dat_te_restate,-c('Price', 'Month'))` to see how (for better or worse) the performance changes. If you want more explanation on why linear model outperformed lasso and ridge, check out (click on) the further explanation below.\n\n\n<details>\n\n\n<summary>Why lm (or step lm) outperformed lasso & ridge </summary>\n\nIn some situations, it is normal to observe that a linear model may perform better than a regularized model, such as a ridge or lasso. This can occur when the number of predictors in the model is small relative to the sample size or when the predictors are highly correlated.\n\nLinear regression assumes that the relationship between the response variable and the predictors is linear and additive. When this assumption holds, a linear model can be a good choice. In contrast, regularized regression methods such as ridge and lasso add a penalty term to the regression objective function to shrink the estimated coefficients towards zero, which can help to avoid overfitting when the number of predictors is large relative to the sample size or when the predictors are highly correlated.\n\nHowever, when the number of predictors is small relative to the sample size or when the predictors are highly correlated, the additional regularization provided by ridge or lasso may not be necessary, and a simple linear model may perform better.\n\nIt is always a good practice to compare the performance of different models using appropriate evaluation metrics and techniques such as cross-validation. The choice of the best model will depend on the specific problem and the goals of the analysis.\n\n</details>\n\n## Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom sklearn.linear_model import Ridge, Lasso, RidgeCV, LassoCV\nimport numpy as np\n\n# Set a seed for reproducibility\nnp.random.seed(123)\n\n# Fit Lasso regression model\nlasso_cv = LassoCV(cv=10)\nlasso_cv.fit(r.dat_tr_re_mat_x, r.dat_tr_re_mat_y);\n\n# Get the optimal regularization parameter\nlasso_optimal_alpha = lasso_cv.alpha_\n\n# Fit the Lasso model with the optimal alpha\nlasso_best_py = Lasso(alpha=lasso_optimal_alpha)\nlasso_best_py.fit(r.dat_tr_re_mat_x, r.dat_tr_re_mat_y);\n\n# Fit Ridge regression model with cross-validation\nridge_cv = RidgeCV(cv=10)\nridge_cv.fit(r.dat_tr_re_mat_x, r.dat_tr_re_mat_y);\n\n# Get the optimal regularization parameter\nridge_optimal_alpha = ridge_cv.alpha_\n\n# Fit the Ridge model with the optimal alpha\nridge_best_py = Ridge(alpha=ridge_optimal_alpha)\nridge_best_py.fit(r.dat_tr_re_mat_x, r.dat_tr_re_mat_y);\n```\n:::\n\n\n\nThe `lambda` argument in `cv.glmnet()` from R corresponds to the `alpha` argument in `RidgeCV()`/`LassoCV()` in python. In `cv.glmnet()`, the lambda argument specifies the range of regularization parameters to be tested in the model selection process. By default, lambda is set to NULL, which means that `glmnet()` will automatically choose a sequence of lambda values to search over. Note that in `glmnet()`, lambda values are used for both $L_1$ (lasso) and $L_2$ (ridge) regularization, whereas in `RidgeCV()`, alpha values are used to control the mix of L1 and L2 regularization, with alpha = 0 corresponding to pure L2 regularization (i.e., ridge regression).\n\nOn the contrary (and to avoid confusion), the `alpha` argument in `cv.glmnet()` corresponds to the `fit_intercept` argument in `RidgeCV()`/`LassoCV()`. In `cv.glmnet()`, the alpha argument specifies the mixing parameter between L1 and L2 regularization. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# python lasso & ridge performance on the training set\ncaret::postResample(py$ridge_best_py$predict(dat_tr_re_mat_x), dat_tr_re_mat_y)\ncaret::postResample(py$lasso_best_py$predict(dat_tr_re_mat_x), dat_tr_re_mat_y)\n\n# python lasso & ridge performance on the test set\ncaret::postResample(py$ridge_best_py$predict( dat_te_re_mat_x), dat_te_re_mat_y)\ncaret::postResample(py$lasso_best_py$predict(dat_te_re_mat_x), dat_te_re_mat_y)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     RMSE  Rsquared       MAE \n9.0705968 0.5467287 6.2383143 \n     RMSE  Rsquared       MAE \n9.7290338 0.4785813 6.7449560 \n     RMSE  Rsquared       MAE \n8.5033987 0.6389774 6.4632568 \n      RMSE   Rsquared        MAE \n10.0543556  0.4971724  7.4724012 \n```\n\n\n:::\n:::\n\n\nThe performance is different in python simply because of different default settings for the python vs R implementations.\n\n:::\n\nTo understand the decision making of lasso, we can check the beta's in a similar fashion to a regression (only shown for the R models).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# running the following can tell you a bit about the impact of different variables\nsmall.lambda.index <- which(lasso_fit$lambda == lasso_fit$lambda.min)\nlasso_fit$glmnet.fit$beta[, small.lambda.index]\n\n# or on the final model\ncoef(lasso_fit , s= 'lambda.min')\nplot(lasso_fit, xvar = \"lambda\", label = TRUE)\n```\n\n::: {.cell-output-display}\n![](Ex_ML_LinLogReg_files/figure-html/unnamed-chunk-28-1.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   TransDate     HouseAge         Dist    NumStores          Lat         Long \n  3.19017320  -0.22968625  -0.00460157   0.99674463 230.81613231   0.00000000 \n7 x 1 sparse Matrix of class \"dgCMatrix\"\n                       s1\n(Intercept) -1.214284e+04\nTransDate    3.190173e+00\nHouseAge    -2.296862e-01\nDist        -4.601570e-03\nNumStores    9.967446e-01\nLat          2.308161e+02\nLong         .           \n```\n\n\n:::\n:::\n\nWe can see that from the first output `Long` value has almost a beta of 0. In the second output, it is confirmed that the coefficient of this variable is indeed 0. This could explain why `mod_lm_sel` also dropped this variable. The rest of the coefficient are similar to `summary(mod_lm_sel)`.  as ridge or lasso in some situations. This can occur when the number of predictors in the model is small relative to the sample size, or when the predictors are highly correlated.\n\n# Your turn to practice\n\n## Linear regression: nursing home data\n\nNow it is your turn. Make an linear regression (also feel free to try lasso and ridge regressions) on the nursing data described below (found also in `/data/nursing_data.csv`). Afterwards, use linear regression to build a predictor of the cost using the other features. Replicate the analysis. Split the data, build a model, make the variable selection, make the predictions and analyze the results. Make also an analysis of the coefficients in terms of the associations between the costs and the features.\n\n<details>\n\n<summary>Data Description</summary>\n\nThe data set is about patients in a nursing home, where elderly people are helped with daily living needs, also known as Activities of Daily Living (ADL, i.e. communication, eating, walking, showering, going to a toilet, etc.).\n\nSince the stay in such facilities is very expensive, it is important to classify the new-coming patient, and estimate the duration of the stay and the corresponding costs.\n\nIn practice, there are different types of patients who require different types of help and, consequently, different duration of the stay. For example, there could be a person with severe mobility issues, who requires the help with most of the needs every day; or a person with mental deviations, who don't need help with daily routine, but requires extra communication hours.\n\nHere, we will focus of total amount of help (measured in minutes of help provided to a person per week) provided and measure the costs of stay of a person.\n\nThe data set on which the analysis is based has the following columns:\n\n-   **gender**: a categorical variable with levels \"*M*\" for male and \"*F*\" for female\n-   **age**: integer variable\n-   **mobil**: categorical variable that represents the physical mobility with levels\n    -   1 = Full mobility\n    -   2 = Reduced mobility\n    -   3 = Restricted mobility in the house\n    -   4 = Null mobility\n-   **orient**: categorical variable that represents the orientation (interactions with the environment) with levels\n    -   1 = Full orientation\n    -   2 = Moderate disturbance of orientation\n    -   3 = Disorientation\n-   **independ**: categorical variable that represents the independence of ADL with levels\n    -   1 = Independent of help\n    -   2 = Dependent less than 24 hours per day\n    -   3 = Dependent at unpredictable time intervals for most of the needs\n-   **minut_mob**: numerical variable that represents the total number of minutes of help with movement per week\n-   **need_comm**: categorical variable with levels \"*Yes*\" for a person who needs extra communication sessions with an employee, and \"*No*\" otherwise\n-   **minut_comm**: numerical variable that represents the total number of minutes of communication per week\n-   **tot_minut**: numerical variable that represents the total number of minutes spent on a patient per week, $tot\\_minut = minut\\_mob + minut\\_comm$\n-   **cost**: numerical variable that represents the total costs of having a patient in the nursing house per month.\n\n</details>\n\nNote: since tot_minut=minut_mob+minut_comm, you may not find any meaningful result using the 3 features. This is perfectly normal. Just use 2 features only among these 3 (arbitrary choice).\n\n## Logistic regression: the credit quality\n\nThe German Credit Quality Dataset consists of a set of attributes as good or bad credit risks. In order to find find a detailed description of the features, please refer to the [original link to the dataset](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)). The `german.csv` file can also be found in `/data/german.csv` whichis the mdified version of the original dataset to simplify the analysis, especially the data loading in **R**.\n\nThe aim here is to predict the credit quality from the other features. The outcome **Quality** is 0 for \"bad\" and 1 for \"good\". Make an analysis of the data and develop the learner. You can follow these notable steps:\n\n-   Make a simple EDA of the features\n-   Split the data and train the model.\n-   Make variable selection and check out the result.\n-   Interpret the coefficients.\n-   Inspect the quality of the model by making the predictions (confusion table and boxplot of the predicted probabilities).\n\nNote that the data are unbalanced again and that you may not find a very good predictor. This issue is quite difficult and will be addressed later.\n",
    "supporting": [
      "Ex_ML_LinLogReg_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}