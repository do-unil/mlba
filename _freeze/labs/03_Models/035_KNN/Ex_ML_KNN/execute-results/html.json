{
  "hash": "290cff24db121c080d67720686937538",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Models: K-Nearest Neighbors\"\nformat: html\n---\n\n\n\n# K-NN: a gentle introduction\n\nIn this first part, we apply the K-NN to the iris data set with examples in both R & python. In both cases, we first load the `iris` built-in data set found `dplyr`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\niris\nstr(iris)\n```\n:::\n\n\n::: panel-tabset\n\n## R\n\nIn R, we can model `iris` using the `knn3` function from the `caret` package (named `knn3` because `knn` is already taken by another package). This function is limited to the Euclidean distance with numerical features.\n\nWe first make the prediction using a 2-NN (with Euclidean distance).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\nmod_knn <- knn3(data=iris, Species~., k=2) ## build the K-NN model\npredict(mod_knn, newdata = iris, type=\"class\")\n```\n:::\n\n\nNow, we want to know if the predictions are good. To do this, we must ensure that the model is not overfitting the data. For this, we must split the data into training and test sets (these concepts will be seen in detail in a future course). To do so, we randomly take 75% of the iris data that will be the training set, and the remaining 25% are the test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123) ## for replication purpose\n\nindex_tr <- sample(1:nrow(iris), replace=FALSE, size=0.75*nrow(iris))\nindex_tr ## the index of the rows of iris that will be in the training set\n\niris_tr <- iris[index_tr,] ## the training set\niris_te <- iris[-index_tr,] ## the test set\n```\n:::\n\n\nNow we can use the 2-NN to predict the test set using the training set. Note that the model is fitted on the training set, and the predictions are computed on the test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_knn <- knn3(data=iris_tr, Species~., k=2)\niris_te_pred <- predict(mod_knn, newdata = iris_te, type=\"class\")\n```\n:::\n\n\nTo compare the predictions above and the true species (the one in the test set), we can build a table. It is called a *confusion matrix* (again, this will be explained in detail later on).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(Pred=iris_te_pred, Observed=iris_te[,5])\n```\n:::\n\n\nThe prediction is almost perfect. It is so good that it is pointless to try to improve the prediction by changing `K` at that point. However, just to illustrate, below we use `K=3`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_knn <- knn3(data=iris_tr, Species~., k=3)\niris_te_pred <- predict(mod_knn, newdata = iris_te, type=\"class\")\n\ntable(Pred=iris_te_pred, Observed=iris_te[,5])\n```\n:::\n\n\nNote that, in th formula \"Species\\~.\", the dot means \"all the other variables\". It is equivalent (but shorter) to \"Species \\~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width\"\n\n## Python\n\nBefore running the python, we must ensure we're using the `MLBA` conda (virtual) environment created during `setup.Rmd`. As we already installed some of the packages, it should suffice for this part of the exercise:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# make sure we're using the right environment\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\npy_config()\n```\n:::\n\n\nFirst, let's load the iris dataset and print its structure. You can access the r data object by using `r.iris` or only the pure python data as shown below (you don't need the chunk below):\n\n``` python\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\niris_df['species'] = iris.target_names[iris.target]\nprint(iris_df)\nprint(iris_df.info())\n```\n\nNow, in python, we will also apply K-NN to the iris data set using the `KNeighborsClassifier` function from the `sklearn` package in python, where we set `K=2`. This function can be applied to both numerical and categorical features. We then make predictions with this python model.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# call iris data from r\ny = r.iris[[\"Species\"]]  # select column \"Species\"\nX = r.iris.drop(columns=[\"Species\"])  # drop column \"Species\"\n\n# if you imported the data directly in python, you can instead run the commands below\n# y = iris.target\n# X = iris.data\n\nk = 2\nknn_model = KNeighborsClassifier(n_neighbors=k)\nknn_model.fit(X, y)\n\npred = knn_model.predict(X)\nprint(pred)\n```\n:::\n\n\nWe will split the data again into training and test sets. We will randomly select 75% of the data for the training set and the remaining 25% for the test set. To achieve this, we will use `numpy`, most often used for efficient numerical computing. This split of training and test sets will only be done once in the exercises as we already created the same objects in R. Nevertheless, this is one way of dividing data into training and test sets in python:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\n\nnp.random.seed(123)\nindex_tr = np.random.choice(range(len(r.iris)), size=int(0.75*len(r.iris)), replace=False)\nindex_te = np.setdiff1d(range(len(r.iris)), index_tr)\n\niris_tr = r.iris.iloc[index_tr, :]\niris_te = r.iris.iloc[index_te, :]\n```\n:::\n\n\nNow, we can fit the K-NN model to the training set and make predictions on the test set.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nknn_model = KNeighborsClassifier(n_neighbors=k)\nknn_model.fit(iris_tr.iloc[:, :-1], iris_tr.iloc[:, -1])\n\npred = knn_model.predict(iris_te.iloc[:, :-1])\nprint(pred)\n```\n:::\n\n\nTo evaluate the performance of our model, we can construct a confusion matrix.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.metrics import confusion_matrix\n\n# for the confusion matrix in python, we need to specify the column names\nlabels = r.iris['Species'].unique()\n\n# create the confusion matrix with the labels as column names\nconf_mat = confusion_matrix(iris_te.iloc[:, -1], pred, labels=labels)\nconf_mat_df = pd.DataFrame(conf_mat, columns=labels, index=labels)\n\n# print the confusion matrix\nprint(conf_mat_df)\n```\n:::\n\n\nThe prediction is almost perfect (as was the case in R), so it is not necessary to try to improve the prediction by changing k at this point. However, just for illustration, we can repeat the process with `K=3`.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nk = 3\n\nknn_model = KNeighborsClassifier(n_neighbors=k)\nknn_model.fit(iris_tr.iloc[:, :-1], iris_tr.iloc[:, -1])\n\npred = knn_model.predict(iris_te.iloc[:, :-1])\nconf_mat = confusion_matrix(iris_te.iloc[:, -1], pred, labels=labels)\nprint(pd.DataFrame(conf_mat, index=labels, columns=labels))\n```\n:::\n\n\n::: callout-note\n## Why results are different in Python vs R\n\nPlease note that the training and test sets we used in python are not the same ones used in R due to a different generator for the seed (i.e., `set.seed(123)` is not generating the same numbers as `np.random.seed(123)`). Therefore, different observations were taken for the test set in the two languages. If you want to see the same performance, you can also run in both languages with the same dataset. For instance, running the code below would give you the same results:\n\n``` {{r}}\nmod_knn_py <- knn3(data=py$iris_tr, Species~., k=2)\niris_te_pred_py <- predict(mod_knn_py, newdata = py$iris_te, type=\"class\")\ntable(Pred=iris_te_pred_py, Observed=py$iris_te[,5])\n```\n:::\n\n:::\n\n# K-NN: regression\n\nIn the case of a regression task, the prediction is obtained by averaging the `K` nearest neighbors. To illustrate this, we will use the `imports.85` data set. The aim is to predict the price using only the numerical features (categorical features are illustrated later).\n\nBelow, we identify the numerical columns (don't forget to load the data first!). The new data frame is named `tmp` (temporary...). Then, we remove all the rows containing an NA.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimports_85 <- read.csv(here::here(\"labs/data/imports-85.data\"), \n                       header=FALSE, na.strings=\"?\")\n\nnames = c(\"symboling\",\"normalized.losses\",\"make\",\"fuel.type\",\n         \"aspiration\",\"num.of.doors\",\"body.style\",\"drive.wheels\",\n         \"engine.location\",\"wheel.base\",\"length\",\"width\",\n         \"height\",\"curb.weight\",\"engine.type\",\"num.of.cylinders\",\n         \"engine.size\",\"fuel.system\",\"bore\",\"stroke\",\n         \"compression.ratio\",\"horsepower\",\"peak.rpm\",\"city.mpg\",\n         \"highway.mpg\",\"price\")\n\nnames(imports_85) = names\n\ntmp <- imports_85 %>% select(where(is.numeric))\ntmp <- filter(tmp, complete.cases(tmp))\ntmp %>% head()\n```\n:::\n\n\nNow, we make a 75%-25% split for our training and testing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nindex_tr <- sample(1:nrow(tmp), size=0.75*nrow(tmp), replace = FALSE)\ntmp_tr <- tmp[index_tr,]\ntmp_te <- tmp[-index_tr,]\n```\n:::\n\n\n::: panel-tabset\n\n## R\n\nWe now fit the `knnreg` function to fit the model in R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_knn <- knnreg(data=tmp_tr, price~., k=3)\ntmp_pred <- predict(mod_knn, newdata=tmp_te)\n```\n:::\n\n\nWe now graphically compare the real prices in the test set with the predicted ones (the diagonal line shows the equality between the prediction and the real price).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_te %>% mutate(pred=tmp_pred) %>%\nggplot(aes(x=price, y=pred)) + \n  geom_point() + geom_abline(slope=1, intercept = 0)\n```\n:::\n\n\nIt looks like there is still room for improvement. Check for yourself if this can be improved by changing `K`.\n\n## Python\n\nAs you already learnt how to import the data in R, and divided into training and test sets, we will use that directly (which also allows for better comparisons between modelling in R vs python).\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsRegressor\n\nX_tr = r.tmp_tr.drop(columns=[\"price\"])\ny_tr = r.tmp_tr[\"price\"]\nmod_knn_py = KNeighborsRegressor(n_neighbors=3)\nmod_knn_py.fit(X_tr, y_tr)\n\nX_te = r.tmp_te.drop(columns=[\"price\"])\ny_te = r.tmp_te[\"price\"]\ntmp_pred = mod_knn_py.predict(X_te)\n```\n:::\n\n\nWe do the same graphical comparison in python, however, we will use `matplotlib` to demonstrate the results.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# we have to make a copy otherwise `r.temp_te` is not directly modified (i.e. we can't add a column)\ntmp_test_plt = r.tmp_te.copy()\ntmp_test_plt[\"pred\"]= tmp_pred\nfig, ax = plt.subplots()\nax.plot(tmp_test_plt[\"price\"], tmp_test_plt[\"pred\"], '.', color='black')\nax.plot(tmp_test_plt[\"price\"], tmp_test_plt[\"price\"], '-', color='blue')\nax.set_xlabel(\"Price\")\nax.set_ylabel(\"Predicted price\")\nplt.show()\n```\n:::\n\n\nWe obtained the same results as R.\n\n:::\n\n# K-NN: mixture of feature types\n\nIn this part, we illustrate how to incorporate categorical variables with K-NN. As a reminder, to use categorical variables, the easiest way is probably to cast them to dummy variables.\n\nThe code below identifies the categorical columns (i.e., non numerical), then create the dummies using and add them to the data frame. In addition, the numerical variables are standardized.\n\nThere are several difficulties in the code below.\n\n-   The price (the outcome) is first set aside because we do not want to standardize it.\n-   Two brand names (variable `make`) contain \"-\", namely \"alfa-romeo\" and \"mercedes-benz\". After the creation of the dummy variables, these are used as titles. However, `knnreg` does not support \"-\" in the variable names. They thus have to be removed first (in the code below they are turned to \"\\_\").\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## tmp is imports.85 without incomplete cases and without price\nimp_comp <- imports_85 %>% filter(complete.cases(imports_85))\ny <- imp_comp$price\nimp_comp <- imp_comp %>% select(!price)\n\n# we will use the standardize function shown by Marc-Olivier during the course (was called `my_fun`)\n# normalize num variable: (x - min(x))/(max(x) - min(x))\nmy_normalize <- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n# additionally, you can use the `scale()` function which centers and scales variables\n\n## the numerical variables in tmp are standardized\nimp_num <- imp_comp %>% select(where(is.numeric)) %>% my_normalize() %>% as.data.frame()\n\n## the dummy variables of the numerical features of tmp are created\nlibrary(fastDummies)\nimp_dumm <- imp_comp %>% select(!where(is.numeric)) %>% \n  dummy_cols(remove_first_dummy = FALSE, remove_selected_columns = TRUE)\n\n## These dummy variables are added to tmp\nimp_dat <- data.frame(imp_num, imp_dumm)\n\n## The names of the two problematic brands are changed\nnames(imp_dat)[c(23)] <- c(\"make_mercedes_benz\")\n\n## The raw price is added to tmp\nimp_dat$price <- y\n```\n:::\n\n\nAt this stage, `tmp` contains all the variables that we want. We once again divide our data into training and test sets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nindex_tr <- sample(1:nrow(imp_dat), size=0.75*nrow(imp_dat), replace = FALSE)\nimp_tr <- imp_dat[index_tr,]\nimp_te <- imp_dat[-index_tr,]\n```\n:::\n\n\n::: panel-tabset\n\n## R\n\nWe can now run our 3-NN directly, like in the previous exercise.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_knn <- knnreg(data=imp_tr, price~., k=3)\nimp_pred <- predict(mod_knn, newdata=imp_te)\nimp_te %>% mutate(pred=imp_pred) %>%\n  ggplot(aes(x=price, y=pred)) + \n  geom_point() + geom_abline(slope=1, intercept = 0)\n```\n:::\n\n\nHere, like always, it is difficult to compare the scales of the scatterplot, and thus to tell if the quality of that 3-NN is better or worst than the previous one. The issue of model scoring will be studied later in the course. For now, it is just about being able to mix categorical and numeric variables in a K-NN.\n\n\n## Python\n\nThe same approach in Python with the same.\n\n\n::: {.cell}\n\n```{.python .cell-code}\nX_tr = r.imp_tr.drop(columns=[\"price\"])\ny_tr = r.imp_tr[\"price\"]\nmod_knn_py = KNeighborsRegressor(n_neighbors=3)\nmod_knn_py.fit(X_tr, y_tr)\n\nX_te = r.imp_te.drop(columns=[\"price\"])\ny_te = r.imp_te[\"price\"]\ntmp_pred = mod_knn_py.predict(X_te)\n\ntmp_test_plt = r.imp_te.copy()\ntmp_test_plt[\"pred\"]= tmp_pred\nfig, ax = plt.subplots()\nax.plot(tmp_test_plt[\"price\"], tmp_test_plt[\"pred\"], '.', color='black')\nax.plot(tmp_test_plt[\"price\"], tmp_test_plt[\"price\"], '-', color='blue')\nax.set_xlabel(\"Price\")\nax.set_ylabel(\"Predicted price\")\nplt.show()\n```\n:::\n\n\n:::\n\n# Analysis of nursing home data\n\nNow it is your turn. Develop a K-NN model to predict the cost using the other variables. Inspect the quality of the prediction using a training set and a test set.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}