{
  "hash": "1562fe5b1c78cf4199a91e5342b367ee",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Models: CART\"\n---\n\n\n\n# Classification tree\n\nIn this exercise, the classification tree method is used to analyze the data set `Carseats` from the package `ISLR`. The exercise took some inspiration from [this video](https://www.youtube.com/watch?v=GOJN9SKl_OE).\n\n## Data preparation\n\nFirst, install the package `ISLR` in order to access the data set `Carseats`. Use `?Carseats` to read its description. To apply a classification of the sales, we first create a categorical outcome `SaleHigh` which equals \"Yes\" if `Sales` \\> 7.5 and \"No\" otherwise. Then we create a data frame `MyCarseats` containing `SaleHigh` and all the features of `Carseats` except `Sales`. Finally, split `MyCarseats` into a training and a test set (2/3 vs 1/3). Below we call them `df_tr` and `df_te`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ISLR)\nlibrary(dplyr)\nMyCarseats <- Carseats %>% mutate(SaleHigh=ifelse(Sales > 7.5, \"Yes\", \"No\"))\nMyCarseats <- MyCarseats %>% select(-Sales)\n\nset.seed(123) # for reproducibility \nindex_tr <- sample(x=1:nrow(MyCarseats), size=2/3*nrow(MyCarseats), replace=FALSE)\ndf_tr <- MyCarseats[index_tr,]\ndf_te <- MyCarseats[-index_tr,]\n```\n:::\n\n\nNote: this is not the point of this exercise but remember that in a real situation the first step of the data analysis would be an EDA.\n\n## Fit & plot\n\n::: panel-tabset\n### R\n\nThe `rpart` function in the package `rpart` can be used to fit a classification tree with the same type of formulas as `naive_bayes`. It can then be plotted using the function `rpart.plot` of the package `rpart.plot`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rpart)\nlibrary(rpart.plot)\nset.seed(1234)\ncarseats_tree <- rpart(SaleHigh ~ ., data=df_tr)\nrpart.plot(carseats_tree)\n```\n\n::: {.cell-output-display}\n![](Ex_ML_Tree_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load MLBA environment\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n```\n:::\n\n\nWe use the `df_train` and `df_te` created in R to carry out our CART training. For this, we will use the `sklearn` library. First, we copy our dataset from the R variable to a python variable. Then we use any encoder (e.g. `OneHotEncoder`, `LabelEncoder`, `OrdinalEncoder`) from `sklearn.preprocessing` to convert categorical data into a numeric form, which many `sklearn` machine learning algorithms require. In this case, we use the `OneHoteEncoder` which is similar to making dummy variables and turn each level of the category into a column. Also, we standardize the data in the case of python for faster computations using `StandardScaler`, which also helps to bring numerical stability and improve the results. After this, we divide the training and test sets into predictors (e.g., `X_train`) and the outcome (e.g., `y_train`) and initialize our classification tree and fit the model to the data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n# We copy the training and testing datasets from R to Python.\ncarset_tr_py = r.df_tr.copy()\ncarset_te_py= r.df_te.copy()\n\n# We encode categorical variables as numeric, which is necessary for CART.\nle = OneHotEncoder()\ncat_vars = [\"ShelveLoc\", \"Urban\", \"US\", \"SaleHigh\"]\nfor var in cat_vars:\n    # Fit the LabelEncoder to the training set and transform the training and testing sets\n    # We need to use the same encoder for both sets to ensure consistency\n    carset_tr_py_encoded = le.fit_transform(carset_tr_py[[var]].values.reshape(-1, 1))\n    carset_te_py_encoded = le.transform(carset_te_py[[var]].values.reshape(-1, 1))\n    carset_tr_py[var] = carset_tr_py_encoded.toarray()\n    carset_te_py[var] = carset_te_py_encoded.toarray()\n\n# Split the data into training and testing sets\nX_train, y_train = carset_tr_py.drop(columns=[\"SaleHigh\"]), carset_tr_py[\"SaleHigh\"]\nX_test, y_test = carset_te_py.drop(columns=[\"SaleHigh\"]), carset_te_py[\"SaleHigh\"]\n\n# Standardize only the continuous variables\ncont_vars = [\"CompPrice\", \"Income\", \"Advertising\", \"Population\", \"Price\", \"Age\", \"Education\"]\nscaler = StandardScaler()\nX_train[cont_vars] = scaler.fit_transform(X_train[cont_vars])\nX_test[cont_vars] = scaler.transform(X_test[cont_vars])\n\n# To speed up the operation you can also transform the inputs\n# X_train = X_train.to_numpy()\n# y_train = y_train.to_numpy()\n```\n:::\n\n\nWe can now train and plot our decision tree using `DecisionTreeClassifier` and `plot_tree` functions.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nimport matplotlib.pyplot as plt\n\n# we clear any previous figures\nplt.clf()\n\nnp.random.seed(1234)\ncarseats_tree = DecisionTreeClassifier().fit(X_train, y_train)\nplt.figure(figsize=(20,10))\nplot_tree(carseats_tree,filled=True, feature_names=X_train.columns, label = 'root',fontsize=5);\nplt.savefig('tree_high_dpi', dpi=300)\n# for a better quality, save the image and load it again\n#plt.show()\n```\n\n::: {.cell-output-display}\n![](Ex_ML_Tree_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=1920}\n:::\n:::\n\n\nAs the image dimensions are not always great for `matplotlib` plots in Rstudio, we saved the image and now load it again below using an R chunk.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknitr::include_graphics(\"tree_high_dpi.png\")\n```\n\n::: {.cell-output-display}\n![](tree_high_dpi.png){fig-align='center' width=3000}\n:::\n:::\n\n:::\n\n## Pruning\n\n::: panel-tabset\n### R\n\nThe analysis of the tree complexity can be obtained using function `plotcp`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplotcp(carseats_tree)\n```\n\n::: {.cell-output-display}\n![](Ex_ML_Tree_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nFrom the graph, we can identify that, according to the 1-SE rule, the tree with 8 nodes is equivalent to the tree with 12 nodes. This 8-nodes tree should be preferred.\n\nTo prune the tree (i.e., extract the tree with 8 nodes), we can use the function `prune` with argument `cp`. The `cp` of the tree can be read on the bottom x-axis of the `plotcp`. The argument in `prune` should be set to any value between the cp of 8-nodes tree (0.031) and the 11-node tree (0.019). Here 0.025 is OK.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncarseats_tree_prune <- prune(carseats_tree, cp=0.025)\nrpart.plot(carseats_tree_prune)\n```\n\n::: {.cell-output-display}\n![](Ex_ML_Tree_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n`Important note`: The CP evaluation relies on a cross-validation procedure, which uses random number generation. This is why we have set the seed to some value (1234). You may not find the same result with another seed or if you do not set it. In any case, just be coherent with your results and prune the tree accordingly.\n\n**Let the computer do the work for you**: Pruning using the 1-SE rule can be automatically obtained with the function `autoprune` in package `adabag`. Note that, because of the randomness involved in the CP evaluation, you may not find exactly the same result as the one obtained by hand. Use `set.seed` to make your result reproducible.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(adabag)\nset.seed(123455)\nrpart.plot(autoprune(SaleHigh ~ ., data=df_tr))\n```\n\n::: {.cell-output-display}\n![](Ex_ML_Tree_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n### Python\n\nThe `rpart.plot` package in R provides a convenient `plotcp()` function to plot the complexity parameter table for a decision tree fit with `rpart()`. Unfortunately, `scikit-learn` doesn't provide an equivalent function out of the box. However, you can still calculate the complexity parameter values for your decision tree and plot them using `matplotlib`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# Get the paths of the leaf nodes for the Car Seats decision tree using cost complexity pruning\npath = carseats_tree.cost_complexity_pruning_path(X_train, y_train)\n# Extract the effective alphas and total impurities of the leaf nodes from the path object\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n# Create a plot to visualize the relationship between effective alphas and total impurities\nfig, ax = plt.subplots()\nax.plot(ccp_alphas, impurities, marker='o', linestyle=\"-\");\nax.set_xlabel(\"Effective alpha\");\nax.set_ylabel(\"Total impurity of leaves\");\nax.set_title(\"Total Impurity vs Effective alpha for Car Seats dataset\");\nax.invert_xaxis();\nplt.show()\n```\n\n::: {.cell-output-display}\n![](Ex_ML_Tree_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output-display}\n![](Ex_ML_Tree_files/figure-html/unnamed-chunk-10-2.png){fig-align='center' width=672}\n:::\n:::\n\n\nThis plot only shows the training set results, which doesn't tell us about over-fitting. A better approach is to compute accuracy (a different metric than `rpart`) on a second test set, called the *validation set* used solely for finding the ideal hyperparameters in machine learning. Once we choose the best hyperparameters, we re-train the model one final time with those parameters and compare everything on the test set (but we should no longer change our models based on the test set). We will learn more about this validation set during the upcoming lectures. Here we'll use the two functions `cross_val_score` and `KFold` from the `sklearn.module_selection` sub-module. We will use ten folds to find the ideal `alpha` (equivalent to `cp` from `rpart::rpart()`). This is not using the `1-SE` rule but proposes a good alternative.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom sklearn.model_selection import cross_val_score, KFold\n\ndef plotcp(X_train, y_train, random_state=123):\n    # Create a decision tree classifier\n    clf = DecisionTreeClassifier(random_state=random_state)\n\n    # Calculate the cross-validation scores for different values of alpha\n    path = clf.cost_complexity_pruning_path(X_train, y_train)\n    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n    # Perform cross-validation for each alpha\n    kfold = KFold(n_splits=10, shuffle=True, random_state=random_state)\n    scores = []\n    for ccp_alpha in ccp_alphas:\n        clf = DecisionTreeClassifier(random_state=random_state, ccp_alpha=ccp_alpha)\n        score = cross_val_score(clf, X_train, y_train, cv=kfold, scoring='accuracy')\n        scores.append(np.mean(score))\n\n    # Plot the cross-validation scores vs alpha\n    fig, ax = plt.subplots()\n    ax.plot(ccp_alphas, scores, marker='o', linestyle=\"-\");\n    ax.set_xlabel(\"ccp_alpha\");\n    ax.set_ylabel(\"Cross-validation score (accuracy)\");\n    ax.set_title(\"Pruning Complexity Parameter (ccp) vs Cross-validation Score\");\n    ax.invert_xaxis();\n    plt.show()\n\nplotcp(X_train, y_train)\n```\n\n::: {.cell-output-display}\n![](Ex_ML_Tree_files/figure-html/unnamed-chunk-11-5.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe can see that the best validation scores are obtained around a `ccp_alpha` of `0.008`. Similar to `cp` from `rpart`, in scikit-learn, the parameter controls the complexity of a classification tree by setting a penalty on the number of leaf nodes. A higher value of results in a simpler tree with fewer splits and more nodes being pruned. More specifically, `alpha` is the regularization parameter used for controlling the cost complexity of the tree. The cost complexity is the sum of the misclassification cost and the complexity cost of the tree. The complexity cost is proportional to the number of terminal nodes (leaves) in the tree. A higher value of `alpha` thus means that the model is less likely to overfit the training data and more likely to generalize better to new, unseen data. However, setting `alpha` too high can result in underfitting and poor model performance on both the training and test data. The optimal `alpha` value depends on the specific dataset and the problem being solved and can be determined through cross-validation or other model selection techniques, as demonstrated above.\n\nOnce you find the ideal alpha, you can specify it with the `ccp_alpha` argument in `DecisionTreeClassifier()`. Here we will take `ccp_alpha` as 0.008 for simplicity.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# Create a decision tree classifier with a ccp_alpha of 0.025\ncarseats_tree_prune = DecisionTreeClassifier(random_state=123, ccp_alpha=0.008)\n\n# Fit the model to the data\ncarseats_tree_prune.fit(X_train, y_train);\n\n# You can again plot the figure with\nplt.clf()\nplt.figure(figsize=(12,10))\nplot_tree(carseats_tree_prune,filled=True, feature_names=X_train.columns, label = 'root',fontsize=5);\nplt.savefig('tree_pruned_high_dpi', dpi=200)\nplt.close()\n```\n\n::: {.cell-output-display}\n![](Ex_ML_Tree_files/figure-html/unnamed-chunk-12-7.png){fig-align='center' width=672}\n:::\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nknitr::include_graphics(\"tree_pruned_high_dpi.png\")\n```\n\n::: {.cell-output-display}\n![](tree_pruned_high_dpi.png){fig-align='center' width=1200}\n:::\n:::\n\n\nThis model is a lot simpler compared to the first tree made using python (where `ccp_alpha` was 0 by default).\n\nFor automatically pruning the tree, unlike the `adabag::autoprune()` function in R's adabag package, scikit-learn does not have a built-in function for automatic pruning of decision trees. Instead, you can use cross-validation to determine the optimal tree depth and use that to prune the tree.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV\n\n# Set up the decision tree classifier\ntree = DecisionTreeClassifier()\n\n# Set up the grid search parameters\nparam_grid = {'max_depth': range(1, 11)}\n\n# Run grid search cross-validation to find optimal tree depth\ngrid_search = GridSearchCV(tree, param_grid=param_grid, cv=10)\ngrid_search.fit(X_train, y_train);\n\n# Use the best estimator to fit and prune the tree\npruned_tree = grid_search.best_estimator_\n\nplt.clf()\nfig, ax = plt.subplots(figsize=(15, 10))\nplot_tree(pruned_tree, ax=ax, feature_names=X_train.columns);\nplt.show()\n# you can choose to re-train the model once again with this new parameter\n```\n\n::: {.cell-output-display}\n![](Ex_ML_Tree_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=1440}\n:::\n\n::: {.cell-output-display}\n![](Ex_ML_Tree_files/figure-html/unnamed-chunk-14-2.png){fig-align='center' width=672}\n:::\n:::\n\n\n::: callout-tip\n## Python approach vs `adabag::autoprune()`\n\nThe technique above does not explicitly use the 1-SE rule for pruning the decision tree. Instead, it uses cross-validation to find the optimal tree depth based on the mean test score across all folds. According to the documentation of `adabag::autoprune()`, *\"The cross validation estimation of the error (xerror) has a random component. To avoid this randomness the 1-SE rule (or 1-SD rule) selects the simplest model with a xerror equal or less than the minimum xerror plus the standard deviation of the minimum xerror.\"* If you're interested in the 1-SE, see the adapted python code for it below.\n\n<details>\n\n<summary>**Implementing GridSearchCV with 1-SE rule**</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# Set up the decision tree classifier\ntree = DecisionTreeClassifier()\n\n# Set up the grid search parameters\nparam_grid = {'max_depth': range(1, 11)}\n\n# Run grid search cross-validation to find optimal tree depth\ngrid_search = GridSearchCV(tree, param_grid=param_grid, cv=10)\ngrid_search.fit(X_train, y_train);\n\n# Calculate the mean and standard error of test scores for each tree depth\nmean_scores = grid_search.cv_results_['mean_test_score']\nstd_scores = grid_search.cv_results_['std_test_score'] / np.sqrt(10)\n\n# Find the optimal depth using the 1-SE rule\noptimal_depth = grid_search.best_params_['max_depth']\noptimal_score = mean_scores[optimal_depth - 1]\nse = std_scores[optimal_depth - 1]\nbest_depth = optimal_depth\nfor depth in range(optimal_depth - 1, -1, -1):\n    score = mean_scores[depth]\n    if score + se < optimal_score:\n        break\n    else:\n        best_depth = depth + 1\n\n# Use the best estimator to fit and prune the tree\npruned_tree = DecisionTreeClassifier(max_depth=best_depth)\npruned_tree.fit(X_train, y_train);\n```\n:::\n\n\nThe results are the same as not using 1-SE.\n\n</details>\n:::\n:::\n\n## Predictions\n\nFirst, use the R plot to determine what is the prediction of the first instance of `MyCarseats`.\n\n<details>\n\n<summary>**Answer**</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nMyCarseats[1,]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  CompPrice Income Advertising Population Price ShelveLoc Age Education Urban\n1       138     73          11        276   120       Bad  42        17   Yes\n   US SaleHigh\n1 Yes      Yes\n```\n\n\n:::\n:::\n\n\nFollow Left, Left, Left =\\> The predicted answer is \"No\".\n\n</details>\n\n::: panel-tabset\n### R\n\nThe function `predict` can be used build the predictions of the test set (use option `type=\"class\"`). This is similar to the previously seen models: the `predict` function used on an object of class `.rpart` (created by the function `rpart`), in fact, calls the function `predict.rpart` which is adapted to the model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npred <- predict(carseats_tree_prune, newdata=df_te, type=\"class\")\ntable(Pred=pred, Obs=df_te$SaleHigh)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     Obs\nPred  No Yes\n  No  58  27\n  Yes  4  45\n```\n\n\n:::\n:::\n\n\nNote that, like most categorical models, you may ask for the probabilities instead of the classes by setting `type=\"prob\"`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict(carseats_tree_prune, newdata=df_te, type=\"prob\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            No       Yes\n1   0.79824561 0.2017544\n3   0.24000000 0.7600000\n6   0.24000000 0.7600000\n8   0.08333333 0.9166667\n12  0.08333333 0.9166667\n15  0.08333333 0.9166667\n17  0.08333333 0.9166667\n18  0.08333333 0.9166667\n19  0.08333333 0.9166667\n21  0.79824561 0.2017544\n28  0.79824561 0.2017544\n30  0.76000000 0.2400000\n31  0.08333333 0.9166667\n37  0.08333333 0.9166667\n38  0.79824561 0.2017544\n44  0.79824561 0.2017544\n46  0.79824561 0.2017544\n47  0.05555556 0.9444444\n49  0.76000000 0.2400000\n50  0.85714286 0.1428571\n56  0.76923077 0.2307692\n58  0.79824561 0.2017544\n59  0.76000000 0.2400000\n60  0.79824561 0.2017544\n62  0.79824561 0.2017544\n65  0.76000000 0.2400000\n68  0.79824561 0.2017544\n70  0.05555556 0.9444444\n71  0.08333333 0.9166667\n73  0.79824561 0.2017544\n75  0.18750000 0.8125000\n79  0.79824561 0.2017544\n82  0.08333333 0.9166667\n88  0.08333333 0.9166667\n92  0.79824561 0.2017544\n95  0.24000000 0.7600000\n96  0.79824561 0.2017544\n97  0.08333333 0.9166667\n99  0.08333333 0.9166667\n100 0.79824561 0.2017544\n103 0.76000000 0.2400000\n108 0.79824561 0.2017544\n114 0.79824561 0.2017544\n119 0.76000000 0.2400000\n120 0.79824561 0.2017544\n122 0.24000000 0.7600000\n123 0.79824561 0.2017544\n124 0.85714286 0.1428571\n126 0.24000000 0.7600000\n128 0.79824561 0.2017544\n131 0.24000000 0.7600000\n132 0.76000000 0.2400000\n138 0.79824561 0.2017544\n139 0.79824561 0.2017544\n140 0.05555556 0.9444444\n144 0.79824561 0.2017544\n147 0.79824561 0.2017544\n149 0.76000000 0.2400000\n150 0.24000000 0.7600000\n156 0.24000000 0.7600000\n167 0.79824561 0.2017544\n169 0.79824561 0.2017544\n170 0.08333333 0.9166667\n171 0.79824561 0.2017544\n175 0.79824561 0.2017544\n176 0.79824561 0.2017544\n180 0.18750000 0.8125000\n181 0.79824561 0.2017544\n188 0.76000000 0.2400000\n189 0.24000000 0.7600000\n190 0.76000000 0.2400000\n191 0.05555556 0.9444444\n192 0.85714286 0.1428571\n193 0.76000000 0.2400000\n206 0.79824561 0.2017544\n207 0.76923077 0.2307692\n208 0.76000000 0.2400000\n219 0.79824561 0.2017544\n220 0.08333333 0.9166667\n221 0.79824561 0.2017544\n225 0.79824561 0.2017544\n234 0.79824561 0.2017544\n237 0.79824561 0.2017544\n241 0.18750000 0.8125000\n246 0.08333333 0.9166667\n247 0.24000000 0.7600000\n248 0.79824561 0.2017544\n259 0.24000000 0.7600000\n264 0.79824561 0.2017544\n266 0.79824561 0.2017544\n268 0.79824561 0.2017544\n270 0.76923077 0.2307692\n271 0.08333333 0.9166667\n274 0.24000000 0.7600000\n281 0.79824561 0.2017544\n283 0.85714286 0.1428571\n284 0.79824561 0.2017544\n289 0.76000000 0.2400000\n293 0.08333333 0.9166667\n301 0.76000000 0.2400000\n303 0.79824561 0.2017544\n304 0.05555556 0.9444444\n305 0.08333333 0.9166667\n307 0.79824561 0.2017544\n310 0.05555556 0.9444444\n318 0.08333333 0.9166667\n322 0.76000000 0.2400000\n325 0.79824561 0.2017544\n327 0.79824561 0.2017544\n329 0.79824561 0.2017544\n333 0.76000000 0.2400000\n334 0.79824561 0.2017544\n335 0.24000000 0.7600000\n338 0.05555556 0.9444444\n339 0.76000000 0.2400000\n342 0.76000000 0.2400000\n344 0.79824561 0.2017544\n345 0.08333333 0.9166667\n356 0.85714286 0.1428571\n358 0.24000000 0.7600000\n360 0.79824561 0.2017544\n367 0.79824561 0.2017544\n368 0.08333333 0.9166667\n369 0.08333333 0.9166667\n370 0.79824561 0.2017544\n371 0.79824561 0.2017544\n373 0.76000000 0.2400000\n375 0.79824561 0.2017544\n386 0.79824561 0.2017544\n387 0.76923077 0.2307692\n393 0.79824561 0.2017544\n394 0.79824561 0.2017544\n396 0.08333333 0.9166667\n400 0.08333333 0.9166667\n```\n\n\n:::\n:::\n\n\n### Python\n\nTo print a confusion matrix of the predicted labels versus the true labels, we can use the `crosstab()` function from the `pandas` library. We pass the predicted and true labels as arguments to `crosstab()`, and use the `rownames` and `colnames` parameters to label the rows and columns of the table, respectively.\n\nFinally, we use the `predict_proba()` method of the decision tree classifier to predict the class probabilities for the test data. The predicted probabilities are stored in the `y_probs` variable, which is a `NumPy` array with shape `(n_samples, n_classes)`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# Predict the class labels for the test data with the python implementation\ny_pred = carseats_tree_prune.predict(X_test)\n\n# Print a confusion matrix of the predicted labels versus the true labels\nprint(pd.crosstab(index=y_pred, columns=y_test, rownames=['Pred'], colnames=['Obs']))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nObs   0.0  1.0\nPred          \n0.0    52   16\n1.0    20   46\n```\n\n\n:::\n:::\n\n\nThis tree is worse than the R implementation, probably due to differences in other default values that we did not tune for.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# Predict the class probabilities for the test data\ny_probs = carseats_tree_prune.predict_proba(X_test)\nprint(pd.DataFrame(y_probs))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            0         1\n0    0.961538  0.038462\n1    0.837838  0.162162\n2    1.000000  0.000000\n3    0.421053  0.578947\n4    0.086207  0.913793\n..        ...       ...\n129  1.000000  0.000000\n130  0.961538  0.038462\n131  0.961538  0.038462\n132  0.961538  0.038462\n133  0.827586  0.172414\n\n[134 rows x 2 columns]\n```\n\n\n:::\n:::\n\n:::\n\n## Interpretation\n\nBy looking at the tree, interpret the most important features for the Sales: the highest in the tree.\n\nNote that, for the level *`Good`* of the `ShelveLoc` variable, only the `Price` drives the `Sales` (according to the tree). Otherwise, it is a subtle mixture between `Price` and `CompPrice`.\n\n# Regression tree\n\nThe regression tree concepts are the same as for classification tree. The (main) difference lies in the outcome being predicted as a value instead of a class.\n\nTo illustrate this we quickly build a tree on the Sales of the `Carseats` data (no training and test set here; it is simply to illustrate the numerical prediction).\n\n::: panel-tabset\n## R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\ncarseats_reg <- rpart(Sales ~ ., data=Carseats)\nrpart.plot(carseats_reg)\n```\n\n::: {.cell-output-display}\n![](Ex_ML_Tree_files/figure-html/unnamed-chunk-21-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe can see from the graph that the final nodes are associated to a numerical prediction. Use the plot to determine what is the prediction of the first instance of `Carseats`. Please note that here we are not dividing between training and test sets, and we only demonstrate how to do a regression tree, but in practice you should always do that.\n\n<details>\n\n<summary>**Answer**</summary>\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nCarseats[1,]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sales CompPrice Income Advertising Population Price ShelveLoc Age Education\n1   9.5       138     73          11        276   120       Bad  42        17\n  Urban  US\n1   Yes Yes\n```\n\n\n:::\n:::\n\n\nFollow Left, Left, Left, Right =\\> the predicted answer is 5.4.\n\n</details>\n\nNow make the prediction for all the data and check the quality.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncarseat_reg_pred <- predict(carseats_reg)\nplot(carseat_reg_pred ~ Carseats$Sales,\n     xlab=\"Sales\", ylab=\"Predictions\")\nabline(0,1, col=\"red\")\n```\n\n::: {.cell-output-display}\n![](Ex_ML_Tree_files/figure-html/unnamed-chunk-23-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## Python\n\nFor the regression task, we need to select the Sales column with continuous values and then encode it via one of the available options. The default encoder for scikit-learn's `DecisionTreeRegressor` is `OrdinalEncoder`. This means that the categorical variables are encoded during training using an ordinal mapping of the categories to integer values. Therefore, for demonstration purposes only (it doesn't make as much sense as `OneHoteEncoder`), we encode them with `OrdinalEncoder`. To read more about this and why it may not make sense for this dataset, click on the blue box below.\n\n::: {.callout-note collapse=\"true\"}\n\n### `OrdinalEncoder` vs `OneHotEncoder`\n\nIf we use `OrdinalEncoder` to encode the categorical features, we would convert each category to a numerical value based on the order of the categories. For example, suppose the categories for `ShelveLoc` are \"Bad\", \"Good\", and \"Medium\". If we use `OrdinalEncoder`, the encoding would be as follows: \"Bad\" = 1, \"Good\" = 2, \"Medium\" = 3. Similarly, for `Urban` and `US`, we might use \"No\" = 1 and \"Yes\" = 2. The resulting encoded features would be numerical, but the ordering might not be meaningful.\n\nOn the other hand, if we use `OneHotEncoder` to encode the categorical features, we would create a binary column for each category in each feature. For example, if we use `OneHotEncoder` to encode `ShelveLoc`, we would create three binary columns: `ShelveLoc_Bad`, `ShelveLoc_Good`, and `ShelveLoc_Medium`. For an instance where `ShelveLoc` is \"Good\", the `ShelveLoc_Good` column would have a value of 1, and the other two columns would have a value of 0. This encoding ensures that each category is treated equally and that there is no implicit ordering between the categories.\n\nIn summary, `OrdinalEncoder` is appropriate when the categories have a meaningful order, such as in the case of \"low\", \"medium\", and \"high\" for a feature like `Price`. On the other hand, OneHotEncoder is appropriate when there is no meaningful ordering between the categories, such as in the case of `Urban`, `US`, and `ShelveLoc` for the CarSeats dataset.\n\n:::\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# Select different columns for the regression task from the original carsets data\nX, y = r.Carseats.drop(columns=[\"Sales\"]), r.Carseats[\"Sales\"]\n\n# Once again select the categorical columns (this time without `SalesHigh`)\ncategorical_cols = ['ShelveLoc', 'Urban', 'US']\n\n# Encode categorical columns as integers\nencoder = OrdinalEncoder()\nX[categorical_cols] = encoder.fit_transform(X[categorical_cols])\n\n# Build the regression tree model\ncarseats_reg = DecisionTreeRegressor(random_state=123)\ncarseats_reg.fit(X, y);\n\n# Visualize the tree (a bit messy)\n# fig, ax = plt.subplots(figsize=(12, 8))\n# plot_tree(carseats_reg, ax=ax, feature_names=X.columns)\n# plt.show()\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# Make predictions on the training data\ncarseat_reg_pred = carseats_reg.predict(X)\n\n# Create a scatter plot of predicted versus true values\nplt.clf()\nfig, ax = plt.subplots(figsize=(8, 8))\nax.scatter(y, carseat_reg_pred, alpha=0.5);\nax.set_xlabel('Sales');\nax.set_ylabel('Predictions');\nax.plot(ax.get_xlim(), ax.get_ylim(), ls=\"--\", c=\".3\");\nplt.show()\n```\n\n::: {.cell-output-display}\n![](Ex_ML_Tree_files/figure-html/unnamed-chunk-25-1.png){fig-align='center' width=768}\n:::\n\n::: {.cell-output-display}\n![](Ex_ML_Tree_files/figure-html/unnamed-chunk-25-2.png){fig-align='center' width=672}\n:::\n:::\n\n\nThis perfect line indicates that we (perfectly) over-fit the data, which is a significant danger (bias-variance trade-off). That's why you need both training and test sets (and preferably a validation set)!\n\n:::\n\n# You turn\n\nUse a regression tree to fit nursing data (outcome = `cost`). Fit the tree using a training set, prune the tree and make the scatterplot \"obs vs pred\" on the test set to analyze the quality.\n",
    "supporting": [
      "Ex_ML_Tree_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}