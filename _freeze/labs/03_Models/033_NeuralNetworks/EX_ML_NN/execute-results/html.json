{
  "hash": "12fc34015fff4af3c36c5ea107098db8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Models: Neural Networks\"\n---\n\n\n\n\nIn this tutorial, we will demonstrate how to use `keras` and `tensorflow` in R for two common tasks in deep learning (DL): regression for structured data and image classification. We will use two datasets for this purpose: the `Boston Housing dataset` for regression and the `CIFAR-10` dataset for image classification.\n\n# Introduction\n\n`Keras` and `TensorFlow` are open-source machine learning frameworks for building and training deep learning models. `TensorFlow` is a low-level framework that provides a foundation for building and training neural networks. `keras`, on the other hand, is a high-level API built on top of `TensorFlow`, making it easier to construct and train deep learning models. This means that `keras` is much more plug-and-play than `TensorFlow`.\n\nIn `keras`, two main ways to build and run deep learning models are the `sequential API` and the `functional API`. The `sequential API` is a simple way to create a neural network by adding layers in a linear sequence. On the other hand, the `functional API` provides more flexibility and allows you to create more complex models with shared layers or multiple inputs and outputs. In this lab session, we deal with the more straightforward `sequential API`; however, feel free to also read about `functional API` [here](https://tensorflow.rstudio.com/guides/keras/functional_api).\n\nThe `sequential API` is suitable for building simple models quickly and easily. Initially, one has to define the desired network architecture\nby one layer at a time to a new `sequential` model. Once the architecture is defined, we need to `compile` the model and then `fit` the model to the data to start training. Here is a summary of all the steps:\n\n1.  *Define the model architecture*: Create a new sequential model object and add layers to it using the pipe operator (`%>%`), which translates to the `add()` method in Python. We can add as many layers as needed where the type of each layer can also be specified (e.g., dense, convolutional) and any necessary parameters for each layer (e.g., number of nodes, activation function).\n\n2.  *Compile the model*: Once the model's architecture has been defined, it has to be compiled using the `compile()` method. This involves specifying the loss function, optimizer, and any additional metrics you want to track during training.\n\n3.  *Train the model*: To train the model, use the `fit()` method and pass in your training data and any necessary hyperparameters (e.g., batch size, number of epochs). A validation set can also be specified to monitor the model's performance during training. Additionally, there are techniques such as early stopping, which stops the training if the model's performance on the validation set does not improve after a certain number of epochs.\n\n4.  *Evaluate the model*: After training, the model's performance on a test set can be evaluated using the `evaluate()` method. This will return the loss and any metrics specified during compilation (step 2).\n\n5.  *Use the model to make predictions*: Finally, one can use the trained model to make predictions on new data using the `predict()` method either from base R or as an attribute of the model (i.e., `model$predict()`). This stage is also known as \"inference\" in DL.\n\n# Setup\n\nWe will use the `keras` and `tensorflow` libraries in R, which are wrappers around `keras` and `TensorFlow` libraries in python. Wrappers provide an abstraction layer that enables users to use a library or framework from a different programming language. Due to this, the syntax of using these libraries in R and python are very close, and maybe except for the `%>%` operator, there are no major differences. For this reason, all examples are provided in R and we only provide one example of using `keras` in with the Boston housing data. For the other part of the lab, if you're more interested in implementing it in python, you can adapt the code yourself based on the provided example.\n\nWe first need to get started with the setup. We will install `keras` into our `MLBA` environment created during `setup.Rmd`. This applies whether you're using only R or both R & python; therefore, if you have not already created a conda virtual environment, please refer to `setup.Rmd`. Once the the virtual environment already exists, we can then install both `keras` and `tensorflow` with `keras::install_keras()`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ninstall.packages(\"keras\")\nkeras::install_keras(method = \"conda\", envname = \"MLBA\")\n```\n:::\n\n\nWe can now check if the installation was successful\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nreticulate::use_condaenv(\"MLBA\")\nlibrary(keras)\nlibrary(tensorflow)\n```\n:::\n\n\nOnce the setup is over, we check if everything works as expected.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntflow <- import(\"tensorflow\")\ntflow$constant(\"Hello Tensorflow!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntf.Tensor(b'Hello Tensorflow!', shape=(), dtype=string)\n```\n\n\n:::\n\n```{.r .cell-code}\n# we typically have to do `tf$` to call tensorflow that is automatically loaded like below\n# tf$constant(\"Hello Tensorflow!\")\n```\n:::\n\nIf you have the same output, you're good to go. Also, sometimes when you run `tf$constant()` command for the first time, you may get `Error in py_module_proxy_import(proxy) : Module proxy does not contain module name` because you just loaded `keras`. If it was the case, just rerun this code chunk.\n\n::: callout-note\n## Getting a warning when loading `tensorflow`\nWhen loading something from `keras`/`tensorflow`, if you get a warning like the one below, you should not worry:\n```\nI tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n```\n`tensorflow` has been optimized to work on graphical processing units (GPUs), and sometimes there are specific configurations for improving the performance of `keras` on central processing units (CPUs). You can safely ignore these warnings. Training on GPUs, especially when using images or high-dimensional data, drastically speeds up the training process and is one of the reasons behind the popularity of DL. If your computer has an Nvidia GPU (it would not work on AMD or Intel GPUs), you can also set it up to work with `tensorflow` by following the instructions from [here](https://tensorflow.rstudio.com/install/local_gpu).\n:::\n\n# Regression for Structured Data\n\n## Boston Housing Dataset\n\nThe [Boston Housing dataset](http://lib.stat.cmu.edu/datasets/boston) is a well-known dataset used for regression tasks. It contains 506 instances and 13 features, including the median value of owner-occupied homes in \\$1000s. We will use this dataset to demonstrate how to perform regression on the sales price.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(keras)\n\nhouses <- dataset_boston_housing()\n\ntrain_x <- houses$train$x\ntrain_y <- houses$train$y\n\ntest_x <- houses$test$x\ntest_y <- houses$test$y\n```\n:::\n\n\n## Data Preparation\n\nWe need to normalize the data. This is especially relevant for neural networks to stabilize the computation of the gradients and consequently improve the training process.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeans <- apply(train_x, 2, mean)\nsds <- apply(train_x, 2, sd)\n\ntrain_x <- scale(train_x, means, sds)\ntest_x <- scale(test_x, means, sds)\n```\n:::\n\n\n## Model Definition\n\nWe will use a simple neural network with two hidden layers to perform regression.\n\n::: callout-tip\n### Learning about some of the hyperparameters\nYou have many options for different hyperparameters; however, one lab session barely scratches the surface of DL and its hyperparameters. There are a couple of points that we need to specify here:\n\n1. You can use any activation function in the middle layers, from a simple linear regression (`leaner`) to more common ones such as `hyperbolic tagnet` or `tanh` (often suitable for tabular data) to more sophisticated ones such as `rectified linear unit` or `relu` (better suited to high dimensional data). What is imperative is that in the last dense layer, the number of units and the activation function determine the kind of task (e.g., classification, regression, etc.) you're trying to accomplish. If you're doing a regression, the last dense layer has to have 1 unit and a `linear` activation function. If you're doing a binary classification (logistic regression), you still use 1 dense unit but must apply the `sigmoid` activation function. If you're doing multi-class classification, then the number of dense units must equal the number of classes, and the activation function is `softmax` (which is a generalization of `sigmoid` for multiple classes). [Google](https://developers.google.com/machine-learning/guides/text-classification/step-4) also provides a nice visual that explains the difference between the classification models. If you remove the `>0.5` rule (i.e., `sigmoid`), you essentially get a linear regression for that layer.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](NNLastLayer.png){fig-align='center' width=600px}\n:::\n:::\n\n\n2. It is imperative that, depending on the task, you use the correct type of loss function. For instance, you can use `mean squared error` (mse) for regression and `categorical cross-entropy` for multi-class classification.\n\nAs mentioned, during the ML course, we cannot cover the details of DL. Suppose you want to understand these hyperparameters better and learn about many new ones. In that case, you can check out the video recordings and the slides for the [Deep learning course previously taught at HEC](https://irudnyts.github.io/deep/lectures/) (last given in 2021).\n:::\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# we set a seed (this sometimes disables GPU performance)\ntensorflow::set_random_seed(123)\n\n# define the model structure\nmodel_reg <- keras_model_sequential() %>%\n  layer_dense(units = 64, activation = \"relu\", input_shape = c(ncol(train_x))) %>%\n  layer_dense(units = 64, activation = \"relu\") %>%\n  layer_dense(units = 1, activation = \"linear\")\n\n# compile the model\nmodel_reg %>% compile(\n  optimizer = \"adam\",\n  loss = \"mean_squared_error\",\n  metrics = c(\"mean_absolute_error\")\n)\n```\n:::\n\n\n### Python\n\nWe can run the same thing in Python with the datasets created in R, e.g. `r.train_x`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# import necessary libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Set random seed\ntf.random.set_seed(123)\n\n# define the model structure\nmodel_reg = keras.Sequential([\n    layers.Dense(units=64, activation='relu', input_shape=(r.train_x.shape[1],)),\n    layers.Dense(units=64, activation='relu'),\n    layers.Dense(units=1, activation='linear')\n])\n\n# compile the model\nmodel_reg.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n```\n:::\n\n\n:::\n\n## Model Training\n\nWe can then train the model:\n\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhistory <- model_reg %>% fit(\n  train_x, train_y,\n  epochs = 10, #300 to get the best results\n  batch_size = 32,\n  validation_split = 0.2,\n  callbacks = callback_early_stopping(patience = 30,restore_best_weights = T),\n  verbose = 1 # to control the printing of the output\n)\n```\n:::\n\n\nYou don't need to assign `model %>% fit()` to `history`. We only do that to plot the results later (e.g., with `plot(history)`.\n\n### Python\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# import necessary libraries\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# fit the model\nmodel_reg.fit(\n    r.train_x, r.train_y,\n    epochs=10, #300\n    batch_size=32,\n    validation_split=0.2,\n    callbacks=[EarlyStopping(patience=30,restore_best_weights=True)],\n    verbose=1\n)\n```\n:::\n\n\n:::\n\n## Model Evaluation\nFinally, we can evaluate the model on the testing set:\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnn_results <- model_reg %>% evaluate(test_x, test_y)\nnn_results\n# alternatively, you can use the `predict` attribute from `model` as shown below\n# nn_results <- model$predict(test_x)\n# caret::MAE(obs = test_y, pred = as.vector(nn_results))\n```\n:::\n\n\n### Python\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nnn_results = model_reg.evaluate(r.test_x, r.test_y)\n```\n\n```{.python .cell-code}\nprint(nn_results)\n```\n:::\n\n\n:::\n\nTo put these results (from R) in context, we can compare it with a simple linear regression:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf_tr <- data.frame(train_y, train_x)\nlm_mod <- lm(train_y ~ ., data = df_tr)\nlm_preds <- as.vector(predict(lm_mod, newdata=data.frame(test_x))) # `predict()` for lm doesn't accept matrices\n\n# calculate mean absolute error with caret `MAE` installed in the `K-NN excercises`\ncaret::MAE(obs = test_y, pred = lm_preds)\n```\n:::\n\n\nWe see that the neural network does significantly better than a regression model. We can also compare it with the trees seen in the CART series.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rpart)\nset.seed(123)\ntree_model <- rpart(train_y ~ ., data=df_tr)\ntree_preds <- predict(tree_model,newdata = data.frame(test_x))\n\n# calculate the performance on tree\ncaret::MAE(obs = test_y, pred = tree_preds)\n```\n:::\n\n\nThe neural network also outperforms CART (if you run NN for 300 epochs). This is due to multiple reasons, including a higher complexity of the neural network (more parameters), using a validation set, and so on. You will learn about ensemble methods (beginning and boosting) in the upcoming lectures. Ensemble methods are the true champions for structured data (at least as of 2023) and usually outperform neural networks for structured/low-dimensional data.\n\n# Image Classification\n\n## CIFAR-10 Dataset\n\nThe [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) is a well-known dataset used for image classification tasks. It contains 50,000 training images and 10,000 testing images of size 32x32 pixels, each belonging to one of ten classes. We will use this dataset to demonstrate how to perform image classification.\n\nIn this case, we had our inputs already prepared, but if it was not the case, you can always do:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load the CIFAR-10 dataset (it'll take a while for it to download)\ncifar10 <- dataset_cifar10()\n\n# Split the data into training and testing sets\ntrain_images <- cifar10$train$x\ntrain_labels <- cifar10$train$y\ntest_images <- cifar10$test$x\ntest_labels <- cifar10$test$y\n```\n:::\n\n\nIf you do an `str(train_images)`, you can see that the output is a four-dimensional array. In this array, the first element is the number of images in the training set, the second and third elements represent the (pixel values) for each image's width and height, respectively, and the last element is the color channel of the image. If you had a black-and-white image (called greyscale), the number of channels would be just one. Typically, an image in color is represented with a combination of three colors known as RGB (red, green, and blue). Therefore, here we have three elements associated with the pixel number for each color, and their combination provides us with the colors you see in the image.\n\n## Data Preparation\n\nBefore building our image classification model, we need to prepare the data by normalizing the values to obtain pixel values between 0-1. Similar to standardization of tabular data, this helps to avoid exploding/vanishing gradients and can improve the convergence of the model.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Normalize the pixel values\ntrain_images <- train_images / 255\ntest_images <- test_images / 255\n```\n:::\n\n\nWe can also plot some of the example images.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Plot the first 9 images\nlabel_names <- c(\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n\npar(mfrow = c(3, 3))\nfor (i in 1:9) {\n  img <- train_images[i,,,]\n  # label <- train_labels[i]\n  label <- label_names[train_labels[i] + 1]  # Add 1 to convert from 0-based to 1-based indexing\n  img <- array_reshape(img, c(32, 32, 3))\n  plot(as.raster(img))\n  title(paste(\"Label:\", label))\n}\n```\n:::\n\n\nWe can now one-hot encode our labels to be better adapted to the loss function we will be using (otherwise you can use \"sparse\" variation of the loss functions).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrain_labels <- to_categorical(train_labels)\ntest_labels <- to_categorical(test_labels)\n```\n:::\n\n\n## Model definition\nWe can build our image classification model using `keras` by defining the layers of the neural network. For this task we'll be using Convolutional neural networks (CNNs). CNNs are a type of deep learning model that are commonly used for image and video processing. They consist of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The convolutional layers use filters to extract features from the input image, and the pooling layers downsample the output of the convolutional layers. The fully connected layers combine the extracted features to produce the final output of the network. CNNs have been shown to be effective in a wide range of applications, including image classification, object detection, and semantic segmentation.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Set a seed again for reproducibility\ntensorflow::set_random_seed(123)\n\n# Define the model\nmodel_cnn <-\n  keras_model_sequential()%>%\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3), input_shape = c(32, 32, 3),  activation = \"relu\") %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_conv_2d(filters = 64, kernel_size = c(5, 5), activation = \"relu\") %>%\n  layer_max_pooling_2d(pool_size = c(3, 3)) %>%\n  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = \"relu\") %>%\n  layer_flatten() %>%\n  layer_dense(units = 64, activation = \"relu\") %>%\n  layer_dense(units = 10, activation = \"softmax\")\n\n# Compile the model\nmodel_cnn %>% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = optimizer_adam(),\n  metrics = c(\"accuracy\")\n)\n```\n:::\n\n\nDepending on youer computer, this may take some time to run.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhistory <- model_cnn %>% fit(\n  x = train_images,\n  y = train_labels,\n  epochs = 10,\n  batch_size = 64,\n  # validation_split = 0.2\n  validation_data= list(test_images, test_labels)\n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmodel_cnn %>% evaluate(\n  x = test_images,\n  y = test_labels\n)\n```\n:::\n\n\nYou may notice that we did not obtain extremely high accuracy, but this is okay for multiple reasons:\n\n1. We only trained the model for a few epochs and stopped the training very early. You're welcome to let it run for more epochs.\n2. We're dealing with at least ten classes; this is a rather complicated problem.\n3. In DL, you can define many different architectures and hyperparameters, and since we did not play around with these values, this could also explain the lower performance.\n\n# Your turn: NNs applied to the nursing data\nUse NNs to fit nursing data, with the **cost** variable once again set as the prediction outcome. Compare the performance against the tree-based methods. Which type of machine learning model would you go for and why? (think in terms of interpretability and performance)\n\n# References\n- [Setting up GPU `tensorflow` in R](https://tensorflow.rstudio.com/install/local_gpu)\n- [Google ML image showing the two types of activation functions for classifier neural networks](https://developers.google.com/machine-learning/guides/text-classification/step-4)\n- [Material for the Deep learning course previously taught at HEC](https://irudnyts.github.io/deep/lectures/)\n",
    "supporting": [
      "EX_ML_NN_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}