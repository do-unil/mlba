{
  "hash": "dbb46416a73d36ee284ad161f71e6483",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Models: Neural Networks\"\n---\n\n\n\nIn this tutorial, we will demonstrate how to use neural networks for two common tasks in deep learning (DL): regression for structured data and image classification. We will use two datasets for this purpose: the `Boston Housing dataset` for regression and the `CIFAR-10` dataset for image classification. It is highly recommended that you use Python for this part since R does not have a very good support for DL. However, we will provide the R code for the regression part to show how you can use the [`keras3`](https://github.com/rstudio/keras3) package to build and train neural networks in R.\n\n# Introduction\n\nNeural networks are powerful machine learning models inspired by the human brain. Two popular frameworks for building them are:\n\n- **PyTorch**: A flexible deep learning framework popular in research, offering an intuitive \"define-by-run\" approach.\n- **Keras** (with TensorFlow backend): A high-level API that makes it easy to construct and train deep learning models. Keras 3 is actively maintained and supports multiple backends (TensorFlow, PyTorch, JAX). Note that raw `TensorFlow` has limited support on some platforms (e.g., Apple Silicon).\n\nIn both frameworks, the basic workflow for building a neural network is:\n\n1.  *Define the model architecture:* Create a model and add layers to it. We can add as many layers as needed where the type of each layer can also be specified (e.g., dense, convolutional) and any necessary parameters for each layer (e.g., number of nodes, activation function).\n\n2.  *Compile/configure the model:* Specify the loss function, optimizer, and any additional metrics you want to track during training.\n\n3.  *Train the model:* Pass in your training data and any necessary hyperparameters (e.g., batch size, number of epochs). A validation set can also be specified to monitor the model's performance during training. Additionally, there are techniques such as early stopping, which stops the training if the model's performance on the validation set does not improve after a certain number of epochs.\n\n4.  *Evaluate the model:* After training, evaluate the model's performance on a test set.\n\n5.  *Use the model to make predictions:* Use the trained model to make predictions on new data. This stage is also known as \"inference\" in DL.\n\n# Setup\n\n::: {.panel-tabset}\n\n### Python (PyTorch)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nprint(f\"PyTorch version: {torch.__version__}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPyTorch version: 2.10.0+cpu\n```\n\n\n:::\n:::\n\n\n### Python (TensorFlow)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport numpy as np\nimport keras\nfrom keras import layers\nprint(f\"Keras version: {keras.__version__}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nKeras version: 3.12.1\n```\n\n\n:::\n:::\n\n\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# keras3 should already be installed via renv::restore()\n# If not, run: install.packages(\"keras3\"); keras3::install_keras(backend = \"tensorflow\")\nlibrary(keras3)\n```\n:::\n\n\n:::\n\n# Regression for Structured Data\n\n## Boston Housing Dataset\n\nThe [Boston Housing dataset](http://lib.stat.cmu.edu/datasets/boston) is a well-known dataset used for regression tasks. It contains 506 instances and 13 features, including the median value of owner-occupied homes in \\$1000s. We will use this dataset to demonstrate how to perform regression on the sales price.\n\n::: {.panel-tabset}\n\n### Python (PyTorch)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom keras.datasets import boston_housing\n\n(train_x_np, train_y_np), (test_x_np, test_y_np) = boston_housing.load_data()\nprint(f\"Train: {train_x_np.shape}, Test: {test_x_np.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrain: (404, 13), Test: (102, 13)\n```\n\n\n:::\n:::\n\n\n### Python (TensorFlow)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom keras.datasets import boston_housing\n\n(train_x, train_y), (test_x, test_y) = boston_housing.load_data()\nprint(f\"Train: {train_x.shape}, Test: {test_x.shape}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrain: (404, 13), Test: (102, 13)\n```\n\n\n:::\n:::\n\n\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhouses <- keras3::dataset_boston_housing()\n\ntrain_x <- houses$train$x\ntrain_y <- houses$train$y\ntest_x <- houses$test$x\ntest_y <- houses$test$y\n```\n:::\n\n\n:::\n\n## Data Preparation\n\nWe need to normalize the data. This is especially relevant for neural networks to stabilize the computation of the gradients and consequently improve the training process.\n\n::: {.panel-tabset}\n\n### Python (PyTorch)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nmeans_np = train_x_np.mean(axis=0)\nstds_np = train_x_np.std(axis=0)\n\ntrain_x_np = (train_x_np - means_np) / stds_np\ntest_x_np = (test_x_np - means_np) / stds_np\n\n# Convert to PyTorch tensors\ntrain_x_t = torch.tensor(train_x_np, dtype=torch.float32)\ntrain_y_t = torch.tensor(train_y_np, dtype=torch.float32).unsqueeze(1)\ntest_x_t = torch.tensor(test_x_np, dtype=torch.float32)\ntest_y_t = torch.tensor(test_y_np, dtype=torch.float32).unsqueeze(1)\n```\n:::\n\n\n### Python (TensorFlow)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nmeans = train_x.mean(axis=0)\nstds = train_x.std(axis=0)\n\ntrain_x = (train_x - means) / stds\ntest_x = (test_x - means) / stds\n```\n:::\n\n\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmeans <- apply(train_x, 2, mean)\nsds <- apply(train_x, 2, sd)\n\ntrain_x <- scale(train_x, means, sds)\ntest_x <- scale(test_x, means, sds)\n```\n:::\n\n\n:::\n\n## Model Definition\n\nWe will use a simple neural network with two hidden layers to perform regression.\n\n::: callout-tip\n### Learning about some of the hyperparameters\nYou have many options for different hyperparameters; however, one lab session barely scratches the surface of DL and its hyperparameters. There are a couple of points that we need to specify here:\n\n1. You can use any activation function in the middle layers, from a simple linear regression (`linear`) to more common ones such as `hyperbolic tangent` or `tanh` (often suitable for tabular data) to more sophisticated ones such as `rectified linear unit` or `relu` (better suited to high dimensional data). What is imperative is that in the last dense layer, the number of units and the activation function determine the kind of task (e.g., classification, regression, etc.) you're trying to accomplish. If you're doing a regression, the last dense layer has to have 1 unit and a `linear` activation function. If you're doing a binary classification (logistic regression), you still use 1 dense unit but must apply the `sigmoid` activation function. If you're doing multi-class classification, then the number of dense units must equal the number of classes, and the activation function is `softmax` (which is a generalization of `sigmoid` for multiple classes). [Google](https://developers.google.com/machine-learning/guides/text-classification/step-4) also provides a nice visual that explains the difference between the classification models. If you remove the `>0.5` rule (i.e., `sigmoid`), you essentially get a linear regression for that layer.\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](NNLastLayer.png){fig-align='center' width=600px}\n:::\n:::\n\n\n2. It is imperative that, depending on the task, you use the correct type of loss function. For instance, you can use `mean squared error` (mse) for regression and `categorical cross-entropy` for multi-class classification.\n\nAs mentioned, during the ML course, we cannot cover the details of DL. If you want to understand these hyperparameters better and explore additional ones, some excellent resources include Stanford's [CS231n: Deep Learning for Computer Vision](https://cs231n.stanford.edu/) and the [PyTorch tutorials](https://pytorch.org/tutorials/).\n:::\n\n::: {.panel-tabset}\n\n### Python (PyTorch)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n_ = torch.manual_seed(123)\n\n# Define the model\nmodel_reg_pt = nn.Sequential(\n    nn.Linear(train_x_t.shape[1], 64),\n    nn.ReLU(),\n    nn.Linear(64, 64),\n    nn.ReLU(),\n    nn.Linear(64, 1)\n)\n\n# Define loss function and optimizer\ncriterion = nn.MSELoss()\noptimizer_pt = optim.Adam(model_reg_pt.parameters())\n```\n:::\n\n\n### Python (TensorFlow)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nkeras.utils.set_random_seed(123)\n\n# Define the model\nmodel_reg = keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=(train_x.shape[1],)),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1, activation='linear')\n])\n\n# Compile the model\nmodel_reg.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\nmodel_reg.summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel: \"sequential\"\n┌─────────────────────────────────┬────────────────────────┬───────────────┐\n│ Layer (type)                    │ Output Shape           │       Param # │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense (Dense)                   │ (None, 64)             │           896 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_1 (Dense)                 │ (None, 64)             │         4,160 │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (Dense)                 │ (None, 1)              │            65 │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n Total params: 5,121 (20.00 KB)\n Trainable params: 5,121 (20.00 KB)\n Non-trainable params: 0 (0.00 B)\n```\n\n\n:::\n:::\n\n\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkeras3::set_random_seed(123)\n\nmodel_reg <- keras_model_sequential(input_shape = ncol(train_x)) |>\n  layer_dense(units = 64, activation = \"relu\") |>\n  layer_dense(units = 64, activation = \"relu\") |>\n  layer_dense(units = 1, activation = \"linear\")\n\nmodel_reg |> compile(\n  optimizer = \"adam\",\n  loss = \"mean_squared_error\",\n  metrics = c(\"mean_absolute_error\")\n)\n```\n:::\n\n\n:::\n\n## Model Training\n\n::: {.panel-tabset}\n\n### Python (PyTorch)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Create data loaders\ntrain_dataset = TensorDataset(train_x_t, train_y_t)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\n# Training loop\nloss_history = []\n_ = model_reg_pt.train()\nfor epoch in range(10):  # 300 for best results\n    epoch_loss = 0\n    for batch_x, batch_y in train_loader:\n        optimizer_pt.zero_grad()\n        pred = model_reg_pt(batch_x)\n        loss = criterion(pred, batch_y)\n        loss.backward()\n        optimizer_pt.step()\n        epoch_loss += loss.item()\n    avg_loss = epoch_loss / len(train_loader)\n    loss_history.append(avg_loss)\n    if (epoch + 1) % 5 == 0:\n        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n\nplt.plot(loss_history);\nplt.xlabel(\"Epoch\");\nplt.ylabel(\"Loss (MSE)\");\nplt.title(\"Training Loss\");\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch 5, Loss: 257.8388\nEpoch 10, Loss: 36.2100\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](EX_ML_NN_files/figure-html/unnamed-chunk-14-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n### Python (TensorFlow)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\n\nhistory = model_reg.fit(\n    train_x, train_y,\n    epochs=10,  # 300 to get the best results\n    batch_size=32,\n    validation_split=0.2,\n    callbacks=[keras.callbacks.EarlyStopping(patience=30, restore_best_weights=True)],\n    verbose=1\n)\n\nplt.plot(history.history['loss'], label='Train');\nplt.plot(history.history['val_loss'], label='Validation');\nplt.xlabel(\"Epoch\");\nplt.ylabel(\"Loss (MSE)\");\nplt.title(\"Training Loss\");\nplt.legend();\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch 1/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 363ms/step - loss: 566.7098 - mean_absolute_error: 21.7820\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 548.3987 - mean_absolute_error: 21.5741 - val_loss: 586.5386 - val_mean_absolute_error: 22.5123\nEpoch 2/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 517.0693 - mean_absolute_error: 20.7190\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 501.7396 - mean_absolute_error: 20.5227 - val_loss: 534.2108 - val_mean_absolute_error: 21.3861\nEpoch 3/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 465.5650 - mean_absolute_error: 19.5448\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 448.2894 - mean_absolute_error: 19.2557 - val_loss: 468.3222 - val_mean_absolute_error: 19.8879\nEpoch 4/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 401.8004 - mean_absolute_error: 18.0079\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 379.8379 - mean_absolute_error: 17.5461 - val_loss: 383.2387 - val_mean_absolute_error: 17.8069\nEpoch 5/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 321.5128 - mean_absolute_error: 15.8981\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 294.5597 - mean_absolute_error: 15.2094 - val_loss: 283.1658 - val_mean_absolute_error: 14.9977\nEpoch 6/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 227.8317 - mean_absolute_error: 13.1221\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 201.2679 - mean_absolute_error: 12.1951 - val_loss: 183.1437 - val_mean_absolute_error: 11.4459\nEpoch 7/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 135.8779 - mean_absolute_error: 9.7947\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 119.6998 - mean_absolute_error: 8.9462 - val_loss: 107.9070 - val_mean_absolute_error: 8.1430\nEpoch 8/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 68.3341 - mean_absolute_error: 6.7204\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 70.8525 - mean_absolute_error: 6.5915 - val_loss: 71.4134 - val_mean_absolute_error: 6.4259\nEpoch 9/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 36.8598 - mean_absolute_error: 5.2102\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 52.5437 - mean_absolute_error: 5.4284 - val_loss: 57.1445 - val_mean_absolute_error: 5.6849\nEpoch 10/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 25.8261 - mean_absolute_error: 4.2970\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 43.6421 - mean_absolute_error: 4.7666 - val_loss: 47.6434 - val_mean_absolute_error: 5.1693\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](EX_ML_NN_files/figure-html/unnamed-chunk-15-3.png){fig-align='center' width=672}\n:::\n:::\n\n\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nhistory <- model_reg |> fit(\n  train_x, train_y,\n  epochs = 10,  # 300 to get the best results\n  batch_size = 32,\n  validation_split = 0.2,\n  callbacks = callback_early_stopping(patience = 30, restore_best_weights = TRUE),\n  verbose = 1\n)\n\nplot(history)\n```\n\n::: {.cell-output-display}\n![](EX_ML_NN_files/figure-html/unnamed-chunk-16-5.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch 1/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 331ms/step - loss: 566.7198 - mean_absolute_error: 21.7822\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 548.4251 - mean_absolute_error: 21.5747 - val_loss: 586.6022 - val_mean_absolute_error: 22.5136\nEpoch 2/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 517.1339 - mean_absolute_error: 20.7204\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 501.8156 - mean_absolute_error: 20.5244 - val_loss: 534.3273 - val_mean_absolute_error: 21.3886\nEpoch 3/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 465.6811 - mean_absolute_error: 19.5475\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 448.4161 - mean_absolute_error: 19.2588 - val_loss: 468.4966 - val_mean_absolute_error: 19.8920\nEpoch 4/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 401.9715 - mean_absolute_error: 18.0122\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 380.0186 - mean_absolute_error: 17.5507 - val_loss: 383.4724 - val_mean_absolute_error: 17.8128\nEpoch 5/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 321.7372 - mean_absolute_error: 15.9045\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 294.7874 - mean_absolute_error: 15.2162 - val_loss: 283.4386 - val_mean_absolute_error: 15.0061\nEpoch 6/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - loss: 228.0873 - mean_absolute_error: 13.1302\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 201.5058 - mean_absolute_error: 12.2035 - val_loss: 183.3969 - val_mean_absolute_error: 11.4563\nEpoch 7/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 136.1075 - mean_absolute_error: 9.8040\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 119.8846 - mean_absolute_error: 8.9541 - val_loss: 108.0732 - val_mean_absolute_error: 8.1505\nEpoch 8/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 68.4765 - mean_absolute_error: 6.7264\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 70.9415 - mean_absolute_error: 6.5963 - val_loss: 71.4844 - val_mean_absolute_error: 6.4290\nEpoch 9/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 36.9140 - mean_absolute_error: 5.2137\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 52.5765 - mean_absolute_error: 5.4308 - val_loss: 57.1857 - val_mean_absolute_error: 5.6866\nEpoch 10/10\n\n\u001b[1m 1/11\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 103ms/step - loss: 25.8527 - mean_absolute_error: 4.2993\n\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 43.6699 - mean_absolute_error: 4.7684 - val_loss: 47.6769 - val_mean_absolute_error: 5.1710\n```\n\n\n:::\n:::\n\n\n:::\n\n## Model Evaluation\n\n::: {.panel-tabset}\n\n### Python (PyTorch)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n_ = model_reg_pt.eval()\nwith torch.no_grad():\n    preds = model_reg_pt(test_x_t)\n    test_mae = torch.mean(torch.abs(preds - test_y_t)).item()\n    test_mse = criterion(preds, test_y_t).item()\nprint(f\"Test MSE: {test_mse:.4f}, Test MAE: {test_mae:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTest MSE: 39.6828, Test MAE: 4.8964\n```\n\n\n:::\n:::\n\n\n### Python (TensorFlow)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nnn_results = model_reg.evaluate(test_x, test_y)\nprint(f\"Test Loss (MSE): {nn_results[0]:.4f}, Test MAE: {nn_results[1]:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 41.2739 - mean_absolute_error: 4.8162\n\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 44.3121 - mean_absolute_error: 5.1213 \nTest Loss (MSE): 44.3121, Test MAE: 5.1213\n```\n\n\n:::\n:::\n\n\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnn_results <- model_reg |> evaluate(test_x, test_y)\nnn_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n4/4 - 0s - 5ms/step - loss: 44.3387 - mean_absolute_error: 5.1227\n$loss\n[1] 44.33867\n\n$mean_absolute_error\n[1] 5.122735\n```\n\n\n:::\n:::\n\n\n:::\n\nTo put these results in context, we can compare it with a simple linear regression in R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndf_tr <- data.frame(train_y, train_x)\nlm_mod <- lm(train_y ~ ., data = df_tr)\nlm_preds <- as.vector(predict(lm_mod, newdata=data.frame(test_x)))\n\ncaret::MAE(obs = test_y, pred = lm_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.464186\n```\n\n\n:::\n:::\n\n\nWe see that the neural network does significantly better than a regression model. We can also compare it with the trees seen in the CART series.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rpart)\nset.seed(123)\ntree_model <- rpart(train_y ~ ., data=df_tr)\ntree_preds <- predict(tree_model,newdata = data.frame(test_x))\n\ncaret::MAE(obs = test_y, pred = tree_preds)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.246767\n```\n\n\n:::\n:::\n\n\nThe neural network also outperforms CART (if you run NN for 300 epochs). This is due to multiple reasons, including a higher complexity of the neural network (more parameters), using a validation set, and so on. You will learn about ensemble methods (bagging and boosting) in the upcoming lectures. Ensemble methods are the true champions for structured data and usually outperform neural networks for structured/low-dimensional data.\n\n# Image Classification\n\n## CIFAR-10 Dataset\n\nThe [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) is a well-known dataset used for image classification tasks. It contains 50,000 training images and 10,000 testing images of size 32x32 pixels, each belonging to one of ten classes. We will use this dataset to demonstrate how to perform image classification.\n\n::: {.panel-tabset}\n\n### Python (PyTorch)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport torchvision\nimport torchvision.transforms as transforms\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\ntrain_loader_img = DataLoader(trainset, batch_size=64, shuffle=True)\ntest_loader_img = DataLoader(testset, batch_size=64, shuffle=False)\n\n# Display some sample images\nclasses = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\nfig, axes = plt.subplots(2, 5, figsize=(10, 4))\nfor i, ax in enumerate(axes.flat):\n    img = testset[i][0].permute(1, 2, 0) * 0.5 + 0.5  # unnormalize\n    ax.imshow(img);\n    ax.set_title(classes[testset[i][1]]);\n    ax.axis('off');\nplt.suptitle(\"Sample CIFAR-10 Images\");\nplt.tight_layout();\nplt.show()\n\n_ = torch.manual_seed(123)\n\nmodel_cnn_pt = nn.Sequential(\n    nn.Conv2d(3, 32, 3, padding=0), nn.ReLU(), nn.MaxPool2d(2),\n    nn.Conv2d(32, 64, 5, padding=0), nn.ReLU(), nn.MaxPool2d(3),\n    nn.Conv2d(64, 64, 3, padding=0), nn.ReLU(),\n    nn.Flatten(),\n    nn.Linear(64, 64), nn.ReLU(),\n    nn.Linear(64, 10)\n)\n\ncriterion_cnn = nn.CrossEntropyLoss()\noptimizer_cnn = optim.Adam(model_cnn_pt.parameters())\n\nfor epoch in range(10):\n    _ = model_cnn_pt.train()\n    running_loss, correct_train, total_train = 0, 0, 0\n    for images, labels in train_loader_img:\n        optimizer_cnn.zero_grad()\n        outputs = model_cnn_pt(images)\n        loss = criterion_cnn(outputs, labels)\n        loss.backward()\n        optimizer_cnn.step()\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted == labels).sum().item()\n    train_acc = correct_train / total_train\n    avg_loss = running_loss / len(train_loader_img)\n    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {train_acc:.4f}\")\n\n# Evaluate\ncorrect, total = 0, 0\n_ = model_cnn_pt.eval()\nwith torch.no_grad():\n    for images, labels in test_loader_img:\n        outputs = model_cnn_pt(images)\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\nprint(f\"Test Accuracy: {correct/total:.4f}\")\n\n# Show predictions on sample images\n_ = model_cnn_pt.eval()\nfig, axes = plt.subplots(2, 5, figsize=(10, 4))\nwith torch.no_grad():\n    for i, ax in enumerate(axes.flat):\n        img = testset[i][0].unsqueeze(0)\n        pred = model_cnn_pt(img).argmax(1).item()\n        true_label = testset[i][1]\n        img_show = testset[i][0].permute(1, 2, 0) * 0.5 + 0.5\n        ax.imshow(img_show);\n        color = 'green' if pred == true_label else 'red'\n        ax.set_title(f\"{classes[pred]}\", color=color);\n        ax.set_xlabel(f\"({classes[true_label]})\", fontsize=8);\n        ax.set_xticks([]); ax.set_yticks([]);\nplt.suptitle(\"Predictions (true label in parentheses)\");\nplt.tight_layout();\nplt.show()\n```\n\n::: {.cell-output-display}\n![](EX_ML_NN_files/figure-html/unnamed-chunk-22-1.png){fig-align='center' width=960}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch 1, Loss: 1.5595, Accuracy: 0.4207\nEpoch 2, Loss: 1.1944, Accuracy: 0.5734\nEpoch 3, Loss: 1.0373, Accuracy: 0.6344\nEpoch 4, Loss: 0.9373, Accuracy: 0.6696\nEpoch 5, Loss: 0.8591, Accuracy: 0.7001\nEpoch 6, Loss: 0.8064, Accuracy: 0.7193\nEpoch 7, Loss: 0.7582, Accuracy: 0.7370\nEpoch 8, Loss: 0.7143, Accuracy: 0.7522\nEpoch 9, Loss: 0.6763, Accuracy: 0.7628\nEpoch 10, Loss: 0.6430, Accuracy: 0.7754\nTest Accuracy: 0.7028\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](EX_ML_NN_files/figure-html/unnamed-chunk-22-2.png){fig-align='center' width=960}\n:::\n:::\n\n\n### Python (TensorFlow)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()\n\n# Normalize\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\n\nclasses = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n\n# Display some sample images\nfig, axes = plt.subplots(2, 5, figsize=(10, 4))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(test_images[i]);\n    ax.set_title(classes[test_labels[i][0]]);\n    ax.axis('off');\nplt.suptitle(\"Sample CIFAR-10 Images\");\nplt.tight_layout();\nplt.show()\n\n# One-hot encode labels\ntrain_labels_cat = keras.utils.to_categorical(train_labels, 10)\ntest_labels_cat = keras.utils.to_categorical(test_labels, 10)\n\nkeras.utils.set_random_seed(123)\n\nmodel_cnn = keras.Sequential([\n    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n    layers.MaxPooling2D((2, 2)),\n    layers.Conv2D(64, (5, 5), activation='relu'),\n    layers.MaxPooling2D((3, 3)),\n    layers.Conv2D(64, (3, 3), activation='relu'),\n    layers.Flatten(),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(10, activation='softmax')\n])\n\nmodel_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nmodel_cnn.fit(train_images, train_labels_cat, epochs=10, batch_size=64,\n              validation_data=(test_images, test_labels_cat))\nmodel_cnn.evaluate(test_images, test_labels_cat)\n\n# Show predictions on sample images\npredictions = model_cnn.predict(test_images[:10])\nfig, axes = plt.subplots(2, 5, figsize=(10, 4))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(test_images[i]);\n    pred = predictions[i].argmax()\n    true_label = test_labels[i][0]\n    color = 'green' if pred == true_label else 'red'\n    ax.set_title(f\"{classes[pred]}\", color=color);\n    ax.set_xlabel(f\"({classes[true_label]})\", fontsize=8);\n    ax.set_xticks([]); ax.set_yticks([]);\nplt.suptitle(\"Predictions (true label in parentheses)\");\nplt.tight_layout();\nplt.show()\n```\n\n::: {.cell-output-display}\n![](EX_ML_NN_files/figure-html/unnamed-chunk-23-5.png){fig-align='center' width=960}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch 1/10\n\n\u001b[1m  1/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:30\u001b[0m 577ms/step - accuracy: 0.0938 - loss: 2.3030\n\u001b[1m  9/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.1227 - loss: 2.3009    \n\u001b[1m 18/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.1238 - loss: 2.3001\n\u001b[1m 28/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.1281 - loss: 2.2977\n\u001b[1m 38/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.1305 - loss: 2.2935\n\u001b[1m 48/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.1329 - loss: 2.2872\n\u001b[1m 59/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1357 - loss: 2.2787\n\u001b[1m 70/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1393 - loss: 2.2682\n\u001b[1m 80/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1434 - loss: 2.2575\n\u001b[1m 90/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1480 - loss: 2.2461\n\u001b[1m100/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1527 - loss: 2.2342\n\u001b[1m110/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1575 - loss: 2.2220\n\u001b[1m121/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1626 - loss: 2.2089\n\u001b[1m131/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1672 - loss: 2.1971\n\u001b[1m141/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1716 - loss: 2.1858\n\u001b[1m152/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1764 - loss: 2.1737\n\u001b[1m162/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1806 - loss: 2.1632\n\u001b[1m173/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1851 - loss: 2.1518\n\u001b[1m184/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1895 - loss: 2.1409\n\u001b[1m195/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.1938 - loss: 2.1304\n\u001b[1m206/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1979 - loss: 2.1203\n\u001b[1m217/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2019 - loss: 2.1106\n\u001b[1m228/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2057 - loss: 2.1013\n\u001b[1m239/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2094 - loss: 2.0923\n\u001b[1m249/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2126 - loss: 2.0844\n\u001b[1m259/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2157 - loss: 2.0767\n\u001b[1m270/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2191 - loss: 2.0682\n\u001b[1m281/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2225 - loss: 2.0599\n\u001b[1m291/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2254 - loss: 2.0526\n\u001b[1m301/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2282 - loss: 2.0455\n\u001b[1m310/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2307 - loss: 2.0394\n\u001b[1m318/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2328 - loss: 2.0340\n\u001b[1m327/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2352 - loss: 2.0281\n\u001b[1m336/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2375 - loss: 2.0224\n\u001b[1m345/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2397 - loss: 2.0167\n\u001b[1m355/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2422 - loss: 2.0106\n\u001b[1m365/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2446 - loss: 2.0046\n\u001b[1m374/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2467 - loss: 1.9993\n\u001b[1m383/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2487 - loss: 1.9942\n\u001b[1m393/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.2509 - loss: 1.9886\n\u001b[1m402/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2529 - loss: 1.9837\n\u001b[1m411/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2548 - loss: 1.9788\n\u001b[1m421/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2569 - loss: 1.9735\n\u001b[1m432/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2592 - loss: 1.9679\n\u001b[1m442/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2612 - loss: 1.9628\n\u001b[1m453/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2633 - loss: 1.9574\n\u001b[1m463/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2652 - loss: 1.9526\n\u001b[1m474/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2673 - loss: 1.9474\n\u001b[1m485/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2693 - loss: 1.9424\n\u001b[1m495/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2711 - loss: 1.9379\n\u001b[1m506/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2731 - loss: 1.9330\n\u001b[1m516/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2748 - loss: 1.9287\n\u001b[1m526/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2765 - loss: 1.9245\n\u001b[1m536/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2782 - loss: 1.9203\n\u001b[1m547/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2800 - loss: 1.9158\n\u001b[1m558/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2818 - loss: 1.9114\n\u001b[1m569/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2835 - loss: 1.9071\n\u001b[1m580/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2852 - loss: 1.9028\n\u001b[1m590/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2868 - loss: 1.8991\n\u001b[1m600/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2883 - loss: 1.8954\n\u001b[1m611/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2899 - loss: 1.8914\n\u001b[1m622/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2915 - loss: 1.8875\n\u001b[1m634/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2932 - loss: 1.8833\n\u001b[1m645/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2947 - loss: 1.8796\n\u001b[1m656/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2962 - loss: 1.8759\n\u001b[1m667/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2977 - loss: 1.8723\n\u001b[1m679/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.2992 - loss: 1.8684\n\u001b[1m690/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3007 - loss: 1.8649\n\u001b[1m701/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3021 - loss: 1.8615\n\u001b[1m712/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3034 - loss: 1.8581\n\u001b[1m723/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3048 - loss: 1.8548\n\u001b[1m735/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3062 - loss: 1.8512\n\u001b[1m746/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3076 - loss: 1.8479\n\u001b[1m757/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3089 - loss: 1.8448\n\u001b[1m769/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3103 - loss: 1.8413\n\u001b[1m780/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3115 - loss: 1.8382\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.4010 - loss: 1.6185 - val_accuracy: 0.4926 - val_loss: 1.4559\nEpoch 2/10\n\n\u001b[1m  1/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.5312 - loss: 1.3719\n\u001b[1m 12/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5200 - loss: 1.3896  \n\u001b[1m 23/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5141 - loss: 1.3950\n\u001b[1m 34/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5102 - loss: 1.3936\n\u001b[1m 44/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5078 - loss: 1.3929\n\u001b[1m 55/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5055 - loss: 1.3935\n\u001b[1m 66/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5045 - loss: 1.3917\n\u001b[1m 78/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5037 - loss: 1.3901\n\u001b[1m 89/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5035 - loss: 1.3882\n\u001b[1m100/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5034 - loss: 1.3862\n\u001b[1m111/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5036 - loss: 1.3839\n\u001b[1m122/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5038 - loss: 1.3819\n\u001b[1m132/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5040 - loss: 1.3804\n\u001b[1m142/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5043 - loss: 1.3790\n\u001b[1m153/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5046 - loss: 1.3778\n\u001b[1m164/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5049 - loss: 1.3767\n\u001b[1m175/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5054 - loss: 1.3754\n\u001b[1m186/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5058 - loss: 1.3743\n\u001b[1m196/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5063 - loss: 1.3732\n\u001b[1m207/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5068 - loss: 1.3721\n\u001b[1m218/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5072 - loss: 1.3710\n\u001b[1m229/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5077 - loss: 1.3699\n\u001b[1m240/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5081 - loss: 1.3689\n\u001b[1m250/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5084 - loss: 1.3681\n\u001b[1m261/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5088 - loss: 1.3671\n\u001b[1m272/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5092 - loss: 1.3660\n\u001b[1m283/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5096 - loss: 1.3649\n\u001b[1m294/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5100 - loss: 1.3638\n\u001b[1m305/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5103 - loss: 1.3628\n\u001b[1m316/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5107 - loss: 1.3618\n\u001b[1m327/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5111 - loss: 1.3609\n\u001b[1m338/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5114 - loss: 1.3599\n\u001b[1m349/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5118 - loss: 1.3590\n\u001b[1m359/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5120 - loss: 1.3582\n\u001b[1m370/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5124 - loss: 1.3573\n\u001b[1m380/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5126 - loss: 1.3565\n\u001b[1m391/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5129 - loss: 1.3556\n\u001b[1m402/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5132 - loss: 1.3547\n\u001b[1m413/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5135 - loss: 1.3538\n\u001b[1m425/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5138 - loss: 1.3528\n\u001b[1m436/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5141 - loss: 1.3519\n\u001b[1m447/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5144 - loss: 1.3510\n\u001b[1m458/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5147 - loss: 1.3501\n\u001b[1m469/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5150 - loss: 1.3493\n\u001b[1m480/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5153 - loss: 1.3485\n\u001b[1m491/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5156 - loss: 1.3476\n\u001b[1m503/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5159 - loss: 1.3467\n\u001b[1m515/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5162 - loss: 1.3459\n\u001b[1m526/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5165 - loss: 1.3451\n\u001b[1m537/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5167 - loss: 1.3443\n\u001b[1m548/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5170 - loss: 1.3435\n\u001b[1m559/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5173 - loss: 1.3428\n\u001b[1m570/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5176 - loss: 1.3420\n\u001b[1m579/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5178 - loss: 1.3414\n\u001b[1m590/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5181 - loss: 1.3407\n\u001b[1m602/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5183 - loss: 1.3400\n\u001b[1m614/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5186 - loss: 1.3392\n\u001b[1m626/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5189 - loss: 1.3385\n\u001b[1m637/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5191 - loss: 1.3379\n\u001b[1m649/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5194 - loss: 1.3372\n\u001b[1m661/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5196 - loss: 1.3366\n\u001b[1m673/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5199 - loss: 1.3359\n\u001b[1m685/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5201 - loss: 1.3352\n\u001b[1m697/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5204 - loss: 1.3346\n\u001b[1m709/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5206 - loss: 1.3339\n\u001b[1m721/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5209 - loss: 1.3332\n\u001b[1m733/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5211 - loss: 1.3325\n\u001b[1m743/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5213 - loss: 1.3320\n\u001b[1m753/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5215 - loss: 1.3314\n\u001b[1m763/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5217 - loss: 1.3309\n\u001b[1m774/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5220 - loss: 1.3302\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5383 - loss: 1.2860 - val_accuracy: 0.5818 - val_loss: 1.1779\nEpoch 3/10\n\n\u001b[1m  1/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.6875 - loss: 1.0634\n\u001b[1m 13/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5878 - loss: 1.1797  \n\u001b[1m 25/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5812 - loss: 1.1939\n\u001b[1m 37/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5813 - loss: 1.1933\n\u001b[1m 48/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.5800 - loss: 1.1948\n\u001b[1m 59/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5792 - loss: 1.1957\n\u001b[1m 69/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5793 - loss: 1.1945\n\u001b[1m 79/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5791 - loss: 1.1941\n\u001b[1m 90/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5790 - loss: 1.1930\n\u001b[1m101/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5790 - loss: 1.1921\n\u001b[1m111/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5791 - loss: 1.1909\n\u001b[1m121/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5792 - loss: 1.1898\n\u001b[1m131/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5794 - loss: 1.1890\n\u001b[1m141/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5794 - loss: 1.1885\n\u001b[1m151/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5794 - loss: 1.1883\n\u001b[1m161/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.5793 - loss: 1.1882\n\u001b[1m171/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5793 - loss: 1.1879\n\u001b[1m181/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5793 - loss: 1.1877\n\u001b[1m191/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5793 - loss: 1.1876\n\u001b[1m201/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5793 - loss: 1.1873\n\u001b[1m211/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5794 - loss: 1.1871\n\u001b[1m221/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5794 - loss: 1.1869\n\u001b[1m232/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5795 - loss: 1.1866\n\u001b[1m243/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5795 - loss: 1.1864\n\u001b[1m254/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5795 - loss: 1.1861\n\u001b[1m263/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5796 - loss: 1.1858\n\u001b[1m273/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5797 - loss: 1.1853\n\u001b[1m284/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5798 - loss: 1.1847\n\u001b[1m294/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5800 - loss: 1.1842\n\u001b[1m304/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5801 - loss: 1.1837\n\u001b[1m314/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5803 - loss: 1.1833\n\u001b[1m324/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5804 - loss: 1.1828\n\u001b[1m334/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5805 - loss: 1.1824\n\u001b[1m344/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5806 - loss: 1.1821\n\u001b[1m354/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5807 - loss: 1.1817\n\u001b[1m364/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5808 - loss: 1.1813\n\u001b[1m374/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5809 - loss: 1.1809\n\u001b[1m385/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.5811 - loss: 1.1805\n\u001b[1m395/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5811 - loss: 1.1801\n\u001b[1m405/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5812 - loss: 1.1797\n\u001b[1m415/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5813 - loss: 1.1793\n\u001b[1m426/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5815 - loss: 1.1789\n\u001b[1m437/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5816 - loss: 1.1784\n\u001b[1m448/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5817 - loss: 1.1780\n\u001b[1m459/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5818 - loss: 1.1775\n\u001b[1m470/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5820 - loss: 1.1771\n\u001b[1m481/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5821 - loss: 1.1766\n\u001b[1m492/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5822 - loss: 1.1762\n\u001b[1m502/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5823 - loss: 1.1758\n\u001b[1m512/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5825 - loss: 1.1754\n\u001b[1m522/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5826 - loss: 1.1750\n\u001b[1m533/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5827 - loss: 1.1746\n\u001b[1m544/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5829 - loss: 1.1742\n\u001b[1m554/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5830 - loss: 1.1739\n\u001b[1m565/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5831 - loss: 1.1735\n\u001b[1m576/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5832 - loss: 1.1732\n\u001b[1m587/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5834 - loss: 1.1728\n\u001b[1m598/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5835 - loss: 1.1725\n\u001b[1m609/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5836 - loss: 1.1722\n\u001b[1m619/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5837 - loss: 1.1719\n\u001b[1m629/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5838 - loss: 1.1717\n\u001b[1m639/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5839 - loss: 1.1714\n\u001b[1m649/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5840 - loss: 1.1712\n\u001b[1m659/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5841 - loss: 1.1710\n\u001b[1m669/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5842 - loss: 1.1707\n\u001b[1m679/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5843 - loss: 1.1705\n\u001b[1m690/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5844 - loss: 1.1702\n\u001b[1m701/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5845 - loss: 1.1699\n\u001b[1m712/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5846 - loss: 1.1696\n\u001b[1m723/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5847 - loss: 1.1693\n\u001b[1m733/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5848 - loss: 1.1690\n\u001b[1m744/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5849 - loss: 1.1687\n\u001b[1m754/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5850 - loss: 1.1684\n\u001b[1m764/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5851 - loss: 1.1681\n\u001b[1m775/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5852 - loss: 1.1678\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.5935 - loss: 1.1456 - val_accuracy: 0.6075 - val_loss: 1.1105\nEpoch 4/10\n\n\u001b[1m  1/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 27ms/step - accuracy: 0.7031 - loss: 0.9245\n\u001b[1m 11/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.6302 - loss: 1.0697  \n\u001b[1m 22/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6256 - loss: 1.0885\n\u001b[1m 33/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6258 - loss: 1.0867\n\u001b[1m 44/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6247 - loss: 1.0876\n\u001b[1m 55/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6225 - loss: 1.0906\n\u001b[1m 66/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6213 - loss: 1.0907\n\u001b[1m 77/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6206 - loss: 1.0910\n\u001b[1m 88/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6201 - loss: 1.0906\n\u001b[1m 99/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6197 - loss: 1.0902\n\u001b[1m110/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6195 - loss: 1.0892\n\u001b[1m121/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6192 - loss: 1.0884\n\u001b[1m132/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6188 - loss: 1.0879\n\u001b[1m143/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6185 - loss: 1.0878\n\u001b[1m154/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6181 - loss: 1.0878\n\u001b[1m165/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6178 - loss: 1.0878\n\u001b[1m176/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6177 - loss: 1.0876\n\u001b[1m187/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6175 - loss: 1.0874\n\u001b[1m198/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6175 - loss: 1.0872\n\u001b[1m209/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6176 - loss: 1.0869\n\u001b[1m219/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6176 - loss: 1.0867\n\u001b[1m229/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6177 - loss: 1.0864\n\u001b[1m239/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6177 - loss: 1.0862\n\u001b[1m250/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6178 - loss: 1.0860\n\u001b[1m260/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6178 - loss: 1.0857\n\u001b[1m271/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6179 - loss: 1.0853\n\u001b[1m282/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6181 - loss: 1.0848\n\u001b[1m293/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6182 - loss: 1.0843\n\u001b[1m304/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6184 - loss: 1.0839\n\u001b[1m315/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6185 - loss: 1.0835\n\u001b[1m325/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6186 - loss: 1.0831\n\u001b[1m335/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6187 - loss: 1.0828\n\u001b[1m345/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6188 - loss: 1.0825\n\u001b[1m355/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6189 - loss: 1.0822\n\u001b[1m366/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6190 - loss: 1.0819\n\u001b[1m376/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6190 - loss: 1.0817\n\u001b[1m387/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6191 - loss: 1.0813\n\u001b[1m398/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6192 - loss: 1.0810\n\u001b[1m409/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6193 - loss: 1.0806\n\u001b[1m420/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6195 - loss: 1.0802\n\u001b[1m431/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6196 - loss: 1.0799\n\u001b[1m442/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6197 - loss: 1.0796\n\u001b[1m452/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6197 - loss: 1.0793\n\u001b[1m462/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6198 - loss: 1.0790\n\u001b[1m473/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6199 - loss: 1.0787\n\u001b[1m484/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6200 - loss: 1.0784\n\u001b[1m495/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6200 - loss: 1.0781\n\u001b[1m506/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6201 - loss: 1.0778\n\u001b[1m517/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6202 - loss: 1.0775\n\u001b[1m528/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6202 - loss: 1.0773\n\u001b[1m538/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6203 - loss: 1.0771\n\u001b[1m549/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6203 - loss: 1.0768\n\u001b[1m560/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6204 - loss: 1.0766\n\u001b[1m571/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6205 - loss: 1.0764\n\u001b[1m582/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6205 - loss: 1.0762\n\u001b[1m592/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6206 - loss: 1.0760\n\u001b[1m603/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6207 - loss: 1.0758\n\u001b[1m613/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6207 - loss: 1.0757\n\u001b[1m623/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6208 - loss: 1.0755\n\u001b[1m633/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6209 - loss: 1.0754\n\u001b[1m643/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6209 - loss: 1.0752\n\u001b[1m654/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6210 - loss: 1.0751\n\u001b[1m664/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6210 - loss: 1.0750\n\u001b[1m674/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6211 - loss: 1.0748\n\u001b[1m685/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6211 - loss: 1.0747\n\u001b[1m696/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6212 - loss: 1.0745\n\u001b[1m706/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6212 - loss: 1.0743\n\u001b[1m717/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6213 - loss: 1.0742\n\u001b[1m728/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6214 - loss: 1.0740\n\u001b[1m738/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6214 - loss: 1.0738\n\u001b[1m749/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6215 - loss: 1.0736\n\u001b[1m760/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6215 - loss: 1.0734\n\u001b[1m771/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6216 - loss: 1.0732\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6217 - loss: 1.0730\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.6257 - loss: 1.0598 - val_accuracy: 0.6350 - val_loss: 1.0409\nEpoch 5/10\n\n\u001b[1m  1/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 30ms/step - accuracy: 0.7500 - loss: 0.8375\n\u001b[1m 12/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6696 - loss: 0.9849  \n\u001b[1m 22/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6590 - loss: 1.0037\n\u001b[1m 32/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6583 - loss: 1.0061\n\u001b[1m 42/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6556 - loss: 1.0101\n\u001b[1m 52/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6525 - loss: 1.0150\n\u001b[1m 62/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6502 - loss: 1.0175\n\u001b[1m 72/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6491 - loss: 1.0182\n\u001b[1m 82/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6480 - loss: 1.0185\n\u001b[1m 93/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6472 - loss: 1.0183\n\u001b[1m104/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6464 - loss: 1.0178\n\u001b[1m115/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6457 - loss: 1.0171\n\u001b[1m126/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6451 - loss: 1.0166\n\u001b[1m137/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6444 - loss: 1.0166\n\u001b[1m147/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6439 - loss: 1.0167\n\u001b[1m158/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6434 - loss: 1.0170\n\u001b[1m169/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6430 - loss: 1.0171\n\u001b[1m179/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6428 - loss: 1.0171\n\u001b[1m190/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6426 - loss: 1.0170\n\u001b[1m201/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6425 - loss: 1.0168\n\u001b[1m212/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6425 - loss: 1.0167\n\u001b[1m222/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6425 - loss: 1.0164\n\u001b[1m232/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6424 - loss: 1.0163\n\u001b[1m242/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6424 - loss: 1.0161\n\u001b[1m251/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6424 - loss: 1.0160\n\u001b[1m261/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6424 - loss: 1.0158\n\u001b[1m271/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6424 - loss: 1.0154\n\u001b[1m281/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6425 - loss: 1.0150\n\u001b[1m292/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6426 - loss: 1.0146\n\u001b[1m303/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6427 - loss: 1.0141\n\u001b[1m314/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6428 - loss: 1.0137\n\u001b[1m325/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6429 - loss: 1.0134\n\u001b[1m336/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6430 - loss: 1.0130\n\u001b[1m346/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6431 - loss: 1.0128\n\u001b[1m356/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6432 - loss: 1.0125\n\u001b[1m367/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6433 - loss: 1.0122\n\u001b[1m377/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6434 - loss: 1.0119\n\u001b[1m387/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6435 - loss: 1.0116\n\u001b[1m398/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6436 - loss: 1.0113\n\u001b[1m409/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6437 - loss: 1.0109\n\u001b[1m420/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6438 - loss: 1.0105\n\u001b[1m431/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6439 - loss: 1.0102\n\u001b[1m442/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6440 - loss: 1.0099\n\u001b[1m452/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6441 - loss: 1.0096\n\u001b[1m462/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6441 - loss: 1.0094\n\u001b[1m472/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6442 - loss: 1.0092\n\u001b[1m482/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6443 - loss: 1.0089\n\u001b[1m492/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6443 - loss: 1.0087\n\u001b[1m502/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6444 - loss: 1.0085\n\u001b[1m512/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6444 - loss: 1.0083\n\u001b[1m522/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6445 - loss: 1.0081\n\u001b[1m532/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6445 - loss: 1.0080\n\u001b[1m542/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6446 - loss: 1.0078\n\u001b[1m552/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6446 - loss: 1.0077\n\u001b[1m562/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6447 - loss: 1.0075\n\u001b[1m572/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6447 - loss: 1.0074\n\u001b[1m582/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6448 - loss: 1.0072\n\u001b[1m592/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6448 - loss: 1.0071\n\u001b[1m602/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6449 - loss: 1.0070\n\u001b[1m613/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6450 - loss: 1.0069\n\u001b[1m624/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6450 - loss: 1.0067\n\u001b[1m635/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6451 - loss: 1.0066\n\u001b[1m646/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6451 - loss: 1.0065\n\u001b[1m657/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6452 - loss: 1.0064\n\u001b[1m667/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6452 - loss: 1.0064\n\u001b[1m678/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6453 - loss: 1.0062\n\u001b[1m689/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6453 - loss: 1.0061\n\u001b[1m699/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6454 - loss: 1.0060\n\u001b[1m710/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6455 - loss: 1.0059\n\u001b[1m721/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6455 - loss: 1.0057\n\u001b[1m732/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6456 - loss: 1.0056\n\u001b[1m743/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6456 - loss: 1.0055\n\u001b[1m754/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6457 - loss: 1.0053\n\u001b[1m764/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6457 - loss: 1.0052\n\u001b[1m775/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6458 - loss: 1.0050\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.6495 - loss: 0.9953 - val_accuracy: 0.6442 - val_loss: 1.0054\nEpoch 6/10\n\n\u001b[1m  1/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 25ms/step - accuracy: 0.7812 - loss: 0.7716\n\u001b[1m 11/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.6947 - loss: 0.9303  \n\u001b[1m 21/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6795 - loss: 0.9520\n\u001b[1m 32/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6749 - loss: 0.9552\n\u001b[1m 43/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6714 - loss: 0.9590\n\u001b[1m 54/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6686 - loss: 0.9636\n\u001b[1m 65/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6674 - loss: 0.9648\n\u001b[1m 76/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6666 - loss: 0.9652\n\u001b[1m 87/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6658 - loss: 0.9650\n\u001b[1m 98/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6654 - loss: 0.9645\n\u001b[1m108/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6652 - loss: 0.9637\n\u001b[1m119/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6650 - loss: 0.9631\n\u001b[1m129/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6646 - loss: 0.9629\n\u001b[1m140/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6642 - loss: 0.9631\n\u001b[1m150/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6639 - loss: 0.9635\n\u001b[1m161/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6635 - loss: 0.9641\n\u001b[1m171/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6634 - loss: 0.9643\n\u001b[1m182/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6632 - loss: 0.9644\n\u001b[1m193/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6631 - loss: 0.9644\n\u001b[1m204/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6631 - loss: 0.9643\n\u001b[1m215/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6631 - loss: 0.9642\n\u001b[1m226/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6631 - loss: 0.9639\n\u001b[1m237/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6630 - loss: 0.9637\n\u001b[1m246/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6630 - loss: 0.9636\n\u001b[1m257/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6630 - loss: 0.9634\n\u001b[1m268/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6631 - loss: 0.9631\n\u001b[1m279/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6632 - loss: 0.9626\n\u001b[1m289/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6632 - loss: 0.9622\n\u001b[1m299/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6633 - loss: 0.9618\n\u001b[1m310/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6634 - loss: 0.9615\n\u001b[1m320/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6635 - loss: 0.9611\n\u001b[1m331/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6636 - loss: 0.9608\n\u001b[1m342/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6637 - loss: 0.9605\n\u001b[1m353/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6638 - loss: 0.9602\n\u001b[1m364/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6638 - loss: 0.9599\n\u001b[1m375/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6639 - loss: 0.9596\n\u001b[1m386/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6641 - loss: 0.9593\n\u001b[1m397/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6642 - loss: 0.9589\n\u001b[1m408/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6643 - loss: 0.9586\n\u001b[1m419/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6644 - loss: 0.9582\n\u001b[1m430/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6645 - loss: 0.9579\n\u001b[1m442/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6645 - loss: 0.9576\n\u001b[1m453/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6646 - loss: 0.9573\n\u001b[1m464/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6647 - loss: 0.9571\n\u001b[1m475/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6647 - loss: 0.9568\n\u001b[1m486/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6647 - loss: 0.9566\n\u001b[1m497/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6648 - loss: 0.9564\n\u001b[1m507/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6648 - loss: 0.9562\n\u001b[1m518/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6649 - loss: 0.9560\n\u001b[1m529/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6649 - loss: 0.9558\n\u001b[1m539/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6649 - loss: 0.9556\n\u001b[1m550/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6650 - loss: 0.9554\n\u001b[1m561/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6650 - loss: 0.9553\n\u001b[1m572/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6650 - loss: 0.9551\n\u001b[1m583/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6651 - loss: 0.9550\n\u001b[1m594/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6651 - loss: 0.9548\n\u001b[1m605/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6652 - loss: 0.9547\n\u001b[1m615/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6652 - loss: 0.9546\n\u001b[1m626/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6652 - loss: 0.9545\n\u001b[1m637/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6653 - loss: 0.9544\n\u001b[1m646/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6653 - loss: 0.9543\n\u001b[1m656/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6654 - loss: 0.9542\n\u001b[1m666/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6654 - loss: 0.9541\n\u001b[1m676/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6654 - loss: 0.9540\n\u001b[1m686/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6655 - loss: 0.9539\n\u001b[1m696/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6655 - loss: 0.9538\n\u001b[1m706/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6656 - loss: 0.9536\n\u001b[1m716/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6656 - loss: 0.9535\n\u001b[1m726/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6657 - loss: 0.9534\n\u001b[1m736/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6657 - loss: 0.9533\n\u001b[1m746/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6657 - loss: 0.9532\n\u001b[1m757/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6658 - loss: 0.9530\n\u001b[1m768/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6658 - loss: 0.9529\n\u001b[1m778/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6658 - loss: 0.9527\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.6684 - loss: 0.9439 - val_accuracy: 0.6434 - val_loss: 1.0087\nEpoch 7/10\n\n\u001b[1m  1/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 29ms/step - accuracy: 0.7812 - loss: 0.7516\n\u001b[1m 12/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7193 - loss: 0.9194  \n\u001b[1m 22/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7086 - loss: 0.9291\n\u001b[1m 32/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7044 - loss: 0.9277\n\u001b[1m 42/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7001 - loss: 0.9279\n\u001b[1m 53/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6963 - loss: 0.9297\n\u001b[1m 64/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6939 - loss: 0.9292\n\u001b[1m 75/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6923 - loss: 0.9279\n\u001b[1m 86/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6907 - loss: 0.9267\n\u001b[1m 97/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6896 - loss: 0.9253\n\u001b[1m109/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6886 - loss: 0.9236\n\u001b[1m121/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6877 - loss: 0.9224\n\u001b[1m132/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6870 - loss: 0.9218\n\u001b[1m144/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6862 - loss: 0.9216\n\u001b[1m156/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6855 - loss: 0.9219\n\u001b[1m168/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6850 - loss: 0.9219\n\u001b[1m180/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6846 - loss: 0.9218\n\u001b[1m192/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6842 - loss: 0.9217\n\u001b[1m204/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6841 - loss: 0.9214\n\u001b[1m216/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6840 - loss: 0.9210\n\u001b[1m227/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6839 - loss: 0.9205\n\u001b[1m239/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6838 - loss: 0.9201\n\u001b[1m251/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6836 - loss: 0.9198\n\u001b[1m263/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6836 - loss: 0.9194\n\u001b[1m275/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6836 - loss: 0.9188\n\u001b[1m287/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6836 - loss: 0.9182\n\u001b[1m299/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6836 - loss: 0.9176\n\u001b[1m310/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6836 - loss: 0.9172\n\u001b[1m321/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6836 - loss: 0.9167\n\u001b[1m333/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6837 - loss: 0.9162\n\u001b[1m344/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6837 - loss: 0.9158\n\u001b[1m355/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6838 - loss: 0.9154\n\u001b[1m366/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6838 - loss: 0.9151\n\u001b[1m377/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6839 - loss: 0.9147\n\u001b[1m387/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6840 - loss: 0.9143\n\u001b[1m398/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6841 - loss: 0.9139\n\u001b[1m409/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6842 - loss: 0.9135\n\u001b[1m420/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6843 - loss: 0.9130\n\u001b[1m432/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6843 - loss: 0.9126\n\u001b[1m443/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6844 - loss: 0.9122\n\u001b[1m454/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6844 - loss: 0.9119\n\u001b[1m466/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6845 - loss: 0.9116\n\u001b[1m478/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6845 - loss: 0.9113\n\u001b[1m490/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6845 - loss: 0.9110\n\u001b[1m502/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6846 - loss: 0.9107\n\u001b[1m514/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6846 - loss: 0.9104\n\u001b[1m526/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6846 - loss: 0.9101\n\u001b[1m538/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6847 - loss: 0.9098\n\u001b[1m549/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6847 - loss: 0.9096\n\u001b[1m561/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6847 - loss: 0.9094\n\u001b[1m573/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6848 - loss: 0.9092\n\u001b[1m585/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6848 - loss: 0.9089\n\u001b[1m597/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6848 - loss: 0.9087\n\u001b[1m608/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6849 - loss: 0.9086\n\u001b[1m620/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6849 - loss: 0.9084\n\u001b[1m631/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6849 - loss: 0.9083\n\u001b[1m643/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6849 - loss: 0.9082\n\u001b[1m654/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6850 - loss: 0.9080\n\u001b[1m666/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6850 - loss: 0.9079\n\u001b[1m677/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6850 - loss: 0.9078\n\u001b[1m689/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6850 - loss: 0.9076\n\u001b[1m701/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6851 - loss: 0.9074\n\u001b[1m712/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6851 - loss: 0.9073\n\u001b[1m723/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6851 - loss: 0.9071\n\u001b[1m735/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6851 - loss: 0.9070\n\u001b[1m747/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6852 - loss: 0.9068\n\u001b[1m759/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6852 - loss: 0.9066\n\u001b[1m771/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6852 - loss: 0.9065\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6852 - loss: 0.9063\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.6865 - loss: 0.8963 - val_accuracy: 0.6585 - val_loss: 0.9752\nEpoch 8/10\n\n\u001b[1m  1/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.7969 - loss: 0.7215\n\u001b[1m 12/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7366 - loss: 0.8730  \n\u001b[1m 23/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7263 - loss: 0.8797\n\u001b[1m 34/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7205 - loss: 0.8785\n\u001b[1m 44/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7151 - loss: 0.8801\n\u001b[1m 55/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7109 - loss: 0.8824\n\u001b[1m 67/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7085 - loss: 0.8816\n\u001b[1m 79/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7067 - loss: 0.8808\n\u001b[1m 91/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7056 - loss: 0.8793\n\u001b[1m103/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7049 - loss: 0.8779\n\u001b[1m115/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7043 - loss: 0.8766\n\u001b[1m127/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7036 - loss: 0.8757\n\u001b[1m139/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7029 - loss: 0.8754\n\u001b[1m150/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7023 - loss: 0.8754\n\u001b[1m162/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7017 - loss: 0.8756\n\u001b[1m172/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7013 - loss: 0.8754\n\u001b[1m184/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7009 - loss: 0.8753\n\u001b[1m196/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7006 - loss: 0.8751\n\u001b[1m208/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7004 - loss: 0.8747\n\u001b[1m220/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7003 - loss: 0.8742\n\u001b[1m232/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7002 - loss: 0.8737\n\u001b[1m244/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7000 - loss: 0.8733\n\u001b[1m256/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7000 - loss: 0.8729\n\u001b[1m268/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6999 - loss: 0.8724\n\u001b[1m280/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6999 - loss: 0.8717\n\u001b[1m291/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6999 - loss: 0.8712\n\u001b[1m302/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6999 - loss: 0.8708\n\u001b[1m313/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6999 - loss: 0.8704\n\u001b[1m324/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7000 - loss: 0.8700\n\u001b[1m336/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7000 - loss: 0.8695\n\u001b[1m348/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7001 - loss: 0.8691\n\u001b[1m360/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7001 - loss: 0.8687\n\u001b[1m372/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7002 - loss: 0.8683\n\u001b[1m384/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7003 - loss: 0.8679\n\u001b[1m396/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7004 - loss: 0.8675\n\u001b[1m407/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7005 - loss: 0.8671\n\u001b[1m419/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7006 - loss: 0.8666\n\u001b[1m431/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7007 - loss: 0.8662\n\u001b[1m443/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7007 - loss: 0.8658\n\u001b[1m454/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7008 - loss: 0.8655\n\u001b[1m466/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7008 - loss: 0.8652\n\u001b[1m477/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7009 - loss: 0.8649\n\u001b[1m488/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7009 - loss: 0.8647\n\u001b[1m500/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7010 - loss: 0.8644\n\u001b[1m511/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7010 - loss: 0.8641\n\u001b[1m522/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7011 - loss: 0.8639\n\u001b[1m534/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7011 - loss: 0.8637\n\u001b[1m546/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7012 - loss: 0.8634\n\u001b[1m557/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7012 - loss: 0.8632\n\u001b[1m569/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7012 - loss: 0.8630\n\u001b[1m580/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7013 - loss: 0.8628\n\u001b[1m590/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7013 - loss: 0.8627\n\u001b[1m601/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7013 - loss: 0.8625\n\u001b[1m612/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7014 - loss: 0.8624\n\u001b[1m623/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7014 - loss: 0.8623\n\u001b[1m634/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7014 - loss: 0.8622\n\u001b[1m646/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7014 - loss: 0.8621\n\u001b[1m657/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7014 - loss: 0.8620\n\u001b[1m669/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7014 - loss: 0.8619\n\u001b[1m681/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7014 - loss: 0.8618\n\u001b[1m692/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7014 - loss: 0.8616\n\u001b[1m704/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7015 - loss: 0.8615\n\u001b[1m717/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7015 - loss: 0.8614\n\u001b[1m729/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7015 - loss: 0.8612\n\u001b[1m741/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7015 - loss: 0.8611\n\u001b[1m753/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7015 - loss: 0.8609\n\u001b[1m765/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7015 - loss: 0.8608\n\u001b[1m777/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7016 - loss: 0.8606\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7023 - loss: 0.8522 - val_accuracy: 0.6647 - val_loss: 0.9679\nEpoch 9/10\n\n\u001b[1m  1/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 24ms/step - accuracy: 0.7812 - loss: 0.7284\n\u001b[1m 12/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7364 - loss: 0.8569  \n\u001b[1m 24/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7297 - loss: 0.8568\n\u001b[1m 35/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7260 - loss: 0.8527\n\u001b[1m 47/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7212 - loss: 0.8536\n\u001b[1m 59/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7179 - loss: 0.8541\n\u001b[1m 71/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7165 - loss: 0.8524\n\u001b[1m 83/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7154 - loss: 0.8510\n\u001b[1m 94/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7150 - loss: 0.8493\n\u001b[1m105/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7147 - loss: 0.8478\n\u001b[1m115/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7143 - loss: 0.8467\n\u001b[1m126/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7138 - loss: 0.8459\n\u001b[1m138/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7133 - loss: 0.8454\n\u001b[1m149/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7129 - loss: 0.8453\n\u001b[1m161/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7124 - loss: 0.8455\n\u001b[1m173/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7121 - loss: 0.8454\n\u001b[1m185/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7118 - loss: 0.8452\n\u001b[1m197/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7116 - loss: 0.8449\n\u001b[1m208/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7114 - loss: 0.8445\n\u001b[1m218/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7113 - loss: 0.8440\n\u001b[1m229/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7112 - loss: 0.8435\n\u001b[1m241/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7111 - loss: 0.8429\n\u001b[1m253/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7110 - loss: 0.8425\n\u001b[1m265/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7109 - loss: 0.8419\n\u001b[1m276/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7109 - loss: 0.8413\n\u001b[1m288/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7109 - loss: 0.8407\n\u001b[1m300/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7109 - loss: 0.8401\n\u001b[1m312/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7108 - loss: 0.8397\n\u001b[1m324/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7109 - loss: 0.8392\n\u001b[1m336/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7109 - loss: 0.8387\n\u001b[1m348/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7110 - loss: 0.8383\n\u001b[1m359/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7110 - loss: 0.8379\n\u001b[1m370/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7111 - loss: 0.8375\n\u001b[1m382/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7112 - loss: 0.8370\n\u001b[1m394/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7113 - loss: 0.8365\n\u001b[1m406/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7114 - loss: 0.8360\n\u001b[1m418/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7115 - loss: 0.8355\n\u001b[1m430/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7116 - loss: 0.8350\n\u001b[1m442/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7117 - loss: 0.8346\n\u001b[1m454/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7118 - loss: 0.8341\n\u001b[1m466/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7118 - loss: 0.8338\n\u001b[1m478/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7119 - loss: 0.8334\n\u001b[1m490/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7120 - loss: 0.8330\n\u001b[1m502/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7121 - loss: 0.8327\n\u001b[1m513/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7122 - loss: 0.8323\n\u001b[1m524/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7122 - loss: 0.8320\n\u001b[1m535/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7123 - loss: 0.8317\n\u001b[1m547/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7124 - loss: 0.8314\n\u001b[1m558/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7124 - loss: 0.8311\n\u001b[1m570/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7125 - loss: 0.8308\n\u001b[1m581/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7126 - loss: 0.8306\n\u001b[1m593/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7126 - loss: 0.8303\n\u001b[1m604/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7127 - loss: 0.8301\n\u001b[1m615/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7127 - loss: 0.8299\n\u001b[1m625/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7128 - loss: 0.8298\n\u001b[1m636/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7128 - loss: 0.8296\n\u001b[1m648/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7128 - loss: 0.8295\n\u001b[1m659/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7129 - loss: 0.8293\n\u001b[1m670/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7129 - loss: 0.8292\n\u001b[1m680/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7129 - loss: 0.8290\n\u001b[1m691/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7130 - loss: 0.8288\n\u001b[1m702/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7130 - loss: 0.8287\n\u001b[1m714/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7131 - loss: 0.8285\n\u001b[1m725/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7131 - loss: 0.8283\n\u001b[1m737/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7131 - loss: 0.8281\n\u001b[1m748/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7132 - loss: 0.8279\n\u001b[1m760/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7132 - loss: 0.8277\n\u001b[1m772/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7132 - loss: 0.8275\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7156 - loss: 0.8162 - val_accuracy: 0.6671 - val_loss: 0.9647\nEpoch 10/10\n\n\u001b[1m  1/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 26ms/step - accuracy: 0.7812 - loss: 0.7365\n\u001b[1m 12/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7312 - loss: 0.8416  \n\u001b[1m 23/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7272 - loss: 0.8343\n\u001b[1m 34/782\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7257 - loss: 0.8278\n\u001b[1m 45/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7235 - loss: 0.8267\n\u001b[1m 56/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7218 - loss: 0.8275\n\u001b[1m 67/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7212 - loss: 0.8258\n\u001b[1m 78/782\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7213 - loss: 0.8244\n\u001b[1m 89/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7217 - loss: 0.8225\n\u001b[1m100/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7218 - loss: 0.8206\n\u001b[1m111/782\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7219 - loss: 0.8189\n\u001b[1m122/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7218 - loss: 0.8176\n\u001b[1m133/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7217 - loss: 0.8167\n\u001b[1m145/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7216 - loss: 0.8160\n\u001b[1m156/782\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7215 - loss: 0.8157\n\u001b[1m168/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7214 - loss: 0.8152\n\u001b[1m180/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7214 - loss: 0.8147\n\u001b[1m192/782\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7214 - loss: 0.8142\n\u001b[1m204/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7214 - loss: 0.8136\n\u001b[1m215/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7215 - loss: 0.8130\n\u001b[1m226/782\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7215 - loss: 0.8123\n\u001b[1m237/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7216 - loss: 0.8116\n\u001b[1m248/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7215 - loss: 0.8111\n\u001b[1m258/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7216 - loss: 0.8106\n\u001b[1m266/782\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7216 - loss: 0.8101\n\u001b[1m276/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7217 - loss: 0.8095\n\u001b[1m286/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7217 - loss: 0.8089\n\u001b[1m295/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7218 - loss: 0.8085\n\u001b[1m305/782\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7218 - loss: 0.8081\n\u001b[1m315/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7219 - loss: 0.8077\n\u001b[1m324/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7219 - loss: 0.8073\n\u001b[1m334/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7220 - loss: 0.8068\n\u001b[1m344/782\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7221 - loss: 0.8064\n\u001b[1m355/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7222 - loss: 0.8059\n\u001b[1m366/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7224 - loss: 0.8054\n\u001b[1m377/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7225 - loss: 0.8049\n\u001b[1m387/782\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7226 - loss: 0.8045\n\u001b[1m398/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7228 - loss: 0.8039\n\u001b[1m409/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7229 - loss: 0.8034\n\u001b[1m420/782\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7230 - loss: 0.8029\n\u001b[1m431/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7232 - loss: 0.8024\n\u001b[1m441/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7233 - loss: 0.8020\n\u001b[1m451/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7234 - loss: 0.8016\n\u001b[1m461/782\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7234 - loss: 0.8012\n\u001b[1m472/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7235 - loss: 0.8008\n\u001b[1m483/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7236 - loss: 0.8005\n\u001b[1m493/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7237 - loss: 0.8001\n\u001b[1m504/782\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7238 - loss: 0.7997\n\u001b[1m515/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7239 - loss: 0.7994\n\u001b[1m525/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7240 - loss: 0.7990\n\u001b[1m535/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7241 - loss: 0.7987\n\u001b[1m546/782\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7242 - loss: 0.7984\n\u001b[1m557/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7242 - loss: 0.7982\n\u001b[1m568/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7243 - loss: 0.7979\n\u001b[1m578/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7244 - loss: 0.7976\n\u001b[1m588/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7244 - loss: 0.7974\n\u001b[1m598/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7245 - loss: 0.7972\n\u001b[1m609/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7245 - loss: 0.7970\n\u001b[1m620/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7246 - loss: 0.7968\n\u001b[1m631/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7246 - loss: 0.7966\n\u001b[1m642/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7247 - loss: 0.7965\n\u001b[1m652/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7247 - loss: 0.7963\n\u001b[1m662/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7247 - loss: 0.7962\n\u001b[1m672/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7247 - loss: 0.7960\n\u001b[1m683/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7248 - loss: 0.7959\n\u001b[1m694/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7248 - loss: 0.7957\n\u001b[1m704/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7249 - loss: 0.7955\n\u001b[1m715/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7249 - loss: 0.7953\n\u001b[1m726/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7250 - loss: 0.7951\n\u001b[1m737/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7250 - loss: 0.7950\n\u001b[1m748/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7250 - loss: 0.7948\n\u001b[1m759/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7251 - loss: 0.7946\n\u001b[1m769/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7251 - loss: 0.7944\n\u001b[1m778/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7251 - loss: 0.7943\n\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7272 - loss: 0.7830 - val_accuracy: 0.6678 - val_loss: 0.9579\n<keras.src.callbacks.history.History object at 0x000001E89C351330>\n\n\u001b[1m  1/313\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.6875 - loss: 0.6795\n\u001b[1m 27/313\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6820 - loss: 0.9369 \n\u001b[1m 54/313\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6808 - loss: 0.9396\n\u001b[1m 82/313\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6786 - loss: 0.9433\n\u001b[1m106/313\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6771 - loss: 0.9455\n\u001b[1m134/313\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6767 - loss: 0.9468\n\u001b[1m159/313\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6769 - loss: 0.9467\n\u001b[1m184/313\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6770 - loss: 0.9464\n\u001b[1m213/313\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6765 - loss: 0.9465\n\u001b[1m242/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6760 - loss: 0.9469\n\u001b[1m269/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6754 - loss: 0.9478\n\u001b[1m297/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6748 - loss: 0.9487\n\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6678 - loss: 0.9579\n[0.9579431414604187, 0.6678000092506409]\n\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](EX_ML_NN_files/figure-html/unnamed-chunk-23-6.png){fig-align='center' width=960}\n:::\n:::\n\n\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncifar10 <- dataset_cifar10()\n\ntrain_images <- cifar10$train$x / 255\ntest_images <- cifar10$test$x / 255\ntrain_labels_raw <- cifar10$train$y\ntest_labels_raw <- cifar10$test$y\ntrain_labels <- to_categorical(train_labels_raw, 10)\ntest_labels <- to_categorical(test_labels_raw, 10)\n\nclasses <- c(\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n\n# Display some sample images\npar(mfrow = c(2, 5), mar = c(1, 0.5, 2, 0.5))\nfor (i in 1:10) {\n  img <- test_images[i, , , ]\n  img <- aperm(img, c(2, 1, 3))  # transpose for correct orientation\n  plot(as.raster(img))\n  title(main = classes[test_labels_raw[i] + 1], cex.main = 1)\n}\n```\n\n::: {.cell-output-display}\n![](EX_ML_NN_files/figure-html/unnamed-chunk-24-9.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\nset_random_seed(123)\n\nmodel_cnn <- keras_model_sequential(input_shape = c(32, 32, 3)) |>\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = \"relu\") |>\n  layer_max_pooling_2d(pool_size = c(2, 2)) |>\n  layer_conv_2d(filters = 64, kernel_size = c(5, 5), activation = \"relu\") |>\n  layer_max_pooling_2d(pool_size = c(3, 3)) |>\n  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = \"relu\") |>\n  layer_flatten() |>\n  layer_dense(units = 64, activation = \"relu\") |>\n  layer_dense(units = 10, activation = \"softmax\")\n\nmodel_cnn |> compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = optimizer_adam(),\n  metrics = c(\"accuracy\")\n)\n\nmodel_cnn |> fit(train_images, train_labels, epochs = 10, batch_size = 64,\n                 validation_data = list(test_images, test_labels))\nmodel_cnn |> evaluate(test_images, test_labels)\n\n# Show predictions on sample images\npredictions <- model_cnn |> predict(test_images[1:10, , , , drop = FALSE])\npar(mfrow = c(2, 5), mar = c(2, 0.5, 2, 0.5))\nfor (i in 1:10) {\n  img <- test_images[i, , , ]\n  img <- aperm(img, c(2, 1, 3))\n  pred <- which.max(predictions[i, ])\n  true_label <- test_labels_raw[i] + 1\n  plot(as.raster(img))\n  color <- if (pred == true_label) \"darkgreen\" else \"red\"\n  title(main = classes[pred], col.main = color, cex.main = 1)\n  mtext(paste0(\"(\", classes[true_label], \")\"), side = 1, cex = 0.7)\n}\n```\n\n::: {.cell-output-display}\n![](EX_ML_NN_files/figure-html/unnamed-chunk-24-10.png){fig-align='center' width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nEpoch 1/10\n782/782 - 5s - 6ms/step - accuracy: 0.4010 - loss: 1.6185 - val_accuracy: 0.4926 - val_loss: 1.4559\nEpoch 2/10\n782/782 - 5s - 6ms/step - accuracy: 0.5383 - loss: 1.2860 - val_accuracy: 0.5818 - val_loss: 1.1779\nEpoch 3/10\n782/782 - 4s - 6ms/step - accuracy: 0.5935 - loss: 1.1456 - val_accuracy: 0.6075 - val_loss: 1.1105\nEpoch 4/10\n782/782 - 4s - 5ms/step - accuracy: 0.6257 - loss: 1.0598 - val_accuracy: 0.6350 - val_loss: 1.0409\nEpoch 5/10\n782/782 - 4s - 5ms/step - accuracy: 0.6495 - loss: 0.9953 - val_accuracy: 0.6442 - val_loss: 1.0054\nEpoch 6/10\n782/782 - 4s - 6ms/step - accuracy: 0.6684 - loss: 0.9439 - val_accuracy: 0.6434 - val_loss: 1.0087\nEpoch 7/10\n782/782 - 4s - 5ms/step - accuracy: 0.6865 - loss: 0.8963 - val_accuracy: 0.6585 - val_loss: 0.9752\nEpoch 8/10\n782/782 - 4s - 6ms/step - accuracy: 0.7023 - loss: 0.8522 - val_accuracy: 0.6647 - val_loss: 0.9679\nEpoch 9/10\n782/782 - 4s - 5ms/step - accuracy: 0.7156 - loss: 0.8162 - val_accuracy: 0.6671 - val_loss: 0.9647\nEpoch 10/10\n782/782 - 4s - 5ms/step - accuracy: 0.7272 - loss: 0.7830 - val_accuracy: 0.6678 - val_loss: 0.9579\n313/313 - 1s - 2ms/step - accuracy: 0.6678 - loss: 0.9579\n$accuracy\n[1] 0.6678\n\n$loss\n[1] 0.9579431\n\n1/1 - 0s - 54ms/step\n```\n\n\n:::\n:::\n\n\n:::\n\nYou may notice that we did not obtain extremely high accuracy, but this is okay for multiple reasons:\n\n1. We only trained the model for a few epochs and stopped the training very early. You're welcome to let it run for more epochs.\n2. We're dealing with at least ten classes; this is a rather complicated problem.\n3. In DL, you can define many different architectures and hyperparameters, and since we did not play around with these values, this could also explain the lower performance.\n\n# Your turn: NNs applied to the nursing data\nUse NNs to fit nursing data, with the **cost** variable once again set as the prediction outcome. Compare the performance against the tree-based methods. Which type of machine learning model would you go for and why? (think in terms of interpretability and performance)\n\n# References\n- [Keras 3 documentation](https://keras.io/)\n- [PyTorch documentation](https://pytorch.org/docs/stable/)\n- [PyTorch tutorials](https://pytorch.org/tutorials/)\n- [Stanford CS231n: Deep Learning for Computer Vision](https://cs231n.stanford.edu/)\n- [Google ML guide: activation functions for classifier neural networks](https://developers.google.com/machine-learning/guides/text-classification/step-4)\n",
    "supporting": [
      "EX_ML_NN_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}