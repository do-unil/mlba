{
  "hash": "c0993d505ea30fc52d8c6e4aee818d82",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Models: Support Vector Machine\"\n---\n\n\n\n# Support Vector Machine (SVM)\n\nIn this exercise, we apply the Support Vector Machines (SVM) to the classification problem of the data set `Carseats` from the package `ISLR` (already used with CART).\n\nSVM are often difficult to interpret. They are typically what we call \"a black box\" model, that is, a model which provides predictions without understanding what is behind (except if you have followed the course...) and how the features influences these predictions. Therefore, and because we do not want to get bored, we will also use the `caret::train()` function to make our first real machine learning application. Note that this application can be done also with the models that have been seen previously.\n\n## Prepare the data\n\nThe lines below are just a repetition/reminder of the CART series to have the data ready. The only difference is the cast of `SalesHigh` into factors rather than characters because the SVM functions require it.\n\nTo proceed we first have to build the data (below is the) Install the package `ISLR` in order to access the data set `Carseats`. Use `?Carseats` to read its description.\n\nTo apply a classification of the sales, we first create a categorical outcome `SaleHigh` which equals \"Yes\" if `Sales` \\> 7.5 and \"No\" otherwise. Then we create a data frame `MyCarseats` containing `SaleHigh` and all the features of `Carseats` except `Sales`. Finally, split `MyCarseats` into a training and a test set (2/3 vs 1/3). Below we call them `df_tr` and `df_te`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ISLR)\nlibrary(dplyr)\nMyCarseats <- Carseats %>% mutate(SaleHigh=ifelse(Sales > 7.5, \"Yes\", \"No\"))\nMyCarseats <- MyCarseats %>% select(-Sales)\nMyCarseats$SaleHigh <- as.factor(MyCarseats$SaleHigh)\n\nset.seed(123) # for reproducibility \nindex_tr <- sample(x=1:nrow(MyCarseats), size=0.8*nrow(MyCarseats), replace=FALSE)\ndf_tr <- MyCarseats[index_tr,]\ndf_te <- MyCarseats[-index_tr,]\n```\n:::\n\n\n## Linear SVM {#linear_svm}\n\n::: panel-tabset\n### R\n\nThe `e1071::svm()` function of the `e1071` package allows to fit SVM to the data with several possible kernels. Below, it is the linear kernel. We fit a linear kernel and check the predictions on the test set.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(e1071)\nset.seed(123)\ncarseats_svm <- svm(SaleHigh ~ ., data=df_tr, kernel=\"linear\")\ncarseats_svm\ncarseats_svm_pred <- predict(carseats_svm, newdata = df_te)\n\ntable(Pred=carseats_svm_pred, obs=df_te$SaleHigh)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nsvm(formula = SaleHigh ~ ., data = df_tr, kernel = \"linear\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  linear \n       cost:  1 \n\nNumber of Support Vectors:  101\n\n     obs\nPred  No Yes\n  No  38   8\n  Yes  3  31\n```\n\n\n:::\n:::\n\n\nTo obtain a better insight about the prediction quality, we will use the accuracy measure. It is simply the proportion of correct predictions. This can be conveniently obtained (and much more) from the function `caret::confusionMatrix()` of the library `caret`. In the parameters, `data` are the predictions, and `reference` are the observations.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(caret)\nconfusionMatrix(data=carseats_svm_pred, reference = df_te$SaleHigh )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction No Yes\n       No  38   8\n       Yes  3  31\n                                          \n               Accuracy : 0.8625          \n                 95% CI : (0.7673, 0.9293)\n    No Information Rate : 0.5125          \n    P-Value [Acc > NIR] : 4.301e-11       \n                                          \n                  Kappa : 0.724           \n                                          \n Mcnemar's Test P-Value : 0.2278          \n                                          \n            Sensitivity : 0.9268          \n            Specificity : 0.7949          \n         Pos Pred Value : 0.8261          \n         Neg Pred Value : 0.9118          \n             Prevalence : 0.5125          \n         Detection Rate : 0.4750          \n   Detection Prevalence : 0.5750          \n      Balanced Accuracy : 0.8609          \n                                          \n       'Positive' Class : No              \n                                          \n```\n\n\n:::\n:::\n\n\nWe should only focus on the accuracy for now (the other measures will be studied later). In our run, it was ≈$86\\%$. That number is obtained using the default parameter, cost $C=1$.\n\n### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# The usual loading of our environment\nlibrary(reticulate)\nuse_condaenv(\"MLBA\", required = TRUE)\n```\n:::\n\n\nSimilar to the CART exercises, we use the `df_train` and `df_te` created in R to carry out our SVM training. Once again, we continue using the `sklearn` library. First, we copy our dataset from the R variable to a python variable. Then we use `LabelEncoder()` from `sklearn.preprocessing` to convert categorical data into a numeric form, which many `sklearn` machine learning algorithms require. The encoding assigns a unique numerical value to each categorical value, which can sometimes help the performance. In the case of `caret::train()`, the function handles this transformation automatically. Also, we standardize the data in the case of python for faster computations using `StandardScaler`, because of the same numerical stability mentioned for CART. After this, we divide the training and test sets into predictors (e.g., `X_train`) and the outcome (e.g., `y_train`) and initialize our linear kernel SVM to fit the model to the data.\n\nWe will use `classification_report` and `accuracy_score` from `sklearn.metrics` to get more information on the performance. (you could also use `confusion_matrix` from the same module.)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn import svm\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n\n# We copy the training and testing datasets from R to Python.\n# Use r.df_tr to access R objects from Python\ncarset_tr_py = r.df_tr.copy()\ncarset_te_py = r.df_te.copy()\n\n# We encode categorical variables as numeric, which is necessary for SVM.\nle = LabelEncoder()\ncat_vars = [\"ShelveLoc\", \"Urban\", \"US\", \"SaleHigh\"]\nfor var in cat_vars:\n    # Fit the LabelEncoder to the training set and transform the training and testing sets\n    # We need to use the same encoder for both sets to ensure consistency\n    carset_tr_py[var] = le.fit_transform(carset_tr_py[var])\n    carset_te_py[var] = le.transform(carset_te_py[var])\n\n# Split the data into training and testing sets\nX_train, y_train = carset_tr_py.drop(columns=[\"SaleHigh\"]), carset_tr_py[\"SaleHigh\"]\nX_test, y_test = carset_te_py.drop(columns=[\"SaleHigh\"]), carset_te_py[\"SaleHigh\"]\n\n# Standardize only the continuous variables\ncont_vars = [\"CompPrice\", \"Income\", \"Advertising\", \"Population\", \"Price\", \"Age\", \"Education\"]\nscaler = StandardScaler()\nX_train[cont_vars] = scaler.fit_transform(X_train[cont_vars])\nX_test[cont_vars] = scaler.transform(X_test[cont_vars])\n\n# To speed up the operation you can also transform the inputs\n# X_train = X_train.to_numpy()\n# y_train = y_train.to_numpy()\n\n# Set random seed for reproducibility\nnp.random.seed(123)\n\n# Initialize a linear SVM model\ncarseats_svm_py = svm.SVC(kernel=\"linear\")\n\n# Fit the SVM model to the training data\ncarseats_svm_py.fit(X_train, y_train);\n\n# Print the model parameters\nprint(carseats_svm_py)\n\n# Make predictions on the test set\ncarseats_svm_pred_py = carseats_svm_py.predict(X_test)\n\n# Print a confusion matrix\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_py, rownames=['True'], colnames=['Predicted']))\n# Alternatively, print a confusion matrix using `sklearn.metrics`\n# print(confusion_matrix(y_test, carseats_svm_pred_py))\n\n# Compute metrics\nreport_linear_py = classification_report(y_test, carseats_svm_pred_py, target_names=['No', 'Yes'])\naccuracy_linear_py = accuracy_score(y_test, carseats_svm_pred_py)\n\nprint(report_linear_py)\nprint(\"Overall accuracy:\", accuracy_linear_py)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSVC(kernel='linear')\nPredicted   0   1\nTrue             \n0          38   3\n1          15  24\n              precision    recall  f1-score   support\n\n          No       0.72      0.93      0.81        41\n         Yes       0.89      0.62      0.73        39\n\n    accuracy                           0.78        80\n   macro avg       0.80      0.77      0.77        80\nweighted avg       0.80      0.78      0.77        80\nOverall accuracy: 0.775\n```\n\n\n:::\n:::\n\n\nThis performance is worse than our model in R and requires more tuning (due to differences in default values and implementations of the functions).\n:::\n\n## Radial basis SVM\n\n::: panel-tabset\n### R\n\nWe try now with a radial basis kernel (the default).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\ncarseats_rb <- svm(SaleHigh ~ ., data=df_tr, kernel=\"radial\")\ncarseats_rb\ncarseats_rb_pred <- predict(carseats_rb, newdata = df_te)\nconfusionMatrix(data=carseats_rb_pred, reference = df_te$SaleHigh )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nsvm(formula = SaleHigh ~ ., data = df_tr, kernel = \"radial\")\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  195\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction No Yes\n       No  39  13\n       Yes  2  26\n                                          \n               Accuracy : 0.8125          \n                 95% CI : (0.7097, 0.8911)\n    No Information Rate : 0.5125          \n    P-Value [Acc > NIR] : 2.438e-08       \n                                          \n                  Kappa : 0.6222          \n                                          \n Mcnemar's Test P-Value : 0.009823        \n                                          \n            Sensitivity : 0.9512          \n            Specificity : 0.6667          \n         Pos Pred Value : 0.7500          \n         Neg Pred Value : 0.9286          \n             Prevalence : 0.5125          \n         Detection Rate : 0.4875          \n   Detection Prevalence : 0.6500          \n      Balanced Accuracy : 0.8089          \n                                          \n       'Positive' Class : No              \n                                          \n```\n\n\n:::\n:::\n\n\nThe accuracy is now ≈$81\\%$. This shows how important that choice can be. In the same vein, we are now relying on the default parameters of the function. For the cost $C$ it is $1$, for the parameter `gamma` of the kernel, it is 1/(data dimension) (see `?svm`). The `train` function allows us to make a better selection.\n\n### Python\n\nThis is same as the linear kernel, and we just need to change the kernel parameter value from `linear` to `rbf`. Here's the radial version:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nnp.random.seed(123) # once again, for reproducibility\n\n# Use a radial kernel in the SVM classifier\ncarseats_svm_rb_py = svm.SVC(kernel=\"rbf\")\ncarseats_svm_rb_py.fit(X_train, y_train);\nprint(carseats_svm_rb_py)\n\n# Predict the values with the radial approach\ncarseats_svm_pred_rb_py = carseats_svm_rb_py.predict(X_test)\n\n# Compute metrics\nreport_radial_py = classification_report(y_test, carseats_svm_pred_rb_py, target_names=['No', 'Yes'])\naccuracy_radial_py = accuracy_score(y_test, carseats_svm_pred_rb_py)\n\n# Print metrics\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_rb_py, rownames=['True'], colnames=['Predicted']))\nprint(report_radial_py)\nprint(\"Overall accuracy:\", accuracy_radial_py)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSVC()\nPredicted   0   1\nTrue             \n0          36   5\n1          15  24\n              precision    recall  f1-score   support\n\n          No       0.71      0.88      0.78        41\n         Yes       0.83      0.62      0.71        39\n\n    accuracy                           0.75        80\n   macro avg       0.77      0.75      0.74        80\nweighted avg       0.77      0.75      0.75        80\nOverall accuracy: 0.75\n```\n\n\n:::\n:::\n\n:::\n\nIn both cases (R & python), with the default parameters, the linear kernel seems to do better than the radial one.\n\n## Tuning the hyperparameter\n\n::: panel-tabset\n### R\n\nIn R, `caret` uses various libraries to run the svm models (check for yourself [by searching for `support vector machine` here](https://topepo.github.io/caret/available-models.html)). For instance, calling `svmLinear` or `svmRadial` uses the library `kernlab`, and the `kernlab::ksvm()` function.\n\nThe `C` hyperparameter (from `kernlab::ksvm()`) accounts for the cost argument and controls the trade-off between allowing misclassifications in the training set and finding a decision boundary that generalizes well to new data. A larger cost value leads to a smaller margin and a more complex model that may overfit the data. On the other hand, a smaller cost value leads to a larger margin and a simpler model that may underfit the data.\n\nSimilarly to `EX_ML_NN`, to select the good hyperparameters, we build a search grid and fit the model with each possible value in the grid. Then, the best model is chosen among all the combinations of the hyperparameters.\n\nAs a reminder, the `train` function from `caret`. Has:\n\n-   a formula.\n-   a dataset.\n-   a method (i.e. the model which in this case is SVM with linear kernel).\n-   a training control procedure.\n\n#### Linear SVM\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrctrl <- trainControl(method = \"cv\", number=10)\nset.seed(143)\nsvm_Linear <- train(SaleHigh ~., data = df_tr, method = \"svmLinear\",\n                    trControl=trctrl)\nsvm_Linear\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSupport Vector Machines with Linear Kernel \n\n320 samples\n 10 predictor\n  2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 288, 289, 288, 288, 288, 288, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.8814333  0.7627961\n\nTuning parameter 'C' was held constant at a value of 1\n```\n\n\n:::\n:::\n\n\nFor now, the validation accuracy is very high (≈$89\\%$). This is normal since this accuracy is computed on the training set.\n\nWe now supply a grid of values for the cost that we want to try and pass to the argugment`tuneGrid`. Be patient, it may take time.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngrid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))\ngrid\nset.seed(143)\nsvm_Linear_Grid <- train(SaleHigh ~., data = df_tr, method = \"svmLinear\",\n                           trControl=trctrl,\n                           tuneGrid = grid)\nsvm_Linear_Grid\nplot(svm_Linear_Grid)\n```\n\n::: {.cell-output-display}\n![](Ex_ML_SVM_files/figure-html/tune-linear-svm-1.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\nsvm_Linear_Grid$bestTune\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      C\n1 1e-02\n2 1e-01\n3 1e+00\n4 1e+01\n5 1e+02\n6 1e+03\nSupport Vector Machines with Linear Kernel \n\n320 samples\n 10 predictor\n  2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 288, 289, 288, 288, 288, 288, ... \nResampling results across tuning parameters:\n\n  C      Accuracy   Kappa    \n  1e-02  0.8248809  0.6494001\n  1e-01  0.8658083  0.7315461\n  1e+00  0.8814333  0.7627961\n  1e+01  0.8751833  0.7502961\n  1e+02  0.8658083  0.7315461\n  1e+03  0.8689333  0.7377961\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was C = 1.\n  C\n3 1\n```\n\n\n:::\n:::\n\n\nWe see that setting the cost to 1 provides the best model. The accuracy apparently reaches a plateau at this value. This is same as our cost parameter in section [1.2](#linear_svm).\n\n#### Radial Basis SVM\n\nThe `sigma` hyperparameter (also from `kernlab::ksvm()`) controls the width of the radial basis function kernel, which is used to transform the input data into a higher-dimensional feature space. A larger value of sigma corresponds to a narrower kernel and a more complex model, while a smaller value corresponds to a wider kernel and a simpler model.\n\nWe repeat the procedure for SVM with a radial basis kernel. Here, there are two parameters ( `sigma` and `C`) to tune. The grid choice is rather arbitrary (often the result of trials and errors), and very few general useful guidelines exist. The code below may take a few minutes to run.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngrid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),\n                           C = c(1, 10, 100, 500, 1000))\ngrid_radial\nset.seed(143)\nsvm_Radial_Grid <- train(SaleHigh ~., data = df_tr, method = \"svmRadial\",\n                           trControl=trctrl,\n                           tuneGrid = grid_radial)\nsvm_Radial_Grid\nplot(svm_Radial_Grid)\n```\n\n::: {.cell-output-display}\n![](Ex_ML_SVM_files/figure-html/tune-radial-svm-1.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\nsvm_Radial_Grid$bestTune\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   sigma    C\n1   0.01    1\n2   0.02    1\n3   0.05    1\n4   0.10    1\n5   0.01   10\n6   0.02   10\n7   0.05   10\n8   0.10   10\n9   0.01  100\n10  0.02  100\n11  0.05  100\n12  0.10  100\n13  0.01  500\n14  0.02  500\n15  0.05  500\n16  0.10  500\n17  0.01 1000\n18  0.02 1000\n19  0.05 1000\n20  0.10 1000\nSupport Vector Machines with Radial Basis Function Kernel \n\n320 samples\n 10 predictor\n  2 classes: 'No', 'Yes' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 288, 289, 288, 288, 288, 288, ... \nResampling results across tuning parameters:\n\n  sigma  C     Accuracy   Kappa    \n  0.01      1  0.8405120  0.6808260\n  0.01     10  0.8750825  0.7500472\n  0.01    100  0.8813325  0.7626682\n  0.01    500  0.8562317  0.7125802\n  0.01   1000  0.8345461  0.6691401\n  0.02      1  0.8375825  0.6750472\n  0.02     10  0.8718567  0.7436694\n  0.02    100  0.8626833  0.7253900\n  0.02    500  0.8221408  0.6444015\n  0.02   1000  0.8251650  0.6504029\n  0.05      1  0.8534030  0.7067635\n  0.05     10  0.8782075  0.7564182\n  0.05    100  0.8249695  0.6501109\n  0.05    500  0.8280945  0.6563609\n  0.05   1000  0.8280945  0.6563609\n  0.10      1  0.8469514  0.6938198\n  0.10     10  0.8280059  0.6560015\n  0.10    100  0.8217498  0.6435996\n  0.10    500  0.8217498  0.6435996\n  0.10   1000  0.8217498  0.6435996\n\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were sigma = 0.01 and C = 100.\n  sigma   C\n3  0.01 100\n```\n\n\n:::\n:::\n\n\nThe optimal model from this search is with `sigma = 0.01` and `C=100`.\n\n### Python\n\nWe can use `GridSearchCV` from `sklearn.model_selection` to achieve the same goal in python and set the argument for cross-validation (`cv` to achieve the same results) and tune both types of kernels at once. `sklearn.svm.SVC()` does contain the two arguments for the `C` and `sigma` but the relationship is slightly different and we'll explain below. Also, please note in this approach, the `linear` kernel by default ignores the `sigma` values.\n\nAs mentioned earlier, in `sklearn.svm.SVC()`, the equivalent parameter to cost is also `C`, which is straightforward. Still, the equivalent parameter to `sigma` is a bit trickier (called `gamma` in `sklearn.svm.SVC()`), which also controls the width of the radial basis function kernel. However, the relationship between `gamma` and `sigma` differs, and the two parameters cannot be directly compared. In particular, `gamma` is defined as the inverse of the width of the kernel, i.e., `gamma = 1/(2 * sigma**2)`.\n\nFor simplicity's sake, we'll directly use the `gamma` with the same values as `sigma`; however, you can always run `'gamma': [1/(2*sigma**2) for sigma in [0.01, 0.02, 0.05, 0.1]]` to get similar values (although you would want to round as this division results in non-terminating repeating decimal numbers).\n\nThe code will take a few minutes to run (longer than the R version).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV\n\nnp.random.seed(123) # for reproducibility\n\n# Define the grid of hyperparameters to search over\nparam_grid = {'C': [1, 10, 100, 500, 1000],\n              'gamma': [0.01, 0.02, 0.05, 0.1],\n              # 'gamma': [round(1/(2*sigma**2),2) for sigma in [0.01, 0.02, 0.05, 0.1]],\n              'kernel': ['linear', 'rbf']}\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(svm.SVC(), param_grid, cv=10, scoring='accuracy', n_jobs=1) # you can also set n_jobs = -1 to use all the cores and obtain the results faster (but unfortunately atm it only works on Mac/Linux and not Windows OS with `reticulate`)\n# for more info, please see https://github.com/rstudio/reticulate/issues/1346\ngrid_search.fit(X_train, y_train);\n\nprint(\"Best hyperparameters:\", grid_search.best_params_)\nprint(\"Best score:\", grid_search.best_score_)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBest hyperparameters: {'C': 500, 'gamma': 0.01, 'kernel': 'rbf'}\nBest score: 0.853125\n```\n\n\n:::\n:::\n\n\n\n\nNote that the accuracy is on the training set. The best parameters are returned by`best_grid_param`. We will use a new plotting library for seeing this evolution called `seaborn`, which offers some great visualization tools.\n\nWe use the installed `seaborn` package from our Setup which allows for grouping our hyperparameters and displaying them with a heatmap.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming `results` is your DataFrame from `grid_search.cv_results_`\nresults = pd.DataFrame(grid_search.cv_results_)\n\n# Before performing the pivot or groupby operation, drop the 'params' column (it's a dictionary)\nresults = results.drop(columns=['params'])\n\n# Now, you can safely group by 'param_C', 'param_gamma', and 'param_kernel' to calculate mean test scores\ngrouped_results = results.pivot_table(index=['param_C', 'param_gamma'], columns='param_kernel', values='mean_test_score')\n\nplt.clf()\n\n# Create the heatmap\nax = sns.heatmap(grouped_results, annot=True, fmt='.3f', cmap='viridis', cbar=False)\nax.invert_yaxis();  # invert the y-axis to match your preference\n\nplt.xlabel('Kernel');\nplt.ylabel('C-Gamma');\nplt.show()\n```\n\n::: {.cell-output-display}\n![](Ex_ML_SVM_files/figure-html/plot-figures-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe can see that `rbf` kernel benefits from changes in `C` & `gamma`, however, for the `linear`, we're always using the same gamma of `0.01` so for this kernel, the only changes are coming from `C` parameter.\n:::\n\n## Best model\n\n::: panel-tabset\n### R\n\nAfter finding the best hyperparameters, it is often good practice to re-train the model with the best hyperparameters on the entire training set before evaluating everything on the test set. We do not need to re-train the model with the entire dataset for the linear SVM, as the best cost matched those used in section [1.2](#linear_svm). We re-train the final model with the entire training set using optimal hyperparameters for the radial basis kernel.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncarseats_rb_tuned <- svm(SaleHigh ~ .,data = df_tr,\n                         kernel = \"radial\", gamma = svm_Radial_Grid$bestTune$sigma,\n                         cost = svm_Radial_Grid$bestTune$C)\ncarseats_rb_tuned_pred <- predict(carseats_rb_tuned, newdata = df_te)\nconfusionMatrix(data=carseats_rb_tuned_pred, reference = df_te$SaleHigh)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nConfusion Matrix and Statistics\n\n          Reference\nPrediction No Yes\n       No  37  10\n       Yes  4  29\n                                          \n               Accuracy : 0.825           \n                 95% CI : (0.7238, 0.9009)\n    No Information Rate : 0.5125          \n    P-Value [Acc > NIR] : 5.687e-09       \n                                          \n                  Kappa : 0.6485          \n                                          \n Mcnemar's Test P-Value : 0.1814          \n                                          \n            Sensitivity : 0.9024          \n            Specificity : 0.7436          \n         Pos Pred Value : 0.7872          \n         Neg Pred Value : 0.8788          \n             Prevalence : 0.5125          \n         Detection Rate : 0.4625          \n   Detection Prevalence : 0.5875          \n      Balanced Accuracy : 0.8230          \n                                          \n       'Positive' Class : No              \n                                          \n```\n\n\n:::\n:::\n\n\nOverall, if we compare all the models, we see that the linear kernel SVM with cost of 1 looks like the best model. We already saw that it provides a $86\\%$ accuracy on the test set. This is what can be expected in the future from that model.\n\n### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# re-train the model with best hyperparameters\nsvm_best_py = svm.SVC(**grid_search.best_params_)\nsvm_best_py.fit(X_train, y_train);\n\n# predict on test dataset\ncarseats_svm_pred_py = svm_best_py.predict(X_test)\n\n# print confusion matrix and accuracy score\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_py, rownames=['True'], colnames=['Predicted']))\nprint(classification_report(y_test, carseats_svm_pred_py, target_names=['No', 'Yes']))\nprint('Accuracy Score:', accuracy_score(y_test, carseats_svm_pred_py))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPredicted   0   1\nTrue             \n0          39   2\n1          12  27\n              precision    recall  f1-score   support\n\n          No       0.76      0.95      0.85        41\n         Yes       0.93      0.69      0.79        39\n\n    accuracy                           0.82        80\n   macro avg       0.85      0.82      0.82        80\nweighted avg       0.85      0.82      0.82        80\nAccuracy Score: 0.825\n```\n\n\n:::\n:::\n\n\nThe results are similar to the R outcome for the linear models (except differences in the confusion matrix).\n:::\n\n## Your turn\n\nRepeat the analysis on the German credit data (`german.csv`). Since dataset is much larger than `MyCarSeats`, the tuning procedure may be longer. For this reason, just limit to a linear SVM model for the tuning with limited range for the grid search.\n",
    "supporting": [
      "Ex_ML_SVM_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}