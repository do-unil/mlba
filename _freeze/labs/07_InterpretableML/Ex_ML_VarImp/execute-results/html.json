{
  "hash": "e539f7d7b970d5b4b88e7519689a35d8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Variable Importance\"\n---\n\n\n# Data & models\n\nThis exercise shows an example of model-agnostic variable importance for a regression problem. The dataset that we will be working with is `Carseats` from the `ISLR` library which has already been used during some of the exercises such as `Ex_ML_Tree` and `Ex_ML_SVM`. It is highly recommended that you try to implement some parts of the exercise yourself before checking the answers.\n\n-   Load the data from ISLR package, then assign 90% of the data for training and the remainder for testing. Please keep in mind that we make a training split running the feature importance (instead of using the entire dataset) as we \"may\" want to re-train the model with only a fewer features rather than biasing our decision by also including the testing data.\n\n-   Create three models including a linear regression, a regression tree and a support-vector machine.\n\n<details>\n\n<summary>\n\n**Answer** </summary>\n<p>\n\n::: panel-tabset\n## R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(rpart)\nlibrary(e1071)\nlibrary(dplyr)\nlibrary(ISLR)\n\n# divide the data into training and testing sets\nset.seed(2022)\ncarseats_index <- sample(x=1:nrow(Carseats), size=0.9*nrow(Carseats), replace=FALSE)\ncarseats_tr <- Carseats[carseats_index,]\ncarseats_te <- Carseats[-carseats_index,]\n\n# define a linear regression\ncarseats_lm <- lm(Sales~., data=carseats_tr)\n\n# define a regression tree (you can also use `adabag::autoprune()`here\ncarseats_rt <- rpart(Sales~., data=carseats_tr)\n\n# define a support-vector machine\ncarseats_svm <- svm(Sales ~ ., data=carseats_tr)\n```\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n```\n:::\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import fetch_openml\n\n# Use the training and test sets created in R (no easy way to get the `Carseats` data in python)\n# Convert the categorical columns to one-hot encoded ones\ncarseats_train, carseats_test = pd.get_dummies(r.carseats_tr.copy()), pd.get_dummies(r.carseats_tr.copy())\n\n# Define a linear regression\ncarseats_lr = LinearRegression()\ncarseats_lr.fit(carseats_train.drop(columns=['Sales']), carseats_train['Sales']);\n\n# Define a decision tree regressor\ncarseats_dtr = DecisionTreeRegressor()\ncarseats_dtr.fit(carseats_train.drop(columns=['Sales']), carseats_train['Sales']);\n\n# Define a support-vector machine\ncarseats_svr = SVR()\ncarseats_svr.fit(carseats_train.drop(columns=['Sales']), carseats_train['Sales']);\n```\n:::\n\n\n:::\n\n</p>\n</details>\n\n# Variable Importance using `DALEX`\n\nFeature importance can be manually computed by permuting one column and computing the drop in the value of the loss function (e.g. RMSE for regression and Accuracy for classification). This permutation can be repeated n-times to calculate a mean drop in loss value across various runs. This form of variable importance is often referred to as \"permutation feature importance\".\n\nFortunately, in R there are some nice packages that can help you with that. Three common libraries (among many) for model-agnostic feature importance in R are `iml`, `DALEX` and `vip` . If you would like to learn about the model-specific and model-agnostic feature importance and the various packages in R, you can refer to the chapter 16 of the book [\"Hands-on Machine Learning with R\"](https://ema.drwhy.ai/featureImportance.html). For the purpose of this part of the exercise, we will be using the `DALEX` library. Please note that the exercise has been largely inspired by the same chapter 16 of the book. You can also check out other variations such as `iml` and `vip` implementations.\n\n## Creating an `explain` object\n\n`DALEX` has an `explain` object which allows you to do various kind of explanatory analysis. Try reading about the required inputs for it by referring to its documentations (`?DALEX::explain()`) and also referring to the book mentioned at the beginning of this exercise ([\"Hands-on Machine Learning with R\"](https://ema.drwhy.ai/featureImportance.html)). Create one explain object per model for the training data and set the inputs that you need which are `model`, `data` (data frame of features) and `y` (vector of observed outcomes). Also, you can give a caption to your model through the `label` argument.\n\n\n::: panel-tabset\n\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(DALEX)\n\nx_train <- select(carseats_tr, -Sales)\ny_train <- pull(carseats_tr, Sales)\n\nexplainer_lm <- DALEX::explain(model = carseats_lm, \n                                 data = x_train, \n                                 y = y_train,\n                                 label = \"Linear Regression\")\n\nexplainer_rt <- DALEX::explain(model = carseats_rt,\n                               data = x_train,\n                               y = y_train,\n                               label = \"Regression Tree\")\n\nexplainer_svm <- DALEX::explain(model = carseats_svm,\n                                data = x_train,\n                                y = y_train,\n                                label = \"Support Vector Machine\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPreparation of a new explainer is initiated\n  -> model label       :  Linear Regression \n  -> data              :  360  rows  10  cols \n  -> target variable   :  360  values \n  -> predict function  :  yhat.lm  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package stats , ver. 4.4.1 , task regression (  default  ) \n  -> predicted values  :  numerical, min =  -0.6248701 , mean =  7.492444 , max =  15.8129  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  -2.846017 , mean =  -1.865625e-15 , max =  3.3867  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -> model label       :  Regression Tree \n  -> data              :  360  rows  10  cols \n  -> target variable   :  360  values \n  -> predict function  :  yhat.rpart  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package rpart , ver. 4.1.23 , task regression (  default  ) \n  -> predicted values  :  numerical, min =  3.70875 , mean =  7.492444 , max =  12.3904  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  -4.03697 , mean =  4.151531e-16 , max =  4.68303  \n  A new explainer has been created!  \nPreparation of a new explainer is initiated\n  -> model label       :  Support Vector Machine \n  -> data              :  360  rows  10  cols \n  -> target variable   :  360  values \n  -> predict function  :  yhat.svm  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package e1071 , ver. 1.7.16 , task regression (  default  ) \n  -> predicted values  :  numerical, min =  2.267277 , mean =  7.498075 , max =  15.21856  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  -2.780185 , mean =  -0.005630512 , max =  2.458701  \n  A new explainer has been created!  \n```\n\n\n:::\n:::\n\n\n### Python\nTo calculate the feature importances using Python, we'll be using the `permutation_importance` function from the `sklearn` library.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the features (relevant to SVM)\nscaler = StandardScaler()\ncarseats_train_scaled = carseats_train.copy()\ncarseats_test_scaled = carseats_test.copy()\n\n# Calculate feature importances for each model\nimportance_lr = permutation_importance(carseats_lr, carseats_train.drop(columns=['Sales']), carseats_train['Sales'], n_repeats=10, random_state=2022)\nimportance_dtr = permutation_importance(carseats_dtr, carseats_train.drop(columns=['Sales']), carseats_train['Sales'], n_repeats=10, random_state=2022)\nimportance_svr = permutation_importance(carseats_svr, carseats_train.drop(columns=['Sales']), carseats_train['Sales'], n_repeats=10, random_state=2022)\n# Train the SVM model on scaled data\ncarseats_svr_scaled = SVR(kernel='linear')\ncarseats_svr_scaled.fit(carseats_train_scaled.drop(columns=['Sales']), carseats_train_scaled['Sales']);\nimportance_svr_scaled = permutation_importance(carseats_svr_scaled, carseats_train_scaled.drop(columns=['Sales']), carseats_train_scaled['Sales'], n_repeats=10, random_state=2022)\n\n# Print the feature importances\nimportance_df = pd.DataFrame(data={\n    'Feature': carseats_train.drop(columns=['Sales']).columns,\n    'Linear Regression': importance_lr.importances_mean,\n    'Decision Tree': importance_dtr.importances_mean,\n    'Support Vector Machine (unscaled)': importance_svr.importances_mean,\n    'Support Vector Machine (scaled)': importance_svr_scaled.importances_mean\n})\n\nprint(importance_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             Feature  ...  Support Vector Machine (scaled)\n0          CompPrice  ...                         0.437965\n1             Income  ...                         0.051003\n2        Advertising  ...                         0.156785\n3         Population  ...                         0.000136\n4              Price  ...                         1.146152\n5                Age  ...                         0.152260\n6          Education  ...                         0.001026\n7      ShelveLoc_Bad  ...                         0.232378\n8     ShelveLoc_Good  ...                         0.257843\n9   ShelveLoc_Medium  ...                         0.004519\n10          Urban_No  ...                         0.000531\n11         Urban_Yes  ...                         0.000531\n12             US_No  ...                         0.000160\n13            US_Yes  ...                         0.000160\n\n[14 rows x 5 columns]\n```\n\n\n:::\n:::\n\n\nYou can observe the value of scaling for SVM. The results seems to agree that `Price` is the most important feature.\n\n:::\n\n## Plotting the feature importance\n\nNow that you have created the `DALEX::explain` objects, we will use another function called `model_parts` which takes care of the feature permutation. Try reading about the function `DALEX::model_parts()` . The main arguments that you need to provide to the `explain` function are:\n\n-   An `explainer` object (what you created above).\n\n-   `B` which is the number of permutations (i.e. how many times you want to randomly shuffle each column).\n\n-   The `type` of scores you would like it to return (raw score vs differences vs ratios) which in this case we set to `ratio` and you can read more the differences in the documentation.\n\n-   `N` argument which you can set to `N=NULL` which essentially asks how many samples you would like to use for calculating the variable importance, where setting it to `NULL` means that we use the entire training set.\n\n-   There is also a `loss_function` which by default is `RMSE` for regression (our case) and `1-AUC` for classification, by there are a few more which you can find out about by referring to the documentations (e.g. through `?DALEX::loss_root_mean_square`).\n\nAfter assigning `model_parts` to a variable, try plotting each model to see the most important variables. What do you see? Are there important features that are in common?\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncalculate_importance <- function(your_model_explainer, n_permutations = 10) {\n  imp <- model_parts(explainer = your_model_explainer,\n                     B = n_permutations,\n                     type = \"ratio\",\n                     N = NULL)\n  return(imp)\n}\n\nimportance_lm  <- calculate_importance(explainer_lm)\nimportance_rt  <- calculate_importance(explainer_rt)\nimportance_svm <- calculate_importance(explainer_svm)\n\nlibrary(ggplot2)\nplot(importance_lm, importance_rt, importance_svm) +\n  ggtitle(\"Mean variable-importance ratio over 10 permutations\", \"\")\n```\n\n::: {.cell-output-display}\n![](Ex_ML_VarImp_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n# Partial Dependence Plots (`PDP`)\n\nPartial dependence plots (PDPs) show the marginal effect of one or two features on the predicted outcome of a machine learning model. They can be used to visualize and interpret the influence of selected features on the model's predictions. We'll continue using the same Carseats data. We first create PDPs for the `Price` and `Advertising` features using the the SVM model `carseats_svm` created earlier:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(caret)\nlibrary(tidyverse)\nlibrary(pdp)\n\npdp::partial(carseats_svm, pred.var = \"Price\", plot = TRUE)\n```\n\n::: {.cell-output-display}\n![](Ex_ML_VarImp_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=672}\n:::\n\n```{.r .cell-code}\npdp::partial(carseats_svm, pred.var = \"Advertising\", plot = TRUE)\n```\n\n::: {.cell-output-display}\n![](Ex_ML_VarImp_files/figure-html/unnamed-chunk-7-2.png){fig-align='center' width=672}\n:::\n:::\n\nWhat we see in the first plot is that for this particular model, the higher the `price`, the number of carseats sold (units) which makes sense. With regards to the advertising, we see that the higher the local advertising budget for the company (see `?Carseats` for more details), the higher the sales number of units for a car, however, there's a plateau after which you cannot sell more units. This means that after a certain point, increasing the advertising budget may no longer bring any benefit in terms of the units sold. It's important to note that in PDP, we study the decision by the model and not the data, however, we can always visualize our results to see if they follow a similar trend or not:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# plot(Carseats$Advertising, Carseats$Sales, xlab = \"Advertising Budget\", ylab = \"Sales Units\")\nggplot(data = carseats_tr, aes(x = Advertising, y = Sales)) +\n  geom_point() +\n  labs(x = \"Advertising Budget\", y = \"Sales Unit\") +\n  ggtitle(\"Relationship between Sales and Advertising Budget\")\n```\n\n::: {.cell-output-display}\n![](Ex_ML_VarImp_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe can see that the trend is not very clear but may be headed in the same direction.\n\nFor categorical features, such as `ShelveLoc`, we can make a similar kind of PDP:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npdp::partial(carseats_svm, pred.var = \"ShelveLoc\", plot = TRUE, plot.engine = \"ggplot\")\n```\n\n::: {.cell-output-display}\n![](Ex_ML_VarImp_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=672}\n:::\n:::\n\nFrom this plot, we can see that the better the quality of shelving location, the high the number of units sold (which again makes sense).\n\n::: {.callout-warning}\n\n## Limitations of PDPs\n\nPDPs can have some limitations, which you, as users, should be aware of. These limitations do partially apply to other techniques presented during lab:\n\n1. *Assuming feature independence*: PDPs assume that the features being plotted are independent of each other. This can lead to misleading results when there is a strong correlation or interaction between features, as the partial dependence function will not capture their combined effect accurately.\n\n2. *Inaccurate representation of complex interactions*: PDPs cannot represent high-order interactions or nonlinear relationships between features.\n\n3. *Unreliable in the presence of outliers*: PDPs can be sensitive to outliers and extreme values in the dataset, which may result in distorted representations of the feature's impact on the model's predictions. Proper preprocessing and outlier detection techniques should be employed to avoid this issue.\n\n4. *Using the average values*: PDPs uses the mean expected value for the final prediction of the target feature. This should be fine in most applications, but it may be inappropriate in some applications.\n\n5. *Computational burden*: Generating PDPs can be computationally expensive, especially for high-dimensional datasets or complex models. It is important to weigh the benefits of this technique against the computational resources available.\n\nKeep in mind these limitations when interpreting PDPs (and other techniques) and consider these limitations when drawing conclusions from them.\n\n:::\n\nWe can now create a two-way PDP to explore the interaction between `Price` and `Advertising`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npdp::partial(carseats_svm, pred.var = c(\"Price\", \"Advertising\"), plot = TRUE)\n```\n\n::: {.cell-output-display}\n![](Ex_ML_VarImp_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nA combination of moderate `advertising` (around 2-18) and low `prices` (from around 0-40) seems to produce the highest sales units by the SVM model.\n\n::: {.callout-note}\nYou may note how no python implementation of PDP has been shown since `sklearn.inspection.plot_partial_dependence`, which was suitable for this task, has recently changed, and the new alternative does not always work well for categorical variables. However, if you are interested, feel free to look up existing alternatives.\n:::\n\n# `LIME` (Local Interpretable Model-agnostic Explanations)\n\n`LIME` helps explain individual predictions of machine learning models by fitting a local, interpretable model around a specific data point. This allows for more transparency and understanding of the model's behavior for individual instances.\n\n## SVM\n\n::: panel-tabset\n### R\n\n`lime` package does not support the SVM model from the `e1071` package out of the box.You can see the list of supported models via ` ?model_type`. There are solutions to this:\n\n- Re-train your model with the `caret` library which we then work directly with this library (also may be good practice to build your models with `caret`).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load the lime library\nlibrary(lime)\n\n# Create a caret model using a support vector machine\nsvm_caret_model <- caret::train(Sales ~ ., data = carseats_tr, method = \"svmLinear2\", trControl = trainControl(method = \"none\"))\n\n# Predict on a test instance\ntest_instance <- carseats_te[1:4, -1]\n\n# Create a lime explainer object for the SVM model\nlime_svm_explainer <- lime::lime(carseats_tr[, -1], \n                                 svm_caret_model)\n\n# Explain a prediction using lime\nlime_svm_explanation <- explain(test_instance, lime_svm_explainer, n_features = 10)\nplot_features(lime_svm_explanation)\n```\n\n::: {.cell-output-display}\n![](Ex_ML_VarImp_files/figure-html/unnamed-chunk-11-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n- Create custom `predict_model` and `model_type` methods for the SVM model. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Custom predict_model function for SVM\npredict_model.svm <- function(x, newdata, type, ...) {\n  if (type == \"raw\") {\n    res <- predict(x, newdata = newdata, ...)\n    return(data.frame(Response = res, stringsAsFactors = FALSE))\n  } else if (type == \"prob\") {\n    res <- predict(x, newdata = newdata, ...)\n    prob <- kernlab::kernel(x, newdata, j = -1)\n    return(as.data.frame(prob, check.names = FALSE))\n  }\n}\n\n# Custom model_type function for SVM\nmodel_type.svm <- function(x, ...) {\n  if (x$type == \"C-classification\") {\n    return(\"classification\")\n  } else {\n    return(\"regression\")\n  }\n}\n\n# Create a LIME explainer for the SVM model\nlime_svm_explainer <- lime(x_train, carseats_svm)\n\n# Choose a specific instance from the test set to explain\ntest_instance <- carseats_te[1:4,-1]\n\n# Generate explanations for the chosen instance\nlime_svm_explanation <- explain(test_instance, lime_svm_explainer, n_features = 10)\n\n# Visualize the explanation\nplot_features(lime_svm_explanation)\n```\n\n::: {.cell-output-display}\n![](Ex_ML_VarImp_files/figure-html/unnamed-chunk-12-1.png){fig-align='center' width=672}\n:::\n:::\n\nYou can see the prediction plot for 4 test observations. We can see several bar charts. On the y-axis, you see the features (and their intervals), while the x-axis shows the relative strength of each feature at a given value or interval. The positive value (blue color) shows that the feature support or increases the value of the prediction, while the negative value (red color) has a negative effect or decreases the prediction value. Please note that the interpretation for each observation can be different (this explanation has been taken from [this blog](https://algotech.netlify.app/blog/interpreting-black-box-regression-model-with-lime/), which you can visit for further details). \n\nWe give the interpretation of the first test observation as an example. The first subplot shows that a `price` of less than 100 results in purchasing a higher quantity than expected. Additionally, people between the ages of 40 and 55 were most likely to buy the seat, which are people who are not too young nor too old. However, in a typical scenario, we would generally expect younger people to buy car seats, but that's probably because of the high ages in our dataset (1st. quantile of age is around 40). If the price by the competitor (`CompPrice`) is also low, it'll impact the sales units badly. Once again, please note that this is specific to the first observation (i.e., the first subplot).\n\nThe next element is `Explanation Fit`. These values indicate how well LIME explains the model, similar to an R-Squared in linear regression. Here we see the explanation Fit only has values around 0.50-0.7 (50%-70%), which can be interpreted that LIME can only explain a little about our model (in some cases, like the 3rd sub-plot, this value is extremely low). You may choose not to trust the LIME output since it only has a low Explanation Fit.\n\n\n### Python\nWe'll be using provide a small demonstration on how this can be achieved in python. Please note the same logic for the \ninterpretation (and explanation) of the R version applies here, therefore, the code is shorter and there's no further comment provided for its output.\n\nWe use `lime` package in python (already installed in the lab `setup`). Then we can run our model in the same way as R:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom sklearn import svm\nfrom sklearn.pipeline import make_pipeline\nfrom lime import lime_tabular\nimport matplotlib.pyplot as plt\n\n# Assuming carseats_tr and carseats_te are already defined as pandas DataFrames\nX_train = carseats_train.drop(columns='Sales')\ny_train = carseats_train['Sales']\nX_test = carseats_test.drop(columns='Sales')\n\n# Create a support vector machine model\nsvm_caret_model = svm.LinearSVR(random_state=2022)\n\n# Train the model\nsvm_caret_model.fit(X_train, y_train);\n\n# Predict on a test instance\ntest_instance = X_test.iloc[0:4]\n\n# Create a lime explainer object for the SVM model\nlime_svm_explainer = lime_tabular.LimeTabularExplainer(X_train.values,\n                                                       feature_names=X_train.columns,\n                                                       class_names=['Sales'],\n                                                       mode='regression')\n\n# Explain a prediction using lime\nlime_svm_explanation = lime_svm_explainer.explain_instance(test_instance.values[0], svm_caret_model.predict, num_features=10)\n\n# Plot the features\n# plt.clf()\nlime_svm_explanation.as_pyplot_figure()\nplt.show()\n```\n\n::: {.cell-output-display}\n![](Ex_ML_VarImp_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n:::\n\n## Bonus: XGBoost\nTo give you an example for a classification problem, we can also train an XGBoost using the `xgboost` library:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(xgboost)\n\n# Load and prepare the data\ncarseats_df <- Carseats\ncarseats_df$High <- ifelse(carseats_df$Sales <= 8, \"yes\", \"no\")\ncarseats_df$High <- as.factor(carseats_df$High)\ncarseats_df$ShelveLoc <- as.factor(carseats_df$ShelveLoc)\ncarseats_df$Urban <- as.factor(carseats_df$Urban)\ncarseats_df$US <- as.factor(carseats_df$US)\n \n# Prepare the data for xgboost (as shown in the boosting excercises)\nxgb_data <- model.matrix(High ~ ., data = carseats_df)[,-1]\nxgb_label <- as.numeric(carseats_df$High) - 1\nxgb_dmatrix <- xgb.DMatrix(data = xgb_data, label = xgb_label)\n\n# Train a gradient boosting model\nset.seed(42)\ncarseats_xgb <- xgboost(data = xgb_dmatrix, nrounds = 100, objective = \"binary:logistic\", eval_metric = \"logloss\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]\ttrain-logloss:0.441670 \n[2]\ttrain-logloss:0.301386 \n[3]\ttrain-logloss:0.212602 \n[4]\ttrain-logloss:0.153032 \n[5]\ttrain-logloss:0.111713 \n[6]\ttrain-logloss:0.082449 \n[7]\ttrain-logloss:0.061431 \n[8]\ttrain-logloss:0.046187 \n[9]\ttrain-logloss:0.035048 \n[10]\ttrain-logloss:0.026860 \n[11]\ttrain-logloss:0.020809 \n[12]\ttrain-logloss:0.016314 \n[13]\ttrain-logloss:0.012954 \n[14]\ttrain-logloss:0.010426 \n[15]\ttrain-logloss:0.008509 \n[16]\ttrain-logloss:0.007042 \n[17]\ttrain-logloss:0.005908 \n[18]\ttrain-logloss:0.005021 \n[19]\ttrain-logloss:0.005013 \n[20]\ttrain-logloss:0.005008 \n[21]\ttrain-logloss:0.005005 \n[22]\ttrain-logloss:0.005003 \n[23]\ttrain-logloss:0.005002 \n[24]\ttrain-logloss:0.005001 \n[25]\ttrain-logloss:0.005000 \n[26]\ttrain-logloss:0.005000 \n[27]\ttrain-logloss:0.005000 \n[28]\ttrain-logloss:0.004999 \n[29]\ttrain-logloss:0.004999 \n[30]\ttrain-logloss:0.004999 \n[31]\ttrain-logloss:0.004999 \n[32]\ttrain-logloss:0.004999 \n[33]\ttrain-logloss:0.004999 \n[34]\ttrain-logloss:0.004999 \n[35]\ttrain-logloss:0.004999 \n[36]\ttrain-logloss:0.004999 \n[37]\ttrain-logloss:0.004999 \n[38]\ttrain-logloss:0.004999 \n[39]\ttrain-logloss:0.004999 \n[40]\ttrain-logloss:0.004999 \n[41]\ttrain-logloss:0.004999 \n[42]\ttrain-logloss:0.004999 \n[43]\ttrain-logloss:0.004999 \n[44]\ttrain-logloss:0.004999 \n[45]\ttrain-logloss:0.004999 \n[46]\ttrain-logloss:0.004999 \n[47]\ttrain-logloss:0.004999 \n[48]\ttrain-logloss:0.004999 \n[49]\ttrain-logloss:0.004999 \n[50]\ttrain-logloss:0.004999 \n[51]\ttrain-logloss:0.004999 \n[52]\ttrain-logloss:0.004999 \n[53]\ttrain-logloss:0.004999 \n[54]\ttrain-logloss:0.004999 \n[55]\ttrain-logloss:0.004999 \n[56]\ttrain-logloss:0.004999 \n[57]\ttrain-logloss:0.004999 \n[58]\ttrain-logloss:0.004999 \n[59]\ttrain-logloss:0.004999 \n[60]\ttrain-logloss:0.004999 \n[61]\ttrain-logloss:0.004999 \n[62]\ttrain-logloss:0.004999 \n[63]\ttrain-logloss:0.004999 \n[64]\ttrain-logloss:0.004999 \n[65]\ttrain-logloss:0.004999 \n[66]\ttrain-logloss:0.004999 \n[67]\ttrain-logloss:0.004999 \n[68]\ttrain-logloss:0.004999 \n[69]\ttrain-logloss:0.004999 \n[70]\ttrain-logloss:0.004999 \n[71]\ttrain-logloss:0.004999 \n[72]\ttrain-logloss:0.004999 \n[73]\ttrain-logloss:0.004999 \n[74]\ttrain-logloss:0.004999 \n[75]\ttrain-logloss:0.004999 \n[76]\ttrain-logloss:0.004999 \n[77]\ttrain-logloss:0.004999 \n[78]\ttrain-logloss:0.004999 \n[79]\ttrain-logloss:0.004999 \n[80]\ttrain-logloss:0.004999 \n[81]\ttrain-logloss:0.004999 \n[82]\ttrain-logloss:0.004999 \n[83]\ttrain-logloss:0.004999 \n[84]\ttrain-logloss:0.004999 \n[85]\ttrain-logloss:0.004999 \n[86]\ttrain-logloss:0.004999 \n[87]\ttrain-logloss:0.004999 \n[88]\ttrain-logloss:0.004999 \n[89]\ttrain-logloss:0.004999 \n[90]\ttrain-logloss:0.004999 \n[91]\ttrain-logloss:0.004999 \n[92]\ttrain-logloss:0.004999 \n[93]\ttrain-logloss:0.004999 \n[94]\ttrain-logloss:0.004999 \n[95]\ttrain-logloss:0.004999 \n[96]\ttrain-logloss:0.004999 \n[97]\ttrain-logloss:0.004999 \n[98]\ttrain-logloss:0.004999 \n[99]\ttrain-logloss:0.004999 \n[100]\ttrain-logloss:0.004999 \n```\n\n\n:::\n:::\n\nIdentify instances with predicted probabilities close to 1, 0, and 0.5:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# LIME explanations for a gradient boosting model\nxgb_preds <- predict(carseats_xgb, xgb_dmatrix)\n\nwhich.max(xgb_preds)\nwhich.min(xgb_preds)\nwhich.min(abs(xgb_preds - 0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4\n[1] 1\n[1] 1\n```\n\n\n:::\n:::\n\n\nGenerate LIME explanations for the selected instances:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# before making the prediction, we need to also one-hot encode the categorical variables\nto_explain <- data.frame(model.matrix(~.,data = carseats_df[c(120, 4, 60), -ncol(carseats_df)])[,-1])\n\n# we can finally run LIME on our results\ncarseats_lime_xgb <- lime(data.frame(xgb_data), carseats_xgb, bin_continuous = TRUE, quantile_bins = FALSE)\ncarseats_expl_xgb <- lime::explain(to_explain, carseats_lime_xgb, n_labels = 1, n_features = 10)\n```\n:::\n\n\nVisualize the LIME explanations\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_features(carseats_expl_xgb, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Ex_ML_VarImp_files/figure-html/unnamed-chunk-17-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWhat can you observe from these subplots?\n\n# Your turn\n\nAfter having done this for the training set, make a selection of the most important variables and run the model again. Would you go for this simpler model that has less features or the more complicated one? (hint: you can compute some scores to see whether dropping features would justify the performance drop).\n\nAs a final remark, the same kind of analysis can also be done for classification and different loss functions.\n",
    "supporting": [
      "Ex_ML_VarImp_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}