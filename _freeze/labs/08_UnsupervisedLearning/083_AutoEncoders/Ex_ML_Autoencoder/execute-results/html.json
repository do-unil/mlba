{
  "hash": "1668e52e534eaeb0c4de7a197f4989f1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Autoencoders\"\n---\n\n\n\n\n# Data preparation\n\nIn this series of exercises, we illustrate autoencoders on the `wine` data already used for clustering & PCA. We first load the data.\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwine <- read.csv(here::here(\"labs/data/Wine.csv\"))\nrow.names(wine) <- paste(\"W\", c(1:nrow(wine)), sep=\"\") \nwine[,-12] <- scale(wine[,-12])\n```\n:::\n\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nwine = pd.read_csv('../../data/Wine.csv')\nwine.index = [\"W\" + str(i) for i in range(1, len(wine)+1)]\nscaler = StandardScaler()\nwine.iloc[:, :-1] = scaler.fit_transform(wine.iloc[:, :-1])\n```\n:::\n\n\n:::\n\nNote that here the scaling of the variables is optional. Scaling the features can sometimes help with the autoencoder, especially when creating lower-dimensional representation of the data.\n\n# Dimension reduction\n\nWe will use `keras` to build an autoencoder for the `wine` dataset. Here, the aim is to represent the 11 wine variables in 2 dimensions. Please note that you can also 'increase' the dimension for some applications where you want to find interactions and interesting relationships between different variables for a supervised task (probably not unsupervised like this case).\n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(keras)\n\n# Define the autoencoder model\ninput_dim <- ncol(wine[,-12])\nencoding_dim <- 2\n\ninput_layer <- layer_input(shape = input_dim)\nencoder_layer <- layer_dense(units = encoding_dim, activation = 'relu')(input_layer)\n\ndecoder_layer <- layer_dense(units = input_dim, activation = 'linear')(encoder_layer)\n\nautoencoder <- keras_model(input_layer, decoder_layer)\nsummary(autoencoder)\n\n# Compile the autoencoder\nautoencoder %>% compile(optimizer = \"adam\", loss = \"mse\")\n\n# Train the autoencoder\nhistory <- autoencoder %>% fit(x = as.matrix(wine[,-12]), y = as.matrix(wine[,-12]), epochs = 500, batch_size = 32, shuffle = TRUE, verbose = 0)\n\n# Extract the encoder model\nencoder <- keras_model(inputs = input_layer, outputs = encoder_layer)\nencoded_wine <- encoder %>% predict(as.matrix(wine[,-12]))\nhead(encoded_wine)\n```\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model\n\n# Define the autoencoder model\ninput_dim = wine.shape[1] - 1\nencoding_dim = 2\n\ninput_layer = Input(shape=(input_dim,))\nencoder_layer = Dense(encoding_dim, activation='relu')(input_layer)\n\ndecoder_layer = Dense(input_dim, activation='linear')(encoder_layer)\n\nautoencoder = Model(input_layer, decoder_layer)\nautoencoder.summary()\n```\n\n```{.python .cell-code}\n# Compile the autoencoder\n\nautoencoder.compile(optimizer='adam', loss='mse')\n# Train the autoencoder\n\nhistory = autoencoder.fit(wine.iloc[:, :-1], wine.iloc[:, :-1], epochs=500, batch_size=32, shuffle=True, verbose=0)\n# Extract the encoder model\n\nencoder = Model(inputs=input_layer, outputs=encoder_layer)\nencoded_wine = encoder.predict(wine.iloc[:, :-1])\n```\n\n```{.python .cell-code}\nprint(encoded_wine[:5])\n```\n:::\n\n:::\n\nIt is important to remember that autoencoders can be more complex and include multiple layers in the encoder and decoder. In this example, we used a simple linear activation function and a single-layer architecture. The choice of the autoencoder architecture and the activation functions will depend on the specific problem and dataset at hand (hence the fine-tuning process).\n\n# Visualize the encoded dimension\n\nNow that we have trained our autoencoder and created a 2-dimensional representation of the data, we can visualize the results of the encoded dimension.\n\n::: panel-tabset\n\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nwine_2d <- data.frame(encoded_wine)\ncolnames(wine_2d) <- c(\"X\", \"Y\")\nwine_2d$Type <- wine$Type\n\nggplot(wine_2d, aes(x = X, y = Y)) +\n  geom_point(size = 2, alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Wine data reduced to 2D using Autoencoder\",\n       x = \"Dimension 1\",\n       y = \"Dimension 2\")\n```\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.clf()\nplt.figure(figsize=(10, 8))\nplt.scatter(encoded_wine[:, 0], encoded_wine[:, 1], s=50, alpha=0.8)\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\nplt.title('2D Representation of Wine Data using Autoencoders')\n\n# OPTIONAL: Add the wine index as labels (you can comment the code below)\nfor i in range(len(wine)):\n  plt.annotate(wine.index[i], (encoded_wine[i, 0], encoded_wine[i, 1]), fontsize=8)\n\nplt.show()\n```\n:::\n\n\n:::\n\nThe plot above shows the 2-dimensional representation of the wine data using the autoencoder. The autoencoder has effectively reduced the dimensionality of the data while preserving the structure and relationships between the samples.\n\n# Inferring missing values\n\nAutoencoders are not only used for dimensionality reduction, but they can also be used to infer missing values in a dataset. In this part of the exercise, we will create an autoencoder to reconstruct the original dataset and use it to infer missing values.\n\nLet's first divide the data into training and test sets. Then, we must simulate a scenario where some values are missing from our `wine` dataset.\n\n## Simulate missing values\nThis step is done only for demonstration since the `wine` data does not contain missing values, and we must artificially create them and place them in a dataframe called `wine_missing`. Applying this technique doesn't require this step since your dataset already should already contain the missing values.\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\nwine_missing <- wine\nwine_missing[sample(1:nrow(wine_missing), size = nrow(wine_missing)*0.2), sample(1:ncol(wine_missing), size = ncol(wine_missing)*0.2)] <- NA\n```\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport numpy as np\nimport random\nnp.random.seed(123)\n# Create a copy of the wine dataset and randomly remove 20% of the data\nwine_missing = wine.copy()\nix = [(row, col) for row in range(wine_missing.shape[0]) for col in range(wine_missing.shape[1])]\nfor row, col in random.sample(ix, int(round(.2*len(ix)))):\n    wine_missing.iat[row, col] = np.nan\n```\n:::\n\n:::\n\n## Train with complete data\nNow, we can use an autoencoder to infer these missing values. To do so, we can train the autoencoder with the complete data and then use it to infer the missing values.\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Re-define the autoencoder model\ninput_layer <- layer_input(shape = input_dim)\nencoder_layer <-\n  layer_dense(units = encoding_dim, activation = 'relu')(input_layer)\ndecoder_layer <-\n  layer_dense(units = input_dim, activation = 'linear')(encoder_layer)\nautoencoder <- keras_model(input_layer, decoder_layer)\nsummary(autoencoder)\nautoencoder %>% compile(optimizer = \"adam\", loss = \"mse\")\n\n# Train the autoencoder with the complete data\nhist <- \n  autoencoder %>% fit(\n  x = as.matrix(wine[, -12]),\n  y = as.matrix(wine[, -12]),\n  epochs = 300,\n  batch_size = 32,\n  shuffle = TRUE,\n  verbose = 0, #set it as `1` if you want to see the training messages\n  validation_split = 0.2,\n  callbacks = callback_early_stopping(\n    monitor = \"val_loss\",\n    patience = 5,\n    restore_best_weights = TRUE\n  ),\n)\n\n## uncomment if you want to see the training plot\n# plot(hist)\n\n# Replace missing values with the inferred values\nwine_missing_original <- wine_missing\n\n# replace the missing values with something before being able to make a prediction on it (model cannot re-construct NA directly)\nwine_missing[is.na(wine_missing)] <- 0  # Mask the missing values as 0\n\n# Use the autoencoder to infer the missing values\npredicted_wine <- autoencoder %>% predict(as.matrix(wine_missing[, -12]))\n```\n:::\n\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# to introduce early stopping\nfrom keras.callbacks import EarlyStopping\n\n# Re-define the autoencoder model\ninput_layer = Input(shape=(input_dim,))\nencoder_layer = Dense(encoding_dim, activation='relu')(input_layer)\ndecoder_layer = Dense(input_dim, activation='linear')(encoder_layer)\nautoencoder = Model(input_layer, decoder_layer)\nautoencoder.compile(optimizer='adam', loss='mse')\n\n# Train the autoencoder with the complete data\nhist = autoencoder.fit(wine.iloc[:, :-1], \n  wine.iloc[:, :-1], \n  epochs=300, \n  batch_size=32, \n  shuffle=True, \n  verbose=0, #set it as `1` if you want to see the training messages\n  validation_split=0.2, \n  callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)])\n\n# # uncomment if you want to see the training plot\n# plt.clf()\n# plt.plot(hist.history['loss'])\n# plt.plot(hist.history['val_loss'])\n# plt.title('Model loss')\n# plt.ylabel('Loss')\n# plt.xlabel('Epoch')\n# plt.legend(['Train', 'Val'], loc='upper right')\n# plt.show()\n\n# Replace missing values with the inferred values\nwine_missing_original = wine_missing.copy()\n\n# replace the missing values with something before being able to make a prediction on it (model cannot re-construct NA directly)\nwine_missing.fillna(0, inplace=True)  # Mask the missing values as 0\n\n# Use the autoencoder to infer the missing values\npredicted_wine = autoencoder.predict(wine_missing.iloc[:, :-1].values)\n```\n:::\n\n\n:::\n\n\n## Predict missing values\n\nFinally, we replace the missing values in the wine dataset with the inferred values from the autoencoder.\n\n::: panel-tabset\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwine_missing[is.na(wine_missing_original)] <- predicted_wine[is.na(wine_missing_original)]\n\nlibrary(dplyr)\n# see the old vs new values by filtering for the rows that contained an NA\nmissing_rows <- wine_missing_original %>% \n  mutate(row_index = row_number()) %>%\n  dplyr::filter_at(vars(dplyr::everything()),any_vars(is.na(.)))\n\n# you can see the new values that were re-constructed\nwine_missing_original[missing_rows$row_index,]\nwine_missing[missing_rows$row_index,]\n```\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# Identify the missing values in the original dataframe\nmissing_mask = wine_missing_original.iloc[:, :-1].isna()\n\n# Replace the missing values with the predicted ones\nfor i in range(wine_missing.shape[1] - 1):  # iterate over all columns except the last one\n    wine_missing.loc[missing_mask.iloc[:, i], wine_missing.columns[i]] = predicted_wine[missing_mask.iloc[:, i], i]\n\n# see the old vs new values by filtering for the rows that contained an NA\nmissing_rows = wine_missing_original[wine_missing_original.iloc[:, :-1].isna().any(axis=1)]\n\n# you can see the new values that were re-constructed\nprint(wine_missing_original.loc[missing_rows.index, :])\n```\n\n```{.python .cell-code}\nprint(wine_missing.loc[missing_rows.index, :])\n```\n:::\n\n\n:::\n\nNote how the dataset `wine_missing` doesn't contain any missing values. The missing values have been inferred by the autoencoder. This method can be a powerful tool to deal with missing data in machine learning projects. \n\n::: {.callout-important}\n\nThere are two essential things to note about using autoencoders for imputation:\n\n- Alternative to our approach, you can train with incomplete data and mask the \"NA\" in the training data with 0s (as shown for inference). In theory, you can put anything for the mask value, for example, the mean or median value for the missing inputs, and then try to recover those (although if you scale your data, 0 may work better in practice). Once the model is trained, you can recover the value for NA while masking the NAs again with 0s (as already implemented by us). In that case, you're pushing your model to predict 0 for the missing instances, which can sometimes be inappropriate.\n\n- Please note for auto-encoders to work well, you'll need a lot of observations. Additionally, you should always compare the performance of this technique against simply using mean and median values for the missing data. Another library in R commonly used for dealing with missing data is called `missForest`, which uses a random forest for imputation. If you need techniques to deal with missing data, feel free to check it out and make sure you understand how it works (it falls beyond the scope of this course).\n\n:::\n",
    "supporting": [
      "Ex_ML_Autoencoder_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}