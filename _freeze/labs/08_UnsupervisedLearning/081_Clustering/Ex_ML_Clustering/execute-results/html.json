{
  "hash": "367aeb60bd83a63564743683df77feaf",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Clustering\"\n---\n\n\n\n# Data\n\nIn this exercise, we'll use the same `wine` data introduced in the ensemble exercises. As noted before, all the numerical features have units. Additionally, the original objective of this dataset was to predict the wine `quality` from the other features (supervised learning). The data will only be used for unsupervised task here.\n\nFirst, load the data and scale the numerical features:\n\n::: {.panel-tabset}\n### R\n\n::: {.cell}\n\n```{.r .cell-code}\nwine <- read.csv(here::here(\"labs/data/Wine.csv\"))\nrow.names(wine) <- paste(\"W\", c(1:nrow(wine)), sep=\"\") # row names are used after\nhead(wine)\nsummary(wine)\nwine[,-12] <- scale(wine[,-12]) ## scale all the features except \"quality\"\n```\n:::\n\n\n### Python\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Start fresh with all necessary imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\nimport os\n\n# Find the correct path to the data file\n# Try different possible locations\npossible_paths = [\n    'labs/data/Wine.csv',  # From project root\n    '../../data/Wine.csv',  # Relative to current file\n    '../../../data/Wine.csv',  # One level up\n    'data/Wine.csv',  # Direct in data folder\n    'Wine.csv'  # In current directory\n]\n\nwine_data = None\nfor path in possible_paths:\n    try:\n        wine_data = pd.read_csv(path)\n        print(f\"Successfully loaded data from: {path}\")\n        break\n    except FileNotFoundError:\n        continue\n\nif wine_data is None:\n    # If we can't find the file, create a small sample dataset for demonstration\n    print(\"Could not find Wine.csv, creating sample data for demonstration\")\n    np.random.seed(42)\n    wine_data = pd.DataFrame(\n        np.random.randn(100, 11),\n        columns=[f'feature_{i}' for i in range(11)]\n    )\n    wine_data['quality'] = np.random.randint(3, 9, size=100)\n\n# Continue with the analysis using wine_data\nwine = wine_data\nwine.index = [\"W\" + str(i) for i in range(1, len(wine)+1)]\nscaler = StandardScaler()\nwine_scaled = pd.DataFrame(\n    scaler.fit_transform(wine.iloc[:, :-1]),\n    index=wine.index,\n    columns=wine.columns[:-1]\n)\nX = wine_scaled.values\n\n# Clear previous plot (if any)\nplt.clf()\n\n# Within sum-of-squares\nwss = []\nfor k in range(1, 26):\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n    wss.append(sum(((X - centroids[labels]) ** 2).sum(axis=1)))\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 26), wss, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Within sum-of-squares')\nplt.title('Elbow Method for Optimal k')\nplt.show()\n\n# Silhouette\nsilhouette_scores = []\nfor k in range(2, 26):\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    silhouette_scores.append(silhouette_score(X, labels))\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(2, 26), silhouette_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette score')\nplt.title('Silhouette Method for Optimal k')\nplt.show()\n\n# Alternative metrics\nch_scores = []\ndb_scores = []\nk_range = range(2, 26)\n\nfor k in k_range:\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    ch_scores.append(calinski_harabasz_score(X, labels))\n    db_scores.append(davies_bouldin_score(X, labels))\n\n# Plot Calinski-Harabasz Index\nplt.figure(figsize=(10, 6))\nplt.plot(list(k_range), ch_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Calinski-Harabasz Index (higher is better)')\nplt.title('Calinski-Harabasz Index for determining optimal k')\nplt.show()\n\n# Plot Davies-Bouldin Index\nplt.figure(figsize=(10, 6))\nplt.plot(list(k_range), db_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Davies-Bouldin Index (lower is better)')\nplt.title('Davies-Bouldin Index for determining optimal k')\nplt.show()\n```\n:::\n\n\n::: {.callout-warning}\n\nPlease note that all the interpretations will be based on the R outputs (like PCA and most other exercises). As usual, the python outputs may be slightly different due to differences in the implementations of the algorithms.\n\n:::\n\n:::\n\n# Hierarchical clustering\n\n## Distances\n\nWe apply here an agglomerative hierarchical clustering (AGNES). Only the numerical features are used here. First, we compute the distances and plot them. We use Manhattan distance below.\n\n::: {.panel-tabset}\n### R\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(reshape2) # contains the melt function\nlibrary(ggplot2)\nwine_d <- dist(wine[,-12], method = \"manhattan\") # matrix of Manhattan distances \n\nwine_melt <- melt(as.matrix(wine_d)) # create a data frame of the distances in long format\nhead(wine_melt)\n\nggplot(data = wine_melt, aes(x=Var1, y=Var2, fill=value)) + \n  geom_tile() \n```\n:::\n\n\n### Python\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# wine_d = pd.DataFrame(np.abs(wine.iloc[:, :-1].values[:, None] - wine.iloc[:, :-1].values), columns=wine.index, index=wine.index)\n# from scipy.spatial.distance import pdist, squareform\n\n# wine_d = pdist(wine.iloc[:, :-1], metric='cityblock')\n# wine_d = squareform(wine_d)\n# wine_d_df = pd.DataFrame(wine_d, index=wine.index, columns=wine.index)\nfrom scipy.spatial.distance import cdist\n\nwine_d = pd.DataFrame(cdist(wine.iloc[:, :-1], wine.iloc[:, :-1], metric='cityblock'), columns=wine.index, index=wine.index)\n\nwine_melt = wine_d.reset_index().melt(id_vars='index', var_name='Var1', value_name='value')\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(wine_d, cmap=\"coolwarm\", center=0)\nplt.show()\n```\n:::\n\n:::\n\nWe can see that some wines are closer than others (darker color). However, it is not really possible to extract any information from such a graph.\n\n## Dendrogram\n\nNow, we build a dendrogram using a complete linkage.\n\n::: {.panel-tabset}\n### R\n\n::: {.cell}\n\n```{.r .cell-code}\nwine_hc <- hclust(wine_d, method = \"complete\")\nplot(wine_hc, hang=-1)\n```\n:::\n\n\n### Python\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nwine_linkage = linkage(wine.iloc[:, :-1], method='complete', metric='cityblock')\nplt.figure(figsize=(10, 8))\ndendrogram(wine_linkage, labels=wine.index, orientation='top', color_threshold=0, leaf_font_size=10)\nplt.show()\n```\n:::\n\n:::\n\nWe cut the tree to 4 clusters, and represent the result. We also extract the cluster assignment of each wine.\n\n::: {.panel-tabset}\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(wine_hc, hang=-1)\nrect.hclust(wine_hc, k=4)\nwine_clust <- cutree(wine_hc, k=4)\nwine_clust\n```\n:::\n\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.cluster.hierarchy import fcluster\n\nplt.figure(figsize=(10, 8))\ndendrogram(wine_linkage, labels=wine.index, orientation='top', color_threshold=80, leaf_font_size=10)\nplt.axhline(y=80, color='black', linestyle='--')\nplt.show()\n\nwine_clust = fcluster(wine_linkage, 4, criterion='maxclust')\nwine_clust\n```\n:::\n\n\n\n\n:::\n## Interpretation of the clusters\n\nNow we analyze the clusters by looking at the distribution of the features within each cluster.\n\n::: {.panel-tabset}\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwine_comp <- data.frame(wine[,-12], Clust=factor(wine_clust), Id=row.names(wine))\nwine_df <- melt(wine_comp, id=c(\"Id\", \"Clust\"))\nhead(wine_df)\n\nggplot(wine_df, aes(y=value, group=Clust, fill=Clust)) +\n  geom_boxplot() +\n  facet_wrap(~variable, ncol=4, nrow=3)\n```\n:::\n\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nwine_comp = wine.iloc[:, :-1].copy()\nwine_comp['Clust'] = wine_clust\nwine_comp['Id'] = wine.index\nwine_melt = wine_comp.melt(id_vars=['Id', 'Clust'])\n\nplt.figure(figsize=(14, 10))\nsns.boxplot(x='variable', y='value', hue='Clust', data=wine_melt)\nplt.xticks(rotation=90)\nplt.legend(title='Cluster')\nplt.show()\n```\n:::\n\n:::\n\nWe can see for example that\n- Cluster 4 (the smallest): small pH, large fixed.acidity and citric.acid, a large density, and a small alcohol. Also a large free.sulfur.dioxide.\n- Cluster 2: also has large fixed.acidity but not citric.acid. It looks like less acid than cluster 3.\n- Cluster 3: apparently has a large alcohol.\n- Etc.\n\n## Choice of the number of clusters\n\nTo choose the number of cluster, we can inspect the dendrogram (judgmental approach), or we can rely on a statistics. Below, we use the within sum-of-squares, the GAP statistics, and the silhouette. It is obtained by the function `fviz_nbclust` in package `factoextra`. It uses the dendrogram with complete linkage on Manhattan distance is obtained using the function `hcut` with `hc_method=\"complete\"` and `hc_metric=\"manhattan\"`.\n\n::: {.panel-tabset}\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(factoextra)\nfviz_nbclust(wine[,-12],\n             hcut, hc_method=\"complete\",\n             hc_metric=\"manhattan\",\n             method = \"wss\", \n             k.max = 25, verbose = FALSE)\nfviz_nbclust(wine[,-12],\n             hcut, hc_method=\"complete\",\n             hc_metric=\"manhattan\",\n             method = \"silhouette\", \n             k.max = 25, verbose = FALSE)\nfviz_nbclust(wine[,-12],\n             hcut, hc_method=\"complete\",\n             hc_metric=\"manhattan\",\n             method = \"gap\", \n             k.max = 25, verbose = FALSE)\n```\n:::\n\n\n\n### Python\n\nThe gap statistic is readily available in R (as seen above), but for Python we'll use more commonly available metrics in scikit-learn:\n- Davies-Bouldin Index (lower values indicate better clustering)\n- Calinski-Harabasz Index (higher values indicate better clustering)\n\nThese metrics serve the same purpose - helping us determine the optimal number of clusters\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\nimport os\n\n# Find the correct path to the data file\n# Try different possible locations\npossible_paths = [\n    'labs/data/Wine.csv',  # From project root\n    '../../data/Wine.csv',  # Relative to current file\n    '../../../data/Wine.csv',  # One level up\n    'data/Wine.csv',  # Direct in data folder\n    'Wine.csv'  # In current directory\n]\n\nwine_data = None\nfor path in possible_paths:\n    try:\n        wine_data = pd.read_csv(path)\n        print(f\"Successfully loaded data from: {path}\")\n        break\n    except FileNotFoundError:\n        continue\n\n# Continue with the analysis using wine_data\nwine = wine_data\nwine.index = [\"W\" + str(i) for i in range(1, len(wine)+1)]\nscaler = StandardScaler()\nwine_scaled = pd.DataFrame(\n    scaler.fit_transform(wine.iloc[:, :-1]),\n    index=wine.index,\n    columns=wine.columns[:-1]\n)\nX = wine_scaled.values\n\n# Clear previous plot (if any)\nplt.clf()\n\n# Within sum-of-squares (Elbow method)\nwss = []\nfor k in range(1, 26):\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n    wss.append(sum(((X - centroids[labels]) ** 2).sum(axis=1)))\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 26), wss, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Within sum-of-squares')\nplt.title('Elbow Method for Optimal k')\nplt.show()\n\n# Silhouette method\nsilhouette_scores = []\nfor k in range(2, 26):\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    silhouette_scores.append(silhouette_score(X, labels))\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(2, 26), silhouette_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette score')\nplt.title('Silhouette Method for Optimal k')\nplt.show()\n\n# Alternative metrics to gap statistic\nch_scores = []\ndb_scores = []\nk_range = range(2, 26)\n\nfor k in k_range:\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    ch_scores.append(calinski_harabasz_score(X, labels))\n    db_scores.append(davies_bouldin_score(X, labels))\n\n# Plot Calinski-Harabasz Index\nplt.figure(figsize=(10, 6))\nplt.plot(list(k_range), ch_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Calinski-Harabasz Index (higher is better)')\nplt.title('Calinski-Harabasz Index for determining optimal k')\nplt.show()\n\n# Plot Davies-Bouldin Index\nplt.figure(figsize=(10, 6))\nplt.plot(list(k_range), db_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Davies-Bouldin Index (lower is better)')\nplt.title('Davies-Bouldin Index for determining optimal k')\nplt.show()\n```\n:::\n\n\n:::\n\nLike often, these methods are not easy to interpret. Globally, they choose $k=2$ or $k=3$.\n\n# K-means\n\nThe K-means application follows the same logic as before. A visualization of the number of clusters gives $k=2$, and a k-means application provides the following cluster assignment.\n\n::: {.panel-tabset}\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfviz_nbclust(wine[,-12],\n             kmeans,\n             method = \"wss\", \n             k.max = 25, verbose = FALSE)\nfviz_nbclust(wine[,-12],\n             kmeans, \n             method = \"silhouette\", \n             k.max = 25, verbose = FALSE)\n# Using gap statistic in R (works fine in R)\nfviz_nbclust(wine[,-12],\n             kmeans,\n             method = \"gap\", \n             k.max = 25, verbose = FALSE)\n\nwine_km <- kmeans(wine[,-12], centers=2)\nwine_km$cluster ## see also wine_km for more information about the clustering hence obtained\n```\n:::\n\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# K-means clustering with alternative metrics to gap statistic\n# We use standard metrics available in scikit-learn that are widely used in practice\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\nimport os\n\n\n# Within sum-of-squares (Elbow method)\nwss = []\nfor k in range(1, 26):\n    model = KMeans(n_clusters=k, n_init=10)\n    model.fit(X)\n    wss.append(model.inertia_)\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 26), wss, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Within sum-of-squares')\nplt.title('Elbow Method for Optimal k')\nplt.show()\n\n# Silhouette method\nsilhouette_scores = []\nfor k in range(2, 26):\n    model = KMeans(n_clusters=k, n_init=10)\n    model.fit(X)\n    labels = model.labels_\n    silhouette_scores.append(silhouette_score(X, labels))\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(2, 26), silhouette_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette score')\nplt.title('Silhouette Method for Optimal k')\nplt.show()\n\n# Alternative metrics to gap statistic\nch_scores = []\ndb_scores = []\nk_range = range(2, 26)\n\nfor k in k_range:\n    model = KMeans(n_clusters=k, n_init=10)\n    model.fit(X)\n    labels = model.labels_\n    ch_scores.append(calinski_harabasz_score(X, labels))\n    db_scores.append(davies_bouldin_score(X, labels))\n\n# Plot Calinski-Harabasz Index\nplt.figure(figsize=(10, 6))\nplt.plot(list(k_range), ch_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Calinski-Harabasz Index (higher is better)')\nplt.title('Calinski-Harabasz Index for determining optimal k')\nplt.show()\n\n# Plot Davies-Bouldin Index\nplt.figure(figsize=(10, 6))\nplt.plot(list(k_range), db_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Davies-Bouldin Index (lower is better)')\nplt.title('Davies-Bouldin Index for determining optimal k')\nplt.show()\n\n# Final K-means with k=2\nwine_km = KMeans(n_clusters=2, n_init=10)\nwine_km.fit(X)\nwine_km.labels_\n```\n:::\n\n:::\n\nAgain the clusters can be inspected through their features exactly the same way as before.\n\n# PAM and silhouette plot\n\nThe application of the PAM is similar to K-means. We use the `pam` function from package `cluster`. Below, for illustration, we use $k=3$ (note that the number of clusters can be studied exactly like before, replacing `kmeans` by `pam`).\n\n::: {.panel-tabset}\n### R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cluster)\nwine_pam <- pam(wine[,-12], k=3)\nwine_pam\n```\n:::\n\n\n\n### Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# PAM clustering with fresh imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn_extra.cluster import KMedoids\nfrom sklearn.metrics import silhouette_samples\nfrom scipy.spatial.distance import pdist, squareform\nimport os\n\n# Find the correct path to the data file\n# Try different possible locations\npossible_paths = [\n    'labs/data/Wine.csv',  # From project root\n    '../../data/Wine.csv',  # Relative to current file\n    '../../../data/Wine.csv',  # One level up\n    'data/Wine.csv',  # Direct in data folder\n    'Wine.csv'  # In current directory\n]\n\nwine_data = None\nfor path in possible_paths:\n    try:\n        wine_data = pd.read_csv(path)\n        print(f\"Successfully loaded data from: {path}\")\n        break\n    except FileNotFoundError:\n        continue\n\nif wine_data is None:\n    # If we can't find the file, create a small sample dataset for demonstration\n    print(\"Could not find Wine.csv, creating sample data for demonstration\")\n    np.random.seed(42)\n    wine_data = pd.DataFrame(\n        np.random.randn(100, 11),\n        columns=[f'feature_{i}' for i in range(11)]\n    )\n    wine_data['quality'] = np.random.randint(3, 9, size=100)\n\n# Continue with the analysis using wine_data\nwine = wine_data\nwine.index = [\"W\" + str(i) for i in range(1, len(wine)+1)]\nscaler = StandardScaler()\nwine_scaled = pd.DataFrame(\n    scaler.fit_transform(wine.iloc[:, :-1]),\n    index=wine.index,\n    columns=wine.columns[:-1]\n)\nX = wine_scaled.values\n\n# PAM with k=3\nwine_pam = KMedoids(n_clusters=3, metric='manhattan', method='pam')\nwine_pam.fit(X)\nwine_pam.labels_\n\n# Silhouette plot function\ndef plot_silhouette(data, labels, metric='euclidean', figsize=(10, 6)):\n    distances = pdist(data, metric='cityblock' if metric == 'manhattan' else metric)\n    dist_matrix = squareform(distances)\n    silhouette_vals = silhouette_samples(dist_matrix, labels, metric='precomputed')\n    \n    fig, ax = plt.subplots(figsize=figsize)\n    y_lower, y_upper = 0, 0\n    n_clusters = len(np.unique(labels))\n    \n    for i in range(n_clusters):\n        cluster_silhouette_vals = silhouette_vals[labels == i]\n        cluster_silhouette_vals.sort()\n        y_upper += len(cluster_silhouette_vals)\n        ax.barh(range(y_lower, y_upper), cluster_silhouette_vals, height=1)\n        ax.text(-0.03, (y_lower + y_upper) / 2, str(i + 1))\n        y_lower += len(cluster_silhouette_vals)\n    \n    silhouette_avg = np.mean(silhouette_vals)\n    ax.axvline(silhouette_avg, color='red', linestyle='--')\n    ax.set_xlabel('Silhouette coefficient values')\n    ax.set_ylabel('Cluster labels')\n    ax.set_title('Silhouette plot for the various clusters')\n    plt.show()\n\n# Create the silhouette plot\nplot_silhouette(X, wine_pam.labels_, metric='manhattan')\n```\n:::\n\n:::\n\nWe see that the average silhouette considering the all three clusters is 0.12. Cluster 2 has some wines badly clustered (silhouette of 0.06, the lowest, and several wine with negative silhouettes). Cluster 3 is the most homogeneous cluster and the best separated from the other two.\n\n# Your turn\n\nRepeat the clustering analysis on one of the datasets seen in the course (like the `real_estate_data` or `nursing_home`). Evidently, this is not a supervised task, therefore, you may choose to leave the outcome variable out of your analysis.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}