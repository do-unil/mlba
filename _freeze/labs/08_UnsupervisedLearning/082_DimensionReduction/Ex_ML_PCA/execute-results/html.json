{
  "hash": "dee86b1e34950d6c928557d87da27f5b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Principal Component Analysis\"\n---\n\n\n\n\n# Data preparation\n\nIn this series of exercises, we illustrate PCA on the wine data already used for clustering. We first load the data.\n\n::: panel-tabset\n### R\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwine <- read.csv(here::here(\"labs/data/Wine.csv\"))\nrow.names(wine) <- paste(\"W\", c(1:nrow(wine)), sep=\"\") \nwine[,-12] <- scale(wine[,-12])\n```\n:::\n\n\n### Python\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nwine = pd.read_csv('../../data/Wine.csv')\nwine.index = [\"W\" + str(i) for i in range(1, len(wine)+1)]\nscaler = StandardScaler()\nwine.iloc[:, :-1] = scaler.fit_transform(wine.iloc[:, :-1])\n```\n:::\n\n:::\n\nNote that here the scaling of the variables is optional. The PCA can be applied on the correlation matrix which is equivalent to use scaled features. We could alternatively use unscaled data. The results would of course be different and dependent on the scales themselves. That choice depends on the practical application.\n\n# Principal Component Analysis\n\nBefore to run the PCA, it is sometimes good to represent the correlations between the features.\n\n::: panel-tabset\n### R\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggcorrplot)\nggcorrplot(cor(wine[,-12]))\n```\n:::\n\n\n### Python\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(wine.iloc[:, :-1].corr(), annot=True, cmap=\"coolwarm\", center=0)\nplt.show()\n```\n:::\n\n::: {.callout-warning}\n\nPlease note that all the interpretations will be based on the R outputs. The python outputs can be slightly different; however, these differences have been explained wherever possible.\n\n:::\n\n:::\n\nWe see that some features are (negatively or positively) correlated, like `alcohol` and `density`. This means that they bring to some extend the same information to the data. They could thus be combined in a single feature. This is in fact what PCA is doing.\n\nWe now compute the principal components. There exist several functions in `R` for this. We use `PCA` available in package `FactoMineR`. It is completed by `factoextra` that provides nice visualizations.\n\n::: panel-tabset\n### R\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(FactoMineR)\nlibrary(factoextra)\nwine_pca <- PCA(wine[,-12], ncp = 11, graph = FALSE)\nwine_pca\n```\n:::\n\n\n### Python\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=11)\nwine_pca_result = pca.fit(wine.iloc[:, :-1])\nwine_pca = pd.DataFrame(pca.transform(wine.iloc[:, :-1]))\nwine_pca\n```\n:::\n\n:::\n\nThe object `wine_pca` contains all the PCA results. We will analyze them now.\n\nNote: we require `ncp=11` because there can be 11 principal components (same as features), and that we want to keep the results for all the principal components. The results will not change if we set `ncp=5` for example. It is just that only 5 PCs will be kept in the `R` object.\n\n## Circle of correlations and interpretation\n\nTo produces a circle of correlations:\n\n::: panel-tabset\n### R\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfviz_pca_var(wine_pca)\n```\n:::\n\n\n### Python\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfeatures = wine.columns[:-1]\nloading_matrix = pd.DataFrame(pca.components_.T, columns=[f\"PC{i+1}\" for i in range(11)], index=features)\nloading_matrix = loading_matrix.iloc[:, :2]\n\n# Normalize the PCA loadings\nloading_matrix_normalized = loading_matrix / np.sqrt(np.sum(loading_matrix**2, axis=0))\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.scatter(loading_matrix_normalized[\"PC1\"], loading_matrix_normalized[\"PC2\"], color=\"r\", marker=\"o\", s=100)\n\nfor i, feature in enumerate(loading_matrix_normalized.index):\n    ax.arrow(0, 0, loading_matrix_normalized.loc[feature, \"PC1\"], loading_matrix_normalized.loc[feature, \"PC2\"], color=\"r\", alpha=0.8)\n    ax.text(loading_matrix_normalized.loc[feature, \"PC1\"], loading_matrix_normalized.loc[feature, \"PC2\"], feature, color=\"black\", fontsize=12)\n\nax.axhline(0, color=\"black\", linewidth=1)\nax.axvline(0, color=\"black\", linewidth=1)\nax.set_xlim(-1.1, 1.1)\n```\n\n```{.python .cell-code}\nax.set_ylim(-1.1, 1.1)\n```\n\n```{.python .cell-code}\ncircle = plt.Circle((0, 0), 1, color='gray', fill=False, linestyle=\"--\", linewidth=1)\nax.add_artist(circle)\n\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.set_title(\"Circle of Correlations\")\nax.grid()\nplt.show()\n```\n:::\n\n::: {.callout-note}\n\nPlease do note for this plot:\n\n- Values on the x-axis have been normalized to be between -1 and 1. For the rest of the python plots, we'll not apply this.\n\n- To get the same results as R, the coordinates of PC2 should be flipped. The results appear mirrored because PCA is sensitive to the orientation of the data. The PCA algorithm calculates eigenvectors as the principal components, and eigenvectors can have either positive or negative signs. This means that the orientation of the principal components can vary depending on the implementation of PCA in different libraries. You can get the same results as R with the code below:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Normalize the PCA loadings\nloading_matrix_normalized = loading_matrix / np.sqrt(np.sum(loading_matrix**2, axis=0))\n\n# Add this if you want the same results\nloading_matrix_normalized[\"PC2\"] = -loading_matrix_normalized[\"PC2\"]\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.scatter(loading_matrix_normalized[\"PC1\"], loading_matrix_normalized[\"PC2\"], color=\"r\", marker=\"o\", s=100)\n\nfor i, feature in enumerate(loading_matrix_normalized.index):\n    ax.arrow(0, 0, loading_matrix_normalized.loc[feature, \"PC1\"], loading_matrix_normalized.loc[feature, \"PC2\"], color=\"r\", alpha=0.8)\n    ax.text(loading_matrix_normalized.loc[feature, \"PC1\"], loading_matrix_normalized.loc[feature, \"PC2\"], feature, color=\"black\", fontsize=12)\n\nax.axhline(0, color=\"black\", linewidth=1)\nax.axvline(0, color=\"black\", linewidth=1)\nax.set_xlim(-1.1, 1.1)\n```\n\n```{.python .cell-code}\nax.set_ylim(-1.1, 1.1)\n```\n\n```{.python .cell-code}\ncircle = plt.Circle((0, 0), 1, color='gray', fill=False, linestyle=\"--\", linewidth=1)\nax.add_artist(circle)\n\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.set_title(\"Circle of Correlations\")\nax.grid()\nplt.show()\n```\n:::\n\n:::\n\n:::\n\nThis is for the two first principal components. We see\n\n-   PC1 explains $31.4\\%$ of the variance of the data, PC2 explains $15.3\\%$. In total, $46.7\\%$ of the variance of the data is explained by these two components.\n-   PC1 is positively correlated with `density`, negatively with `alcohol`. Which confirms that these two features are negatively correlated. It is also positively correlated with `residual.sugar`.\n-   PC2 is positively correlated with `pH`, negatively with `fixed.acidity` and, a little less, with `citric.acid`.\n-   Features with shorts arrows are not explained here: `volatile.acidity`, `sulphates`, etc.\n\nTo even better interpret the dimensions, we can extract the contributions of each features in the dimension. Below, for PC1.\n\n::: panel-tabset\n### R\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfviz_contrib(wine_pca, choice = \"var\", axes = 1)\n```\n:::\n\n\n### Python\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nexplained_variance = pca.explained_variance_ratio_\nsquare_loading_matrix = loading_matrix**2\ncontributions = square_loading_matrix * 100 / explained_variance[:2]\n\ncontributions = contributions[\"PC1\"].sort_values(ascending=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=contributions.values, y=contributions.index, palette=\"viridis\")\nplt.xlabel(\"Contribution (%)\")\nplt.ylabel(\"Features\")\nplt.title(\"Feature Contributions for PC1\")\nplt.show()\n```\n:::\n\n:::\n\nWe recover our conclusions from the circle (for PC1).\n\n## Map of the individual and biplot\n\nWe can represent the wines in the (PC1,PC2) map. To better interpret the map, we add on it the correlation circle: a biplot.\n\n::: panel-tabset\n\n### R\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## fviz_pca_ind(wine_pca) ## only the individuals\nfviz_pca_biplot(wine_pca) ## biplot\n```\n:::\n\n### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the scores for the first two principal components\nscores = wine_pca.iloc[:, :2]\n\n# Define the loadings for the first two principal components\nloadings = pca.components_.T[:, :2]\n\n# Scale the loadings by the square root of the variance\nloadings_scaled = loadings * np.sqrt(pca.explained_variance_[:2])\n\n# Calculate the scaling factor for the arrows\narrow_max = 0.9 * np.max(np.max(np.abs(scores)))\nscale_factor = arrow_max / np.max(np.abs(loadings_scaled))\n\n# Create a scatter plot of the scores\nplt.figure(figsize=(10, 8))\nplt.scatter(scores.iloc[:, 0], scores.iloc[:, 1], s=50, alpha=0.8)\n\n# Add arrows for each variable's loadings\nfor i, variable in enumerate(wine.columns[:-1]):\n    plt.arrow(0, 0, loadings_scaled[i, 0]*scale_factor, loadings_scaled[i, 1]*scale_factor, color='r', alpha=0.8)\n    plt.text(loadings_scaled[i, 0]*scale_factor, loadings_scaled[i, 1]*scale_factor, variable, color='black', fontsize=12)\n\n# Add axis labels and title\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Biplot of Wine Dataset')\n\n# Add grid lines\nplt.grid()\n\n# Show the plot\nplt.show()\n```\n:::\n\n\n:::\n\nIt's a bit difficult to see all the patterns, but for instance\n\n-   Wine 168 has a large `fixed.acidity` and a low `pH`.\n-   Wine 195 has a large `alcohol` and a low `density`.\n-   etc.\n\nWhen we say \"large\" or \"low\", it is not in absolute value but relative to the data set, i.e., \"larger than the average\"; the average being at the center of the graph (PC1=0, PC2=0).\n\n## How many dimensions\n\nFor graphical representation one a single graph, we need to keep only two PCs. But if we use it to reduce the dimension of our data set, or if we want to represent the data on several graphs, then we need to know how many components are needed to reach a certain level of variance. This can be achieved by looking at the eigenvalues (*screeplot*).\n\n::: panel-tabset\n### R\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfviz_eig(wine_pca, addlabels = TRUE, ncp=11)\n```\n:::\n\n\n### Python\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 12), explained_variance * 100, 'o-')\nplt.xticks(range(1, 12), [f\"PC{i}\" for i in range(1, 12)])\n```\n\n```{.python .cell-code}\nplt.xlabel(\"Principal Components\")\nplt.ylabel(\"Explained Variance (%)\")\nplt.title(\"Scree Plot\")\nplt.grid()\nplt.show()\n```\n:::\n\n:::\n\nIf we want to achieve $75\\%$ of representation of the data (i.e., of the variance of the data), we need 5 dimensions. This means that the three biplots below represent $>75\\%$ of the data (in fact $84.6\\%$).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(gridExtra)\np1 <- fviz_pca_biplot(wine_pca, axes = 1:2) \np2 <- fviz_pca_biplot(wine_pca, axes = 3:4) \np3 <- fviz_pca_biplot(wine_pca, axes = 5:6) \ngrid.arrange(p1, p2, p3, nrow = 2, ncol=2)\n```\n:::\n\n\n# Using PCA to represent clustering results\n\nWe now combine clustering and PCA: we make clusters and represent them on the map of the individuals.\n\nFirst, we make a clustering (below $k=4$ for the example). Then, we use the group (as factors) to color the individuals in the biplot.\n\n::: panel-tabset\n### R\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwine_hc <- hclust(dist(wine[,-12], method = \"manhattan\"))\nwine_clust <- cutree(wine_hc, k = 4)\nfviz_pca_biplot(wine_pca,\n             col.ind = factor(wine_clust))\n```\n:::\n\n\n### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom sklearn.cluster import AgglomerativeClustering\nfrom matplotlib.cm import get_cmap\nfrom matplotlib.patches import Patch\n\nclustering = AgglomerativeClustering(n_clusters=4, metric='manhattan', linkage='average')\nwine_clust = clustering.fit_predict(wine.iloc[:, :-1])\n\ncmap = get_cmap(\"viridis\", 4)\ncolors = cmap(np.array(wine_clust) / 3)\n\n# Define legend labels based on cluster number\nlabels = ['Cluster {}'.format(i+1) for i in range(4)]\n\n# Rescale the loading matrix\nloading_matrix_rescaled = loading_matrix.div(loading_matrix.std())\n\nplt.figure(figsize=(10, 10))\nplt.scatter(wine_pca[0], wine_pca[1], c=colors, marker=\"o\", s=100, alpha=0.7)\n\nfor i, feature in enumerate(loading_matrix_rescaled.index):\n    plt.arrow(0, 0, loading_matrix_rescaled.loc[feature, \"PC1\"], loading_matrix_rescaled.loc[feature, \"PC2\"], color=\"r\", alpha=0.8)\n    plt.text(loading_matrix_rescaled.loc[feature, \"PC1\"], loading_matrix_rescaled.loc[feature, \"PC2\"], feature, color=\"black\", fontsize=12)\n\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.title(\"PCA Biplot with Clusters\")\nplt.grid()\n\n# Create legend patches and group them in a Legend instance\nlegend_patches = [Patch(color=cmap(i/3), label=labels[i]) for i in range(4)]\nplt.legend(handles=legend_patches, title='', loc='upper right', bbox_to_anchor=(1.0, 1.0))\n\nplt.show()\n```\n:::\n\n\n:::\n\nWe can see that the clusters are almost separated by the dimensions:\n\n-   Cluster 1: larger PC1 and PC2. That is, large pH (low acidity) and large density, residual sugar (low alcohol), etc.\n-   Cluster 3: larger PC1 and smaller PC2: That is, larger alcohol (lower density and sugar) and large pH (low acidity).\n-   Cluster 2 is fuzzier. Larger acidity (lower pH) and larger alcohol.\n-   Cluster 4 is apparently linked to larger citric acidity.\n\n",
    "supporting": [
      "Ex_ML_PCA_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}