{
  "hash": "f16553f5cfe27113cf334b8383b30878",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Ensemble Methods\"\n---\n\n\n\n\n# Random Forest\n\nIn this part of the lab, we will look at how the `randomForest` library (alternative to `ranger`) can be applied for classification and regression tasks. At the very end, please feel free to apply these techniques to one of your favorite datasets seen in class (classification or regression).\n\n::: {.callout-note}\n## Hyperparameters of RF \n\nR (using the `randomForest` library): \n\n1. `ntree`: The number of trees in the forest (equivalent to `n_estimators` in python). \n2. `mtry`: The number of features to consider when looking for the best split. (similar to `max_features` in python)\n3. `max.depth`: The maximum depth of each tree. \n4. `nodesize`: The minimum number of samples required to split an internal node (equivalent to `min_samples_split` in python).\n\nPython (using the `sklearn` library): \n\n1. `n_estimators`: The number of trees in the forest. \n2. `max_features`: The number of features to consider when looking for the best split. \n3. `max_depth`: The maximum depth of each tree. \n4. `min_samples_split`: The minimum number of samples required to split an internal node. \n5. `min_samples_leaf`: The minimum number of samples required to be at a leaf node.\n:::\n\n:::{.callout-tip}\n## Few tips on RF hyperparameters\n\nA few (among many) tips for finding the ideal hyperparameters for RF:\n\n1. `ntree` (R) / `n_estimators` (python): Use a large number of trees in the forest to improve model performance, but be aware of the increased computation time. The default value is usually a good starting point. \n2. `mtry` (R) / `max_features` (python): Experiment with different values, usually starting with the default (square root of the number of features for classification or one-third of the number of features for regression). Increasing this value may improve model performance but can also increase computation time. \n3. `max.depth`: Control the depth of each tree to manage overfitting. Deeper trees capture more complex patterns but can lead to overfitting. Experiment with different values, keeping in mind that a shallower tree can be more interpretable and less prone to overfitting. \n4. `nodesize` (R) / `min_samples_split` (python): Increasing this value can help reduce overfitting, but setting it too high might lead to underfitting. Experiment with different values to find the optimal balance.\n:::\n\n## Classification\n\n### Data preparation for Random Forest\n\nLoad the library `randomForest` in R. Then, load the `wine` data set. This dataset is about white wine quality (in fact Portuguese vinho verde). The data contains 11 numerical features and 1 factor variable:\n\n-   `fixed.acidity`\n-   `volatile.acidity`\n-   `citric.acid`\n-   `residual.sugar`\n-   `chlorides`\n-   `free.sulfur.dioxide`\n-   `total.sulfur.dioxide`\n-   `density`\n-   `pH`\n-   `sulphates`\n-   `alcohol`\n-   `quality: Good/Bad`\n\nAll the numerical features have units. The data source can be found [here](https://archive.ics.uci.edu/ml/datasets/wine+quality). For simplicity, only an extraction of 200 wines are used in this exercise. Note that in the original data set, the `quality` is a score (0 to 10) that was turned as factor here for the exercise (Bad: 0 to 5, Good: 6 to 10). Also, note that in the data source, the objective is to predict the quality from the other features (supervised learning).\n\nAs mentioned, the outcome variable used for this dataset is the wine `quality`. We should first coerce the classes as factors. Then, we make the training/test set random split with a 75/25 scheme.\n\n::: panel-tabset\n#### R\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(randomForest)\nwine <- read.csv(here::here(\"data/Wine.csv\"))\nwine$quality <- as.factor(wine$quality)\n\n# define a function to get the splitting index (training and testing) of a given dataset\nget_split_index <- function(dataset, train_proportion = 0.75) {\n  set.seed(123)\n  index <-\n    sample(\n      x = 1:2,\n      size = nrow(dataset),\n      replace = TRUE,\n      prob = c(train_proportion, 1 - train_proportion)\n    )\n  return(index)\n}\n\nwine_index <- get_split_index(wine)\nwine_tr <- wine[wine_index == 1, ]\nwine_te <- wine[wine_index == 2, ]\n```\n:::\n\n\n#### Python\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Load the course python environment as usual with a r code chunks.\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n```\n:::\n\n\nSimilar to the previous labs, in python, we can use the usual `sklearn` library to do all our modelling. Please note that we will load the data again in python to make the demo easier. Additionally, we'll load all the necessary libraries for this lab in this code chunk.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# we first move up one directory to achieve relative paths\nwine = pd.read_csv('../data/Wine.csv')\n\nwine['quality'] = wine['quality'].astype('category')\n\n# Split wine dataset into train and test\ntrain_wine, test_wine = train_test_split(wine, test_size=0.25, random_state=123)\n```\n:::\n\n\n:::\n\n**Note:** Here, we have written a function to make the split as we will also need to apply it also for another dataset in the regression part.\n\n### Training and testing the model\n\nFit a random forest on the train set. The target is the `taste` variable that we want to predict. Specify for the number of trees `ntree=1000` (by default, the function selects $500$ trees). Remember to exclude `quality` in the predictors of the formula. Also, use the option `importance=TRUE`, we will need it afterward. Then test the model by computing the accuracy on the test set. You may use `confusionMatrix` from `caret`.\n\n::: panel-tabset\n\n#### R\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nwine_rf <- randomForest(quality~., data=wine_tr, ntree=1000, importance=TRUE)\nwine.pred_rf <- predict(wine_rf, newdata=wine_te)\n\nlibrary(caret)\nconfusionMatrix(data=wine.pred_rf, reference = wine_te$quality)\n```\n:::\n\n\n#### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# Fit a Random Forest classifier on the train set\nwine_rf = RandomForestClassifier(n_estimators=1000, random_state=123)\nwine_rf.fit(train_wine.drop('quality', axis=1), train_wine['quality'])\n\n# Test the model and compute the accuracy\nwine_pred_rf = wine_rf.predict(test_wine.drop('quality', axis=1))\nprint(confusion_matrix(test_wine['quality'], wine_pred_rf))\nprint(\"Accuracy:\", accuracy_score(test_wine['quality'], wine_pred_rf))\n```\n:::\n\n\nThis model is worse than the R version mostly because of the different defaults.\n:::\n\n### Variable importance\n\nExtract the model-specific variable importance using the functions `varImpPlot` (plots) and `importance` (values) on the model. Observe well that the mean decrease in accuracy of each variable is also computed for each specific class. In particular, what makes `density` special for predicting `Good` compare to another variable (like for example `citric.acid`)?\n\n::: panel-tabset\n#### R\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvarImpPlot(wine_rf)\nimportance(wine_rf)\n```\n:::\n\n\n#### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# Variable importance\nwine_importances = pd.Series(wine_rf.feature_importances_, index=train_wine.drop('quality', axis=1).columns)\nwine_importances.sort_values(ascending=False).plot(kind='bar')\n```\n:::\n\n\n:::\n\n`density` is important for predicting the `Good` since their predictions is much less accurate if we do not use it. `citric.acid` is both overall less important than `density` but especially for prediction of `Good`.\n\n\n## Regression\n\nIn this part, we will be using the `real_estate_data.csv` once again. After reading the data, apply a random forest to predict `price` using all the other variables except `No`, `Month` and `Year`. Compute the RMSE and inspect the prediction quality with a graph. Note that the importance is not specific to any class here.\n\n::: panel-tabset\n#### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(magrittr)\nreal_estate_data <- read.csv(here::here(\"labs/data/real_estate_data.csv\"))\n\n# select the columns of interest\nreal_estate_data <- \n  real_estate_data %>% \n  select(-c(No, Month, Year))\n\n# once again, divide the data into training and testing sets using the function created earlier\nrestate_index <- get_split_index(real_estate_data)\nrestate_tr <- real_estate_data[restate_index == 1, ]\nrestate_te <- real_estate_data[restate_index == 2, ]\n\n# apply the RF model as a regression\nrestate_rf <- randomForest(Price~., data=restate_tr, ntree=1000, importance=TRUE)\nrestate.pred_rf<-predict(restate_rf, newdata=restate_te)\n\n# compute rmse and plot the results as well the VarImp\n(rmse <- sqrt(mean((restate_te$Price - restate.pred_rf)^2)))\nplot(restate_te$Price ~ restate.pred_rf)\nabline(0,1)\nvarImpPlot(restate_rf)\nimportance(restate_rf)\n```\n:::\n\n\n#### Python\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# Load real estate dataset\nreal_estate_data = pd.read_csv(\"../data/real_estate_data.csv\")\n\nreal_estate_data = real_estate_data.drop(['No', 'Month', 'Year'], axis=1)\n\n# Split real estate dataset into train and test\ntrain_restate, test_restate = train_test_split(real_estate_data, test_size=0.25, random_state=123)\n\n# Fit a Random Forest regressor on the train set\nrestate_rf = RandomForestRegressor(n_estimators=1000, random_state=123)\nrestate_rf.fit(train_restate.drop('Price', axis=1), train_restate['Price'])\n\n# Test the model and compute the RMSE\nrestate_pred_rf = restate_rf.predict(test_restate.drop('Price', axis=1))\nrmse = np.sqrt(mean_squared_error(test_restate['Price'], restate_pred_rf))\nprint(\"RMSE:\", rmse)\n\n# Plot the prediction quality\nplt.scatter(test_restate['Price'], restate_pred_rf)\nplt.xlabel('Actual Price')\nplt.ylabel('Predicted Price')\nplt.plot([min(test_restate['Price']), max(test_restate['Price'])], [min(test_restate['Price']), max(test_restate['Price'])], color='red')\nplt.show()\n\n# Variable importance\nrestate_importances = pd.Series(restate_rf.feature_importances_, index=train_restate.drop('Price', axis=1).columns)\nrestate_importances.sort_values(ascending=False).plot(kind='bar')\n```\n:::\n\n\n:::\n\nCompare this model with the one you came up with in `Ex_ML_LinLogReg` . Which one would you go for?\n\n# Gradient Boosting Machines (GBM)\n\nIn this part of the lab, we will look at how the `gbm` library in R and the `GradientBoostingClassifier` and `GradientBoostingRegressor` in Python can be applied for classification and regression tasks. We will continue using the `wine` dataset for classification and `real_estate_data` for regression.\n\n::: {.callout-note}\n## Hyperparameters of GBM \n\nR (using the `gbm` library): \n\n1. `n.trees`: The number of boosting stages to perform (equivalent to `n_estimators` in python). \n2. `interaction.depth`: The maximum depth of each tree (equivalent to `max_depth` in python). \n3. `shrinkage`: The learning rate. \n4. `n.minobsinnode`: The minimum number of samples required to split an internal node (equivalent to `min_samples_split` in python). \n5. `bag.fraction`: The fraction of samples to be used for fitting individual base learners (equivalent to `subsample` in python).\n\nPython (using the `sklearn` library):\n\n1. `n_estimators`: The number of boosting stages to perform. Similar to Random Forest, increasing the number of estimators can improve the model's performance but may also increase the computational complexity and training time. \n2. `learning_rate`: The learning rate shrinks the contribution of each tree. A smaller learning rate requires more boosting stages to achieve the same performance as a larger learning rate, but it can also result in a more robust model. \n3. `max_depth`: The maximum depth of each tree. Similar to Random Forest, a higher depth can capture more complex patterns in the data, but it may also lead to overfitting. \n4. `min_samples_split`: The minimum number of samples required to split an internal node. Similar to Random Forest, a smaller value allows the model to capture finer details in the data, while a larger value can help prevent overfitting. \n5. `min_samples_leaf`: The minimum number of samples required to be at a leaf node. Similar to Random Forest, a smaller value allows the model to capture finer details, while a larger value can help prevent overfitting.\n6. `subsample`: The fraction of samples to be used for fitting individual base learners. A value smaller than 1.0 can lead to a reduction in variance and an increase in bias, resulting in a more robust model.\n:::\n\n\n:::{.callout-tip}\n## Few tips on GBM hyperparameters\n\nSimilar to R, here are some tips for finding the best combination of the hyperparameters:\n\n1. `n.trees` (R) / `n_estimators` (python): Start with a lower number of trees and increase it until no further improvement in performance is observed. Be aware of the increased computation time with a larger number of trees. \n2. `interaction.depth` (R) / `max_depth` (python): Keep the depth of each tree relatively shallow (3-5 levels) to prevent overfitting. Deeper trees can capture more complex patterns but may lead to overfitting. \n3. `shrinkage` (R) / `learning_rate` (python): Use a smaller learning rate for better model performance, but be prepared for slower convergence and increased computation time. Typically, values range between 0.01 and 0.1. \n4. `n.minobsinnode` (R) / `min_samples_split` (python): Similar to Random Forest, experiment with different values to find the optimal balance between overfitting and underfitting. \n5. `bag.fraction` (R) / `subsample` (python): Using a subsample of the data (e.g., 0.5-0.8) can help reduce overfitting and speed up the training process. Experiment with different values to find the best trade-off between performance and computation time.\n:::\n\n## Classification\n\n### Training and testing the model\n\nWe now fit a GBM model on the `wine` training set and apply it to the same target variable `quality`. We can train the model and add\n\n::: panel-tabset\n#### R\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(gbm)\nset.seed(123)\nwine_gbm <- gbm(quality~., data=wine_tr, distribution=\"multinomial\", n.trees=1000, interaction.depth=4, shrinkage=0.01)\nwine.pred_gbm <- predict(wine_gbm, newdata=wine_te, n.trees=1000, type=\"response\")\nwine.pred_gbm_class <- apply(wine.pred_gbm, 1, which.max)\nlevels(wine_te$quality) <- 1:length(levels(wine_te$quality))\nconfusionMatrix(factor(wine.pred_gbm_class), wine_te$quality)\n```\n:::\n\n\n\n#### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Fit a Gradient Boosting classifier on the train set\nwine_gbm = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.01, max_depth=4, random_state=123)\nwine_gbm.fit(train_wine.drop('quality', axis=1), train_wine['quality'])\n\n# Test the model and compute the accuracy\nwine_pred_gbm = wine_gbm.predict(test_wine.drop('quality', axis=1))\nprint(confusion_matrix(test_wine['quality'], wine_pred_gbm))\nprint(\"Accuracy:\", accuracy_score(test_wine['quality'], wine_pred_gbm))\n```\n:::\n\n\n:::\n## Regression\n\nIn this part, we will continue using the `real_estate_data.csv`. Fit a GBM model on the real estate training set to predict `price` using all the other variables except `No`, `Month`, and `Year`. Then compute the metrics and plot the predictions.\n\n::: panel-tabset\n#### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\nrestate_gbm <- gbm(Price~., data=restate_tr, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.01)\nrestate.pred_gbm<-predict(restate_gbm, newdata=restate_te, n.trees=1000)\n\n# compute rmse and plot the results\n(rmse_gbm <- sqrt(mean((restate_te$Price - restate.pred_gbm)^2)))\nplot(restate_te$Price ~ restate.pred_gbm)\nabline(0,1)\n```\n:::\n\n\n\n#### Python\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# run the code below if you have not cleared the plot yet\nplt.clf()\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n# Fit a Gradient Boosting regressor on the train set\nrestate_gbm = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01, max_depth=4, random_state=123)\nrestate_gbm.fit(train_restate.drop('Price', axis=1), train_restate['Price'])\n# Test the model and compute the RMSE\n\nrestate_pred_gbm = restate_gbm.predict(test_restate.drop('Price', axis=1))\nrmse_gbm = np.sqrt(mean_squared_error(test_restate['Price'], restate_pred_gbm))\nprint(\"RMSE:\", rmse_gbm)\n# Plot the prediction quality\n\nplt.scatter(test_restate['Price'], restate_pred_gbm)\nplt.xlabel('Actual Price')\nplt.ylabel('Predicted Price')\nplt.plot([min(test_restate['Price']), max(test_restate['Price'])], [min(test_restate['Price']), max(test_restate['Price'])], color='red')\nplt.show()\n```\n:::\n\n\n:::\n\nCompare the GBM model with the Random Forest model you came up with earlier. Which one would you go for?\n\n# Bonus: XGBoost\n\n## What is XGBoost?\n\nXGBoost (Extreme Gradient Boosting) is an optimized implementation of the gradient boosting algorithm. It is designed for high performance and efficient memory usage. XGBoost improves upon the base Gradient Boosting Machine (GBM) by incorporating regularization to prevent overfitting and implementing parallel processing techniques for faster training. The algorithm also offers built-in cross-validation and early stopping to save time and resources during model training.\n\n\n## Modelling with XGBoost\n\nWe'll use the [`xgboost`](https://xgboost.readthedocs.io/en/stable/index.html) library in both R and python. You can see some of the hyperparameters below:\n\n::: {.callout-note}\n## Hyperparameters of XGBoost\n\n1. `eta`: Controls the learning rate, which determines the step size at each iteration while updating the model weights. Smaller values make the model more robust to overfitting but require more iterations to converge. Typical values range from 0.01 to 0.3. \n2. `max_depth`: Controls the maximum depth of each tree. Deeper trees can model more complex relationships but are more prone to overfitting. Experiment with different values, keeping in mind that a shallower tree can be more interpretable and less prone to overfitting. \n3. `min_child_weight`: Controls the minimum sum of instance weights needed in a child node. Increasing this value helps to prevent overfitting by making the model more conservative.\n\n:::\n\nYou can read more about the package in its documentation.\n\n::: panel-tabset\n\n### R\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Install and load the package\n# install.packages(\"xgboost\")\nlibrary(xgboost)\n\n# Prepare data for XGBoost\ndtrain <- xgb.DMatrix(data = as.matrix(restate_tr[, -ncol(restate_tr)]), label = restate_tr$Price)\ndtest <- xgb.DMatrix(data = as.matrix(restate_te[, -ncol(restate_te)]), label = restate_te$Price)\n\n# Set hyperparameters\nparams <- list(\n  objective = \"reg:squarederror\",\n  eta = 0.1,\n  max_depth = 5,\n  min_child_weight = 1,\n  subsample = 1,\n  colsample_bytree = 1\n)\n\n# Train the model\nxgb_model <- xgb.train(params, dtrain, nrounds = 1000)\n\n# Test the model and compute the RMSE\nrestate_pred_xgb <- predict(xgb_model, dtest)\nrmse_xgb <- sqrt(mean((restate_te$Price - restate_pred_xgb)^2))\nprint(paste(\"RMSE:\", rmse_xgb))\n```\n:::\n\n\n### Python\n\nWe used the python installation of `xgboost` from our lab `setup`.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.python .cell-code}\n# Install and load the package\nimport xgboost as xgb\n\n# Prepare data for XGBoost\ndtrain = xgb.DMatrix(train_restate.drop(\"Price\", axis=1), label=train_restate[\"Price\"])\ndtest = xgb.DMatrix(test_restate.drop(\"Price\", axis=1), label=test_restate[\"Price\"])\n\n# Set hyperparameters\nparams = {\n    \"objective\": \"reg:squarederror\",\n    \"eta\": 0.1,\n    \"max_depth\": 3,\n    \"min_child_weight\": 1,\n    \"subsample\": 1,\n    \"colsample_bytree\": 1,\n}\n\n# Train the model\nxgb_model = xgb.train(params, dtrain, num_boost_round=1000)\n\n# Test the model and compute the RMSE\nrestate_pred_xgb = xgb_model.predict(dtest)\nrmse_xgb = np.sqrt(mean_squared_error(test_restate[\"Price\"], restate_pred_xgb))\nprint(\"RMSE:\", rmse_xgb)\n```\n:::\n\n\nAlthough initially our GBM suffered compared to the RF, we can see that XGBoost can help improve the result (the case for the python implementation). However, random forest still outperforms all the other models. \n\n:::\n\nFeel free to apply XGBoost to the dataset of your choice.\n",
    "supporting": [
      "Ex_ML_Ensemble_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}