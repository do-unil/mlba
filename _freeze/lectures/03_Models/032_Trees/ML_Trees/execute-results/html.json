{
  "hash": "3611366e5223950d9b4480d35b3362f1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Trees\nformat: html\nexecute:\n  freeze: auto\n  output: true\ncode-tools: false\ncode-fold: false\ncode-link: false\n---\n\nConcept\n=======\n\n### CART\n\nCART stands for **C**lassification **A**nd **R**egression **T**rees. It\nconsists of\n\n-   A hierarchical set of binary rules,\n\n-   A representation in a shape of a tree.\n\nIt applies to both regression and classification tasks.\n\nIris data (on 125 data; 35/35/35).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# knitr::include_graphics![](../../Graphs/IRIS_CART.pdf){width=\"10cm\" fig-align=\"center\"}\nknitr::include_graphics(\"../../Graphs/IRIS_CART.pdf\")\n```\n\n::: {.cell-output-display}\n![](../../Graphs/IRIS_CART.pdf)\n:::\n:::\n\n\n- If Petal length $< 2.45$ then Setosa, else if Petal width $< 1.65$ then Versicolor, else Virginica.\n\n### The prediction formula\n\nThe set of rules including the order of the features and the thresholds\nat each node can be embedded into a set of parameters $\\theta$.\\\nAfter the training, the tree contains of final nodes. We use the\nnotation below,\n\n-   Applying the rules to features $x$ send the instance $(y,x)$ to the\n    final node that we call $N(x)$.\n\n-   In the training set $(y_1,x_1), \\ldots, (y_n,x_n)$, the set of\n    instances that were sent to node $N$ is called $I(N)$. E.g.,\n    $I(N)=\\{1,3,4\\}$ means that instances\n    $(y_1,x_1),(y_3,x_3),(y_4,x_4)$ were sent to node $N$.\n\n-   The prediction for features $x$ is noted $f(x;\\theta)$. That\n    prediction only depends on the node to which $x$ is sent. In other\n    words,\n    $$N(x) = N(x') \\quad \\Longrightarrow \\quad f(x;\\theta) = f(x';\\theta).$$\n\n### The prediction formula: regression\n\nFor regression, $f(x;\\theta)$, the prediction for $x$, is the mean of\nthe $y_i$'s that were sent to the node of $x$ during the training.\\\nIn more fancy terms,\n$$f(x;\\theta) = \\frac{1}{|I(N(x))|}\\sum_{i \\in I(N(x))} y_i.$$\n\n### The prediction formula: classification\n\nFor classification,\n\n-   The node $N(x)$ has a predicted probability vector: probabilities\n    one each possible class\n    $$p(x;\\theta) = (p_1(x;\\theta), \\ldots, p_C(x;\\theta)),$$\n\n-   The prediction $f(x;\\theta)$ is the class that has highest\n    probability:\n    $$f(x;\\theta) = \\arg\\max_{c=1,\\ldots,C} p_c(x;\\theta).$$\n\nThe predicted probabilities $p(x;\\theta)$ at node $N(x)$ are the\nproportions of each class taken on all the instances sent to $N(x)$\nduring the training.\\\nIn more fancy terms, for any class $c$,\n$$p_c(x;\\theta) = \\frac{1}{|I(N(x))|}\\sum_{i \\in I(N(x))} 1_{\\{y_i = c\\}}.$$\n\n### The loss function: regression\n\nFor the regression case, the most used loss function is the MSE:\n$$\\bar{\\cal L}(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\left\\{y_i - f(x_i;\\theta)\\right\\}^2$$\n\nFor the classification case, the most used loss is the **entropy**. It\nis different from the one of logistic regression[^1]. In the case of $C$\nclasses, the entropy is $$\\begin{aligned}\n{\\cal L}(y, p) &=& - p_1 \\log p_1 - p_2 \\log p_2 - \\cdots - p_C \\log p_C\\\\\n&=& - \\sum_{c=1}^C p_C \\log p_c.\\end{aligned}$$ A the global entropy is\nthus $$\\begin{aligned}\n\\bar{\\cal L}(\\theta) &=& - \\sum_{i=1}^n\\sum_{c=1}^C p_c(x_i;\\theta) \\log p_c(x_i;\\theta).\\end{aligned}$$\n\n### The loss function: classification\n\nOther possible choices for the loss function are:\\\nThe **classification error**: $${\\cal L}(y,p) = 1 - p_y.$$ If $y=c$,\nthen $p_c$ should be large and $1-p_c$ should be small.\\\nThe **Gini index**:\n$${\\cal L}(y,p) = p_1(1-p_1) + \\cdots + p_C(1-p_C) = \\sum_{c=1}^C p_c(1-p_c).$$\nThis index is smaller if $p$ has one large component and the other ones\nsmall (e.g., $(1,0,0,0)$). It is thus minimum when the prediction is\ncertain. For this, it is similar to the entropy.\n\n### The greedy algorithm\n\nFinding the optimal $\\theta$ cannot be obtained using a Newton-Raphson\nalgorithm because $f(x;\\theta)$ is not differentiable.\\\nTo obtain the optimal $\\hat{\\theta}$, one should search among all\npossible splitting combinations. The complexity of such task is enormous\nand cannot be achieved even with the most powerful imaginable\ncomputers.\\\nAn approximate solution is obtained by the **greedy algorithm**.\n\nThe greedy algorithm\n====================\n\n### Splitting the data space\n\nThe tree splits the space into rectangles[^2]. Below with two features:\n\n![](../../Graphs/CART_Picture.png){width=\"10cm\" fig-align=\"center\"}\n\nEach rectangle is associated to one node and one prediction.\n\nThe greedy algorithm grows the tree step by step as follows:\n\n-   Start with no partition,\n\n-   Along each feature $x_j$, find the best split of the training set by\n    this feature. This gives $p$ splits (one for each feature).\n\n-   Compare the $p$ splits and select the best one: the one diminishing\n    the loss the most.\n\nYou obtain two nodes, $N_1$ and $N_2$, say. Repeat the above rule on\neach node until a stopping rule is reached (see later).\n\nExample of classification:\n\n![](../../Graphs/Cart_Build.png){width=\"12cm\" fig-align=\"center\"}\n\nNote: by convention $0\\times \\ln 0 = 0$.\n\n### CART: a greedy algorithm\n\n![](../../Graphs/Cart_Build2.png){width=\"12cm\" fig-align=\"center\"}\n\netc.\n\n### A greedy algorithm\n\nExample of regression:\n\n![](../../Graphs/RT_Build.png){width=\"12cm\" fig-align=\"center\"}\n\n### Stopping rule\n\nThe algorithm could go on splitting the feature space forever. Several\nstopping rules can be used. For example in `R` with `rpart` (see\n`?rpart.control`):\n\n-   `minsplit`: the minimum number of observations that must exist in a\n    node in order for a split to be attempted (default is 20).\n\n-   `cp`: Any split that does not decrease the overall lack of fit by a\n    factor of cp is not attempted (default to 0.1).\n\n-   `minbucket`: the minimum number of observations in any terminal node\n    (default to `minsplit/3`).\n\n-   etc.\n\nEven with these rules, once the tree is grown, it might be too large.\nThe procedure of cutting nodes is called `pruning` the tree.\n\nInterpretation\n==============\n\n### Interpretation\n\nTrees can be interpreted by reading directly on the graph the link\nbetween the features and the outcome.\n\n-   The most important feature is at the top of the graph. The first\n    split influence the other ones.\n\n-   The final nodes should be organized in a logical way to ease the\n    reading. E.g.,\n\n    -   (regression) Low predictions to the left, high predictions to\n        the right.\n\n    -   (binary) predictions \\\"0\\\" to the left, and \\\"1\\\" to the right.\n\nInterpretation must be done with cautious. The tree structure is\nunstable: several trees can produce similar predictions. The greedy\nalgorithm may just pick one at random.\n\nPruning\n=======\n\n### Occam's razor\n\nFor linear and logistic regressions, the more variables are in the\nmodel, the more complex that model is.\\\nFor trees, model complexities can be measured by the length of the tree:\na shorter tree is more simple than a longer one.\\\nTo apply Occan's razor, one needs to shorten the tree without impeding\ntoo much its prediction quality.\\\nTo **prune** the tree consists of cutting branches while maintaining\nenough prediction quality.\n\n### 1-SE rule\n\nThere exist several ways to prune the tree. One of the most used is the\n**1-SE** rule.\n\n-   Build a long tree (using the default stopping rules)\n\n-   For all the sub-trees compute the error of the tree\n\n-   For the best tree (lowest error) compute the uncertainty on that\n    error measure: the **standard error** SE.\n\n-   Any tree whose error is below the lowest error plus one SE is\n    considered equivalent to the best tree.\n\n-   Select the shortest tree among those that are equivalent to the best\n    one.\n\n### Technical details\n\n-   The SE is computed using cross-validation (see later).\n\n-   The 1-SE rule can be replaced by other rules (like cutting at a\n    given CP, etc.).\n\nIn practice,\n\n-   the pruning is not a guarantee to obtain a good model.\n\n-   It should be used to simplify the model and avoid overfitting.\n\n-   (empirical) it works well to build a very large tree then to prune\n    it.\n\n[^1]: For logistic regression, it is the cross-entropy.\n\n[^2]: Several trees may be equivalent.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}