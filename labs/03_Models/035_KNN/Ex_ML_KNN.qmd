---
title: "Models: K-Nearest Neighbors"
format: html
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(fig.align="center", results = 'hide', fig.show = 'hide')
```

# K-NN: a gentle introduction

In this first part, we apply the K-NN to the iris data set with examples in both R & python. In both cases, we first load the `iris` built-in data set found `dplyr`.

```{r, results = 'hide'}
library(dplyr)
iris
str(iris)
```

::: panel-tabset

## R

In R, we can model `iris` using the `knn3` function from the `caret` package (named `knn3` because `knn` is already taken by another package). This function is limited to the Euclidean distance with numerical features.

We first make the prediction using a 2-NN (with Euclidean distance).

```{r}
library(caret)
mod_knn <- knn3(data=iris, Species~., k=2) ## build the K-NN model
predict(mod_knn, newdata = iris, type="class")
```

Now, we want to know if the predictions are good. To do this, we must ensure that the model is not overfitting the data. For this, we must split the data into training and test sets (these concepts will be seen in detail in a future course). To do so, we randomly take 75% of the iris data that will be the training set, and the remaining 25% are the test set.

```{r}
set.seed(123) ## for replication purpose

index_tr <- sample(1:nrow(iris), replace=FALSE, size=0.75*nrow(iris))
index_tr ## the index of the rows of iris that will be in the training set

iris_tr <- iris[index_tr,] ## the training set
iris_te <- iris[-index_tr,] ## the test set
```

Now we can use the 2-NN to predict the test set using the training set. Note that the model is fitted on the training set, and the predictions are computed on the test set.

```{r}
mod_knn <- knn3(data=iris_tr, Species~., k=2)
iris_te_pred <- predict(mod_knn, newdata = iris_te, type="class")
```

To compare the predictions above and the true species (the one in the test set), we can build a table. It is called a *confusion matrix* (again, this will be explained in detail later on).

```{r}
table(Pred=iris_te_pred, Observed=iris_te[,5])
```

The prediction is almost perfect. It is so good that it is pointless to try to improve the prediction by changing `K` at that point. However, just to illustrate, below we use `K=3`.

```{r}
mod_knn <- knn3(data=iris_tr, Species~., k=3)
iris_te_pred <- predict(mod_knn, newdata = iris_te, type="class")

table(Pred=iris_te_pred, Observed=iris_te[,5])
```

Note that, in th formula "Species\~.", the dot means "all the other variables". It is equivalent (but shorter) to "Species \~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width"

## Python

Before running the python, we must ensure we're using the `MLBA` conda (virtual) environment created during `setup.Rmd`. As we already installed some of the packages, it should suffice for this part of the exercise:

```{r}
# make sure we're using the right environment
library(reticulate)
use_condaenv("MLBA")
py_config()
```

First, let's load the iris dataset and print its structure. You can access the r data object by using `r.iris` or only the pure python data as shown below (you don't need the chunk below):

``` python
import pandas as pd
from sklearn.datasets import load_iris

iris = load_iris()
iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)
iris_df['species'] = iris.target_names[iris.target]
print(iris_df)
print(iris_df.info())
```

Now, in python, we will also apply K-NN to the iris data set using the `KNeighborsClassifier` function from the `sklearn` package in python, where we set `K=2`. This function can be applied to both numerical and categorical features. We then make predictions with this python model.

```{python}
import pandas as pd
from sklearn.neighbors import KNeighborsClassifier

# call iris data from r
y = r.iris[["Species"]]  # select column "Species"
X = r.iris.drop(columns=["Species"])  # drop column "Species"

# if you imported the data directly in python, you can instead run the commands below
# y = iris.target
# X = iris.data

k = 2
knn_model = KNeighborsClassifier(n_neighbors=k)
knn_model.fit(X, y)

pred = knn_model.predict(X)
print(pred)
```

We will split the data again into training and test sets. We will randomly select 75% of the data for the training set and the remaining 25% for the test set. To achieve this, we will use `numpy`, most often used for efficient numerical computing. This split of training and test sets will only be done once in the exercises as we already created the same objects in R. Nevertheless, this is one way of dividing data into training and test sets in python:

```{python}
import numpy as np

np.random.seed(123)
index_tr = np.random.choice(range(len(r.iris)), size=int(0.75*len(r.iris)), replace=False)
index_te = np.setdiff1d(range(len(r.iris)), index_tr)

iris_tr = r.iris.iloc[index_tr, :]
iris_te = r.iris.iloc[index_te, :]
```

Now, we can fit the K-NN model to the training set and make predictions on the test set.

```{python}
knn_model = KNeighborsClassifier(n_neighbors=k)
knn_model.fit(iris_tr.iloc[:, :-1], iris_tr.iloc[:, -1])

pred = knn_model.predict(iris_te.iloc[:, :-1])
print(pred)
```

To evaluate the performance of our model, we can construct a confusion matrix.

```{python}
from sklearn.metrics import confusion_matrix

# for the confusion matrix in python, we need to specify the column names
labels = r.iris['Species'].unique()

# create the confusion matrix with the labels as column names
conf_mat = confusion_matrix(iris_te.iloc[:, -1], pred, labels=labels)
conf_mat_df = pd.DataFrame(conf_mat, columns=labels, index=labels)

# print the confusion matrix
print(conf_mat_df)
```

The prediction is almost perfect (as was the case in R), so it is not necessary to try to improve the prediction by changing k at this point. However, just for illustration, we can repeat the process with `K=3`.

```{python}
k = 3

knn_model = KNeighborsClassifier(n_neighbors=k)
knn_model.fit(iris_tr.iloc[:, :-1], iris_tr.iloc[:, -1])

pred = knn_model.predict(iris_te.iloc[:, :-1])
conf_mat = confusion_matrix(iris_te.iloc[:, -1], pred, labels=labels)
print(pd.DataFrame(conf_mat, index=labels, columns=labels))
```

::: callout-note
## Why results are different in Python vs R

Please note that the training and test sets we used in python are not the same ones used in R due to a different generator for the seed (i.e., `set.seed(123)` is not generating the same numbers as `np.random.seed(123)`). Therefore, different observations were taken for the test set in the two languages. If you want to see the same performance, you can also run in both languages with the same dataset. For instance, running the code below would give you the same results:

``` {{r}}
mod_knn_py <- knn3(data=py$iris_tr, Species~., k=2)
iris_te_pred_py <- predict(mod_knn_py, newdata = py$iris_te, type="class")
table(Pred=iris_te_pred_py, Observed=py$iris_te[,5])
```
:::

:::

# K-NN: regression

In the case of a regression task, the prediction is obtained by averaging the `K` nearest neighbors. To illustrate this, we will use the `imports.85` data set. The aim is to predict the price using only the numerical features (categorical features are illustrated later).

Below, we identify the numerical columns (don't forget to load the data first!). The new data frame is named `tmp` (temporary...). Then, we remove all the rows containing an NA.

```{r results='hide'}
imports_85 <- read.csv(here::here("labs/data/imports-85.data"), 
                       header=FALSE, na.strings="?")

names = c("symboling","normalized.losses","make","fuel.type",
         "aspiration","num.of.doors","body.style","drive.wheels",
         "engine.location","wheel.base","length","width",
         "height","curb.weight","engine.type","num.of.cylinders",
         "engine.size","fuel.system","bore","stroke",
         "compression.ratio","horsepower","peak.rpm","city.mpg",
         "highway.mpg","price")

names(imports_85) = names

tmp <- imports_85 %>% select(where(is.numeric))
tmp <- filter(tmp, complete.cases(tmp))
tmp %>% head()
```

Now, we make a 75%-25% split for our training and testing.

```{r}
set.seed(123)
index_tr <- sample(1:nrow(tmp), size=0.75*nrow(tmp), replace = FALSE)
tmp_tr <- tmp[index_tr,]
tmp_te <- tmp[-index_tr,]
```

::: panel-tabset

## R

We now fit the `knnreg` function to fit the model in R.

```{r}
mod_knn <- knnreg(data=tmp_tr, price~., k=3)
tmp_pred <- predict(mod_knn, newdata=tmp_te)
```

We now graphically compare the real prices in the test set with the predicted ones (the diagonal line shows the equality between the prediction and the real price).

```{r}
tmp_te %>% mutate(pred=tmp_pred) %>%
ggplot(aes(x=price, y=pred)) + 
  geom_point() + geom_abline(slope=1, intercept = 0)
```

It looks like there is still room for improvement. Check for yourself if this can be improved by changing `K`.

## Python

As you already learnt how to import the data in R, and divided into training and test sets, we will use that directly (which also allows for better comparisons between modelling in R vs python).

```{python}
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsRegressor

X_tr = r.tmp_tr.drop(columns=["price"])
y_tr = r.tmp_tr["price"]
mod_knn_py = KNeighborsRegressor(n_neighbors=3)
mod_knn_py.fit(X_tr, y_tr)

X_te = r.tmp_te.drop(columns=["price"])
y_te = r.tmp_te["price"]
tmp_pred = mod_knn_py.predict(X_te)
```

We do the same graphical comparison in python, however, we will use `matplotlib` to demonstrate the results.

```{python}
import matplotlib.pyplot as plt

# we have to make a copy otherwise `r.temp_te` is not directly modified (i.e. we can't add a column)
tmp_test_plt = r.tmp_te.copy()
tmp_test_plt["pred"]= tmp_pred
fig, ax = plt.subplots()
ax.plot(tmp_test_plt["price"], tmp_test_plt["pred"], '.', color='black');
ax.plot(tmp_test_plt["price"], tmp_test_plt["price"], '-', color='blue');
ax.set_xlabel("Price");
ax.set_ylabel("Predicted price");
plt.show()
```

We obtained the same results as R.

:::

# K-NN: mixture of feature types

In this part, we illustrate how to incorporate categorical variables with K-NN. As a reminder, to use categorical variables, the easiest way is probably to cast them to dummy variables.

The code below identifies the categorical columns (i.e., non numerical), then create the dummies using and add them to the data frame. In addition, the numerical variables are standardized.

There are several difficulties in the code below.

-   The price (the outcome) is first set aside because we do not want to standardize it.
-   Two brand names (variable `make`) contain "-", namely "alfa-romeo" and "mercedes-benz". After the creation of the dummy variables, these are used as titles. However, `knnreg` does not support "-" in the variable names. They thus have to be removed first (in the code below they are turned to "\_").

```{r}
## tmp is imports.85 without incomplete cases and without price
imp_comp <- imports_85 %>% filter(complete.cases(imports_85))
y <- imp_comp$price
imp_comp <- imp_comp %>% select(!price)

# we will use the standardize function shown by Marc-Olivier during the course (was called `my_fun`)
# normalize num variable: (x - min(x))/(max(x) - min(x))
my_normalize <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}
# additionally, you can use the `scale()` function which centers and scales variables

## the numerical variables in tmp are standardized
imp_num <- imp_comp %>% select(where(is.numeric)) %>% my_normalize() %>% as.data.frame()

## the dummy variables of the numerical features of tmp are created
library(fastDummies)
imp_dumm <- imp_comp %>% select(!where(is.numeric)) %>% 
  dummy_cols(remove_first_dummy = FALSE, remove_selected_columns = TRUE)

## These dummy variables are added to tmp
imp_dat <- data.frame(imp_num, imp_dumm)

## The names of the two problematic brands are changed
names(imp_dat)[c(23)] <- c("make_mercedes_benz")

## The raw price is added to tmp
imp_dat$price <- y
```

At this stage, `tmp` contains all the variables that we want. We once again divide our data into training and test sets.

```{r}
set.seed(123)
index_tr <- sample(1:nrow(imp_dat), size=0.75*nrow(imp_dat), replace = FALSE)
imp_tr <- imp_dat[index_tr,]
imp_te <- imp_dat[-index_tr,]
```

::: panel-tabset

## R

We can now run our 3-NN directly, like in the previous exercise.

```{r}
mod_knn <- knnreg(data=imp_tr, price~., k=3)
imp_pred <- predict(mod_knn, newdata=imp_te)
imp_te %>% mutate(pred=imp_pred) %>%
  ggplot(aes(x=price, y=pred)) + 
  geom_point() + geom_abline(slope=1, intercept = 0)
```

Here, like always, it is difficult to compare the scales of the scatterplot, and thus to tell if the quality of that 3-NN is better or worst than the previous one. The issue of model scoring will be studied later in the course. For now, it is just about being able to mix categorical and numeric variables in a K-NN.


## Python

The same approach in Python with the same.

```{python}
X_tr = r.imp_tr.drop(columns=["price"])
y_tr = r.imp_tr["price"]
mod_knn_py = KNeighborsRegressor(n_neighbors=3)
mod_knn_py.fit(X_tr, y_tr)

X_te = r.imp_te.drop(columns=["price"])
y_te = r.imp_te["price"]
tmp_pred = mod_knn_py.predict(X_te)

tmp_test_plt = r.imp_te.copy()
tmp_test_plt["pred"]= tmp_pred
fig, ax = plt.subplots()
ax.plot(tmp_test_plt["price"], tmp_test_plt["pred"], '.', color='black');
ax.plot(tmp_test_plt["price"], tmp_test_plt["price"], '-', color='blue');
ax.set_xlabel("Price");
ax.set_ylabel("Predicted price");
plt.show()
```

:::

# Analysis of nursing home data

Now it is your turn. Develop a K-NN model to predict the cost using the other variables. Inspect the quality of the prediction using a training set and a test set.
