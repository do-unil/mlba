[
  {
    "objectID": "labs/00_lab/setup.html",
    "href": "labs/00_lab/setup.html",
    "title": "General Instructions",
    "section": "",
    "text": "Objectives\n\n\n\nThis setup tutorial is optional for those who are interested in the following:\n\nLearning about (and using) virtual environments in R to ensure reproducibility (benefits explained below).\nLearning about using python üêç in (and with) R. This is useful for some ML lab sessions, and cutting-edge ML is often first implemented in python."
  },
  {
    "objectID": "labs/00_lab/setup.html#the-what-the-why",
    "href": "labs/00_lab/setup.html#the-what-the-why",
    "title": "General Instructions",
    "section": "The What & The Why",
    "text": "The What & The Why\nAs you start working on machine learning in R, you‚Äôll be using various R packages that provide different functionalities. As you might know, managing multiple packages and their dependencies can be a daunting task, especially when you‚Äôre new to programming.\nThis is where renv comes in handy. renv is a package management tool that helps you manage the packages used in an R project, making it easier to handle dependencies and ensuring that your project is reproducible. Here are some specific reasons why renv is particularly useful for machine learning lab sessions:\n\nConsistent environment: When you work on machine learning lab sessions, you‚Äôll often be working with complex models that use multiple R packages. It‚Äôs crucial to ensure that all the packages used in your project are compatible with each other. With renv, you can create a consistent environment by isolating the packages used in your project and making sure they work well together.\nEasy installation and setup: renv makes it easy to set up a new R project with the required packages. Once you‚Äôve created an renv project, you can easily install all the required packages with a single command. This saves you time and ensures that you have all the necessary packages for your machine learning lab sessions.\nReproducibility: Reproducibility is critical in machine learning lab sessions. With renv, you can easily share your code and the exact packages used in your project with your peers or instructors, making it easy for them to reproduce your results.\n\nIn summary, renv is an essential tool for managing packages in R and ensuring that your machine learning lab sessions are efficient and reproducible. It helps you avoid compatibility issues, simplifies installation and setup, and makes it easy to share your work with others. In python, you have similar tools such as virtualenv, venv and conda. We hope you find renv useful as you begin your journey into machine learning with R!\n\n# Check if renv is installed\nif (!require(\"renv\")) {\n  # Install renv if it is not already installed\n  install.packages(\"renv\")\n}"
  },
  {
    "objectID": "labs/00_lab/setup.html#the-how",
    "href": "labs/00_lab/setup.html#the-how",
    "title": "General Instructions",
    "section": "The How",
    "text": "The How\nTo create a new renv project, first set your working directory to the location where you want to create the project, and then run the command renv::init() which has to be executed only once. We recommend creating a main folder for all your ML-related exercises and running the command at the top directory. Running the initialization will create a bunch of auxiliary files, such as renv.lock to keep track of the packages that you‚Äôre using and all their versions. If you move to another computer or something goes wrong with your packages, you can always re-install the packages listed in your renv.lock to the previous versions using renv::restore(). To occasionally update your packages, you can always use renv::snapshot(); however, every time that you start a new R session, if renv detects any changes, it will automatically ask you to run renv::status() to see if the list in renv.lock needs updating.\n\n# Check if renv is already initialized\nif (!file.exists(\"renv.lock\")) {\n  # Initialize renv project\n  renv::init()\n\n  # Restore packages\n  renv::restore()\n}"
  },
  {
    "objectID": "labs/00_lab/setup.html#small-motivation",
    "href": "labs/00_lab/setup.html#small-motivation",
    "title": "General Instructions",
    "section": "Small Motivation",
    "text": "Small Motivation\nMost careers after HEC require data literacy and data-driven insights to solve business problems. To that end, R is a potent tool; however, in the realm of machine learning, Python is arguably more demanded and, therefore, can be a good tool in your toolbox. Additionally, python is a widely-used language in the industry and offers powerful libraries for data manipulation, analysis, and modeling. Furthermore, you don‚Äôt need to be a technical person to learn python as there are many resources available online, and it‚Äôs a language that is relatively easy to pick up, even for absolute beginners to programming. Combining the strengths of both R and Python can enhance your workflow and improve your ability to work with data. To understand how R and Python compare, you are highly encouraged to watch this video by IBM on R vs Python."
  },
  {
    "objectID": "labs/00_lab/setup.html#configuration",
    "href": "labs/00_lab/setup.html#configuration",
    "title": "General Instructions",
    "section": "Configuration",
    "text": "Configuration\nYou need to follow a few steps to ensure that everything runs smoothly. There are several solutions for running the python part of the exercises. Our preferred method is to run python in R using the reticulate package as recently there has been a smooth integration between the two languages, especially if your IDE of choice (integrated development environment) is Rstudio. This library provides a comprehensive set of tools for interoperability (i.e., exchanging languages) between python and R. You can either do this in your own central python installation or use a virtual environment where we‚Äôll install the desired python packages. In python, virtual environments are similar to those of R (i.e.¬†renv), allowing you to create isolated python installations, making it easier to manage different projects, and avoiding compatibility issues between dependencies. This setup usually works well, but if there are any individual issues, do not hesitate to contact me (Ilia). With that said, let‚Äôs get started with python!\nFirst, load all the corresponding libraries to install and run python:\n\nlibrary(reticulate)\nlibrary(tidyverse)\n\nTo install python in R, we will use miniconda, a smaller version of Anaconda package manager. If you don‚Äôt have Miniconda or Anaconda, first run reticulate::install_miniconda(), which will automatically create a virtual environment for you.\n\n# reticulate::install_miniconda() # if you got an error, you could also try `install_python()` or installing conda seperately on your OS\n\nNow, we will create our virtual environment with the command reticulate::conda_create(). We will then make sure our rstudio is using this correct conda environment by enforcing it via reticulate::use_condaenv().\n\n# assign the right virtual environment for the exercises\nenv_name <- \"MLBA\"\n\n# if the virtual enviroment does not already exist, only then create then\nif (!env_name %in% reticulate::conda_list()$name) {\n  reticulate::conda_create(env_name, pip = TRUE) # we use pip for installations\n}\n\n# Print the conda list\nprint(reticulate::conda_list())\n\n# make sure we're using the right environment\nreticulate::use_condaenv(env_name)\n\n# if you preferred, you can also use your own version of python with `use_python()`\n# you can see all the versions of your path here and which one has been assigned\nreticulate::py_config()\n\n# Check if python is setup properly now\nreticulate::py_available()\n\nIf you have made it here so far and see the name MLBA in your python path, you have successfully installed/configured python and setup the virtual environment(s)ü•≥.\nBonus: Using Rstudio to select Python path\nIn case you wanted to use the Rstudio interface, you can always go to Tools > Project Options (or Global Options if you‚Äôre not using renv) and then select the version inside the Python field as shown below."
  },
  {
    "objectID": "labs/00_lab/setup.html#python-very-brief-overview",
    "href": "labs/00_lab/setup.html#python-very-brief-overview",
    "title": "General Instructions",
    "section": "Python (very brief) overview",
    "text": "Python (very brief) overview\nWe do not teach the principles of python programming during this course. With that said, if you already have a background in R from your previous courses (e.g., DSFBA, QMM), we provide a few links to help you get started. If you‚Äôre new to python, feel free to continue reading; otherwise, skip to the section Calling Python in R.\nIf you would like to see a crash course in python on different data structures and types, check out this video on Python for Data Science [Crash Course] (you can skip the installation part). You may notice that much of the syntax is similar to R.\nPython data-oriented libraries\nPython provides a range of libraries for data manipulation, analysis, and modeling, including Pandas (similar to tibble+dplyr), which offers easy-to-use data structures for working with tabular data, and NumPy (dplyr+data.table), which provides powerful tools for array manipulation, linear algebra, and Fourier analysis. For data visualization, python offers Matplotlib (similar to plot() in base R). For machine learning, Scikit-learn (similar to the caret package in R which we will be introduced later) provides a wide range of tools for classification, regression, clustering, and dimensionality reduction, while Tensorflow & Keras (both used for deep learning and neural networks) are also popular libraries in this space which are available in both R and Python. These libraries are just a few examples of the many tools available in python that can help you work with data and build machine learning models. If you‚Äôre interested to learn about these libraries and how to manipulate Pandas dataframes, you can check out this other video on Data Analysis with Python - Full Course for Beginners (Numpy, Pandas, Matplotlib, Seaborn) (it‚Äôs slightly long).\nAssigning variables in Python vs.¬†R\n\n\n\nFor those of you new to python, there‚Äôs an important difference between R and Python when assignment variables. In Python, when you assign a variable to another variable, you are creating a reference to the same object in memory, so any changes made to one variable will be reflected in the other variable as well. This is demonostrated in the example below:\n\na = [1, 2, 3]\nb = a\nprint(a, b)\nb[0] = 4 # we only change `b`\nprint(a, b) # the variable `a` also changed unlike how R treats the variable\n\n\n\n\n\n\n\nIndexing in R vs.¬†Python\n\n\n\nAnother difference between R & Python is that the index (first value of any object) starts from 0 (python) rather than 1 (R). So if you have a list, the first element is python is starting at element 0.\n\n\nTo create a separate copy of the object, you need to use the .copy() method (python way of saying a function).\n\na = [1, 2, 3]\nb = a.copy() # create a separate copy of the list\nprint(a, b)\nb[0] = 4 # modify only `b`\nprint(a, b) # variable `a` does not change\n\nIn R, on the other hand, assignment creates a copy by default, so you don‚Äôt need to use a .copy() method. Any changes you make to one variable will not affect the other variable, because they are separate copies of the same object."
  },
  {
    "objectID": "labs/00_lab/setup.html#use-python-in-r",
    "href": "labs/00_lab/setup.html#use-python-in-r",
    "title": "General Instructions",
    "section": "Calling Python in R",
    "text": "Calling Python in R\nInstalling Python libraries\nYou can install any python package in R using the reticulate::py_install() command. This is similar to calling install.packages() in R. You may have heard of CRAN in R, a central repository for all R packages. In Python, the equivalent of CRAN is PyPI (Python Package Index). If you want the latest version of the packages, it is recommended to install packages using pip. To do so, you must set the argument pip=TRUE inside reticulate::py_install(), as demonstrated below.\nWe will install all the packages we need for this particular setup. This should be go smoothly with the following command:\n\n# Install python package into virtual environment\nreticulate::py_install(c(\"jupyter\", \"pandas\", \"matplotlib\",\"statsmodels\",\"scikit-learn\", \"seaborn\", \"mlxtend\", \"lime\", \"mkl-service\", \"xgboost\",\"scikit-learn-extra\"), envname = \"MLBA\", pip=TRUE)\n\n\n\n\n\n\n\nmkl-service for Mac Users\n\n\n\nPlease note that the mkl-service python package from above is specific to windows/linux users and is not needed for Mac users. In case of any errors (especially on M1 chips), feel free to remove it.\n\n\nLet‚Äôs checked if the package is installed successfully.\n\n# import package that is used for dealing with data.frames in Python (equivalent of tibble+dplyr)\npd <- reticulate::import(\"pandas\")\n# import the package for plotting in python\nplt <- reticulate::import(\"matplotlib.pyplot\")\n# import the library which we will use for linear regression\nsm <- reticulate::import(\"statsmodels.api\")\n\nNo error messages? Then installation was successful!\nCalling Python and R objects interchangeably\nAll the data types in R vs.¬†Python are explained below (image from reticulate‚Äôs home page). If you need to explicitly change between objects in R and Python (usually R handles that automatically) to get the objects from the image above, you can use reticulate::r_to_py() and reticulate::py_to_r() (e.g., R dataframes to pandas dataframes).\n\nTo run objects from python in R, you have to use $ to access their elements. For instance, when you want to load a python library, you can use reticulate::import() function and then assign it to a variable with the name of your choice as we did in the previous part. If that library contains a function (in python called module or sub-module), it always follows the format LIBRARY$FUNCTION(). We can see an example of that below:\n\n# Using R\n## load mtcars dataset\ndata(mtcars)\n\n## plot it using base R plot function (or ggplot)\nplot(mtcars$mpg, mtcars$disp)\n\n# Using Python\n# plot it using matplotlib in python (or another python library for plots)\nplt$scatter(mtcars$mpg,mtcars$disp)\nplt$xlabel('mpg', fontsize = 12)\nplt$ylabel('disp', fontsize = 12)\n\n# save the figure and then include in the Rmd\nplt$savefig(\"pyplot.png\")\nknitr::include_graphics(\"pyplot.png\")\n# alternatively, when not knitting, you can uncomment and run the two following lines\n# instead of save the figure\n# plt$show() # you always have to call this for the plot to be made\n# plt$clf() #this means clear figure\n\nNow, this is using R inside Python, but if you wanted to do it the other way around, that‚Äôs also possible by using a dot . instead of $. If you‚Äôre running python in markdown, you can replace {r...} at the beginning of the code chunk with {python...}, and it‚Äôll run python code. Additionally, if you would like to run a script instead (interactively or the whole script), you can do it by going to file > New File > Python Script in your Rstudio, and then running any part of your python script starts the interactive python session using reticulate::repl_python(). Alternatively, you can call the repl_python() to start the interactive session.\n```{python}\n# Your python code goes here\n```\nWe will now create a code chunk that purely runs Python and accesses the objects object with r.OBJECT_NAME:\n\n# we access mtcars dataset from R\nprint(r.mtcars.head())\n\nfrom sklearn import datasets\nimport pandas as pd\n\n# we open iris data from `sklearn` python package\niris = datasets.load_iris()\niris_data = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n\n# Get the head of the DataFrame\nprint(iris_data.head())\n\nWe can do the same thing in R by calling py$OBJECT_NAME by using reticulate::py$OBJECT_NAME:\n\n# plotting the iris data from python\nplot(py$iris_data)"
  },
  {
    "objectID": "labs/00_lab/setup.html#modelling-in-r-python",
    "href": "labs/00_lab/setup.html#modelling-in-r-python",
    "title": "General Instructions",
    "section": "Modelling in R & Python",
    "text": "Modelling in R & Python\nLet‚Äôs take a simple use case of making a regression in R and Python so you can see how the two languages compare:\n\n# remove the spaces and `(cm)` from the column names\nnames(py$iris_data) <- gsub(' ', '_', names(py$iris_data))\nnames(py$iris_data) <- gsub('_\\\\(cm\\\\)', '', names(py$iris_data))\n\n# example of running a model on iris data\nr_lm <- lm(\"sepal_length ~. \", data = py$iris_data)\nsummary(r_lm)\n\nWithin R, you can still use python to run the same linear regression with the statsmodels python library.\n\n# example of runnning lm model in python -> firstly, process the data\n# specify your dependent variable and independent variables\ny_iris = select(py$iris_data, \"sepal_length\")\nx_iris = select(py$iris_data, -\"sepal_length\")\n\n# for python approach, we need to add a constant to predictor variables\nx_iris = sm$add_constant(x_iris)\n\n# create a linear regression model and fit it to the data\npy_lm = sm$OLS(y_iris, x_iris)$fit()\n\n# get the model summary\nprint(py_lm$summary())\n\nAs you can see, the results are the same (those of statsmodels have been rounded). This could have also been done purely in python, as demonstrated below. Please note that here we have brought the iris data from python to R, then manipulated it in R (with the select() operation), and now we have two options to use these objects in pure python (just one is needed):\n\nWe can simply call the objects from R by r.x_iris and r.y_axis.\nWe can do the same select() operation in python with it‚Äôs own syntax on the original python iris data:\n\n\npy_y_iris = iris_data[\"sepal_length\"]\npy_x_iris = iris_data.drop(\"sepal_length\", axis=1)\n\nNeedless to say that in this case, the most efficient approach is the first one as demonstrated below:\n\n# load the sm library with it's alias\nimport statsmodels.api as sm\n\n# add the constant again\nx_iris = sm.add_constant(r.x_iris)\n\n# create a linear regression model and fit it to the data\npy_lm = sm.OLS(r.y_iris, r.x_iris).fit()\n\n# get the model summary\nprint(py_lm.summary())\n\nThe outcome is the same as calling python within R."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Important dates (more info during the class)\n\n\n\n\nMonday the 5th of May: Written exam (in-class)\nWednesday the 21th of May: Project report deadline\nMonday the 26th of May: Presentations of the projects & slides submission deadline\n\n\n\nNote that the schedule below way be updated after the first week. The lecture notes, i.e., lecture_qmd, will definitely be updated.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n    \n    \n    \n    \n  \n  \n    \n      Week\n      Date\n      Topic\n      Lecture Pdf\n      Lecture Qmd\n      Lab\n    \n  \n  \n    1\nMon, Feb 17\nIntroduction + EDA\nüßëüèª‚Äçüè´ Introduction\nüßëüèª‚Äçüè´ EDA\nüìÑ Introduction\nüìÑ EDA\n\n    2\nMon, Feb 24\nModels(regression & classification)\nüßëüèª‚Äçüè´ Intro to Models\nüßëüèª‚Äçüè´ Linear & Logistic Models\nü§ñ ML_LinLogReg.R\nüßëüèª‚Äçüè´ Trees\nü§ñ ML_Trees.R\nüìÑ Intro to Models\nüìÑ Linear & Logistic Models\nüìÑ Trees\n\n    3\nMon, Mar 3\nLab 1\n\n\nüìù Lab 1: Slides\nüíª Setup\nüíª LinLogReg\nüíª Tree\n    4\nMon, Mar 10\nMetrics(& Overfitting detection)\nüßëüèª‚Äçüè´ Neural Networks\nüßëüèª‚Äçüè´ Support Vector Machines\nüßëüèª‚Äçüè´ Metrics\nüìÑ Neural Networks\nüìÑ Support Vector Machines\nüìÑ Metrics\n\n    5\nMon, Mar 17\nLab 2\n\n\nüíª NN\nüíª SVM\nüíª Scoring\n    6\nMon, Mar 24\nData splitting + Ensemble methods\nüßëüèª‚Äçüè´ Data Splitting\nüßëüèª‚Äçüè´ Ensemble Methods\nüìÑ Data Splitting\nüìÑ Ensemble Methods\n\n    \nTue, Apr 1\nno lecture üò¢\n\n\n\n    7\nMon, Apr 7\nInterpretable ML + Unsupervised Learning\nüßëüèª‚Äçüè´ Interpretable ML\nüßëüèª‚Äçüè´ Intro to Unsupervised Learning\nüßëüèª‚Äçüè´ Clustering\nüßëüèª‚Äçüè´ Dimension Reduction\nüìÑ Interpretable ML\nüìÑ Intro to Unsupervised Learning\nüìÑ Clustering\nüìÑ Dimension Reduction\n\n    8\nMon, Apr 14\nLab 3\n\n\nüíª Splitting\nüíª Ensemble\n    9\nMon, Apr 21\nEaster(i.e., no lecture) üòä\n\n\n\n    10\nMon, Apr 28\nLab 4 + revisions\n\n\nüíª VarImp\nüíª Clustering\nüíª PCA\nüíª Autoencoders\n    11\nMon, May 5\nWritten exam(on site) ‚ùó\n\n\n\n    12\nMon, May 12\nWorking session(projects)\n\n\n\n    13\nMon, May 19\nWorking session(projects)\n\n\n\n    \n1¬†Wed, May 21\nProject report deadline(moodle) ‚ùó\n\n\n\n    14\nMon, May 26\nProject presentation deadline(moodle) + final presentations(on site) ‚ùó\n\n\n\n  \n  \n  \n    \n      1 üì© Report submission at 23:59\n    \n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe content of the schedule may be adapted (faster or slower)."
  },
  {
    "objectID": "assessments/Exam.html",
    "href": "assessments/Exam.html",
    "title": "Exam",
    "section": "",
    "text": "Exam questions with the solutions üìÑ\nAnswer booklet üìÑ\nAbove is a recent past exam. The course content varies from year to year, as well as its focus. Therefore, it must not be considered as a mock exam but as an example of the type of questions that can be asked. Furthermore, the answer booklet is an example of the file on which you will solve the exam. During the exam, a lot of flexibility is accepted in its use (copy-paste, include images, adapt answer space, etc.) as long as the presentation of the solutions follows the question orders."
  },
  {
    "objectID": "assessments/Presentation_Guidelines.html",
    "href": "assessments/Presentation_Guidelines.html",
    "title": "Presentation Guidelines",
    "section": "",
    "text": "A typical plan:\n\nTitle, date and team\nContext\nAgenda\n\nData description\nMethodology\nResults\nConclusion and discussion\nReferences if needed\n‚ÄúThank you + Any Question‚Äù slide\n\nMaterial:\n\n1-3 minutes per slide (e.g., around 10 to 20 slides for a 30-minutes presentation with only slides).\nKeep additional slides hidden (typical technical ones, extra results, etc.) and show them in case of questions/discussion.\nConsider also live demo, showing codes, etc.\n\nOrganization:\n\nIn group, define clearly who is going to explain what.\n\nMake fluent transition (useless to start over again the context when the speaker change).\n\nWalk away from these guidelines if they are not adapted in your case. Also, feel free to let your originality speaks.\n\nThe further guidelines below are based on the following link:\n\nKnow Your Audience\n\nAdapt your speech to your audience. If people think often about adapting the technical content, it is equally important to adapt the context. Including a deep explanation of a context to an audience who knows it can be as counter-productive and time consuming as not explaining this context when it is due.\n\nTell audience members up front why they should care and what is in it for them Explain the issue and why it is important.\nConvey your excitement. If any‚Ä¶ but do not overplay this.\nBuild a story:\n\nExplain the context\nFrame the problem\nProvide highlights: some parts in the analysis/project were striking and determinant. Find and explain these key points.\nConclude by summing up key points and open to new area, problems, etc.\n\nKeep it simple Depending on your audience, some terms may be difficult to understand. If you use difficult terms, you will not impress the audience, you will annoy them.\nPrepare, prepare, prepare:\n\nSet the stage (equipment ready, have a charger in case, have your headphone ready, etc.)\nGet ready (concentrate and calm down, breathe, visualize)\n\nSpeak up, talk to the audience (not to the screen)\nStart and finish on time. Nothing worse than a late presentation. If you need more time, mention it during the presentation and apologize, before to end of the presentation.\nDo not drift at the end The end of your presentation should be clear. Do not start a new conversation.\n\nFor the slides/support materials:\n\nLess Is More. Though, it is difficult to gives a general rule, a regular slide (not the title or the ‚ÄúThank you. Any Question?‚Äù slide) is 1 to 3 minutes. Thus, a 30 minutes presentation cannot have more than 30 slides, and it is already very long. If you are afraid to miss something, have some extra slides with the details that you may show if anyone has a question about it.\nCreate sections Pretty much like the report. It is especially important for group presentation that must be well structured.\nAvoid clutter A slide should be light and easy to read. It is a support for your speech. It is not to be read in detail neither by you nor by the audience.\nMake it readable It is useless to bold or to capitalize every word. Use one appropriate and coherent font. An incomprehensible slide does not become comprehensible or joyful if written in colors.\nUse visual Consider using image, figures, etc. ‚ÄúA drawing is worth a thousand words‚Äù. Of course, this is not an objective. Always choose the best way to convey information. If an image is only here to be ‚Äúnice‚Äù, prefer words.\nCheck your spelling We all make typos, especially when it is not our mother tongue. But having too many misspelled words on 20 slides with few words will make you lose credibility. Use correctors, re-read and re-read, ask for help, etc."
  },
  {
    "objectID": "assessments/Project_Directives.html",
    "href": "assessments/Project_Directives.html",
    "title": "Project Directives",
    "section": "",
    "text": "The project grade accounts for 70% of the final grade: 30% for the presentation and 40% for the report. The remaining 30% are for the exam. Grades are from 1 to 6 (Absent=0), rounded to the closest 0.1. The final grade (40% report + 30% presentation + 30% exam) is rounded to the closest 0.5.\n\n\nThe size of groups is 3 students. You may now register via our moodle page. Because we do not know the final number of participants yet, we may create groups of 4 by the end of the grouping process. By default, only groups of 3 can register for the moment.\n\n\n\n\nReportPresentation\n\n\n\n {.responsive-md .hover}\n\n\n\n\n\n\n\n\n\n\nAspect\nChoice of techniques\nImplementation\nInterpretation\nReporting quality (texts, graphs, tables)\nWeight\n\n\n\n\nData preprocessing: cleaning, scaling, feature engineering, missing data, exploration, representation\n2\n3\n2\n3\n10\n\n\nModel selection and implementation: model choices, splitting methods, metrics, variable selection\n7\n7\n6\n5\n25\n\n\nTuning: hyperparameters, solving overfitting, balancing\n6\n5\n3\n1\n15\n\n\nInterpretability: methods and application\n7\n3\n7\n3\n20\n\n\nUnsupervised method: clustering, dimension reduction\n3\n2\n4\n1\n10\n\n\nGlobal appreciation: clarity, structure, concept integration, appropriate vocabulary, originality\n-\n-\n-\n-\n20\n\n\nOverall weight\n-\n-\n-\n-\n100\n\n\n\n\n\n\n{.responsive-md .hover}\n\n\n\n\n\n\nAspect\nWeight\n\n\n\n\nDelivery: quality of slides, completeness\n25\n\n\nClarity: organization, logic, engagement, voice.\n15\n\n\nTime management\n15\n\n\nQ&A session: understanding, correct answer, clarity (no ambiguity)\n25\n\n\nOverall impression\n20\n\n\nOverall weight\n100\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nGrade (Absent=0, min=1, max=6) weighted average\nEach graded 1 (absent/non satisfactory) up to 5 (completely); eventually 5.5 (outstanding)\n\n\n\n\n\n\nThe project consists of an original analysis of one data set. We encourage the data set to be extracted from the sources provided on the webpage of the course or alternatively use web scraping to gather novel data. If you decide to use another one, please ask us a validation. In any case, the data source must be given.\nIf the data comes from a previous analysis, then your analysis must have new elements (new models, features, outcome labels, approaches, interpretations, etc.). In such case, this analysis must be explained, and the original aspects of the new analysis must be made clear, for example, being transparent on the important differences between your and the previous analysis.\n\n\n\nYour analysis must be comprehensive and not leave major aspects of your dataset unaddressed. The project must include at least:\n\nCleaning of the data, creation of new features if needed.\nExploratory Data Analysis.\nSupervised learning analysis: two or more models, two or more meaningful metrics, a tuning of one or more hyperparameters per model, a data splitting (if a training/test set split is enough for the global analysis, at least one CV or bootstrap must be used), addressing of the overfitting problem and unbalance, if applicable.\nInterpretability methods (variable importance and PDP).\nUnsupervised analysis: clustering and/or dimension reduction.\n\n\n\n\n\nReport: a full PDF or HTML report of your analysis with reproducible code and figures, not exceeding 30 pages (or equivalent for HTML), including title page and appendices.\n\nPresentation: the slides of your presentation.\n\n\n\n\nYou must meet the following deadlines (by 23h59):\n\n\n\nProject report: Sunday the 19th of May 2024 at 23h59\nPresentation slide: Thursday the 23rd of May 2024 at 23h59\n\nNot meeting the deadline (presentation and/or report) penalizes the grade by 0.1 per started hour of delay. No maximum penalty for the project report and presentation slide.\n\n\n\nPresentations will be organized on site on Monday the 27th of May 2024 in a 15+5-minutes format (presentation + questions). If the number of groups is large, then either the presentation time will be reduced, or, if not possible, another oral presentation session will be organized later (possibly during the exam session)."
  },
  {
    "objectID": "assessments/Proposal_guidelines.html",
    "href": "assessments/Proposal_guidelines.html",
    "title": "Proposal Guidelines",
    "section": "",
    "text": "ATTENTION\n\n\n\nüèó This section is not relevant for 2024 üèó\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe subsection titles belove are indicative\n\n\n\nIntroduction\n\nThe context of the data, domain, and/or field of application.\nThe source and/or reference of the data (where it was found, where/when it was previously used).\nThe global objective of the project and data analysis (i.e., research question).\n\n\nData description\n\nThe features: variable names, their description, their modality, etc.\nThe instances: what they represent (unit of observation), how many, etc.\nAny planned cleaning or data modification: removing, imputation, modality merging, etc.\n\n\nPlanned analysis\n\nSupervised learning: models, splitting strategy, metrics, etc.\n(optional) Unsupervised learning: methods used, how they will interact with supervised learning."
  },
  {
    "objectID": "assessments/Report_Guideline.html",
    "href": "assessments/Report_Guideline.html",
    "title": "Report Guidelines",
    "section": "",
    "text": "For your report, you can use the Quarto template we provide here. Please note that the Personal Information And Non-Plagiarism Statement indicated below must be integrated into the final report during your submission."
  },
  {
    "objectID": "assessments/Report_Guideline.html#why-do-you-write-a-technical-report",
    "href": "assessments/Report_Guideline.html#why-do-you-write-a-technical-report",
    "title": "Report Guidelines",
    "section": "Why Do You Write A (Technical) Report?",
    "text": "Why Do You Write A (Technical) Report?\nThe aim of writing a report is to communicate results from a study to someone who did not participate to it.\nThe report should be relevant, easy to read and convincing. Advices:\n\nTake some time to ask yourself what the aims of your report are, and to whom it is addressed.\nAsk yourself which messages you report should convey and which is the best way to convey it."
  },
  {
    "objectID": "assessments/Report_Guideline.html#level-of-details",
    "href": "assessments/Report_Guideline.html#level-of-details",
    "title": "Report Guidelines",
    "section": "Level Of Details",
    "text": "Level Of Details\nIncluding all the minutes of your research is NOT interesting. Relevance and reproducibility are the key words here. On the one hand, the report should include all the needed details for the reader to be able to repeat on his own the relevant part of the study. On the other hand, the report is not a course about a given subject. In particular, it is not expected that you include in the report all the theory about a method that is used. Of course, it is not easy to find the good balance, which anyway probably varies from one reader to another. Often, this contradiction can be solved by referencing to clear and accessible sources (paper, book chapter, webpage, etc.)."
  },
  {
    "objectID": "assessments/Report_Guideline.html#the-structure",
    "href": "assessments/Report_Guideline.html#the-structure",
    "title": "Report Guidelines",
    "section": "The Structure",
    "text": "The Structure\nThere are several ways to write a report. The following structure can be used and adapted according to the specificity of the study.\n\nA. Title Page (Mandatory)\nOne separate page. It should include\n\nTitle: large format. It should be short AND informative. Never more than a sentence.\nLogo: UNIL/HEC at least; optional: logo of the company\nDate of edition\nCourse name and the year\nGroup name\n\n\n\nB. Personal Information And Non-Plagiarism Statement (Mandatory)\nOne separated page containing\n\nFull names of all the group members and their email address\nThe following text hand-signed by all the authors. This project was written by us and in our own words, except for quotations from published and unpublished sources, which are clearly indicated and acknowledged as such. We are conscious that the incorporation of material from other works or a paraphrase of such material without acknowledgement will be treated as plagiarism, subject to the custom and usage of the subject, according to the University Regulations. The source of any picture, map or other illustration is also indicated, as is the source, published or unpublished, of any material not resulting from our own research.\n\n\n\nC. Summary Or Abstract\nUsually one page, it is limited to one-and-a-half-page maximum. The summary states very succinctly and clearly the issue, including a data brief description, the methods used, the main results, and the main conclusions. This summary allows someone to know, without to read your work, what has been done, why and what are the results and implications. Advises for writing it:\n\nWrite it at the very end the work, after the report body is written and clean.\nTake each section (Introduction, etc.) and summarize it in one, two, three, sentences, one paragraph or so. Use the needed space. State the important results.\nOnce finished, read it again. Cut/shorten long and useless words (e.g.¬†‚Äúhas been‚Äù -> ‚Äúwas‚Äù, ‚Äútherefore‚Äù -> ‚Äúhence‚Äù, etc.).\nIf it is still too long, cut some part by order of importance.\nThe summary is not a teaser to read the report. It contains the results. It should summary them. It should not be only inviting the reader to read the report but propose an alternative for readers who do not have time to read it all.\nTypically, ‚ÄúWe have made an analysis of the data in this report about the influential factors on the sales. We decided to use an ARIMA model to make the forecasts.‚Äù It contains a piece of information but is quite frustrating. More useful: ‚ÄúUsing an ARIMA model, the [what] sales were found influenced by the [what] factors. Such model achieved an RMSE of [what] on forecasting a [what] test set.‚Äù.\n\n\n\nD. Table Of Content (Toc), Table Of Figures (Tof) And Table Of Tables (Tot)\nToC is mandatory. ToF and ToT are nice to have especially for very long reports, or when these elements are central to the report. ToC must be up to date. You gain generating it automatically (easy with tools like word, LateX, Rmarkdown, etc.). ToF and ToT should also be generated automatically. This implies that figures and tables are correctly inserted with a caption.\n\n\nE. Body Of The Report\n\n1. Introduction\nIn brief, the introduction explains why the theme is interesting, what is done in the study and what it will bring. After reading it, the reader should have a broad view on the study, why it is relevant, and how it is organized. The purpose of the introduction is not to detail all theoretical aspects. Introduction is a section that should contain\n\nThe context and background: course, company name, business context.\nAim of the investigation: major terms should be defined, the question of research (more generally the issue), why it is of interest and relevant in that context.\nDescription of the data and the general material provided and how it was made available (and/or collected, if it is relevant). Only in broad terms however, the data will be further described in a following section. Typically, the origin/source of the data (the company, webpage, etc.), the type of files (Excel files, etc.), and what it contains in broad terms (e.g.¬†‚Äúa file containing weekly sales with the factors of interest including in particular the promotion characteristics‚Äù).\n\nThe method that is used, in broad terms, no details needed at this point. E.g. ‚ÄúModel based machine learning will help us quantifying the important factors on the sales‚Äù.\n\nAn outlook: a short paragraph indicating from now what will be treated in each following sections/chapters. E.g. ‚Äúin Section 3, we describe the data. Section 4 is dedicated to the presentation of the text mining methods‚Ä¶‚Äù\n\n\n\n2. Data Description\nThe data description section may be optional. It is typical of report for project oriented toward statistical and machine learning. The data description section can be included as a subsection of the introduction or be a whole section by itself, depending on the complexity and the size of the database. If relevant, it should contain\n\nDescription of the data file format (xlsx, csv, text, video, etc.)\nThe features or variables: type, units, the range (e.g.¬†the time, numerical, in weeks from January 1, 2012 to December 31, 2015), their coding (numerical, the levels for categorical, etc.), etc.\nThe instances: customers, company, products, subjects, etc.\nMissing data pattern: if there are missing data, if they are specific to some features, etc.\nAny modification to the initial data: aggregation, imputation in replacement of missing data, recoding of levels, etc.\nIf only a subset was used, it should be mentioned and explained; e.g.¬†inclusion criteria. Note that if inclusion criteria do not exist and the inclusion was an arbitrary choice, it should be stated as such. One should not try to invent unreal justifications.\n\nIn some cases, data structure can be more complex like images, videos, etc. This should be adapted accordingly. Any unknown element should be mentioned as such. Overall, this description can be quite short or very long, depending on the format of the data. For example, one xlsx file with 10 features and 50‚Äô000 instances representing customer behaviors is in general quite easy to describe. If you have 25 files with different formats from different stores representing these customers, with features and instances only partially matching, then it may be more difficult to deal with. Typically, the first and simple case would be included into the introduction as a subsection while the more complex case would deserve a whole data description section.\n\n\n3. Methods\nA very important section describing in detail all the methods that are used in the report. This section explains how the study was conducted. It aims at giving relevant information and tools for others to be able to reproduce the study. It explains the global strategy of the study, and describes each tool used to achieve this goal. For a comprehensive and structured presentation, often it is important to give the general structure of the study, and then to drill down each step. The individual tools (e.g.¬†machine learning methods, etc.) should be described in a concise and relevant way. This section is not a course. For example, one can give a brief definition and the important properties (important for the study), then refer to an external source for the details. In addition, the description should be balanced. Do not spend three pages on explaining the boxplots when a predictive model of random forest is explained in one-half page. It should describe and duly cite\n\nStatistical and machine learning methods to analyze the data\nSoftware, computer programs, function packages (typically for R).\nThe sequence of analysis, the reason of each method.\n\nHowever,\n\nIt does not include methods that are not used, even if you spent a lot of time on it. Except maybe if the fact that a particular method is not useful is of interest for the report.\nIt is not a course about a method. It should contain all the relevant pieces of information, but it should also be stopped at some points. Clear references and well-chosen are of major importance here.\n\n\n\n4. Results\nObviously, this section presents the results: application of the methods to the data. It presents all the results (even negative ones) and only the results (no more data or method descriptions). This section should be very well structured especially if there are lot of results. Use subsections and clear references. In addition, use the appendix. For example, if 30 time series were analyzed and the corresponding 30 figures produced. Then only the most interesting ones should be included in the results section. The other ones should be reported in the appendix (or supplementary material) and appropriately cited. The interpretation can be included at this point but should be limited to technical aspects. Keep discussions for the next section.\n\n\n5. Recommendations And Discussion\nIn this section, the interpretations of the results are given:\n\nSummary of what was observed from the results\nImplications for the business (link with the context and the initial objectives of the study).\nLimitations (this may be the subject of a separated section). An opening is often expected here: ‚ÄúWhat next?‚Äù ‚ÄúWhat can be done for improvement‚Äù, etc.?\n\n\n\n6. References\nAll external sources should be cited. The APA system is a good and safe one. One can use the Reference tool of Word. For more details, see http://www.apastyle.org/manual/index.aspx.\n\n\n7. Appendix\nThe appendix section is not a trash in which one puts all what was done but not integrated in the body of the report. All the elements in the appendix must be cited in the text. Appendix can contain graphs, texts, tables, codes, etc. For example, in the body of the report: ‚ÄúFig. 1 presents the analysis of the times of the process A [‚Ä¶]. The 29 other processes are represented in Fig. 40 to 68 in Appendix B.‚Äù\n\n\n\nF. Supplementary Material\nIn this section, you mention and briefly describe all the external documents given with the report. For example, a code file name ‚ÄúOurSuperRCode.R‚Äù attached to the report should be mentioned here."
  },
  {
    "objectID": "assessments/Report_Guideline.html#further-remarks",
    "href": "assessments/Report_Guideline.html#further-remarks",
    "title": "Report Guidelines",
    "section": "Further Remarks",
    "text": "Further Remarks\n\nGeneral Format And Style\n\nFont: Times New Roman, 11pts\nPage format: A4\nText justification: even on both side\nMargins: 2.5cm\nMax. 40 pages for the body of the report (i.e.¬†not including title, toc, introduction, reference, appendix, sup. mat.). It is not an expected value. It is an upper bound.\n\n\n\nText editor\nUsing Word is probably the simplest choice, but \\(LaTeX\\) is also welcomed (see Overleaf for easy and free usage) Freely available alternatives exist like LibreOffice. It should be noted that Quarto and Rmarkdown could also be a useful tool. However, be mindful of mixing research (i.e.¬†working R codes, running complex R code or procedure) and the writing of the report. Often, one loses more time and encounter lots of difficulties. Nevertheless, Quarto is very valuable for writing reproducible reports. In any cases, pay attention to use cache in order to avoid running again and again very heavy computations each time you want to compile the report.\n\n\nMaths and formulae\nIf you need to edit equations or formulae, you may gain using an equation editor like MathWord. Free alternatives exist like LibreOffice.\n\n\nNumbers, figures and other remarks\nUsually, in the text, numbers zero (0) to ten (10) are written in letters, then 11, 12, etc. This not for reference like ‚Äúin Table 1‚Äù should never be written ‚Äúin Table one‚Äù. Figures, tables and sections referenced in the text are names an should have a capital letter. One should write ‚Äúin Table 1‚Äù, ‚Äúin Figure 3 shown below‚Äù, ‚ÄúSection 3.2 shows [‚Ä¶]‚Äù. It is not the case when you refer to an object like ‚Äúthe next table‚Äù, ‚Äúin the following section we can see [‚Ä¶]‚Äù, etc.\nDo not use don‚Äôt (except when quoting speeches for example). For a better scientific writing, check out this blog."
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Tip\n\n\n\nIf you do not find the answer to your question here, you can use the search bar on the top left to search the entire site. If you are still unable to find the answer, you can ask your question during the exercise hours. If you found the solution and think it would be useful to other students, you can report an issue (see on the right) with the questions and the solution, and we will add it to the FAQ."
  },
  {
    "objectID": "faq.html#organizational-issues",
    "href": "faq.html#organizational-issues",
    "title": "FAQ",
    "section": "Organizational issues",
    "text": "Organizational issues\n\nDo we use moodle?\nWe will be using moodle to collect the assessments, and any communication with you. On the other hand, we will use this website to upload the content for the course. You are encouraged to continuously check the website for any material update.\n\n\nWhat is the difference between Pdf and Qmd of the lectures?\nThey contain the same material, except that format-wise, Qmd files are more accessible to navigate and go through when preparing for the exam.\n\n\nHow are the lab sessions organized?\nThe lab exercises on different topics can be followed at your own pace, hence why all the materials are available on this website. You may go slower or faster through the content and then show up for support during one of the four physical lab sessions to ask your questions.\n\n\nWhich programming language will we be using?\nYou can use either R or Python. We will be providing code examples and support in both languages.\n\n\nWhat software do I need to install?\nYou need a text editor or an Integrated Development Environment (IDE). RStudio or VS Code are both good options. We also recommend the use of git and version control."
  },
  {
    "objectID": "faq.html#technical-issues",
    "href": "faq.html#technical-issues",
    "title": "FAQ",
    "section": "Technical issues",
    "text": "Technical issues\n\nHow do I ask a question or report an issue or suggestion regarding the content?\nFollow these steps to propose changes/suggestions to correct the material. First, start off by making a GitHub account:\n\nCreate a GitHub account at https://github.com. You can use whatever username you like, but here is some helpful username advice.1. With your UNIL email address, you can also get the GitHub Student Developer Pack\nGo to https://github.com/do-unil/mlba/issues/new/choose and select either ‚ÄúSuggestion‚Äù or ‚ÄúTypo‚Äù.\n\nIf Suggestion, describe in detail your suggestion, giving as much information and justification as possible.\nIf Typo, fill in the blanks to provide as much information as possible for the typo to be fixed.\n\nWhen done, hit Submit. \nBefore opening an issue, you must the existing ones at https://github.com/do-unil/mlba/issues to make sure you‚Äôre not reporting something someone else has already reported.\nIssues can refer to the text of the lecture or the labs.\n\n\n\nI am lost with Github, what should I do?\nGitHub has possibly the best tutorials and documentation of any software out there. You can find the Hello world there. Most of the time googling your issue will lead you to the right place.\n\n\n\n\n\n\nAdvice for you\n\n\n\nGitHub is super useful for your projects, and working in teams. Ilia will provide you with a tutorial on how to use it.\n\n\n\n\nI have bugs in my code/ something doesn‚Äôt work/ I don‚Äôt know how to do something, what should I do?\nFirst google your issue. 90 times out of 100 this will solve your issues. Another 9% can be resolved by reading the documentation of the software you are using. For the very last percent you can ask your question during the exercises hours."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning in Business Analytics 2024-2025",
    "section": "",
    "text": "üßëüèª‚Äçüè´ Teaching staff: Dr.¬†Marc-Olivier Boldi (Lecturer) & Ilia Azizi (TA).\nüïõ Time: Mondays 8:30-12:00, starting on Monday the 17th of February 2025.\nüè´ Room: Anthropole 3021 (no live broadcasting or recording).\n\nüìñ Content\nThis course presents several machine learning techniques in business and management contexts. The list of topics is meant to cover mainly supervised methods of classification and prediction although unsupervised methods are also seen. Below is a tentative lists of topics. It will be adapted according to the pace of the class:\n\nSupervised learning models: regressions, trees, support vector machine, neural networks.\nData splitting: training/test sets, cross-validation, bootstrap.\nMetrics: MSE, Accuracy, ‚Ä¶\nEnsemble methods: bagging, random forests, boosting.\nUnsupervised learning: clustering, PCA, FAMD, Auto-Encoder.\nInterpretable machine learning: variale importance, partial dependence plots, LIME.\n\nExercises and theory are equally crucial for the success of the class. For the labs, we use primarily R, with equivalent code in Python.\n\n\nüìù Evaluation\n\nPartial written exam: organized during the semester.\nApplied project: individual or in group (depending on the number of participants to the course).\n\nOne report (incl.¬†supplementary material, codes, etc.)\nOne final presentation will be organized during the semester or during the exam session (depending on the number of participants).\n\n\nFinal grade = (0.3 x exam) + (0.3 x presentation) + (0.4 x report)\n\n\n\n\n\n\nProject member contributions\n\n\n\nThe project grade is a group grade. However, if the contribution of the members of the groups to the project is unbalanced, an individual adaptation of the grade will be made, e.g., absence of one of the members of the group during the presentation (in catch-up, a subsequent and adapted presentation of the absent member may be required).\n\n\nFurther directives and guidelines are provided in the Assessments section."
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html",
    "title": "Models: Linear and logistic regressions",
    "section": "",
    "text": "The dataset we‚Äôll be using for the first part of the exercise is real estate transaction prices in Taiwan, which can be accessed from this link this link. This dataset was modified for this exercise. The modified file real_estate_data.csv is in the exercise folder under /data/.\nThe aim is to predict the house prices from available features: No, Month, Year, TransDate, HouseAge, Dist, NumStores, Lat, Long, Price. No is the transaction number and will not be used.\n\nFirst, an EDA of the data is needed. After exploring the structure, the Price is shown with the year and month.\n\nreal_estate_data <- read.csv(here::here(\"labs/data/real_estate_data.csv\"))\n\n## adapt the path to the data\n# if you encountered any error with the encoding of the data (`Error in gregexpr...`), just re-run the code again\nstr(real_estate_data)\nlibrary(summarytools)\ndfSummary(real_estate_data)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nreal_estate_data %>% ggplot(aes(x=Month, y=Price, fill=as.factor(Year))) + \n  geom_boxplot()+ facet_wrap(~as.factor(Year))\n\nThe results show how important it is to make an EDA! It appears that the data does not contain transactions for all the months of 2012 and 2013, but just some months by the end of 2012 and the first half of 2013. This shows that it is pointless to use month and year here. This is why we prefer TransDate, a value indicating the transaction time on a linear scale (e.g., 2013.250 is March 2013).\nNow we focus on the link between Price and the other features.\n\nlibrary(GGally)\nreal_estate_data %>% \n  select(Price, HouseAge, Dist, Lat, Long, TransDate) %>% \n  ggpairs()\n\nNo clear link appears. The linear regression will help to discover if a combination of the features can predict the price.\n\nFirst, we split the data into training/test set (75/25).\n\nset.seed(234)\nindex <- sample(x=c(1,2), size=nrow(real_estate_data), replace=TRUE, prob=c(0.75,0.25)) # 1==training set, 2==test set\ndat_tr_restate <- real_estate_data[index==1,]\ndat_te_restate <- real_estate_data[index==2,]\n\nThen, we fit the linear regression to the training set.\n\n\nR\nPython\n\n\n\n\nmod_lm <- lm(Price~TransDate+\n               HouseAge+\n               Dist+\n               NumStores+\n               Lat+\n               Long, data=dat_tr_restate)\nsummary(mod_lm)\n\n\n\n\n# In R, we load the conda environment as usual\nlibrary(reticulate)\nreticulate::use_condaenv(\"MLBA\", required = TRUE)\ngc(full = TRUE)\n\nIn python, we then use the statsmodels library to fit a linear regression model to the training data and perform feature elimination. We use the .fit() method to fit the model with the formula for the variable names. Note that python‚Äôs summary() function is unique to the statsmodels libraries and produces similar information to its R counterpart.\n\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"\n\nimport mkl\n# %env OMP_NUM_THREADS=1\n# set the number of threads. Here we set it to 1 to avoid parallelization when rendering quarto, but you can set it to higher values.\nmkl.set_num_threads(1)\n\n\n# Import necessary library\nimport statsmodels.formula.api as smf\n\n# Fit a linear regression model to the training data & print the summary\nmod_lm_py = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long', data=r.dat_tr_restate).fit()\nprint(mod_lm_py.summary())\n\nIt‚Äôs not a suprise that the results are same as the ones obtained in R.\n\n\n\n\nThe stepwise variable selection can be performed using the function step. By default, it is a backward selection; see ?step for details (parameter direction is backward when scope is empty).\n\n\nR\nPython\n\n\n\n\nstep(mod_lm) # see the result\nmod_lm_sel <- step(mod_lm) # store the final model into mod_lm_sel\nsummary(mod_lm_sel)\n\n\n\nAs python does not have an exact equivalent of stats::step() function, which performs both forward and backward selection based on AIC, we have to implement it manually. We start with the full model and iteratively remove the feature with the highest p-value and add the feature with the lowest AIC until we can no longer improve the AIC. The final model is stored in mod_lm_sel. For an extensive explanation of what this while loop is doing and how the backward+forward is computed, check the code below:\n\nExplaining feature elimination in python (while loop)\nWe start by setting mod_lm_sel_py to the full model mod_lm_py. Then, we enter a while loop that continues until we break out of it. In each iteration of the loop, we store the current model in prev_model for later comparison. We start by dropping the feature with the highest p-value from the current model using idxmax(), which returns the label of the maximum value in the pvalues attribute of the mod_lm_sel_py object. We exclude the intercept term from the list of labels by specifying labels=['Intercept']. We then create a new model using smf.ols() with the feature removed and fit it to the training data using fit(). We store this new model in mod_lm_sel_py.\nNext, we check whether the AIC of the new model is larger than the previous model‚Äôs. If it is, we break out of the while loop and use the previous model (prev_model) as the final model. If not, we continue to the next step of the loop. Here, we look for the feature with the lowest AIC among the remaining features using idxmin() on the pvalues attribute, again excluding the intercept term. We create a new model by adding this feature to the current model using smf.ols(), fit it to the training data using fit(), and store it in mod_lm_sel_new.\nWe then check whether the AIC of the new model is larger than that of the current model. If it is, we break out of the while loop and use the current model (mod_lm_sel_py) as the final model. If not, we update mod_lm_sel_py with the new model and continue to the next iteration of the loop. This way, we iteratively remove the feature with the highest p-value and add the feature with the lowest AIC until we can no longer improve the AIC. The final model is stored in mod_lm_sel_py.\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# perform both forward and backward selection using AIC\nmod_lm_sel_py = mod_lm_py\n\nwhile True:\n    prev_model = mod_lm_sel_py\n    # drop the feature with the highest p-value\n    feature_to_drop = mod_lm_sel_py.pvalues.drop(labels=['Intercept']).idxmax()\n    mod_lm_sel_py = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long - ' + feature_to_drop, data=r.dat_tr_restate).fit()\n    # check if AIC has increased, if yes, break the loop and use the previous model\n    if mod_lm_sel_py.aic > prev_model.aic:\n        mod_lm_sel_py = prev_model\n        break\n    \n    # add the feature with the lowest AIC\n    feature_to_add = mod_lm_sel_py.pvalues.drop(labels=['Intercept']).idxmin()\n    mod_lm_sel_py_new = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long + ' + feature_to_add, data=r.dat_tr_restate).fit()\n    # check if AIC has increased, if yes, break the loop and use the previous model\n    if mod_lm_sel_py_new.aic > mod_lm_sel_py.aic:\n        break\n    mod_lm_sel_py = mod_lm_sel_py_new\n    \nprint(mod_lm_sel_py.summary())\n\n\n\n\nAfter identifying the most important features, you can fit a new model using only those features and evaluate its performance using the test set.\nThe final model does not contain Long. In terms of interpretations, for example:\n\nThe price increased on average by 3.7 per year (TransDate)\nIt diminishes in average by (-2)2.4 per year (HouseAge)\netc.\n\nWe now predict the prices in the test set. We can make a scatter plot of the predictions versus the observed prices to inspect that. We already know by looking at the \\(R^2\\) in the summary that the prediction quality is not good.\n\n\nR\nPython\n\n\n\n\nmod_lm_pred <- predict(mod_lm_sel, newdata=dat_te_restate)\nplot(dat_te_restate$Price ~ mod_lm_pred, xlab=\"Prediction\", ylab=\"Observed prices\")\nabline(0,1) # line showing the obs -- pred agreement\n\n\n\n\nmod_lm_sel_pred = mod_lm_sel_py.predict(r.dat_te_restate)\nfig, ax = plt.subplots()\nax.scatter(x=mod_lm_sel_pred, y=r.dat_te_restate['Price'])\nax.set_xlabel('Prediction')\nax.set_ylabel('Observed prices')\nax.plot(ax.get_xlim(), ax.get_ylim(), ls=\"--\", c=\".3\")\nplt.show()\n\n\n\n\nIt appears that the lowest and the highest prices are underestimated. At the center (around 30), the prices are slightly overestimated.\nAs an exercise, write down the prediction equation of the selected model. Use this equation to explain how instances 1 and 2 (test set) are predicted and calculate the predictions manually. Verify your results using the predict function from the previous R code.\n\nAnswer\n\nmod_lm_pred[c(1,2)]\n\n\\[\ny = -0.000133 + 3.66\\times TransDate -0.243\\times HouseAge \\\\-0.00464\\times Dist + 1.027\\times NumStores + 237.8\\times Lat\n\\]"
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#modelling-1",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#modelling-1",
    "title": "Models: Linear and logistic regressions",
    "section": "Modelling",
    "text": "Modelling\nWe can split our data and fit the logistic regression. The function for this is glm. This function encompasses a larger class of models (namely, the generalized linear models) which includes the logistic regression, accessible with family=‚Äúbinomial‚Äù.\n\nset.seed(234)\nindex <- sample(x=c(1,2), size=nrow(DocVis), replace=TRUE, prob=c(0.75,0.25)) # 1==training set, 2==test set\ndat_tr_visit <- DocVis[index==1,]\ndat_te_visit <- DocVis[index==2,]\n\n\n\nR\nPython\n\n\n\n\nvis_logr <- glm(visits~., data=dat_tr_visit, family=\"binomial\")\nsummary(vis_logr)\n\n\n\n\n# a hack around this technique to not type all the variable names\nvis_formula = 'visits ~ ' + ' + '.join(r.dat_tr_visit.columns.difference(['visits']))\n\n# create a logistic regression model\nvis_logr_py = sm.formula.logit(formula= vis_formula, data=r.dat_tr_visit).fit()\nprint(vis_logr_py.summary())\n\n\n\n\n\n\n\nUsing . for formulas in R vs Python\n\n\n\nIn R, the dot . is used as shorthand to indicate that we want to include all other variables in the formula as predictors except for the outcome variable. So, if our outcome variable is y and we want to include all other variables in our data frame as predictors, we can write y ~ . in the formula.\nIn Python, however, the dot . is not used in the same way in formulas. Instead, to include all other variables as predictors except for y, we would write y ~ x1 + x2 + ... where x1, x2, etc. represent the names of the predictor variables. Also, statsmodels has a similar syntax to R base regressions. In most other typical ML libraries in Python, you must provide the column values instead of using the column names.\n\n\nNote that the family=\"binomial\" argument in R is not needed in Python since sm.formula.logit() assumes the logistic regression model is fitted using a binomial distribution by default."
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#variable-selection-interpretation-1",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#variable-selection-interpretation-1",
    "title": "Models: Linear and logistic regressions",
    "section": "Variable selection & interpretation",
    "text": "Variable selection & interpretation\nNow, we can apply the variable selection:\n\n\nR\nPython\n\n\n\n\nvis_logr_sel <- step(vis_logr)\nsummary(vis_logr_sel)\n\n\n\nAs already seen in the linear regression part, in python, we don‚Äôt have the same implementation of the step function, hence why we designed the while loop earlier. It is good practice to create a single function with this step while loop to handle all cases (linear, logistic etc); however, we only implement it here for logistic regression. Therefore, to tackle this, we will create a function that does step-wise elimination for us. We define a function called forward_selected that performs forward selection on a given dataset to select the best predictors for a response variable based on AIC. The function takes two arguments: data, a pandas DataFrame containing the predictors and response variable, and response, a string specifying the name of the response variable.\n\nFor more explanation of the code, click on me\nThe function first initializes two sets: remaining and selected. remaining contains the names of all columns in the data DataFrame except for the response variable, while selected is initially empty. The function then initializes current_aic and best_new_aic to infinity. The main loop of the function continues as long as remaining is not empty and current_aic is equal to best_new_aic. At each iteration, the function iterates over all columns in remaining and computes the AIC for a logistic regression model that includes the response variable and the currently selected predictors, as well as the current candidate predictor. The function then adds the candidate predictor and its AIC to a list of (aic, candidate) tuples, and sorts the list by increasing AIC. The function then selects the candidate with the lowest AIC and adds it to the selected set, removes it from the remaining set, and updates current_aic to the new lowest AIC. The function continues this process until no candidate can improve the AIC. Finally, the function fits a logistic regression model using the selected predictors and returns the resulting model.\n\n# code taken from the link below and adjusted for logistic regression with AIC criteria\n# https://planspace.org/20150423-forward_selection_with_statsmodels/\n\ndef forward_selected(data, response):\n    \"\"\"Linear model designed by forward selection.\n\n    Parameters:\n    -----------\n    data : pandas DataFrame with all possible predictors and response\n\n    response: string, name of response column in data\n\n    Returns:\n    --------\n    model: an \"optimal\" fitted statsmodels linear model\n           with an intercept\n           selected by forward selection\n           evaluated by AIC\n    \"\"\"\n    remaining = set(data.columns)\n    remaining.remove(response)\n    selected = []\n    current_aic, best_new_aic = float(\"inf\"), float(\"inf\")\n    while remaining and current_aic == best_new_aic:\n        aics_with_candidates = []\n        for candidate in remaining:\n            formula = \"{} ~ {} + 1\".format(response,\n                                           ' + '.join(selected + [candidate]))\n            model = smf.logit(formula, data).fit(disp=0)\n            aic = model.aic\n            aics_with_candidates.append((aic, candidate))\n        aics_with_candidates.sort()\n        best_new_aic, best_candidate = aics_with_candidates.pop(0)\n        if current_aic > best_new_aic:\n            remaining.remove(best_candidate)\n            selected.append(best_candidate)\n            current_aic = best_new_aic\n    formula = \"{} ~ {} + 1\".format(response,\n                                   ' + '.join(selected))\n    model = smf.logit(formula, data).fit(disp=0)\n    return model\n\n\nmod_logit_sel_py = forward_selected(r.dat_tr_visit, 'visits')\n\nprint(mod_logit_sel_py.summary())\n\nWe can see that the results of mod_logit_sel_py model are slightly different from the R version, but nevertheless, we have reduced the features and the interpretations (see below) with both R and python versions remain the same.\n\n\n\nWe can see that the probability of a visit is\n\nsmaller for males\nincreasing with age\nlarger with illness\netc."
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#inference-1",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#inference-1",
    "title": "Models: Linear and logistic regressions",
    "section": "Inference",
    "text": "Inference\n\n\nR\nPython\n\n\n\nThe predict function with type=‚Äúresponse‚Äù will predict the probability of the positive class (‚Äú1‚Äù). If it is set to ‚Äúlink‚Äù it produces the linear predictor (i.e., the \\(z\\)). To make the prediction, we thus have to identify if the predicted probability is larger or lower than 0.5.\n\nprob_te_visit <- predict(vis_logr_sel, newdata = dat_te_visit, type=\"response\")\npred_te_visit <- ifelse(prob_te_visit >= 0.5, 1, 0)\ntable(Pred=pred_te_visit, Obs=dat_te_visit$visits)\n\n\n\nThe explanation is similar to that of R, with a slight different that here we use pandas.crosstab to make our confusion matrix.\n\nimport pandas as pd\n\nprob_te_visit = mod_logit_sel_py.predict(r.dat_te_visit)\npred_te_visit = [1 if p >= 0.5 else 0 for p in prob_te_visit]\nconf_mat = pd.crosstab(pred_te_visit, r.dat_te_visit['visits'], rownames=['Pred'], colnames=['Obs'])\nprint(conf_mat)\n\nThe results are extremely close to the R version.\n\n\n\nThe predictions are not really good. It is in fact a difficult data set. Indeed, the number of 0 is so large compare to the 1, that predicting a 0 always provides a good model overall. That issue will be addressed further later on in the course.\nFor now, this can be further inspected by looking at the predicted probabilities per observed label.\n\n\nR\nPython\n\n\n\n\nboxplot(prob_te_visit~dat_te_visit$visits)\n\n\n\n\nfig, ax = plt.subplots()\nax.boxplot([prob_te_visit[r.dat_te_visit['visits']==0], prob_te_visit[r.dat_te_visit['visits']==1]])\nax.set_xticklabels(['No Visit', 'Visit'])\nax.set_ylabel('Predicted Probability')\nax.set_title('Predicted Probabilities by Visit Status')\nplt.show()\n\n\n\n\nWe see that if the lowest predicted probabilities are usually assigned to 0-observations, most of the probabilities remain below 0.5 (even for the 1-observations). A good model would have two well separated boxplots, well away from 0.5.\nNow, as an exercise, write down the prediction equation of the selected model, like you did for linear regression. Use this equation to explain how instance 1 and 2 (test set) are predicted, and calculate the predictions manually. Verify your results using the function predict used before.\n\nAnswer\n\nprob_te_visit[c(1,2)]\n\n\\[\nz(x) = -2.31795-0.31838\\times gender\\_male+0.39762\\times age+\\\\0.28431\\times illness+0.16340\\times reduced+0.05589\\times health+\\\\0.27249\\times private\\_eyes -0.65344\\times freepoor\\_yes+\\\\0.38038\\times freerepat\\_yes  \n\\] Then \\[\nP(Y=1 | X=x) = \\frac{e^{z(x)}}{1+e^{z(x)}}\n\\]"
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#linear-regression-nursing-home-data",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#linear-regression-nursing-home-data",
    "title": "Models: Linear and logistic regressions",
    "section": "Linear regression: nursing home data",
    "text": "Linear regression: nursing home data\nNow it is your turn. Make an linear regression (also feel free to try lasso and ridge regressions) on the nursing data described below (found also in /data/nursing_data.csv). Afterwards, use linear regression to build a predictor of the cost using the other features. Replicate the analysis. Split the data, build a model, make the variable selection, make the predictions and analyze the results. Make also an analysis of the coefficients in terms of the associations between the costs and the features.\n\nData Description\nThe data set is about patients in a nursing home, where elderly people are helped with daily living needs, also known as Activities of Daily Living (ADL, i.e.¬†communication, eating, walking, showering, going to a toilet, etc.).\nSince the stay in such facilities is very expensive, it is important to classify the new-coming patient, and estimate the duration of the stay and the corresponding costs.\nIn practice, there are different types of patients who require different types of help and, consequently, different duration of the stay. For example, there could be a person with severe mobility issues, who requires the help with most of the needs every day; or a person with mental deviations, who don‚Äôt need help with daily routine, but requires extra communication hours.\nHere, we will focus of total amount of help (measured in minutes of help provided to a person per week) provided and measure the costs of stay of a person.\nThe data set on which the analysis is based has the following columns:\n\n\ngender: a categorical variable with levels ‚ÄúM‚Äù for male and ‚ÄúF‚Äù for female\n\nage: integer variable\n\nmobil: categorical variable that represents the physical mobility with levels\n\n1 = Full mobility\n2 = Reduced mobility\n3 = Restricted mobility in the house\n4 = Null mobility\n\n\n\norient: categorical variable that represents the orientation (interactions with the environment) with levels\n\n1 = Full orientation\n2 = Moderate disturbance of orientation\n3 = Disorientation\n\n\n\nindepend: categorical variable that represents the independence of ADL with levels\n\n1 = Independent of help\n2 = Dependent less than 24 hours per day\n3 = Dependent at unpredictable time intervals for most of the needs\n\n\n\nminut_mob: numerical variable that represents the total number of minutes of help with movement per week\n\nneed_comm: categorical variable with levels ‚ÄúYes‚Äù for a person who needs extra communication sessions with an employee, and ‚ÄúNo‚Äù otherwise\n\nminut_comm: numerical variable that represents the total number of minutes of communication per week\n\ntot_minut: numerical variable that represents the total number of minutes spent on a patient per week, \\(tot\\_minut = minut\\_mob + minut\\_comm\\)\n\n\ncost: numerical variable that represents the total costs of having a patient in the nursing house per month.\nNote: since tot_minut=minut_mob+minut_comm, you may not find any meaningful result using the 3 features. This is perfectly normal. Just use 2 features only among these 3 (arbitrary choice)."
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#logistic-regression-the-credit-quality",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#logistic-regression-the-credit-quality",
    "title": "Models: Linear and logistic regressions",
    "section": "Logistic regression: the credit quality",
    "text": "Logistic regression: the credit quality\nThe German Credit Quality Dataset consists of a set of attributes as good or bad credit risks. In order to find find a detailed description of the features, please refer to the original link to the dataset. The german.csv file can also be found in /data/german.csv whichis the mdified version of the original dataset to simplify the analysis, especially the data loading in R.\nThe aim here is to predict the credit quality from the other features. The outcome Quality is 0 for ‚Äúbad‚Äù and 1 for ‚Äúgood‚Äù. Make an analysis of the data and develop the learner. You can follow these notable steps:\n\nMake a simple EDA of the features\nSplit the data and train the model.\nMake variable selection and check out the result.\nInterpret the coefficients.\nInspect the quality of the model by making the predictions (confusion table and boxplot of the predicted probabilities).\n\nNote that the data are unbalanced again and that you may not find a very good predictor. This issue is quite difficult and will be addressed later."
  },
  {
    "objectID": "labs/03_Models/032_Trees/Ex_ML_Tree.html",
    "href": "labs/03_Models/032_Trees/Ex_ML_Tree.html",
    "title": "Models: CART",
    "section": "",
    "text": "In this exercise, the classification tree method is used to analyze the data set Carseats from the package ISLR. The exercise took some inspiration from this video.\n\nFirst, install the package ISLR in order to access the data set Carseats. Use ?Carseats to read its description. To apply a classification of the sales, we first create a categorical outcome SaleHigh which equals ‚ÄúYes‚Äù if Sales > 7.5 and ‚ÄúNo‚Äù otherwise. Then we create a data frame MyCarseats containing SaleHigh and all the features of Carseats except Sales. Finally, split MyCarseats into a training and a test set (2/3 vs 1/3). Below we call them df_tr and df_te.\n\nlibrary(ISLR)\nlibrary(dplyr)\nMyCarseats <- Carseats %>% mutate(SaleHigh=ifelse(Sales > 7.5, \"Yes\", \"No\"))\nMyCarseats <- MyCarseats %>% select(-Sales)\n\nset.seed(123) # for reproducibility \nindex_tr <- sample(x=1:nrow(MyCarseats), size=2/3*nrow(MyCarseats), replace=FALSE)\ndf_tr <- MyCarseats[index_tr,]\ndf_te <- MyCarseats[-index_tr,]\n\nNote: this is not the point of this exercise but remember that in a real situation the first step of the data analysis would be an EDA.\n\n\n\nR\nPython\n\n\n\nThe rpart function in the package rpart can be used to fit a classification tree with the same type of formulas as naive_bayes. It can then be plotted using the function rpart.plot of the package rpart.plot.\n\nlibrary(rpart)\nlibrary(rpart.plot)\nset.seed(1234)\ncarseats_tree <- rpart(SaleHigh ~ ., data=df_tr)\nrpart.plot(carseats_tree)\n\n\n\n\n# Load MLBA environment\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\nWe use the df_train and df_te created in R to carry out our CART training. For this, we will use the sklearn library. First, we copy our dataset from the R variable to a python variable. Then we use any encoder (e.g.¬†OneHotEncoder, LabelEncoder, OrdinalEncoder) from sklearn.preprocessing to convert categorical data into a numeric form, which many sklearn machine learning algorithms require. In this case, we use the OneHoteEncoder which is similar to making dummy variables and turn each level of the category into a column. Also, we standardize the data in the case of python for faster computations using StandardScaler, which also helps to bring numerical stability and improve the results. After this, we divide the training and test sets into predictors (e.g., X_train) and the outcome (e.g., y_train) and initialize our classification tree and fit the model to the data.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n# We copy the training and testing datasets from R to Python.\ncarset_tr_py = r.df_tr.copy()\ncarset_te_py= r.df_te.copy()\n\n# We encode categorical variables as numeric, which is necessary for CART.\nle = OneHotEncoder()\ncat_vars = [\"ShelveLoc\", \"Urban\", \"US\", \"SaleHigh\"]\nfor var in cat_vars:\n    # Fit the LabelEncoder to the training set and transform the training and testing sets\n    # We need to use the same encoder for both sets to ensure consistency\n    carset_tr_py_encoded = le.fit_transform(carset_tr_py[[var]].values.reshape(-1, 1))\n    carset_te_py_encoded = le.transform(carset_te_py[[var]].values.reshape(-1, 1))\n    carset_tr_py[var] = carset_tr_py_encoded.toarray()\n    carset_te_py[var] = carset_te_py_encoded.toarray()\n\n# Split the data into training and testing sets\nX_train, y_train = carset_tr_py.drop(columns=[\"SaleHigh\"]), carset_tr_py[\"SaleHigh\"]\nX_test, y_test = carset_te_py.drop(columns=[\"SaleHigh\"]), carset_te_py[\"SaleHigh\"]\n\n# Standardize only the continuous variables\ncont_vars = [\"CompPrice\", \"Income\", \"Advertising\", \"Population\", \"Price\", \"Age\", \"Education\"]\nscaler = StandardScaler()\nX_train[cont_vars] = scaler.fit_transform(X_train[cont_vars])\nX_test[cont_vars] = scaler.transform(X_test[cont_vars])\n\n# To speed up the operation you can also transform the inputs\n# X_train = X_train.to_numpy()\n# y_train = y_train.to_numpy()\n\nWe can now train and plot our decision tree using DecisionTreeClassifier and plot_tree functions.\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nimport matplotlib.pyplot as plt\n\n# we clear any previous figures\nplt.clf()\n\nnp.random.seed(1234)\ncarseats_tree = DecisionTreeClassifier().fit(X_train, y_train)\nplt.figure(figsize=(20,10))\nplot_tree(carseats_tree,filled=True, feature_names=X_train.columns, label = 'root',fontsize=5)\nplt.savefig('tree_high_dpi', dpi=300)\n# for a better quality, save the image and load it again\n#plt.show()\n\nAs the image dimensions are not always great for matplotlib plots in Rstudio, we saved the image and now load it again below using an R chunk.\n\nknitr::include_graphics(\"tree_high_dpi.png\")\n\n\n\n\n\n\n\nR\nPython\n\n\n\nThe analysis of the tree complexity can be obtained using function plotcp.\n\nplotcp(carseats_tree)\n\nFrom the graph, we can identify that, according to the 1-SE rule, the tree with 8 nodes is equivalent to the tree with 12 nodes. This 8-nodes tree should be preferred.\nTo prune the tree (i.e., extract the tree with 8 nodes), we can use the function prune with argument cp. The cp of the tree can be read on the bottom x-axis of the plotcp. The argument in prune should be set to any value between the cp of 8-nodes tree (0.031) and the 11-node tree (0.019). Here 0.025 is OK.\n\ncarseats_tree_prune <- prune(carseats_tree, cp=0.025)\nrpart.plot(carseats_tree_prune)\n\nImportant note: The CP evaluation relies on a cross-validation procedure, which uses random number generation. This is why we have set the seed to some value (1234). You may not find the same result with another seed or if you do not set it. In any case, just be coherent with your results and prune the tree accordingly.\nLet the computer do the work for you: Pruning using the 1-SE rule can be automatically obtained with the function autoprune in package adabag. Note that, because of the randomness involved in the CP evaluation, you may not find exactly the same result as the one obtained by hand. Use set.seed to make your result reproducible.\n\nlibrary(adabag)\nset.seed(123455)\nrpart.plot(autoprune(SaleHigh ~ ., data=df_tr))\n\n\n\nThe rpart.plot package in R provides a convenient plotcp() function to plot the complexity parameter table for a decision tree fit with rpart(). Unfortunately, scikit-learn doesn‚Äôt provide an equivalent function out of the box. However, you can still calculate the complexity parameter values for your decision tree and plot them using matplotlib.\n\n# Get the paths of the leaf nodes for the Car Seats decision tree using cost complexity pruning\npath = carseats_tree.cost_complexity_pruning_path(X_train, y_train)\n# Extract the effective alphas and total impurities of the leaf nodes from the path object\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n# Create a plot to visualize the relationship between effective alphas and total impurities\nfig, ax = plt.subplots()\nax.plot(ccp_alphas, impurities, marker='o', linestyle=\"-\")\nax.set_xlabel(\"Effective alpha\")\nax.set_ylabel(\"Total impurity of leaves\")\nax.set_title(\"Total Impurity vs Effective alpha for Car Seats dataset\")\nax.invert_xaxis()\nplt.show()\n\nThis plot only shows the training set results, which doesn‚Äôt tell us about over-fitting. A better approach is to compute accuracy (a different metric than rpart) on a second test set, called the validation set used solely for finding the ideal hyperparameters in machine learning. Once we choose the best hyperparameters, we re-train the model one final time with those parameters and compare everything on the test set (but we should no longer change our models based on the test set). We will learn more about this validation set during the upcoming lectures. Here we‚Äôll use the two functions cross_val_score and KFold from the sklearn.module_selection sub-module. We will use ten folds to find the ideal alpha (equivalent to cp from rpart::rpart()). This is not using the 1-SE rule but proposes a good alternative.\n\nfrom sklearn.model_selection import cross_val_score, KFold\n\ndef plotcp(X_train, y_train, random_state=123):\n    # Create a decision tree classifier\n    clf = DecisionTreeClassifier(random_state=random_state)\n\n    # Calculate the cross-validation scores for different values of alpha\n    path = clf.cost_complexity_pruning_path(X_train, y_train)\n    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n    # Perform cross-validation for each alpha\n    kfold = KFold(n_splits=10, shuffle=True, random_state=random_state)\n    scores = []\n    for ccp_alpha in ccp_alphas:\n        clf = DecisionTreeClassifier(random_state=random_state, ccp_alpha=ccp_alpha)\n        score = cross_val_score(clf, X_train, y_train, cv=kfold, scoring='accuracy')\n        scores.append(np.mean(score))\n\n    # Plot the cross-validation scores vs alpha\n    fig, ax = plt.subplots()\n    ax.plot(ccp_alphas, scores, marker='o', linestyle=\"-\")\n    ax.set_xlabel(\"ccp_alpha\")\n    ax.set_ylabel(\"Cross-validation score (accuracy)\")\n    ax.set_title(\"Pruning Complexity Parameter (ccp) vs Cross-validation Score\")\n    ax.invert_xaxis()\n    plt.show()\n\nplotcp(X_train, y_train)\n\nWe can see that the best validation scores are obtained around a ccp_alpha of 0.008. Similar to cp from rpart, in scikit-learn, the parameter controls the complexity of a classification tree by setting a penalty on the number of leaf nodes. A higher value of results in a simpler tree with fewer splits and more nodes being pruned. More specifically, alpha is the regularization parameter used for controlling the cost complexity of the tree. The cost complexity is the sum of the misclassification cost and the complexity cost of the tree. The complexity cost is proportional to the number of terminal nodes (leaves) in the tree. A higher value of alpha thus means that the model is less likely to overfit the training data and more likely to generalize better to new, unseen data. However, setting alpha too high can result in underfitting and poor model performance on both the training and test data. The optimal alpha value depends on the specific dataset and the problem being solved and can be determined through cross-validation or other model selection techniques, as demonstrated above.\nOnce you find the ideal alpha, you can specify it with the ccp_alpha argument in DecisionTreeClassifier(). Here we will take ccp_alpha as 0.008 for simplicity.\n\n# Create a decision tree classifier with a ccp_alpha of 0.025\ncarseats_tree_prune = DecisionTreeClassifier(random_state=123, ccp_alpha=0.008)\n\n# Fit the model to the data\ncarseats_tree_prune.fit(X_train, y_train)\n\n\n\n\nDecisionTreeClassifier(ccp_alpha=0.008, random_state=123)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nDecisionTreeClassifier\n\n?Documentation for DecisionTreeClassifieriFitted\nDecisionTreeClassifier(ccp_alpha=0.008, random_state=123) \n\n\n\n# You can again plot the figure with\nplt.clf()\nplt.figure(figsize=(12,10))\nplot_tree(carseats_tree_prune,filled=True, feature_names=X_train.columns, label = 'root',fontsize=5)\nplt.savefig('tree_pruned_high_dpi', dpi=200)\nplt.close()\n\n\nknitr::include_graphics(\"tree_pruned_high_dpi.png\")\n\nThis model is a lot simpler compared to the first tree made using python (where ccp_alpha was 0 by default).\nFor automatically pruning the tree, unlike the adabag::autoprune() function in R‚Äôs adabag package, scikit-learn does not have a built-in function for automatic pruning of decision trees. Instead, you can use cross-validation to determine the optimal tree depth and use that to prune the tree.\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Set up the decision tree classifier\ntree = DecisionTreeClassifier()\n\n# Set up the grid search parameters\nparam_grid = {'max_depth': range(1, 11)}\n\n# Run grid search cross-validation to find optimal tree depth\ngrid_search = GridSearchCV(tree, param_grid=param_grid, cv=10)\ngrid_search.fit(X_train, y_train)\n\n\n\n\nGridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n             param_grid={'max_depth': range(1, 11)})\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\nGridSearchCV\n\n?Documentation for GridSearchCViFitted\nGridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n             param_grid={'max_depth': range(1, 11)}) \n\n\nbest_estimator_: DecisionTreeClassifierDecisionTreeClassifier(max_depth=3) \n\nDecisionTreeClassifier\n?Documentation for DecisionTreeClassifierDecisionTreeClassifier(max_depth=3) \n\n\n\n\n# Use the best estimator to fit and prune the tree\npruned_tree = grid_search.best_estimator_\n\nplt.clf()\nfig, ax = plt.subplots(figsize=(15, 10))\nplot_tree(pruned_tree, ax=ax, feature_names=X_train.columns)\nplt.show()\n# you can choose to re-train the model once again with this new parameter\n\n\n\n\n\n\n\nPython approach vs adabag::autoprune()\n\n\n\nThe technique above does not explicitly use the 1-SE rule for pruning the decision tree. Instead, it uses cross-validation to find the optimal tree depth based on the mean test score across all folds. According to the documentation of adabag::autoprune(), ‚ÄúThe cross validation estimation of the error (xerror) has a random component. To avoid this randomness the 1-SE rule (or 1-SD rule) selects the simplest model with a xerror equal or less than the minimum xerror plus the standard deviation of the minimum xerror.‚Äù If you‚Äôre interested in the 1-SE, see the adapted python code for it below.\nImplementing GridSearchCV with 1-SE rule\n\n# Set up the decision tree classifier\ntree = DecisionTreeClassifier()\n\n# Set up the grid search parameters\nparam_grid = {'max_depth': range(1, 11)}\n\n# Run grid search cross-validation to find optimal tree depth\ngrid_search = GridSearchCV(tree, param_grid=param_grid, cv=10)\ngrid_search.fit(X_train, y_train)\n\n\n\n\nGridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n             param_grid={'max_depth': range(1, 11)})\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\nGridSearchCV\n\n?Documentation for GridSearchCViFitted\nGridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n             param_grid={'max_depth': range(1, 11)}) \n\n\nbest_estimator_: DecisionTreeClassifierDecisionTreeClassifier(max_depth=3) \n\nDecisionTreeClassifier\n?Documentation for DecisionTreeClassifierDecisionTreeClassifier(max_depth=3) \n\n\n\n\n# Calculate the mean and standard error of test scores for each tree depth\nmean_scores = grid_search.cv_results_['mean_test_score']\nstd_scores = grid_search.cv_results_['std_test_score'] / np.sqrt(10)\n\n# Find the optimal depth using the 1-SE rule\noptimal_depth = grid_search.best_params_['max_depth']\noptimal_score = mean_scores[optimal_depth - 1]\nse = std_scores[optimal_depth - 1]\nbest_depth = optimal_depth\nfor depth in range(optimal_depth - 1, -1, -1):\n    score = mean_scores[depth]\n    if score + se < optimal_score:\n        break\n    else:\n        best_depth = depth + 1\n\n# Use the best estimator to fit and prune the tree\npruned_tree = DecisionTreeClassifier(max_depth=best_depth)\npruned_tree.fit(X_train, y_train)\n\n\n\n\nDecisionTreeClassifier(max_depth=3)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nDecisionTreeClassifier\n\n?Documentation for DecisionTreeClassifieriFitted\nDecisionTreeClassifier(max_depth=3) \n\n\n\nThe results are the same as not using 1-SE.\n\n\n\n\n\n\n\nFirst, use the R plot to determine what is the prediction of the first instance of MyCarseats.\nAnswer\n\nMyCarseats[1,]\n\nFollow Left, Left, Left => The predicted answer is ‚ÄúNo‚Äù.\n\n\nR\nPython\n\n\n\nThe function predict can be used build the predictions of the test set (use option type=\"class\"). This is similar to the previously seen models: the predict function used on an object of class .rpart (created by the function rpart), in fact, calls the function predict.rpart which is adapted to the model.\n\npred <- predict(carseats_tree_prune, newdata=df_te, type=\"class\")\ntable(Pred=pred, Obs=df_te$SaleHigh)\n\nNote that, like most categorical models, you may ask for the probabilities instead of the classes by setting type=\"prob\".\n\npredict(carseats_tree_prune, newdata=df_te, type=\"prob\")\n\n\n\nTo print a confusion matrix of the predicted labels versus the true labels, we can use the crosstab() function from the pandas library. We pass the predicted and true labels as arguments to crosstab(), and use the rownames and colnames parameters to label the rows and columns of the table, respectively.\nFinally, we use the predict_proba() method of the decision tree classifier to predict the class probabilities for the test data. The predicted probabilities are stored in the y_probs variable, which is a NumPy array with shape (n_samples, n_classes).\n\n# Predict the class labels for the test data with the python implementation\ny_pred = carseats_tree_prune.predict(X_test)\n\n# Print a confusion matrix of the predicted labels versus the true labels\nprint(pd.crosstab(index=y_pred, columns=y_test, rownames=['Pred'], colnames=['Obs']))\n\nThis tree is worse than the R implementation, probably due to differences in other default values that we did not tune for.\n\n# Predict the class probabilities for the test data\ny_probs = carseats_tree_prune.predict_proba(X_test)\nprint(pd.DataFrame(y_probs))\n\n\n\n\n\nBy looking at the tree, interpret the most important features for the Sales: the highest in the tree.\nNote that, for the level Good of the ShelveLoc variable, only the Price drives the Sales (according to the tree). Otherwise, it is a subtle mixture between Price and CompPrice."
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html",
    "title": "Models: Neural Networks",
    "section": "",
    "text": "In this tutorial, we will demonstrate how to use keras and tensorflow in R for two common tasks in deep learning (DL): regression for structured data and image classification. We will use two datasets for this purpose: the Boston Housing dataset for regression and the CIFAR-10 dataset for image classification."
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#boston-housing-dataset",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#boston-housing-dataset",
    "title": "Models: Neural Networks",
    "section": "Boston Housing Dataset",
    "text": "Boston Housing Dataset\nThe Boston Housing dataset is a well-known dataset used for regression tasks. It contains 506 instances and 13 features, including the median value of owner-occupied homes in $1000s. We will use this dataset to demonstrate how to perform regression on the sales price.\n\nlibrary(keras)\n\nhouses <- dataset_boston_housing()\n\ntrain_x <- houses$train$x\ntrain_y <- houses$train$y\n\ntest_x <- houses$test$x\ntest_y <- houses$test$y"
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#data-preparation",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#data-preparation",
    "title": "Models: Neural Networks",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe need to normalize the data. This is especially relevant for neural networks to stabilize the computation of the gradients and consequently improve the training process.\n\nmeans <- apply(train_x, 2, mean)\nsds <- apply(train_x, 2, sd)\n\ntrain_x <- scale(train_x, means, sds)\ntest_x <- scale(test_x, means, sds)"
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-definition",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-definition",
    "title": "Models: Neural Networks",
    "section": "Model Definition",
    "text": "Model Definition\nWe will use a simple neural network with two hidden layers to perform regression.\n\n\n\n\n\n\nLearning about some of the hyperparameters\n\n\n\nYou have many options for different hyperparameters; however, one lab session barely scratches the surface of DL and its hyperparameters. There are a couple of points that we need to specify here:\n\nYou can use any activation function in the middle layers, from a simple linear regression (linear) to more common ones such as hyperbolic tagnet or tanh (often suitable for tabular data) to more sophisticated ones such as rectified linear unit or relu (better suited to high dimensional data). What is imperative is that in the last dense layer, the number of units and the activation function determine the kind of task (e.g., classification, regression, etc.) you‚Äôre trying to accomplish. If you‚Äôre doing a regression, the last dense layer has to have 1 unit and a linear activation function. If you‚Äôre doing a binary classification (logistic regression), you still use 1 dense unit but must apply the sigmoid activation function. If you‚Äôre doing multi-class classification, then the number of dense units must equal the number of classes, and the activation function is softmax (which is a generalization of sigmoid for multiple classes). Google also provides a nice visual that explains the difference between the classification models. If you remove the >0.5 rule (i.e., sigmoid), you essentially get a linear regression for that layer.\n\n\n\n\n\n\n\n\n\n\nIt is imperative that, depending on the task, you use the correct type of loss function. For instance, you can use mean squared error (mse) for regression and categorical cross-entropy for multi-class classification.\n\nAs mentioned, during the ML course, we cannot cover the details of DL. Suppose you want to understand these hyperparameters better and learn about many new ones. In that case, you can check out the video recordings and the slides for the Deep learning course previously taught at HEC (last given in 2021).\n\n\n\n\nR\nPython\n\n\n\n\n# we set a seed (this sometimes disables GPU performance)\ntensorflow::set_random_seed(123)\n\n# define the model structure\nmodel_reg <- keras_model_sequential() %>%\n  layer_dense(units = 64, activation = \"relu\", input_shape = c(ncol(train_x))) %>%\n  layer_dense(units = 64, activation = \"relu\") %>%\n  layer_dense(units = 1, activation = \"linear\")\n\n# compile the model\nmodel_reg %>% compile(\n  optimizer = \"adam\",\n  loss = \"mean_squared_error\",\n  metrics = c(\"mean_absolute_error\")\n)\n\n\n\nWe can run the same thing in Python with the datasets created in R, e.g.¬†r.train_x.\n\n# import necessary libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Set random seed\ntf.random.set_seed(123)\n\n# define the model structure\nmodel_reg = keras.Sequential([\n    layers.Dense(units=64, activation='relu', input_shape=(r.train_x.shape[1],)),\n    layers.Dense(units=64, activation='relu'),\n    layers.Dense(units=1, activation='linear')\n])\n\n# compile the model\nmodel_reg.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])"
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-training",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-training",
    "title": "Models: Neural Networks",
    "section": "Model Training",
    "text": "Model Training\nWe can then train the model:\n\n\nR\nPython\n\n\n\n\nhistory <- model_reg %>% fit(\n  train_x, train_y,\n  epochs = 10, #300 to get the best results\n  batch_size = 32,\n  validation_split = 0.2,\n  callbacks = callback_early_stopping(patience = 30,restore_best_weights = T),\n  verbose = 1 # to control the printing of the output\n)\n\nYou don‚Äôt need to assign model %>% fit() to history. We only do that to plot the results later (e.g., with plot(history).\n\n\n\n# import necessary libraries\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# fit the model\nmodel_reg.fit(\n    r.train_x, r.train_y,\n    epochs=10, #300\n    batch_size=32,\n    validation_split=0.2,\n    callbacks=[EarlyStopping(patience=30,restore_best_weights=True)],\n    verbose=1\n)"
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-evaluation",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-evaluation",
    "title": "Models: Neural Networks",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nFinally, we can evaluate the model on the testing set:\n\n\nR\nPython\n\n\n\n\nnn_results <- model_reg %>% evaluate(test_x, test_y)\nnn_results\n# alternatively, you can use the `predict` attribute from `model` as shown below\n# nn_results <- model$predict(test_x)\n# caret::MAE(obs = test_y, pred = as.vector(nn_results))\n\n\n\n\nnn_results = model_reg.evaluate(r.test_x, r.test_y)\nprint(nn_results)\n\n\n\n\nTo put these results (from R) in context, we can compare it with a simple linear regression:\n\ndf_tr <- data.frame(train_y, train_x)\nlm_mod <- lm(train_y ~ ., data = df_tr)\nlm_preds <- as.vector(predict(lm_mod, newdata=data.frame(test_x))) # `predict()` for lm doesn't accept matrices\n\n# calculate mean absolute error with caret `MAE` installed in the `K-NN excercises`\ncaret::MAE(obs = test_y, pred = lm_preds)\n\nWe see that the neural network does significantly better than a regression model. We can also compare it with the trees seen in the CART series.\n\nlibrary(rpart)\nset.seed(123)\ntree_model <- rpart(train_y ~ ., data=df_tr)\ntree_preds <- predict(tree_model,newdata = data.frame(test_x))\n\n# calculate the performance on tree\ncaret::MAE(obs = test_y, pred = tree_preds)\n\nThe neural network also outperforms CART (if you run NN for 300 epochs). This is due to multiple reasons, including a higher complexity of the neural network (more parameters), using a validation set, and so on. You will learn about ensemble methods (bagging and boosting) in the upcoming lectures. Ensemble methods are the true champions for structured data and usually outperform neural networks for structured/low-dimensional data."
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#cifar-10-dataset",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#cifar-10-dataset",
    "title": "Models: Neural Networks",
    "section": "CIFAR-10 Dataset",
    "text": "CIFAR-10 Dataset\nThe CIFAR-10 dataset is a well-known dataset used for image classification tasks. It contains 50,000 training images and 10,000 testing images of size 32x32 pixels, each belonging to one of ten classes. We will use this dataset to demonstrate how to perform image classification.\nIn this case, we had our inputs already prepared, but if it was not the case, you can always do:\n\n# Load the CIFAR-10 dataset (it'll take a while for it to download)\ncifar10 <- dataset_cifar10()\n\n# Split the data into training and testing sets\ntrain_images <- cifar10$train$x\ntrain_labels <- cifar10$train$y\ntest_images <- cifar10$test$x\ntest_labels <- cifar10$test$y\n\nIf you do an str(train_images), you can see that the output is a four-dimensional array. In this array, the first element is the number of images in the training set, the second and third elements represent the (pixel values) for each image‚Äôs width and height, respectively, and the last element is the color channel of the image. If you had a black-and-white image (called greyscale), the number of channels would be just one. Typically, an image in color is represented with a combination of three colors known as RGB (red, green, and blue). Therefore, here we have three elements associated with the pixel number for each color, and their combination provides us with the colors you see in the image."
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#data-preparation-1",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#data-preparation-1",
    "title": "Models: Neural Networks",
    "section": "Data Preparation",
    "text": "Data Preparation\nBefore building our image classification model, we need to prepare the data by normalizing the values to obtain pixel values between 0-1. Similar to standardization of tabular data, this helps to avoid exploding/vanishing gradients and can improve the convergence of the model.\n\n# Normalize the pixel values\ntrain_images <- train_images / 255\ntest_images <- test_images / 255\n\nWe can also plot some of the example images.\n\n# Plot the first 9 images\nlabel_names <- c(\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n\npar(mfrow = c(3, 3))\nfor (i in 1:9) {\n  img <- train_images[i,,,]\n  # label <- train_labels[i]\n  label <- label_names[train_labels[i] + 1]  # Add 1 to convert from 0-based to 1-based indexing\n  img <- array_reshape(img, c(32, 32, 3))\n  plot(as.raster(img))\n  title(paste(\"Label:\", label))\n}\n\nWe can now one-hot encode our labels to be better adapted to the loss function we will be using (otherwise you can use ‚Äúsparse‚Äù variation of the loss functions).\n\ntrain_labels <- to_categorical(train_labels)\ntest_labels <- to_categorical(test_labels)"
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-definition-1",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-definition-1",
    "title": "Models: Neural Networks",
    "section": "Model definition",
    "text": "Model definition\nWe can build our image classification model using keras by defining the layers of the neural network. For this task we‚Äôll be using Convolutional neural networks (CNNs). CNNs are a type of deep learning model that are commonly used for image and video processing. They consist of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The convolutional layers use filters to extract features from the input image, and the pooling layers downsample the output of the convolutional layers. The fully connected layers combine the extracted features to produce the final output of the network. CNNs have been shown to be effective in a wide range of applications, including image classification, object detection, and semantic segmentation.\n\n# Set a seed again for reproducibility\ntensorflow::set_random_seed(123)\n\n# Define the model\nmodel_cnn <-\n  keras_model_sequential()%>%\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3), input_shape = c(32, 32, 3),  activation = \"relu\") %>%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %>%\n  layer_conv_2d(filters = 64, kernel_size = c(5, 5), activation = \"relu\") %>%\n  layer_max_pooling_2d(pool_size = c(3, 3)) %>%\n  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = \"relu\") %>%\n  layer_flatten() %>%\n  layer_dense(units = 64, activation = \"relu\") %>%\n  layer_dense(units = 10, activation = \"softmax\")\n\n# Compile the model\nmodel_cnn %>% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = optimizer_adam(),\n  metrics = c(\"accuracy\")\n)\n\nDepending on youer computer, this may take some time to run.\n\nhistory <- model_cnn %>% fit(\n  x = train_images,\n  y = train_labels,\n  epochs = 10,\n  batch_size = 64,\n  # validation_split = 0.2\n  validation_data= list(test_images, test_labels)\n)\n\n\nmodel_cnn %>% evaluate(\n  x = test_images,\n  y = test_labels\n)\n\nYou may notice that we did not obtain extremely high accuracy, but this is okay for multiple reasons:\n\nWe only trained the model for a few epochs and stopped the training very early. You‚Äôre welcome to let it run for more epochs.\nWe‚Äôre dealing with at least ten classes; this is a rather complicated problem.\nIn DL, you can define many different architectures and hyperparameters, and since we did not play around with these values, this could also explain the lower performance."
  },
  {
    "objectID": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html",
    "href": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html",
    "title": "Models: Support Vector Machine",
    "section": "",
    "text": "In this exercise, we apply the Support Vector Machines (SVM) to the classification problem of the data set Carseats from the package ISLR (already used with CART).\nSVM are often difficult to interpret. They are typically what we call ‚Äúa black box‚Äù model, that is, a model which provides predictions without understanding what is behind (except if you have followed the course‚Ä¶) and how the features influences these predictions. Therefore, and because we do not want to get bored, we will also use the caret::train() function to make our first real machine learning application. Note that this application can be done also with the models that have been seen previously.\n\nThe lines below are just a repetition/reminder of the CART series to have the data ready. The only difference is the cast of SalesHigh into factors rather than characters because the SVM functions require it.\nTo proceed we first have to build the data (below is the) Install the package ISLR in order to access the data set Carseats. Use ?Carseats to read its description.\nTo apply a classification of the sales, we first create a categorical outcome SaleHigh which equals ‚ÄúYes‚Äù if Sales > 7.5 and ‚ÄúNo‚Äù otherwise. Then we create a data frame MyCarseats containing SaleHigh and all the features of Carseats except Sales. Finally, split MyCarseats into a training and a test set (2/3 vs 1/3). Below we call them df_tr and df_te.\n\nlibrary(ISLR)\nlibrary(dplyr)\nMyCarseats <- Carseats %>% mutate(SaleHigh=ifelse(Sales > 7.5, \"Yes\", \"No\"))\nMyCarseats <- MyCarseats %>% select(-Sales)\nMyCarseats$SaleHigh <- as.factor(MyCarseats$SaleHigh)\n\nset.seed(123) # for reproducibility \nindex_tr <- sample(x=1:nrow(MyCarseats), size=0.8*nrow(MyCarseats), replace=FALSE)\ndf_tr <- MyCarseats[index_tr,]\ndf_te <- MyCarseats[-index_tr,]\n\n\n\n\nR\nPython\n\n\n\nThe e1071::svm() function of the e1071 package allows to fit SVM to the data with several possible kernels. Below, it is the linear kernel. We fit a linear kernel and check the predictions on the test set.\n\nlibrary(e1071)\nset.seed(123)\ncarseats_svm <- svm(SaleHigh ~ ., data=df_tr, kernel=\"linear\")\ncarseats_svm\ncarseats_svm_pred <- predict(carseats_svm, newdata = df_te)\n\ntable(Pred=carseats_svm_pred, obs=df_te$SaleHigh)\n\nTo obtain a better insight about the prediction quality, we will use the accuracy measure. It is simply the proportion of correct predictions. This can be conveniently obtained (and much more) from the function caret::confusionMatrix() of the library caret. In the parameters, data are the predictions, and reference are the observations.\n\nlibrary(caret)\nconfusionMatrix(data=carseats_svm_pred, reference = df_te$SaleHigh )\n\nWe should only focus on the accuracy for now (the other measures will be studied later). In our run, it was ‚âà\\(86\\%\\). That number is obtained using the default parameter, cost \\(C=1\\).\n\n\n\n# The usual loading of our environment\nlibrary(reticulate)\nuse_condaenv(\"MLBA\", required = TRUE)\n\nSimilar to the CART exercises, we use the df_train and df_te created in R to carry out our SVM training. Once again, we continue using the sklearn library. First, we copy our dataset from the R variable to a python variable. Then we use LabelEncoder() from sklearn.preprocessing to convert categorical data into a numeric form, which many sklearn machine learning algorithms require. The encoding assigns a unique numerical value to each categorical value, which can sometimes help the performance. In the case of caret::train(), the function handles this transformation automatically. Also, we standardize the data in the case of python for faster computations using StandardScaler, because of the same numerical stability mentioned for CART. After this, we divide the training and test sets into predictors (e.g., X_train) and the outcome (e.g., y_train) and initialize our linear kernel SVM to fit the model to the data.\nWe will use classification_report and accuracy_score from sklearn.metrics to get more information on the performance. (you could also use confusion_matrix from the same module.)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import svm\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n\n# We copy the training and testing datasets from R to Python.\ncarset_tr_py = r.df_tr.copy()\ncarset_te_py= r.df_te.copy()\n\n# We encode categorical variables as numeric, which is necessary for SVM.\nle = LabelEncoder()\ncat_vars = [\"ShelveLoc\", \"Urban\", \"US\", \"SaleHigh\"]\nfor var in cat_vars:\n    # Fit the LabelEncoder to the training set and transform the training and testing sets\n    # We need to use the same encoder for both sets to ensure consistency\n    carset_tr_py[var] = le.fit_transform(carset_tr_py[var])\n    carset_te_py[var] = le.transform(carset_te_py[var])\n\n# Split the data into training and testing sets\nX_train, y_train = carset_tr_py.drop(columns=[\"SaleHigh\"]), carset_tr_py[\"SaleHigh\"]\nX_test, y_test = carset_te_py.drop(columns=[\"SaleHigh\"]), carset_te_py[\"SaleHigh\"]\n\n# Standardize only the continuous variables\ncont_vars = [\"CompPrice\", \"Income\", \"Advertising\", \"Population\", \"Price\", \"Age\", \"Education\"]\nscaler = StandardScaler()\nX_train[cont_vars] = scaler.fit_transform(X_train[cont_vars])\nX_test[cont_vars] = scaler.transform(X_test[cont_vars])\n\n# To speed up the operation you can also transform the inputs\n# X_train = X_train.to_numpy()\n# y_train = y_train.to_numpy()\n\n# Set random seed for reproducibility\nnp.random.seed(123)\n\n# Initialize a linear SVM model\ncarseats_svm_py = svm.SVC(kernel=\"linear\")\n\n# Fit the SVM model to the training data\ncarseats_svm_py.fit(X_train, y_train)\n\n\n\n\nSVC(kernel='linear')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nSVC\n\n?Documentation for SVCiFitted\nSVC(kernel='linear') \n\n\n# Print the model parameters\nprint(carseats_svm_py)\n# Make predictions on the test set\ncarseats_svm_pred_py = carseats_svm_py.predict(X_test)\n\n# Print a confusion matrix\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_py, rownames=['True'], colnames=['Predicted']))\n# Alternatively, print a confusin matrix using `sklearn.metrics`\n# print(confusion_matrix(y_test, carseats_svm_pred_py))\n\n# Compute metrics\nreport_linear_py = classification_report(y_test, carseats_svm_pred_py, target_names=['No', 'Yes'])\naccuracy_linear_py = accuracy_score(y_test, carseats_svm_pred_py)\n\nprint(report_linear_py)\nprint(\"Overall accuracy:\", accuracy_linear_py)\n\nThis performance is worse than our model in R and requires more tuning (due to differences in default values and implementations of the functions).\n\n\n\n\n\n\nR\nPython\n\n\n\nWe try now with a radial basis kernel (the default).\n\nset.seed(123)\ncarseats_rb <- svm(SaleHigh ~ ., data=df_tr, kernel=\"radial\")\ncarseats_rb\ncarseats_rb_pred <- predict(carseats_rb, newdata = df_te)\nconfusionMatrix(data=carseats_rb_pred, reference = df_te$SaleHigh )\n\nThe accuracy is now ‚âà\\(81\\%\\). This shows how important that choice can be. In the same vein, we are now relying on the default parameters of the function. For the cost \\(C\\) it is \\(1\\), for the parameter gamma of the kernel, it is 1/(data dimension) (see ?svm). The train function allows us to make a better selection.\n\n\nThis is same as the linear kernel, and we just need to change the kernel parameter value from linear to rbf. Here‚Äôs the radial version:\n\nnp.random.seed(123) # once again, for reproducibility\n\n# Use a radial kernel in the SVM classifier\ncarseats_svm_rb_py = svm.SVC(kernel=\"rbf\")\ncarseats_svm_rb_py.fit(X_train, y_train)\n\n\n\n\nSVC()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nSVC\n\n?Documentation for SVCiFitted\nSVC() \n\n\nprint(carseats_svm_rb_py)\n# Predict the values with the radial approach\ncarseats_svm_pred_rb_py = carseats_svm_rb_py.predict(X_test)\n\n# Compute metrics\nreport_radial_py = classification_report(y_test, carseats_svm_pred_rb_py, target_names=['No', 'Yes'])\naccuracy_radial_py = accuracy_score(y_test, carseats_svm_pred_rb_py)\n\n# Print metrics\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_rb_py, rownames=['True'], colnames=['Predicted']))\nprint(report_radial_py)\nprint(\"Overall accuracy:\", accuracy_radial_py)\n\n\n\n\nIn both cases (R & python), with the default parameters, the linear kernel seems to do better than the radial one.\n\n\n\nR\nPython\n\n\n\nIn R, caret uses various libraries to run the svm models (check for yourself by searching for support vector machine here). For instance, calling svmLinear or svmRadial uses the library kernlab, and the kernlab::ksvm() function.\nThe C hyperparameter (from kernlab::ksvm()) accounts for the cost argument and controls the trade-off between allowing misclassifications in the training set and finding a decision boundary that generalizes well to new data. A larger cost value leads to a smaller margin and a more complex model that may overfit the data. On the other hand, a smaller cost value leads to a larger margin and a simpler model that may underfit the data.\nSimilarly to EX_ML_NN, to select the good hyperparameters, we build a search grid and fit the model with each possible value in the grid. Then, the best model is chosen among all the combinations of the hyperparameters.\nAs a reminder, the train function from caret. Has:\n\na formula.\na dataset.\na method (i.e.¬†the model which in this case is SVM with linear kernel).\na training control procedure.\n\n\n\ntrctrl <- trainControl(method = \"cv\", number=10)\nset.seed(143)\nsvm_Linear <- train(SaleHigh ~., data = df_tr, method = \"svmLinear\",\n                    trControl=trctrl)\nsvm_Linear\n\nFor now, the validation accuracy is very high (‚âà\\(89\\%\\)). This is normal since this accuracy is computed on the training set.\nWe now supply a grid of values for the cost that we want to try and pass to the argugmenttuneGrid. Be patient, it may take time.\n\ngrid <- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))\ngrid\nset.seed(143)\nsvm_Linear_Grid <- train(SaleHigh ~., data = df_tr, method = \"svmLinear\",\n                           trControl=trctrl,\n                           tuneGrid = grid)\nsvm_Linear_Grid\nplot(svm_Linear_Grid)\nsvm_Linear_Grid$bestTune\n\nWe see that setting the cost to 1 provides the best model. The accuracy apparently reaches a plateau at this value. This is same as our cost parameter in section 1.2.\n\nThe sigma hyperparameter (also from kernlab::ksvm()) controls the width of the radial basis function kernel, which is used to transform the input data into a higher-dimensional feature space. A larger value of sigma corresponds to a narrower kernel and a more complex model, while a smaller value corresponds to a wider kernel and a simpler model.\nWe repeat the procedure for SVM with a radial basis kernel. Here, there are two parameters ( sigma and C) to tune. The grid choice is rather arbitrary (often the result of trials and errors), and very few general useful guidelines exist. The code below may take a few minutes to run.\n\ngrid_radial <- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),\n                           C = c(1, 10, 100, 500, 1000))\ngrid_radial\nset.seed(143)\nsvm_Radial_Grid <- train(SaleHigh ~., data = df_tr, method = \"svmRadial\",\n                           trControl=trctrl,\n                           tuneGrid = grid_radial)\nsvm_Radial_Grid\nplot(svm_Radial_Grid)\nsvm_Radial_Grid$bestTune\n\nThe optimal model from this search is with sigma = 0.01 and C=100.\n\n\n\nWe can use GridSearchCV from sklearn.model_selection to achieve the same goal in python and set the argument for cross-validation (cv to achieve the same results) and tune both types of kernels at once. sklearn.svm.SVC() does contain the two arguments for the C and sigma but the relationship is slightly different and we‚Äôll explain below. Also, please note in this approach, the linear kernel by default ignores the sigma values.\nAs mentioned earlier, in sklearn.svm.SVC(), the equivalent parameter to cost is also C, which is straightforward. Still, the equivalent parameter to sigma is a bit trickier (called gamma in sklearn.svm.SVC()), which also controls the width of the radial basis function kernel. However, the relationship between gamma and sigma differs, and the two parameters cannot be directly compared. In particular, gamma is defined as the inverse of the width of the kernel, i.e., gamma = 1/(2 * sigma**2).\nFor simplicity‚Äôs sake, we‚Äôll directly use the gamma with the same values as sigma; however, you can always run 'gamma': [1/(2*sigma**2) for sigma in [0.01, 0.02, 0.05, 0.1]] to get similar values (although you would want to round as this division results in non-terminating repeating decimal numbers).\nThe code will take a few minutes to run (longer than the R version).\n\nfrom sklearn.model_selection import GridSearchCV\n\nnp.random.seed(123) # for reproducibility\n\n# Define the grid of hyperparameters to search over\nparam_grid = {'C': [1, 10, 100, 500, 1000],\n              'gamma': [0.01, 0.02, 0.05, 0.1],\n              # 'gamma': [round(1/(2*sigma**2),2) for sigma in [0.01, 0.02, 0.05, 0.1]],\n              'kernel': ['linear', 'rbf']}\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(svm.SVC(), param_grid, cv=10, scoring='accuracy', n_jobs = 1) # you can also set n_jobs = -1 to use all the cores and obtain the results faster (but unfortunately atm it only works on Mac/Linux and not Windows OS with `reticulate`)\n# for more info, please see https://github.com/rstudio/reticulate/issues/1346\ngrid_search.fit(X_train, y_train)\n\n\n\n\nGridSearchCV(cv=10, estimator=SVC(), n_jobs=1,\n             param_grid={'C': [1, 10, 100, 500, 1000],\n                         'gamma': [0.01, 0.02, 0.05, 0.1],\n                         'kernel': ['linear', 'rbf']},\n             scoring='accuracy')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\nGridSearchCV\n\n?Documentation for GridSearchCViFitted\nGridSearchCV(cv=10, estimator=SVC(), n_jobs=1,\n             param_grid={'C': [1, 10, 100, 500, 1000],\n                         'gamma': [0.01, 0.02, 0.05, 0.1],\n                         'kernel': ['linear', 'rbf']},\n             scoring='accuracy') \n\n\nbest_estimator_: SVCSVC(C=500, gamma=0.01) \n\nSVC\n?Documentation for SVCSVC(C=500, gamma=0.01) \n\n\n\n\nprint(\"Best hyperparameters: \", grid_search.best_params_)\nprint(\"Best score: \", grid_search.best_score_)\n\nNote that the accuracy of 85% is on the training set. The best parameters are C = 500, gamma = 0.01, kernel = rbf. We will use a new plotting library for seeing this evolution called searborn, which offers some great visualization tools.\nNote that the accuracy of is on the training set. The best parameters are C = 500, gamma = 0.01, kernel = rbf. We will use a new plotting library for seeing this evolution called searborn, which offers some great visualization tools.\nWe use the installed seaborn package from our Setup which allows for grouping our hyperparameters and displaying them with a heatmap.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming `results` is your DataFrame from `grid_search.cv_results_`\nresults = pd.DataFrame(grid_search.cv_results_)\n\n# Before performing the pivot or groupby operation, drop the 'params' column (it's a dictionary)\nresults = results.drop(columns=['params'])\n\n# Now, you can safely group by 'param_C', 'param_gamma', and 'param_kernel' to calculate mean test scores\ngrouped_results = results.pivot_table(index=['param_C', 'param_gamma'], columns='param_kernel', values='mean_test_score')\n\nplt.clf()\n\n# Create the heatmap\nax = sns.heatmap(grouped_results, annot=True, fmt='.3f', cmap='viridis', cbar=False)\nax.invert_yaxis()  # invert the y-axis to match your preference\n\nplt.xlabel('Kernel')\nplt.ylabel('C-Gamma')\nplt.show()\n\nWe can see that rbf kernel benefits from changes in C & gamma, however, for the linear, we‚Äôre always using the same gamma of 0.01 so for this kernel, the only changes are coming from C parameter.\n\n\n\n\n\n\nR\nPython\n\n\n\nAfter finding the best hyperparameters, it is often good practice to re-train the model with the best hyperparameters on the entire training set before evaluating everything on the test set. We do not need to re-train the model with the entire dataset for the linear SVM, as the best cost matched those used in section 1.2. We re-train the final model with the entire training set using optimal hyperparameters for the radial basis kernel.\n\ncarseats_rb_tuned <- svm(SaleHigh ~ .,data = df_tr,\n                         kernel = \"radial\", gamma = svm_Radial_Grid$bestTune$sigma,\n                         cost = svm_Radial_Grid$bestTune$C)\ncarseats_rb_tuned_pred <- predict(carseats_rb_tuned, newdata = df_te)\nconfusionMatrix(data=carseats_rb_tuned_pred, reference = df_te$SaleHigh)\n\nOverall, if we compare all the models, we see that the linear kernel SVM with cost of 1 looks like the best model. We already saw that it provides a \\(86\\%\\) accuracy on the test set. This is what can be expected in the future from that model.\n\n\n\n# re-train the model with best hyperparameters\nsvm_best_py = svm.SVC(**grid_search.best_params_)\nsvm_best_py.fit(X_train, y_train)\n\n\n\n\nSVC(C=500, gamma=0.01)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nSVC\n\n?Documentation for SVCiFitted\nSVC(C=500, gamma=0.01) \n\n\n# predict on test dataset\ncarseats_svm_pred_py = svm_best_py.predict(X_test)\n\n# print confusion matrix and accuracy score\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_py, rownames=['True'], colnames=['Predicted']))\nprint(classification_report(y_test, carseats_svm_pred_py, target_names=['No', 'Yes']))\nprint('Accuracy Score:', accuracy_score(y_test, carseats_svm_pred_py))\n\nThe results are similar to the R outcome for the linear models (except differences in the confusion matrix).\n\n\n\n\nRepeat the analysis on the German credit data (german.csv). Since dataset is much larger than MyCarSeats, the tuning procedure may be longer. For this reason, just limit to a linear SVM model for the tuning with limited range for the grid search."
  },
  {
    "objectID": "labs/03_Models/035_KNN/Ex_ML_KNN.html",
    "href": "labs/03_Models/035_KNN/Ex_ML_KNN.html",
    "title": "Models: K-Nearest Neighbors",
    "section": "",
    "text": "K-NN: a gentle introduction\nIn this first part, we apply the K-NN to the iris data set with examples in both R & python. In both cases, we first load the iris built-in data set found dplyr.\n\nlibrary(dplyr)\niris\nstr(iris)\n\n\n\nR\nPython\n\n\n\nIn R, we can model iris using the knn3 function from the caret package (named knn3 because knn is already taken by another package). This function is limited to the Euclidean distance with numerical features.\nWe first make the prediction using a 2-NN (with Euclidean distance).\n\nlibrary(caret)\nmod_knn <- knn3(data=iris, Species~., k=2) ## build the K-NN model\npredict(mod_knn, newdata = iris, type=\"class\")\n\nNow, we want to know if the predictions are good. To do this, we must ensure that the model is not overfitting the data. For this, we must split the data into training and test sets (these concepts will be seen in detail in a future course). To do so, we randomly take 75% of the iris data that will be the training set, and the remaining 25% are the test set.\n\nset.seed(123) ## for replication purpose\n\nindex_tr <- sample(1:nrow(iris), replace=FALSE, size=0.75*nrow(iris))\nindex_tr ## the index of the rows of iris that will be in the training set\n\niris_tr <- iris[index_tr,] ## the training set\niris_te <- iris[-index_tr,] ## the test set\n\nNow we can use the 2-NN to predict the test set using the training set. Note that the model is fitted on the training set, and the predictions are computed on the test set.\n\nmod_knn <- knn3(data=iris_tr, Species~., k=2)\niris_te_pred <- predict(mod_knn, newdata = iris_te, type=\"class\")\n\nTo compare the predictions above and the true species (the one in the test set), we can build a table. It is called a confusion matrix (again, this will be explained in detail later on).\n\ntable(Pred=iris_te_pred, Observed=iris_te[,5])\n\nThe prediction is almost perfect. It is so good that it is pointless to try to improve the prediction by changing K at that point. However, just to illustrate, below we use K=3.\n\nmod_knn <- knn3(data=iris_tr, Species~., k=3)\niris_te_pred <- predict(mod_knn, newdata = iris_te, type=\"class\")\n\ntable(Pred=iris_te_pred, Observed=iris_te[,5])\n\nNote that, in th formula ‚ÄúSpecies~.‚Äù, the dot means ‚Äúall the other variables‚Äù. It is equivalent (but shorter) to ‚ÄúSpecies ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width‚Äù\n\n\nBefore running the python, we must ensure we‚Äôre using the MLBA conda (virtual) environment created during setup.Rmd. As we already installed some of the packages, it should suffice for this part of the exercise:\n\n# make sure we're using the right environment\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\npy_config()\n\nFirst, let‚Äôs load the iris dataset and print its structure. You can access the r data object by using r.iris or only the pure python data as shown below (you don‚Äôt need the chunk below):\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\niris_df['species'] = iris.target_names[iris.target]\nprint(iris_df)\nprint(iris_df.info())\nNow, in python, we will also apply K-NN to the iris data set using the KNeighborsClassifier function from the sklearn package in python, where we set K=2. This function can be applied to both numerical and categorical features. We then make predictions with this python model.\n\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# call iris data from r\ny = r.iris[[\"Species\"]]  # select column \"Species\"\nX = r.iris.drop(columns=[\"Species\"])  # drop column \"Species\"\n\n# if you imported the data directly in python, you can instead run the commands below\n# y = iris.target\n# X = iris.data\n\nk = 2\nknn_model = KNeighborsClassifier(n_neighbors=k)\nknn_model.fit(X, y)\n\n\n\n\nKNeighborsClassifier(n_neighbors=2)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nKNeighborsClassifier\n\n?Documentation for KNeighborsClassifieriFitted\nKNeighborsClassifier(n_neighbors=2) \n\n\npred = knn_model.predict(X)\nprint(pred)\n\nWe will split the data again into training and test sets. We will randomly select 75% of the data for the training set and the remaining 25% for the test set. To achieve this, we will use numpy, most often used for efficient numerical computing. This split of training and test sets will only be done once in the exercises as we already created the same objects in R. Nevertheless, this is one way of dividing data into training and test sets in python:\n\nimport numpy as np\n\nnp.random.seed(123)\nindex_tr = np.random.choice(range(len(r.iris)), size=int(0.75*len(r.iris)), replace=False)\nindex_te = np.setdiff1d(range(len(r.iris)), index_tr)\n\niris_tr = r.iris.iloc[index_tr, :]\niris_te = r.iris.iloc[index_te, :]\n\nNow, we can fit the K-NN model to the training set and make predictions on the test set.\n\nknn_model = KNeighborsClassifier(n_neighbors=k)\nknn_model.fit(iris_tr.iloc[:, :-1], iris_tr.iloc[:, -1])\n\n\n\n\nKNeighborsClassifier(n_neighbors=2)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nKNeighborsClassifier\n\n?Documentation for KNeighborsClassifieriFitted\nKNeighborsClassifier(n_neighbors=2) \n\n\npred = knn_model.predict(iris_te.iloc[:, :-1])\nprint(pred)\n\nTo evaluate the performance of our model, we can construct a confusion matrix.\n\nfrom sklearn.metrics import confusion_matrix\n\n# for the confusion matrix in python, we need to specify the column names\nlabels = r.iris['Species'].unique()\n\n# create the confusion matrix with the labels as column names\nconf_mat = confusion_matrix(iris_te.iloc[:, -1], pred, labels=labels)\nconf_mat_df = pd.DataFrame(conf_mat, columns=labels, index=labels)\n\n# print the confusion matrix\nprint(conf_mat_df)\n\nThe prediction is almost perfect (as was the case in R), so it is not necessary to try to improve the prediction by changing k at this point. However, just for illustration, we can repeat the process with K=3.\n\nk = 3\n\nknn_model = KNeighborsClassifier(n_neighbors=k)\nknn_model.fit(iris_tr.iloc[:, :-1], iris_tr.iloc[:, -1])\n\n\n\n\nKNeighborsClassifier(n_neighbors=3)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nKNeighborsClassifier\n\n?Documentation for KNeighborsClassifieriFitted\nKNeighborsClassifier(n_neighbors=3) \n\n\npred = knn_model.predict(iris_te.iloc[:, :-1])\nconf_mat = confusion_matrix(iris_te.iloc[:, -1], pred, labels=labels)\nprint(pd.DataFrame(conf_mat, index=labels, columns=labels))\n\n\n\n\n\n\n\nWhy results are different in Python vs R\n\n\n\nPlease note that the training and test sets we used in python are not the same ones used in R due to a different generator for the seed (i.e., set.seed(123) is not generating the same numbers as np.random.seed(123)). Therefore, different observations were taken for the test set in the two languages. If you want to see the same performance, you can also run in both languages with the same dataset. For instance, running the code below would give you the same results:\n```{r}\nmod_knn_py <- knn3(data=py$iris_tr, Species~., k=2)\niris_te_pred_py <- predict(mod_knn_py, newdata = py$iris_te, type=\"class\")\ntable(Pred=iris_te_pred_py, Observed=py$iris_te[,5])\n```\n\n\n\n\n\nK-NN: regression\nIn the case of a regression task, the prediction is obtained by averaging the K nearest neighbors. To illustrate this, we will use the imports.85 data set. The aim is to predict the price using only the numerical features (categorical features are illustrated later).\nBelow, we identify the numerical columns (don‚Äôt forget to load the data first!). The new data frame is named tmp (temporary‚Ä¶). Then, we remove all the rows containing an NA.\n\nimports_85 <- read.csv(here::here(\"labs/data/imports-85.data\"), \n                       header=FALSE, na.strings=\"?\")\n\nnames = c(\"symboling\",\"normalized.losses\",\"make\",\"fuel.type\",\n         \"aspiration\",\"num.of.doors\",\"body.style\",\"drive.wheels\",\n         \"engine.location\",\"wheel.base\",\"length\",\"width\",\n         \"height\",\"curb.weight\",\"engine.type\",\"num.of.cylinders\",\n         \"engine.size\",\"fuel.system\",\"bore\",\"stroke\",\n         \"compression.ratio\",\"horsepower\",\"peak.rpm\",\"city.mpg\",\n         \"highway.mpg\",\"price\")\n\nnames(imports_85) = names\n\ntmp <- imports_85 %>% select(where(is.numeric))\ntmp <- filter(tmp, complete.cases(tmp))\ntmp %>% head()\n\nNow, we make a 75%-25% split for our training and testing.\n\nset.seed(123)\nindex_tr <- sample(1:nrow(tmp), size=0.75*nrow(tmp), replace = FALSE)\ntmp_tr <- tmp[index_tr,]\ntmp_te <- tmp[-index_tr,]\n\n\n\nR\nPython\n\n\n\nWe now fit the knnreg function to fit the model in R.\n\nmod_knn <- knnreg(data=tmp_tr, price~., k=3)\ntmp_pred <- predict(mod_knn, newdata=tmp_te)\n\nWe now graphically compare the real prices in the test set with the predicted ones (the diagonal line shows the equality between the prediction and the real price).\n\ntmp_te %>% mutate(pred=tmp_pred) %>%\nggplot(aes(x=price, y=pred)) + \n  geom_point() + geom_abline(slope=1, intercept = 0)\n\nIt looks like there is still room for improvement. Check for yourself if this can be improved by changing K.\n\n\nAs you already learnt how to import the data in R, and divided into training and test sets, we will use that directly (which also allows for better comparisons between modelling in R vs python).\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsRegressor\n\nX_tr = r.tmp_tr.drop(columns=[\"price\"])\ny_tr = r.tmp_tr[\"price\"]\nmod_knn_py = KNeighborsRegressor(n_neighbors=3)\nmod_knn_py.fit(X_tr, y_tr)\n\n\n\n\nKNeighborsRegressor(n_neighbors=3)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nKNeighborsRegressor\n\n?Documentation for KNeighborsRegressoriFitted\nKNeighborsRegressor(n_neighbors=3) \n\n\n\nX_te = r.tmp_te.drop(columns=[\"price\"])\ny_te = r.tmp_te[\"price\"]\ntmp_pred = mod_knn_py.predict(X_te)\n\nWe do the same graphical comparison in python, however, we will use matplotlib to demonstrate the results.\n\nimport matplotlib.pyplot as plt\n\n# we have to make a copy otherwise `r.temp_te` is not directly modified (i.e. we can't add a column)\ntmp_test_plt = r.tmp_te.copy()\ntmp_test_plt[\"pred\"]= tmp_pred\nfig, ax = plt.subplots()\nax.plot(tmp_test_plt[\"price\"], tmp_test_plt[\"pred\"], '.', color='black')\nax.plot(tmp_test_plt[\"price\"], tmp_test_plt[\"price\"], '-', color='blue')\nax.set_xlabel(\"Price\")\nax.set_ylabel(\"Predicted price\")\nplt.show()\n\nWe obtained the same results as R.\n\n\n\nK-NN: mixture of feature types\nIn this part, we illustrate how to incorporate categorical variables with K-NN. As a reminder, to use categorical variables, the easiest way is probably to cast them to dummy variables.\nThe code below identifies the categorical columns (i.e., non numerical), then create the dummies using and add them to the data frame. In addition, the numerical variables are standardized.\nThere are several difficulties in the code below.\n\nThe price (the outcome) is first set aside because we do not want to standardize it.\nTwo brand names (variable make) contain ‚Äú-‚Äù, namely ‚Äúalfa-romeo‚Äù and ‚Äúmercedes-benz‚Äù. After the creation of the dummy variables, these are used as titles. However, knnreg does not support ‚Äú-‚Äù in the variable names. They thus have to be removed first (in the code below they are turned to ‚Äú_‚Äù).\n\n\n## tmp is imports.85 without incomplete cases and without price\nimp_comp <- imports_85 %>% filter(complete.cases(imports_85))\ny <- imp_comp$price\nimp_comp <- imp_comp %>% select(!price)\n\n# we will use the standardize function shown by Marc-Olivier during the course (was called `my_fun`)\n# normalize num variable: (x - min(x))/(max(x) - min(x))\nmy_normalize <- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n# additionally, you can use the `scale()` function which centers and scales variables\n\n## the numerical variables in tmp are standardized\nimp_num <- imp_comp %>% select(where(is.numeric)) %>% my_normalize() %>% as.data.frame()\n\n## the dummy variables of the numerical features of tmp are created\nlibrary(fastDummies)\nimp_dumm <- imp_comp %>% select(!where(is.numeric)) %>% \n  dummy_cols(remove_first_dummy = FALSE, remove_selected_columns = TRUE)\n\n## These dummy variables are added to tmp\nimp_dat <- data.frame(imp_num, imp_dumm)\n\n## The names of the two problematic brands are changed\nnames(imp_dat)[c(23)] <- c(\"make_mercedes_benz\")\n\n## The raw price is added to tmp\nimp_dat$price <- y\n\nAt this stage, tmp contains all the variables that we want. We once again divide our data into training and test sets.\n\nset.seed(123)\nindex_tr <- sample(1:nrow(imp_dat), size=0.75*nrow(imp_dat), replace = FALSE)\nimp_tr <- imp_dat[index_tr,]\nimp_te <- imp_dat[-index_tr,]\n\n\n\nR\nPython\n\n\n\nWe can now run our 3-NN directly, like in the previous exercise.\n\nmod_knn <- knnreg(data=imp_tr, price~., k=3)\nimp_pred <- predict(mod_knn, newdata=imp_te)\nimp_te %>% mutate(pred=imp_pred) %>%\n  ggplot(aes(x=price, y=pred)) + \n  geom_point() + geom_abline(slope=1, intercept = 0)\n\nHere, like always, it is difficult to compare the scales of the scatterplot, and thus to tell if the quality of that 3-NN is better or worst than the previous one. The issue of model scoring will be studied later in the course. For now, it is just about being able to mix categorical and numeric variables in a K-NN.\n\n\nThe same approach in Python with the same.\n\nX_tr = r.imp_tr.drop(columns=[\"price\"])\ny_tr = r.imp_tr[\"price\"]\nmod_knn_py = KNeighborsRegressor(n_neighbors=3)\nmod_knn_py.fit(X_tr, y_tr)\n\n\n\n\nKNeighborsRegressor(n_neighbors=3)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nKNeighborsRegressor\n\n?Documentation for KNeighborsRegressoriFitted\nKNeighborsRegressor(n_neighbors=3) \n\n\nX_te = r.imp_te.drop(columns=[\"price\"])\ny_te = r.imp_te[\"price\"]\ntmp_pred = mod_knn_py.predict(X_te)\n\ntmp_test_plt = r.imp_te.copy()\ntmp_test_plt[\"pred\"]= tmp_pred\nfig, ax = plt.subplots()\nax.plot(tmp_test_plt[\"price\"], tmp_test_plt[\"pred\"], '.', color='black')\nax.plot(tmp_test_plt[\"price\"], tmp_test_plt[\"price\"], '-', color='blue')\nax.set_xlabel(\"Price\")\nax.set_ylabel(\"Predicted price\")\nplt.show()\n\n\n\n\nAnalysis of nursing home data\nNow it is your turn. Develop a K-NN model to predict the cost using the other variables. Inspect the quality of the prediction using a training set and a test set."
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html",
    "title": "MLBA - S24 ",
    "section": "",
    "text": "How can I render the following?‚Äî title: ‚ÄúModel scoring‚Äù"
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#data",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#data",
    "title": "MLBA - S24 ",
    "section": "Data",
    "text": "Data\nThe data set is the one used in the series on linear regressions.\n\nlibrary(readr)\nreal_estate_data <- read.csv(here::here(\"labs/data/real_estate_data.csv\"))\n\nThen we split the data in a training and a test set (0.8/0.2). For this, we use the createDataPartition function of the caret package.\n\nlibrary(caret)\nset.seed(234)\nindex_tr <- createDataPartition(y = real_estate_data$Price, p= 0.8, list = FALSE)\ndf_tr <- real_estate_data[index_tr,]\ndf_te <- real_estate_data[-index_tr,]"
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#models",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#models",
    "title": "MLBA - S24 ",
    "section": "Models",
    "text": "Models\nWe will compare a linear regression, a regression tree and a 3-NN (KNN).\n\n\nR\nPython\n\n\n\n\nlibrary(rpart)\nest_lm <- lm(Price~TransDate+HouseAge+Dist+\n               NumStores+Lat+Long, data=df_tr)\nest_rt <- rpart(Price~TransDate+HouseAge+Dist+\n                      NumStores+Lat+Long, data=df_tr)\nest_knn <- knnreg(Price~TransDate+HouseAge+Dist+\n                      NumStores+Lat+Long, data=df_tr, k = 3)\n\n\n\n\n# Load the course python environment as usual with a r code chunks.\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\n\n# Fit the models: linear regression, regression tree, and KNN\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Define predictors and target variable\npredictors = ['TransDate', 'HouseAge', 'Dist', 'NumStores', 'Lat', 'Long']\ntarget = 'Price'\n\n# Fit models\nest_lm = LinearRegression().fit(r.df_tr[predictors], r.df_tr[target])\nest_rt = DecisionTreeRegressor(random_state=234).fit(r.df_tr[predictors], r.df_tr[target])\nest_knn = KNeighborsRegressor(n_neighbors=3).fit(r.df_tr[predictors], r.df_tr[target])"
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#r-squared",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#r-squared",
    "title": "MLBA - S24 ",
    "section": "R-squared",
    "text": "R-squared\n\n\nR\nPython\n\n\n\nWe now compute the R2 for each model using the R2 function (in caret).\n\nR2(predict(est_lm, newdata = df_te), df_te$Price)\nR2(predict(est_rt, newdata = df_te), df_te$Price)\nR2(predict(est_knn, newdata = df_te), df_te$Price)\n\nJust for the exercise, we can compute it by hand (square of the correlation)\n\ncor(predict(est_lm, newdata = df_te), df_te$Price)^2\n\n\n\n\n# Same thing as the R code\nfrom sklearn.metrics import r2_score\nimport numpy as np\n\n# Only to demonostrate which argument goes where (different from `caret::R2`)\nprint(r2_score(y_true = r.df_te[target], y_pred = est_lm.predict(r.df_te[predictors])))\nprint(r2_score(r.df_te[target], est_rt.predict(r.df_te[predictors])))\nprint(r2_score(r.df_te[target], est_knn.predict(r.df_te[predictors])))\n# Computing it by hand gives us the same result as R\nnp.corrcoef(est_lm.predict(r.df_te[predictors]), r.df_te[target])[0][1]**2\n\nTo understand why the results are different in caret::R2() vs.¬†sklearn.metrics.r2_score(), see this post on stackoverflow. If you want to get the same results in both, you can set the argument form = 'corr' to form = \"traditional\" in caret::R2() so that for instance R2(predict(est_lm, newdata = df_te), df_te$Price, form = \"traditional\"), produces the same result of 0.642401.\nAdditionally, please note that the performance of the tree is highly dependent on the seed, so setting a different seed can lead to different results."
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#rmse",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#rmse",
    "title": "MLBA - S24 ",
    "section": "RMSE",
    "text": "RMSE\nNow, we compute the RMSE.\n\n\nR\nPython\n\n\n\n\nRMSE(predict(est_lm, newdata = df_te), df_te$Price)\nRMSE(predict(est_rt, newdata = df_te), df_te$Price)\nRMSE(predict(est_knn, newdata = df_te), df_te$Price)\n\nThe formula would be:\n\nsqrt(mean((predict(est_lm, newdata = df_te)-df_te$Price)^2))\n\n\n\n\nfrom sklearn.metrics import mean_squared_error, root_mean_squared_error\nimport numpy as np\n\nprint(root_mean_squared_error(r.df_te[target], est_lm.predict(r.df_te[predictors])))\n# alternatively in the older version of `sklearn`, you had to run the code below\n# print(np.sqrt(mean_squared_error(r.df_te[target], est_lm.predict(r.df_te[predictors]))))\nprint(root_mean_squared_error(r.df_te[target], est_rt.predict(r.df_te[predictors])))\nprint(root_mean_squared_error(r.df_te[target], est_knn.predict(r.df_te[predictors])))"
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#mae",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#mae",
    "title": "MLBA - S24 ",
    "section": "MAE",
    "text": "MAE\nNow, we compute the MAE.\n\n\nR\nPython\n\n\n\n\nMAE(predict(est_lm, newdata = df_te), df_te$Price)\nMAE(predict(est_rt, newdata = df_te), df_te$Price)\nMAE(predict(est_knn, newdata = df_te), df_te$Price)\n\nThe formula would be:\n\nmean(abs(predict(est_lm, newdata = df_te)-df_te$Price))\n\n\n\n\n# Compute MAE for each model\nfrom sklearn.metrics import mean_absolute_error\n\nprint(mean_absolute_error(r.df_te[target], est_lm.predict(r.df_te[predictors])))\nprint(mean_absolute_error(r.df_te[target], est_rt.predict(r.df_te[predictors])))\nprint(mean_absolute_error(r.df_te[target], est_knn.predict(r.df_te[predictors])))"
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#best-model",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#best-model",
    "title": "MLBA - S24 ",
    "section": "Best model",
    "text": "Best model\nThese three measures agree on the fact that the regression tree is the best model. To inspect further the predictions, we use scatterplots:\n\n\nR\nPython\n\n\n\n\npar(mfrow=c(2,2))\nplot(df_te$Price ~ predict(est_lm, newdata = df_te), xlab=\"Prediction\", \n     ylab=\"Observed prices\", main=\"Lin. Reg.\")\nabline(0,1)\nplot(df_te$Price ~ predict(est_rt, newdata = df_te), xlab=\"Prediction\", \n     ylab=\"Observed prices\", main=\"Lin. Reg.\")\nabline(0,1)\nplot(df_te$Price ~ predict(est_knn, newdata = df_te), xlab=\"Prediction\", \n     ylab=\"Observed prices\", main=\"Lin. Reg.\")\nabline(0,1)\npar(mfrow=c(1,1))\n\n\n\n\n# visualize also in Python\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\n\nplt.subplot(221)\nplt.scatter(est_lm.predict(r.df_te[predictors]), r.df_te[target], alpha=0.5)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Observed prices\")\nplt.title(\"Lin. Reg.\")\nplt.plot(r.df_te[target], r.df_te[target], color='red')\n\nplt.subplot(222)\nplt.scatter(est_rt.predict(r.df_te[predictors]), r.df_te[target], alpha=0.5)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Observed prices\")\nplt.title(\"Regression Tree\")\nplt.plot(r.df_te[target], r.df_te[target], color='red')\n\nplt.subplot(223)\nplt.scatter(est_knn.predict(r.df_te[predictors]), r.df_te[target], alpha=0.5)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Observed prices\")\nplt.title(\"KNN\")\nplt.plot(r.df_te[target], r.df_te[target], color='red')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe scatterplots are in line with the conclusion that KNN is the best, even though it is not easy to declare from a plot. We can in addition see that the regression tree (RT) has made more error on the larger prices."
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#data-1",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#data-1",
    "title": "MLBA - S24 ",
    "section": "Data",
    "text": "Data\nThe data set is the visit data (already used in previous exercises). For simplicity, we turn the outcome (visits) into factor. Like before, that are also split into a training and a test set.\n\nDocVis <- read.csv(here::here(\"labs/data/DocVis.csv\"))\nDocVis$visits <- as.factor(DocVis$visits)\n\nlibrary(caret)\nset.seed(346)\nindex_tr <- createDataPartition(y = DocVis$visits, p= 0.8, list = FALSE)\ndf_tr <- DocVis[index_tr,]\ndf_te <- DocVis[-index_tr,]"
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#models-1",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#models-1",
    "title": "MLBA - S24 ",
    "section": "Models",
    "text": "Models\nWe will compare a logistic regression, a classification tree (pruned) and a SVM with radial basis (cost and gamma tuned).\n\n\nR\nPython\n\n\n\nNote that the code for tuning the SVM is provided below in comments because of the time it takes to run. The final parameters have been selected accordingly. Also, the SVM fit includes the argument probability=TRUE to allow the calculations of predicted probabilities later.\n\nlibrary(e1071)\nlibrary(adabag)\n\n## Logistic regression\nDoc_lr <- glm(visits~., data=df_tr, family=\"binomial\")\nDoc_lr <- step(Doc_lr)\n\n## Classification tree \nDoc_ct <- autoprune(visits~., data=df_tr)\n\n## SVM radial basis\n# grid_radial <- expand.grid(sigma = c(0.0001, 0.001, 0.01, 0.1),\n#                           C = c(0.1, 1, 10, 100, 1000))\n# trctrl <- trainControl(method = \"cv\", number=10)\n# set.seed(143)\n# Doc_svm <- train(visits ~., data = df_tr, method = \"svmRadial\",\n#                          trControl=trctrl,\n#                          tuneGrid = grid_radial)\nDoc_svm <- svm(visits~., data=df_tr, gamma=0.001, cost=1000, probability=TRUE)\n\n\n\n\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\n# We first put the data in a nice format by one-hot encoding the categorical variables\nX_train = pd.get_dummies(r.df_tr.drop('visits', axis=1))\ny_train = r.df_tr['visits']\nX_test = pd.get_dummies(r.df_te.drop('visits', axis=1))\ny_test = r.df_te['visits']\n\n## Logistic regression\ndoc_lr = LogisticRegression()\ndoc_lr.fit(X_train, y_train)\n\n\n\n\nLogisticRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nLogisticRegression\n\n?Documentation for LogisticRegressioniFitted\nLogisticRegression() \n\n\n## Classification tree\ndoc_ct = DecisionTreeClassifier(random_state=123)\ndoc_ct.fit(X_train, y_train)\n\n\n\n\nDecisionTreeClassifier(random_state=123)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nDecisionTreeClassifier\n\n?Documentation for DecisionTreeClassifieriFitted\nDecisionTreeClassifier(random_state=123) \n\n\n## SVM radial basis\ndoc_svm = SVC(kernel='rbf', gamma=0.001, C=1000, probability=True, random_state=123)\ndoc_svm.fit(X_train, y_train)\n\n\n\n\nSVC(C=1000, gamma=0.001, probability=True, random_state=123)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nSVC\n\n?Documentation for SVCiFitted\nSVC(C=1000, gamma=0.001, probability=True, random_state=123)"
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#predictions",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#predictions",
    "title": "MLBA - S24 ",
    "section": "Predictions",
    "text": "Predictions\nWe now compute the predicted probabilities and the predictions of all the models.\n\n\nR\nPython\n\n\n\nNote that, for SVM, we need to extract the attribute ‚Äúprobabilities‚Äù from the predicted object. This can be done with the attr function.\n\n## Logistic regression\nDoc_lr_prob <- predict(Doc_lr, newdata=df_te, type=\"response\")\nDoc_lr_pred <- ifelse(Doc_lr_prob>0.5,\"Yes\",\"No\")\n\n## Classification tree \nDoc_ct_prob <- predict(Doc_ct, newdata=df_te, type=\"prob\")\nDoc_ct_pred <- predict(Doc_ct, newdata=df_te, type=\"class\")\n\n## SVM radial basis\nlibrary(dplyr)\nDoc_svm_prob <- predict(Doc_svm, newdata=df_te, probability=TRUE) %>% attr(\"probabilities\")\nDoc_svm_pred <- predict(Doc_svm, newdata=df_te, type=\"class\")\n\n\n\n\n## Logistic regression\n## the second column represents the `no` values, to make sure of that, you can run `doc_lr.classes_`\ndoc_lr_prob = doc_lr.predict_proba(X_test)[:,1]\ndoc_lr_pred = np.where(doc_lr_prob>0.5, \"Yes\", \"No\")\n\n## Classification tree\ndoc_ct_prob = doc_ct.predict_proba(X_test)[:,1]\ndoc_ct_pred = doc_ct.predict(X_test)\n\n## SVM radial basis\ndoc_svm_prob = doc_svm.predict_proba(X_test)[:,1]\ndoc_svm_pred = doc_svm.predict(X_test)"
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#confusion-matrices-prediction-based-measures",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#confusion-matrices-prediction-based-measures",
    "title": "MLBA - S24 ",
    "section": "Confusion matrices & prediction-based measures",
    "text": "Confusion matrices & prediction-based measures\n\n\nR\nPython\n\n\n\nThe confusionMatrix function provides all the accuracy measures that we want.\n\nconfusionMatrix(data=as.factor(Doc_lr_pred), reference = df_te$visits)\nconfusionMatrix(data=as.factor(Doc_ct_pred), reference = df_te$visits)\nconfusionMatrix(data=as.factor(Doc_svm_pred), reference = df_te$visits)\n\n\n\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, cohen_kappa_score\n\n## Logistic regression\nprint(confusion_matrix(y_test, doc_lr_pred))\nprint(f\"Accuracy: {accuracy_score(y_test, doc_lr_pred):.3f}\")\nprint(f\"Kappa: {cohen_kappa_score(y_test, doc_lr_pred):.3f}\")\nprint(f\"Balanced accuracy: {balanced_accuracy_score(y_test, doc_lr_pred):.3f}\")\n## Classification tree\nprint(confusion_matrix(y_test, doc_ct_pred))\nprint(f\"Accuracy: {accuracy_score(y_test, doc_ct_pred):.3f}\")\nprint(f\"Kappa: {cohen_kappa_score(y_test, doc_ct_pred):.3f}\")\nprint(f\"Balanced accuracy: {balanced_accuracy_score(y_test, doc_ct_pred):.3f}\")\n## SVM radial basis\nprint(confusion_matrix(y_test, doc_svm_pred))\nprint(f\"Accuracy: {accuracy_score(y_test, doc_svm_pred):.3f}\")\nprint(f\"Kappa: {cohen_kappa_score(y_test, doc_svm_pred):.3f}\")\nprint(f\"Balanced accuracy: {balanced_accuracy_score(y_test, doc_svm_pred):.3f}\")\n\nDifferent results for the tree and CSV due to randomness, but even with that, SVM remains the best model in terms of accuracy.\n\n\n\nThe conclusion may be different from one measure to another\n\nAccuracy: the SVM reaches the highest accuracy\nKappa: the CT is the highest.\nBalanced accuracy: the CT is the highest.\netc.\n\nLooking at the confusion matrix, we see that the data is highly unbalanced (many more ‚ÄúNo‚Äù than ‚ÄúYes‚Äù). Therefore, measures like balanced accuracy and kappa are interesting because they take this characteristics into account. This shows that the CT is probably better than the SVM because it reaches a better balance between predicting ‚ÄúYes‚Äù and ‚ÄúNo‚Äù.\nBy looking at the sensitivity and specificity (!! here the positive class is ‚ÄúNo‚Äù), we see that the best model to recover the ‚ÄúNo‚Äù is the logistic regression (largest sensitivity) and the best model to recover the ‚ÄúYes‚Äù is the classification tree (largest specificity)."
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#probability-based-measures",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#probability-based-measures",
    "title": "MLBA - S24 ",
    "section": "Probability-based measures",
    "text": "Probability-based measures\n\n\nR\nPython\n\n\n\nTo compute the AUC (area under the ROC curve) we can use the caret::twoClassSummary function. The use of this function can be tricky. Its argument should be a data frame with columns (names are fixed):\n\n‚Äúobs‚Äù: the observed classes\n‚Äúpred‚Äù: the predicted classes\ntwo columns with names being the levels of the classes, here ‚ÄúYes‚Äù and ‚ÄúNo‚Äù, containing the predicted probabilities.\n\n\ndf_pred_lr <- data.frame(obs=df_te$visits,\n                         Yes=Doc_lr_prob,\n                         No=1-Doc_lr_prob,\n                         pred=as.factor(Doc_lr_pred))\nhead(df_pred_lr)\n\ndf_pred_ct <- data.frame(obs=df_te$visits,\n                         Doc_ct_prob,\n                         pred=as.factor(Doc_ct_pred))\nhead(df_pred_ct)\ndf_pred_svm <- data.frame(obs=df_te$visits,\n                          Doc_svm_prob,\n                          pred=as.factor(Doc_svm_pred))\nhead(df_pred_svm)\n\nThen we pass these objects to the function, and levels of the classes to be predicted (for the function to be able to recover them in the data frame). The function compute the AUC by default (under the name ROC_.. not very wise) as well as sensitivity and specificity (that we already have).\n\ntwoClassSummary(df_pred_lr, lev = levels(df_pred_lr$obs))\ntwoClassSummary(df_pred_ct, lev = levels(df_pred_lr$obs))\ntwoClassSummary(df_pred_svm, lev = levels(df_pred_lr$obs))\n\nThis brings us another view: the logistic regression has the highest AUC. This shows that varying the prediction threshold provides a good potential of improving the specificity and the sensitivity (in fine, the balanced accuracy).\nNow we compute the entropy using the mnLogLoss function (entropy is also called log-loss).\n\nmnLogLoss(df_pred_lr, lev = levels(df_pred_lr$obs))\nmnLogLoss(df_pred_ct, lev = levels(df_pred_lr$obs))\nmnLogLoss(df_pred_svm, lev = levels(df_pred_lr$obs))\n\nHere again, the entropy selects the logistic regression as the best model, though close to classification tree and SVM.\n\n\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\n## Logistic regression\nprint(f\"AUC: {roc_auc_score(y_test, doc_lr_prob):.3f}\")\n## Classification tree\nprint(f\"AUC: {roc_auc_score(y_test, doc_ct_prob):.3f}\")\n## SVM radial basis\nprint(f\"AUC: {roc_auc_score(y_test, doc_svm_prob):.3f}\")\n# Now we compute the entropy using the `log_loss` function (entropy is also called *log-loss*).\n\nfrom sklearn.metrics import log_loss\n\n## Logistic regression\nprint(f\"Log-loss: {log_loss(y_test, doc_lr_prob):.3f}\")\n## Classification tree\nprint(f\"Log-loss: {log_loss(y_test, doc_ct_prob):.3f}\")\n## SVM radial basis\nprint(f\"Log-loss: {log_loss(y_test, doc_svm_prob):.3f}\")"
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#roc-curve-prob-threshold-tuning",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#roc-curve-prob-threshold-tuning",
    "title": "MLBA - S24 ",
    "section": "ROC curve & prob threshold tuning",
    "text": "ROC curve & prob threshold tuning\n\n\nR\nPython\n\n\n\nTo go deeper in the analysis, we now produce the ROC curve of each model using the roc function of the proc package.\n\nlibrary(pROC)\nROC_lr <- roc(obs ~ Yes, data=df_pred_lr)\nROC_ct <- roc(obs ~ Yes, data=df_pred_ct)\nROC_svm <- roc(obs ~ Yes, data=df_pred_svm)\n\nplot(ROC_lr, print.thres=\"best\")\nplot(ROC_ct, print.thres=\"best\", add=TRUE)\nplot(ROC_svm, print.thres=\"best\", add=TRUE)\n\nThe plotting function provides an ‚Äúoptimal‚Äù threshold that reaches the best trade-off between sensitivity and specificity (according to some criterion). We see that there is room to improve this trade-off.\nNow, to tune this threshold, we need to do it on the training set to avoid overfitting. To do this, we just repeat the previous calculations (predictions) on the training set. To simplify, we only do this on the logistic regression (note that you can try on the other models; you may find that logistic regression is the best one).\n\nDoc_lr_prob_tr <- predict(Doc_lr, newdata=df_tr, type=\"response\")\ndf_pred_lr_tr <- data.frame(obs=df_tr$visits,\n                            Yes=Doc_lr_prob_tr)\nROC_lr_tr <- roc(obs ~ Yes, data=df_pred_lr_tr)\nplot(ROC_lr_tr, print.thres=\"best\")\n\nThe best threshold is 0.193. Now let us compute the confusion table with this threshold.\n\nDoc_lr_pred_opt <- ifelse(Doc_lr_prob>0.193,\"Yes\",\"No\")\nconfusionMatrix(data=as.factor(Doc_lr_pred_opt), reference = df_te$visits)\n\nWe now have a model with an accuracy of circa \\(70\\%\\) but with a balanced accuracy of \\(67\\%\\). Far from perfect, this is still an interesting improvement compare to the CT \\(62\\%\\). The specificity and sensitivity are now respectively \\(62\\%\\) and \\(72\\%\\). The specificity in particular made a huge improvement (from around \\(29\\%\\) at best - by CT - to \\(62\\%\\) - by log. reg).\nIf the aim is to predict both ‚ÄúYes‚Äù and ‚ÄúNo‚Äù, this last model (log. reg. with tuned threshold) is the best one to use.\n\n\n\n## Logistic regression\n## We need to turn back our results into binary values to be plotted\ndoc_lr_prob_dict = {'Yes': 1, 'No': 0}\ny_test_binary = np.array([doc_lr_prob_dict[x] for x in y_test])\nfpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test_binary, doc_lr_prob)\nplt.plot(fpr_lr, tpr_lr, label=\"Logistic Regression\")\n\n## Classification tree\ndoc_ct_prob_dict = {'Yes': 1, 'No': 0}\ny_test_binary = np.array([doc_ct_prob_dict[x] for x in y_test])\nfpr_ct, tpr_ct, thresholds_ct = roc_curve(y_test_binary, doc_ct_prob)\nplt.plot(fpr_ct, tpr_ct, label=\"Classification Tree\")\n\n## SVM radial basis\ndoc_svm_prob_dict = {'Yes': 1, 'No': 0}\ny_test_binary = np.array([doc_svm_prob_dict[x] for x in y_test])\nfpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test_binary, doc_svm_prob)\n\n# Clear the last plot (if any)\n# plt.clf()\n\nplt.plot(fpr_svm, tpr_svm, label=\"SVM Radial Basis\")\n# Plot the ROC curve\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Classifier\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()\n\nWe can then plot the results in the similar way to R:\n\ndoc_lr_prob_tr = doc_lr.predict_proba(X_train)[:,1]\ndoc_lr_prob_tr_dict = {'Yes': 1, 'No': 0}\ny_train_binary = np.array([doc_lr_prob_tr_dict[x] for x in y_train])\nfpr_lr_tr, tpr_lr_tr, thresholds_lr_tr = roc_curve(y_train_binary, doc_lr_prob_tr)\noptimal_idx = np.argmax(tpr_lr_tr - fpr_lr_tr)\noptimal_threshold = thresholds_lr_tr[optimal_idx]\nprint(f\"Optimal threshold: {optimal_threshold:.3f}\")\n\nFinally, we print the confusion matrix again:\n\ndoc_lr_pred_opt = np.where(doc_lr_prob > optimal_threshold, \"Yes\", \"No\")\nprint(confusion_matrix(y_test, doc_lr_pred_opt))\n\nThe logistic regression produced with R was better."
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#classification",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#classification",
    "title": "MLBA - S24 ",
    "section": "Classification",
    "text": "Classification\nRepeat the analysis on the German credit data. Put several models in competition. Tune them and try to optimize their threshold. Select the best one and analyze its performance."
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#regression",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#regression",
    "title": "MLBA - S24 ",
    "section": "Regression",
    "text": "Regression\nRepeat the analysis on the nursing cost data. Put several models in competition. Tune them and select the best one. Analyze its performance using a scatterplot."
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html",
    "title": "Data splitting",
    "section": "",
    "text": "In this series, we practice the data splitting strategies seen in class. The data are the doctor visits, already used in previous applications: cross-validation, bootstrap, and balancing.\nWe‚Äôll be using the DocVis dataset for this lab session.\n\nDocVis <- read.csv(here::here(\"labs/data/DocVis.csv\"))\nDocVis$visits <- as.factor(DocVis$visits) ## make sure that visits is a factor\n\nWe need to set aside a test set. This will be used after to check that there was no overfitting during the training of the model and to ensure that the score we have obtained generalizes outside the training data.\n\nlibrary(caret)\nlibrary(dplyr)\nset.seed(346)\nindex_tr <- createDataPartition(y = DocVis$visits, p= 0.8, list = FALSE)\ndf_tr <- DocVis[index_tr,]\ndf_te <- DocVis[-index_tr,]\n\nNote that the splitting techniques used are applied on the training set only."
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#first-fold",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#first-fold",
    "title": "Data splitting",
    "section": "First fold",
    "text": "First fold\n\n\nR\nPython\n\n\n\nFirst, we create the folds by using the createFolds function of caret.\n\nindex_CV <- createFolds(y = df_tr$visits, k=10)\nindex_CV[[1]]\n\nAs seen before, the index_CV object is a list of row indices. The first element of the list index_CV[[1]] corresponds to the first fold. It is the vector of row indices of the validation set for the first fold (i.e., the validation is made of the rows of the training set that are in this vector). All the indices that are not in index_CV[[1]] will be in the training set (for this fold).\n\ndf_cv_tr <- df_tr[-index_CV[[1]],]\ndf_cv_val <- df_tr[index_CV[[1]],]\n\nFor this fold, df_cv_tr is the training set (it contains 9/10 of the original training set df_tr) and df_cv_val is the validation set (it contains 1/10 of the original training set df_tr). These two sets are disjoints.\nNow, we simply fit the model with the training set and compute its accuracy on the validation set. For this exercise, we use a logistic regression with AIC-based variable selection.\n\nDoc_cv <- glm(visits~., data=df_cv_tr, family=\"binomial\") %>% step()\nDoc_cv_prob <- predict(Doc_cv, newdata=df_cv_val, type=\"response\")\nDoc_cv_pred <- ifelse(Doc_cv_prob>0.5,\"Yes\",\"No\")\nconfusionMatrix(data=as.factor(Doc_cv_pred), reference = df_cv_val$visits)$overall[1]\n\n\n\n\n# Load the course python environment as usual with a r code chunks.\nlibrary(reticulate)\nuse_condaenv(\"MLBA\", required = TRUE)\n\nWe will one-hot encode the categorical variables using pd.get_dummies() and then divide the data into X_train, y_train, X_test and y_test.\n\nimport pandas as pd\n\n# One-hot encoding the categorical columns\nX_train = pd.get_dummies(r.df_tr.drop('visits', axis=1))\ny_train = r.df_tr['visits']\nX_test = pd.get_dummies(r.df_te.drop('visits', axis=1))\ny_test = r.df_te['visits']\n\nThen, we create the folds by using the KFold function of scikit-learn.\n\nfrom sklearn.model_selection import KFold\n# We setup the 10-k fold\nkf = KFold(n_splits=10, random_state=346, shuffle=True)\nfold_indices = list(kf.split(X_train, y_train))\nfirst_fold_train, first_fold_val = fold_indices[0]\n\nAs seen before, the fold_indices object is a list of tuple pairs. The first element of the list fold_indices[0] corresponds to the first fold. It is the tuple of row indices of the training set and the validation set for the first fold. All the indices that are not in first_fold_val will be in the training set (for this fold).\n\nX_cv_tr = X_train.iloc[first_fold_train, :]\ny_cv_tr = y_train.iloc[first_fold_train]\n\nX_cv_val = X_train.iloc[first_fold_val, :]\ny_cv_val = y_train.iloc[first_fold_val]\n\nThis part, will be slightly different from the R approach. Here, we fit the model with the training set and compute its accuracy on the validation set. For the python approach, we use a logistic regression with recursive feature elimination to select the best number of features.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.feature_selection import RFE\nimport numpy as np\n\nmodel = LogisticRegression(solver='liblinear')\nrfe = RFE(model)\nrfe.fit(X_cv_tr, y_cv_tr)\n\n\n\n\nRFE(estimator=LogisticRegression(solver='liblinear'))\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\nRFE\n\n?Documentation for RFEiFitted\nRFE(estimator=LogisticRegression(solver='liblinear')) \n\n\nestimator: LogisticRegressionLogisticRegression(solver='liblinear') \n\nLogisticRegression\n?Documentation for LogisticRegressionLogisticRegression(solver='liblinear') \n\n\n\n\npred_probs = rfe.predict_proba(X_cv_val)[:, 1]\nDoc_cv_pred = np.where(pred_probs > 0.5, \"Yes\", \"No\")\nacc = accuracy_score(y_cv_val, Doc_cv_pred)\nacc\n\n# alternatively, you could use `cross_val_score`\n# from sklearn.model_selection import cross_val_score\n# cv_scores = cross_val_score(rfe, X_cv_tr, y_cv_tr, cv=kf, scoring=\"accuracy\")\n# cv_scores"
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#loop-on-the-10-folds",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#loop-on-the-10-folds",
    "title": "Data splitting",
    "section": "Loop on the 10 folds",
    "text": "Loop on the 10 folds\nNow we repeat the previous steps for all the folds.\n\n\nR\nPython\n\n\n\nIn order to track the 10 accuracy measures obtained, we store them into a vector (acc_vec). Note also that the option trace=0 was set in the function step to avoid all the print outs of the AIC selections.\n\nacc_vec <- numeric(10)\nfor (i in 1:10){\n  df_cv_tr <- df_tr[-index_CV[[i]],]\n  df_cv_val <- df_tr[index_CV[[i]],]\n  \n  Doc_cv <- glm(visits~., data=df_cv_tr, family=\"binomial\") %>% step(trace=0)\n  Doc_cv_prob <- predict(Doc_cv, newdata=df_cv_val, type=\"response\")\n  Doc_cv_pred <- ifelse(Doc_cv_prob>0.5,\"Yes\",\"No\")\n  acc_vec[i] <- confusionMatrix(data=as.factor(Doc_cv_pred), reference = df_cv_val$visits)$overall[1]\n}\nacc_vec\n\nBy definition of the CV, all the 10 validations sets in this loop are disjoints. Thus, these 10 accuracy measures are in a way representative of what can be expected on the test set, except if we are very unlucky when we created the test set.\nNow we can estimate the expected accuracy (i.e., the mean) and its variation (below we use the standard deviation).\n\nmean(acc_vec)\nsd(acc_vec)\n\nThe small SD shows that the results are reliable and that we have good chance that the model, trained on the whole training set, will have this accuracy on the test set.\n\n\nFor python, in order to track the 10 accuracy measures obtained, we store them into a list (acc_list).\n\nacc_list = []\nfor train_idx, val_idx in kf.split(X_train, y_train):\n    X_cv_tr, y_cv_tr = X_train.iloc[train_idx, :], y_train.iloc[train_idx]\n    X_cv_val, y_cv_val = X_train.iloc[val_idx, :], y_train.iloc[val_idx]\n    \n    rfe.fit(X_cv_tr, y_cv_tr)\n    pred_probs = rfe.predict_proba(X_cv_val)[:, 1]\n    Doc_cv_pred = np.where(pred_probs > 0.5, \"Yes\", \"No\")\n    acc = accuracy_score(y_cv_val, Doc_cv_pred)\n    acc_list.append(acc)\n\n\n\n\nRFE(estimator=LogisticRegression(solver='liblinear'))\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\nRFE\n\n?Documentation for RFEiFitted\nRFE(estimator=LogisticRegression(solver='liblinear')) \n\n\nestimator: LogisticRegressionLogisticRegression(solver='liblinear') \n\nLogisticRegression\n?Documentation for LogisticRegressionLogisticRegression(solver='liblinear') \n\n\n\n\nacc_list\n\nOnce again, we can estimate the expected accuracy and its variation.\n\nmean_acc = np.mean(acc_list)\nstd_acc = np.std(acc_list)\nmean_acc, std_acc\n\nNow, we fit the final model using the whole training set and evaluate its performance on the test set.\n\nrfe.fit(X_train, y_train)\n\n\n\n\nRFE(estimator=LogisticRegression(solver='liblinear'))\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\nRFE\n\n?Documentation for RFEiFitted\nRFE(estimator=LogisticRegression(solver='liblinear')) \n\n\nestimator: LogisticRegressionLogisticRegression(solver='liblinear') \n\nLogisticRegression\n?Documentation for LogisticRegressionLogisticRegression(solver='liblinear') \n\n\n\n\ny_test_pred = rfe.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\ntest_cm = confusion_matrix(y_test, y_test_pred)\ntest_accuracy, test_cm"
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#automated-approach",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#automated-approach",
    "title": "Data splitting",
    "section": "Automated approach",
    "text": "Automated approach\n\n\nR - caret\nPython - sklearn\n\n\n\nThe 10-CV can be easily obtained from caret. First, set up the splitting data method using the trainControl function.\n\ntrctrl <- trainControl(method = \"cv\", number=10)\n\nThen pass this method to the train function (from caret). In addition, we use the model (below unhappily called ‚Äúmethod‚Äù also) glmStepAIC which, combined with the binomial family, applies a logistic regression and a AIC-based variable selection (backward; exactly like the step function used above). Of course, we also provide the model formula.\n\nset.seed(346)\nDoc_cv <- train(visits ~., data = df_tr, method = \"glmStepAIC\", family=\"binomial\",\n                    trControl=trctrl, trace=0)\nDoc_cv\n\nNote that the function ‚Äúonly‚Äù provides the expected accuracy and the expected kappa. It does not provides their standard deviations.\nThe final model (i.e., the model trained on the whole training set df_tr) is stored in Doc_cv$finalModel. It can be used to compute the accuracy on the test set.\n\nDoc_pred <- predict(Doc_cv, newdata = df_te)\nconfusionMatrix(data=Doc_pred, reference = df_te$visits)\n\n\n\nIn python, a similar approach demonstrated in caret would be using GridSearchCV from scikit-learn. We use the same 10-CV kf object created earlier. Then pass this method to the GridSearchCV function with LogisticRegression model with RFE for feature selection with the grid of parameters we would like to search for (in this case the number of features). Then, we output the (hyper-)parameters with the best performance.\n\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = LogisticRegression(solver='liblinear')\nrfe = RFE(model)\nparam_grid = {'n_features_to_select': list(range(1, X_train.shape[1] + 1))}\ngrid_search = GridSearchCV(rfe, param_grid, scoring='accuracy', cv=kf)\ngrid_search.fit(X_train, y_train)\n\n\n\n\nGridSearchCV(cv=KFold(n_splits=10, random_state=346, shuffle=True),\n             estimator=RFE(estimator=LogisticRegression(solver='liblinear')),\n             param_grid={'n_features_to_select': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n                                                  11, 12, 13, 14, 15, 16, 17]},\n             scoring='accuracy')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\nGridSearchCV\n\n?Documentation for GridSearchCViFitted\nGridSearchCV(cv=KFold(n_splits=10, random_state=346, shuffle=True),\n             estimator=RFE(estimator=LogisticRegression(solver='liblinear')),\n             param_grid={'n_features_to_select': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n                                                  11, 12, 13, 14, 15, 16, 17]},\n             scoring='accuracy') \n\n\nbest_estimator_: RFERFE(estimator=LogisticRegression(solver='liblinear'), n_features_to_select=17) \n\n\nestimator: LogisticRegressionLogisticRegression(solver='liblinear') \n\nLogisticRegression\n?Documentation for LogisticRegressionLogisticRegression(solver='liblinear') \n\n\n\n\n\nprint(grid_search.best_score_, grid_search.best_params_)\n\nThe final model (i.e., the model trained on the whole training set X_train) is stored in grid_search.best_estimator_. It can be used to compute the accuracy on the test set.\n\ny_test_pred = grid_search.best_estimator_.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\ntest_cm = confusion_matrix(y_test, y_test_pred)\nprint(test_accuracy, test_cm)\n\nOur results did improve compared to the last python model."
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#first-sample",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#first-sample",
    "title": "Data splitting",
    "section": "First sample",
    "text": "First sample\n\n\nR\nPython\n\n\n\nWe need to first create the replicates using the caret function createResample.\n\nset.seed(897)\nindex_boot <- createResample(y=df_tr$visits, times=100)\nindex_boot[[1]]\n\nAgain, it creates a list of indices. The first element of the list, index_boot[[1]], contains the row indices that will be in the training set. Note that, it is of length 4,153. In other words, the training set during this first replicate is of the same dimension as the whole training set df_tr. Note also that, in index_boot[[1]], there are indices that are replicated. This is the bootstrap sampling process. Some rows will be replicated in the training set. This also means that some rows of df_tr will not be in index_boot[[1]]. These rows are said to be out-of-bag and form the validation set. See below the dimensions of the data frames.\n\ndf_boot_tr <- df_tr[index_boot[[1]],]\ndim(df_boot_tr)\ndf_boot_val <- df_tr[-index_boot[[1]],]\ndim(df_boot_val)\n\nWe now fit the data to this first sample training set.\n\nDoc_boot <- glm(visits~., data=df_boot_tr, family=\"binomial\") %>% step()\n\nThe accuracy is then computed with the 632-rule: first, the apparent accuracy is computed (accuracy on the sample training set), then the out-of-bag accuracy (the accuracy on the validation set), then the final accuracy estimate is the 0.368/0.632-combination of the two.\n\nDoc_boot_prob_val <- predict(Doc_boot, newdata=df_boot_val, type=\"response\")\nDoc_boot_pred_val <- ifelse(Doc_boot_prob_val>0.5,\"Yes\",\"No\")\noob_acc <- confusionMatrix(data=as.factor(Doc_boot_pred_val), reference = df_boot_val$visits)$overall[1]\n\nDoc_boot_prob_tr <- predict(Doc_boot, newdata=df_boot_tr, type=\"response\")\nDoc_boot_pred_tr <- ifelse(Doc_boot_prob_tr>0.5,\"Yes\",\"No\")\napp_acc <- confusionMatrix(data=as.factor(Doc_boot_pred_tr), reference = df_boot_tr$visits)$overall[1]\n\noob_acc ## out-of-bag accuracy\napp_acc ## apparent accuracy\n0.368*app_acc + 0.632*oob_acc ## accuracy estimate\n\n\n\nFirst, we create the replicates using the resample function from sklearn.utils. Please note that unlike caret::createResample(), in sklearn (to the best of our knowledge), there‚Äôs no method that returns a list of the samples, so with resample, we get one set of resampled data points. This doesn‚Äôt matter for now, but in the following sub-section, we will write a function to do the same thing in python.\n\nfrom sklearn.utils import resample\n\nnp.random.seed(897)\ndf_boot_tr = resample(r.df_tr, n_samples=len(r.df_tr), random_state=897)\n\nThe resample function returns a new data frames with the same number of samples as the original r.df_tr, but some rows will be replicated. This also means that some rows of r.df_tr will not be in the bootstrapped data frames These rows are said to be out-of-bag and form the validation set. See below the dimensions of the data frames.\n\ndf_boot_tr.shape\noob_mask = ~r.df_tr.index.isin(df_boot_tr.index.values)\ndf_boot_val = r.df_tr[oob_mask]\ndf_boot_val.shape\n\nThere‚Äôs a difference between the shape of df_boot_val because of the difference in random generators between R & python. If you change the number for the random generator in python (i.e., np.random.seed(897)) or in R (i.e., set.seed(897)), you‚Äôll see that the results will be slightly different."
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#loop-on-the-100-sample",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#loop-on-the-100-sample",
    "title": "Data splitting",
    "section": "Loop on the 100 sample",
    "text": "Loop on the 100 sample\n\n\nR\nPython\n\n\n\nThe previous code is looped. The accuracy measures are stored in vectors. The code can be quite long to run.\n\noob_acc_vec <- numeric(100)\napp_acc_vec <- numeric(100)\nacc_vec <- numeric(100)\nfor (i in 1:100){\n  df_boot_tr <- df_tr[index_boot[[i]],]\n  df_boot_val <- df_tr[-index_boot[[i]],]\n  \n  Doc_boot <- glm(visits~., data=df_boot_tr, family=\"binomial\") %>% step(trace=0)\n  Doc_boot_prob_val <- predict(Doc_boot, newdata=df_boot_val, type=\"response\")\n  Doc_boot_pred_val <- ifelse(Doc_boot_prob_val>0.5,\"Yes\",\"No\")\n  oob_acc_vec[i] <- confusionMatrix(data=as.factor(Doc_boot_pred_val), reference = df_boot_val$visits)$overall[1]\n  \n  Doc_boot_prob_tr <- predict(Doc_boot, newdata=df_boot_tr, type=\"response\")\n  Doc_boot_pred_tr <- ifelse(Doc_boot_prob_tr>0.5,\"Yes\",\"No\")\n  app_acc_vec[i] <- confusionMatrix(data=as.factor(Doc_boot_pred_tr), reference = df_boot_tr$visits)$overall[1]\n  \n  acc_vec[i] <- 0.368*app_acc_vec[i] + 0.632*oob_acc_vec[i]\n}\n\nacc_vec\n\nLike for the CV, we can estimate the expected accuracy and its dispersion.\n\nmean(acc_vec)\nsd(acc_vec)\n\n\n\nIn this part of the code, we perform the bootstrap procedure with 100 samples. To that, we will implement our own function to create the index n-times for a given dataset. This ensures that we get a similar output to caret::createResample(). In this case, we apply this function to our main training dataframe 100 times.\n\ndef create_resample(data, times=100, random_seed=None):\n    # If you're not familiar with the documentation below, they are called\n    # `docstrings` and whenever you ask help for a function or see it's documentation,\n    # they are generated from that\n    \"\"\"\n    Generate bootstrap sample indices for data.\n    \n    Args:\n    - data (array-like): The data to bootstrap.\n    - times (int): The number of bootstrap samples to generate.\n    - random_seed (int): The random seed to use for reproducibility.\n    \n    Returns:\n    - samples (list of arrays): A list of times bootstrap sample indices.\n    \"\"\"\n    np.random.seed(random_seed)\n    n_samples = len(data)\n    samples = []\n    for _ in range(times):\n        indices = np.random.choice(n_samples, n_samples, replace=True)\n        samples.append(indices)\n    return samples\n\n# apply the new created function\nindex_boot = create_resample(r.df_tr, times=100, random_seed = 123)\n# we can see if we successfully replicated the sampling process 100 times\nprint(len(index_boot))\n# check if we have the correct number of rows (e.g. for the first element)\nprint(len(index_boot[0]))\n# alternatively to see this information, you can uncomment & run the code below\n# np.asarray(index_boot).shape\n\n\n\n\n\n\n\nNote\n\n\n\nOne thing to note is that we could have used the sklearn.utils.resample introduced earlier to directly get a list of dataframes with the randomly chosen indices. The issue here would be rather a computational one, as we have to extract the rows many times from the dataset and then hold all this data in memory, which is redundant. So although this may not be a problem for 100 replications, it can quickly start to become an issue if you want to replicated many more times (e.g., 100,000 times). The best approach is to get the indices, and then subset the rows only when needed.\n\nimport numpy as np\nfrom sklearn.utils import resample\n\ndef create_n_resamples(data, times, random_seed=None):\n    \"\"\"\n    Generate n_bootstraps bootstrap samples of data.\n    \n    Args:\n    - data (array-like): The data to bootstrap.\n    - n_bootstraps (int): The number of bootstrap samples to generate.\n    - random_seed (int): The random seed to use for reproducibility.\n    \n    Returns:\n    - bootstrap_samples (list of lists): A list of n_bootstraps bootstrap samples.\n    \"\"\"\n    np.random.seed(random_seed)\n    bootstrap_samples = []\n    for i in range(times):\n        sample = resample(data)\n        bootstrap_samples.append(sample)\n    return bootstrap_samples\n\ndfs_boot = create_n_resamples(r.df_tr, times=100, random_seed = 123) \n\n\n\nFor each sample, we calculate the out-of-bag accuracy, the apparent accuracy, and the final accuracy estimate using the 0.368/0.632 rule. This is done using a loop that iterates 100 times, once for each bootstrap sample. The steps that the code follows are similar to R, and are outlined below:\n\nWe set up three arrays to store the out-of-bag accuracy, the apparent accuracy, and the final accuracy estimate for each of the 100 bootstrap samples.\nIn the loop, we perform the following steps for each bootstrap sample:\n\nUse the generate a random list of indices with replacement, which forms the bootstrap training set.\nCreate a mask to extract the out-of-bag (validation) set from the original training set.\nTrain the logistic regression model with RFE on the bootstrap training set.\nCompute the out-of-bag accuracy by predicting on the validation set and comparing the predicted labels to the true labels.\nCompute the apparent accuracy by predicting on the bootstrap training set and comparing the predicted labels to the true labels.\nCalculate the final accuracy estimate for the current bootstrap sample using the 0.368/0.632 rule.\n\n\nOnce the loop is complete, the acc_vec array will contain the final accuracy estimates for all 100 bootstrap samples. We can then calculate the mean and standard deviation of these accuracy estimates to get an overall understanding of the model‚Äôs performance.\n\n\n# we one-hote encode the categorical variables\n## notice that we didn't use the argument `drop_first` before, since this is like\n## making dummy variable m - 1 where m is the number of variables you have\nr.df_tr_encoded = pd.get_dummies(r.df_tr, drop_first=True)\n\noob_acc_vec = np.zeros(100)\napp_acc_vec = np.zeros(100)\nacc_vec = np.zeros(100)\n\nfor i in range(100):\n    df_boot_tr = r.df_tr_encoded.iloc[index_boot[i]]\n    y_boot_tr = df_boot_tr[\"visits_Yes\"].astype(int)\n    X_boot_tr = df_boot_tr.drop(\"visits_Yes\", axis=1)\n\n    oob_mask = ~r.df_tr_encoded.index.isin(df_boot_tr.index.values)\n    df_boot_val = r.df_tr_encoded[oob_mask]\n    y_boot_val = df_boot_val[\"visits_Yes\"].astype(int)\n    X_boot_val = df_boot_val.drop(\"visits_Yes\", axis=1)\n\n    model = LogisticRegression(solver='liblinear')\n    rfe = RFE(model)\n    rfe.fit(X_boot_tr, y_boot_tr)\n\n    pred_probs_val = rfe.predict_proba(X_boot_val)[:, 1]\n    Doc_boot_pred_val = (pred_probs_val > 0.5).astype(int)\n    oob_acc = accuracy_score(y_boot_val, Doc_boot_pred_val)\n    oob_acc_vec[i] = oob_acc\n\n    pred_probs_tr = rfe.predict_proba(X_boot_tr)[:, 1]\n    Doc_boot_pred_tr = (pred_probs_tr > 0.5).astype(int)\n    app_acc = accuracy_score(y_boot_tr, Doc_boot_pred_tr)\n    app_acc_vec[i] = app_acc\n\n    acc_vec[i] = 0.368 * app_acc + 0.632 * oob_acc\n\n\n\n\nRFE(estimator=LogisticRegression(solver='liblinear'))\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\nRFE\n\n?Documentation for RFEiFitted\nRFE(estimator=LogisticRegression(solver='liblinear')) \n\n\nestimator: LogisticRegressionLogisticRegression(solver='liblinear') \n\nLogisticRegression\n?Documentation for LogisticRegressionLogisticRegression(solver='liblinear') \n\n\n\n\nprint(acc_vec)"
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#automated-approach-1",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#automated-approach-1",
    "title": "Data splitting",
    "section": "Automated approach",
    "text": "Automated approach\n\n\nR - caret\nPython - sklearn & mlxtend\n\n\n\nWe only need to change the method in the trainControl function. The corresponding method is ‚Äúboot632‚Äù.\n\nset.seed(346)\ntrctrl <- trainControl(method = \"boot632\", number=100)\nDoc_boot <- train(visits ~., data = df_tr, method = \"glmStepAIC\", family=\"binomial\",\n                   trControl=trctrl, trace = 0)\nDoc_boot\n\n\n\nAs sklearn does not offer bootstrap with the 0.632 rule, we use bootstrap_point632_score function from the mlxtend library to perform bootstrapping with the 0.632 rule for our Logistic Regression model. We will use mlxtend with R for bootstrapping with the 0.632 rule.\nPlease note for this part, we don‚Äôt make any step-wise feature selection here as in the case of caret (i.e., glmStepAIC), but similar feature selections such as sklearn.feature_selection.RFE can be implemented since, as mentioned in the Ex_ML_LinLogReg exercises, there are no exact implementations of step-wise AIC regression with the libraries of interest in python.\n\nfrom mlxtend.evaluate import bootstrap_point632_score\n\nnp.random.seed(346)\n\n# Fit the logistic regression model\nmodel = LogisticRegression(solver='liblinear')\n\n# Compute bootstrap point 632 scores\nscores = bootstrap_point632_score(estimator=model, X=X_train, y=y_train, n_splits=100, random_seed=123)\n\n# Print the mean accuracy and standard deviation\nprint(\"Mean accuracy:\", np.mean(scores))\nprint(\"Standard deviation:\", np.std(scores))\n\nThe results are now very close to our model in caret."
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#sub-sampling",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#sub-sampling",
    "title": "Data splitting",
    "section": "Sub-sampling",
    "text": "Sub-sampling\nBalancing using sub-sampling consists of taking all the cases in the smallest class (i.e., Yes) and extract at random the same amount of cases in the largest category (i.e., No).\n\n\nR\nPython\n\n\n\n\nn_yes <- min(table(df_tr$visits)) ## 840\n\ndf_tr_no <- filter(df_tr, visits==\"No\") ## the \"No\" cases\ndf_tr_yes <- filter(df_tr, visits==\"Yes\") ## The \"Yes\" cases\n\nindex_no <- sample(size=n_yes, x=1:nrow(df_tr_no), replace=FALSE) ## sub-sample 840 instances from the \"No\"\n\ndf_tr_subs <- data.frame(rbind(df_tr_yes, df_tr_no[index_no,])) ## Bind all the \"Yes\" and the sub-sampled \"No\"\ntable(df_tr_subs$visits) ## The cases are balanced\n\nNow let us see the result on the accuracy measures.\n\nDoc_lr_subs <- glm(visits~., data=df_tr_subs, family=\"binomial\") %>% step(trace=0)\nDoc_lr_subs_prob <- predict(Doc_lr_subs, newdata=df_te, type=\"response\")\nDoc_lr_subs_pred <- ifelse(Doc_lr_subs_prob>0.5,\"Yes\",\"No\")\nconfusionMatrix(data=as.factor(Doc_lr_subs_pred), reference = df_te$visits)\n\n\n\n\nn_yes = min(r.df_tr['visits'].value_counts()) ## 840\n\ndf_tr_no = r.df_tr[r.df_tr['visits'] == \"No\"] ## the \"No\" cases\ndf_tr_yes = r.df_tr[r.df_tr['visits'] == \"Yes\"] ## The \"Yes\" cases\n\nindex_no = np.random.choice(df_tr_no.index, size=n_yes, replace=False)\n\ndf_tr_subs = pd.concat([df_tr_yes, df_tr_no.loc[index_no]])\ndf_tr_subs['visits'].value_counts() ## The cases like R are balanced\n\nNow to the calculating the scores again:\n\nX_train_subs = pd.get_dummies(df_tr_subs.drop(columns=['visits']))\ny_train_subs = df_tr_subs['visits']\n\nlr_subs = LogisticRegression(solver='liblinear')\nlr_subs.fit(X_train_subs, y_train_subs)\n\n\n\n\nLogisticRegression(solver='liblinear')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nLogisticRegression\n\n?Documentation for LogisticRegressioniFitted\nLogisticRegression(solver='liblinear') \n\n\nlr_subs_pred = lr_subs.predict(X_test)\nlr_subs_cf = confusion_matrix(y_test, lr_subs_pred)\n\ntn_subs, fp_subs, fn_subs, tp_subs = lr_subs_cf.ravel()\n\nspecificity_subs = tn_subs / (tn_subs + fp_subs)\nsensitivity_subs = recall_score(y_test, lr_subs_pred, pos_label='Yes')\nbalanced_acc_subs = balanced_accuracy_score(y_test, lr_subs_pred)\naccuracy_subs = accuracy_score(y_test, lr_subs_pred)\n\nprint(lr_subs_cf)\nprint(f\"Accuracy: {accuracy_subs:.3f}\")\nprint(f\"Balanced Accuracy: {balanced_acc_subs:.3f}\")\nprint(f\"Specificity: {specificity_subs:.3f}\")\nprint(f\"Sensitivity: {sensitivity_subs:.3f}\")\n\nSame conclusion as R (albeit with slightly different values).\n\n\n\nAs expected, the accuracy has decreased but the balanced accuracy has increased. Depending on the aim of the prediction, this model may be much better to use than the one trained on the unbalanced data."
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#resampling",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#resampling",
    "title": "Data splitting",
    "section": "Resampling",
    "text": "Resampling\nBalancing by resampling follows the same aim. The difference with sub-sampling is that the resampling increases the number of cases in the smallest class by resampling at random from them. The codes below are explicit:\n\n\nR\nPython\n\n\n\n\nn_no <- max(table(df_tr$visits)) ## 3313\n\ndf_tr_no <- filter(df_tr, visits==\"No\")\ndf_tr_yes <- filter(df_tr, visits==\"Yes\")\n\nindex_yes <- sample(size=n_no, x=1:nrow(df_tr_yes), replace=TRUE)\ndf_tr_res <- data.frame(rbind(df_tr_no, df_tr_yes[index_yes,]))\ntable(df_tr_res$visits)\n\nNow, we have a balanced data set where each class has the same amount as the largest class (i.e., ‚ÄúNo‚Äù) in the original training set. The effect on the model fit is very similar to the subsampling:\n\nDoc_lr_res <- glm(visits~., data=df_tr_res, family=\"binomial\") %>% step(trace=0)\nDoc_lr_res_prob <- predict(Doc_lr_res, newdata=df_te, type=\"response\")\nDoc_lr_res_pred <- ifelse(Doc_lr_res_prob>0.5,\"Yes\",\"No\")\nconfusionMatrix(data=as.factor(Doc_lr_res_pred), reference = df_te$visits)\n\n\n\n\nn_no = max(r.df_tr['visits'].value_counts()) ## 3313\n\ndf_tr_no = r.df_tr[r.df_tr['visits'] == \"No\"]\ndf_tr_yes = r.df_tr[r.df_tr['visits'] == \"Yes\"]\n\nindex_yes = np.random.choice(df_tr_yes.index, size=n_no, replace=True)\ndf_tr_res = pd.concat([df_tr_no, df_tr_yes.loc[index_yes]])\ndf_tr_res['visits'].value_counts()\n\nNow we can model again with the resampled data\n\nX_train_res = pd.get_dummies(df_tr_res.drop(columns=['visits']))\ny_train_res = df_tr_res['visits']\n\nlr_res = LogisticRegression(solver='liblinear')\nlr_res.fit(X_train_res, y_train_res)\n\n\n\n\nLogisticRegression(solver='liblinear')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nLogisticRegression\n\n?Documentation for LogisticRegressioniFitted\nLogisticRegression(solver='liblinear') \n\n\nlr_res_pred = lr_res.predict(X_test)\n\nlr_res_cf = confusion_matrix(y_test, lr_res_pred)\n\ntn_res, fp_res, fn_res, tp_res = lr_res_cf.ravel()\n\nspecificity_res = tn_res / (tn_res + fp_res)\nsensitivity_res = recall_score(y_test, lr_res_pred, pos_label='Yes')\nbalanced_acc_res = balanced_accuracy_score(y_test, lr_res_pred)\naccuracy_res = accuracy_score(y_test, lr_res_pred)\n\nprint(lr_res_cf)\nprint(f\"Accuracy: {accuracy_res:.3f}\")\nprint(f\"Balanced Accuracy: {balanced_acc_res:.3f}\")\nprint(f\"Specificity: {specificity_res:.3f}\")\nprint(f\"Sensitivity: {sensitivity_res:.3f}\")\n\n\n\n\nWhether one should prefer sub-sampling or resampling depends on the amount and the richness of the data."
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html",
    "title": "Ensemble Methods",
    "section": "",
    "text": "In this part of the lab, we will look at how the randomForest library (alternative to ranger) can be applied for classification and regression tasks. At the very end, please feel free to apply these techniques to one of your favorite datasets seen in class (classification or regression).\n\n\n\n\n\n\nHyperparameters of RF\n\n\n\nR (using the randomForest library):\n\n\nntree: The number of trees in the forest (equivalent to n_estimators in python).\n\nmtry: The number of features to consider when looking for the best split. (similar to max_features in python)\n\nmax.depth: The maximum depth of each tree.\n\nnodesize: The minimum number of samples required to split an internal node (equivalent to min_samples_split in python).\n\nPython (using the sklearn library):\n\n\nn_estimators: The number of trees in the forest.\n\nmax_features: The number of features to consider when looking for the best split.\n\nmax_depth: The maximum depth of each tree.\n\nmin_samples_split: The minimum number of samples required to split an internal node.\n\nmin_samples_leaf: The minimum number of samples required to be at a leaf node.\n\n\n\n\n\n\n\n\n\nFew tips on RF hyperparameters\n\n\n\nA few (among many) tips for finding the ideal hyperparameters for RF:\n\n\nntree (R) / n_estimators (python): Use a large number of trees in the forest to improve model performance, but be aware of the increased computation time. The default value is usually a good starting point.\n\nmtry (R) / max_features (python): Experiment with different values, usually starting with the default (square root of the number of features for classification or one-third of the number of features for regression). Increasing this value may improve model performance but can also increase computation time.\n\nmax.depth: Control the depth of each tree to manage overfitting. Deeper trees capture more complex patterns but can lead to overfitting. Experiment with different values, keeping in mind that a shallower tree can be more interpretable and less prone to overfitting.\n\nnodesize (R) / min_samples_split (python): Increasing this value can help reduce overfitting, but setting it too high might lead to underfitting. Experiment with different values to find the optimal balance.\n\n\n\n\n\nLoad the library randomForest in R. Then, load the wine data set. This dataset is about white wine quality (in fact Portuguese vinho verde). The data contains 11 numerical features and 1 factor variable:\n\nfixed.acidity\nvolatile.acidity\ncitric.acid\nresidual.sugar\nchlorides\nfree.sulfur.dioxide\ntotal.sulfur.dioxide\ndensity\npH\nsulphates\nalcohol\nquality: Good/Bad\n\nAll the numerical features have units. The data source can be found here. For simplicity, only an extraction of 200 wines are used in this exercise. Note that in the original data set, the quality is a score (0 to 10) that was turned as factor here for the exercise (Bad: 0 to 5, Good: 6 to 10). Also, note that in the data source, the objective is to predict the quality from the other features (supervised learning).\nAs mentioned, the outcome variable used for this dataset is the wine quality. We should first coerce the classes as factors. Then, we make the training/test set random split with a 75/25 scheme.\n\n\nR\nPython\n\n\n\n\nlibrary(randomForest)\nwine <- read.csv(here::here(\"data/Wine.csv\"))\nwine$quality <- as.factor(wine$quality)\n\n# define a function to get the splitting index (training and testing) of a given dataset\nget_split_index <- function(dataset, train_proportion = 0.75) {\n  set.seed(123)\n  index <-\n    sample(\n      x = 1:2,\n      size = nrow(dataset),\n      replace = TRUE,\n      prob = c(train_proportion, 1 - train_proportion)\n    )\n  return(index)\n}\n\nwine_index <- get_split_index(wine)\nwine_tr <- wine[wine_index == 1, ]\nwine_te <- wine[wine_index == 2, ]\n\n\n\n\n# Load the course python environment as usual with a r code chunks.\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\nSimilar to the previous labs, in python, we can use the usual sklearn library to do all our modelling. Please note that we will load the data again in python to make the demo easier. Additionally, we‚Äôll load all the necessary libraries for this lab in this code chunk.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# we first move up one directory to achieve relative paths\nwine = pd.read_csv('../data/Wine.csv')\n\nwine['quality'] = wine['quality'].astype('category')\n\n# Split wine dataset into train and test\ntrain_wine, test_wine = train_test_split(wine, test_size=0.25, random_state=123)\n\n\n\n\nNote: Here, we have written a function to make the split as we will also need to apply it also for another dataset in the regression part.\n\nFit a random forest on the train set. The target is the taste variable that we want to predict. Specify for the number of trees ntree=1000 (by default, the function selects \\(500\\) trees). Remember to exclude quality in the predictors of the formula. Also, use the option importance=TRUE, we will need it afterward. Then test the model by computing the accuracy on the test set. You may use confusionMatrix from caret.\n\n\nR\nPython\n\n\n\n\nwine_rf <- randomForest(quality~., data=wine_tr, ntree=1000, importance=TRUE)\nwine.pred_rf <- predict(wine_rf, newdata=wine_te)\n\nlibrary(caret)\nconfusionMatrix(data=wine.pred_rf, reference = wine_te$quality)\n\n\n\n\n# Fit a Random Forest classifier on the train set\nwine_rf = RandomForestClassifier(n_estimators=1000, random_state=123)\nwine_rf.fit(train_wine.drop('quality', axis=1), train_wine['quality'])\n\n# Test the model and compute the accuracy\nwine_pred_rf = wine_rf.predict(test_wine.drop('quality', axis=1))\nprint(confusion_matrix(test_wine['quality'], wine_pred_rf))\nprint(\"Accuracy:\", accuracy_score(test_wine['quality'], wine_pred_rf))\n\nThis model is worse than the R version mostly because of the different defaults.\n\n\n\n\nExtract the model-specific variable importance using the functions varImpPlot (plots) and importance (values) on the model. Observe well that the mean decrease in accuracy of each variable is also computed for each specific class. In particular, what makes density special for predicting Good compare to another variable (like for example citric.acid)?\n\n\nR\nPython\n\n\n\n\nvarImpPlot(wine_rf)\nimportance(wine_rf)\n\n\n\n\n# Variable importance\nwine_importances = pd.Series(wine_rf.feature_importances_, index=train_wine.drop('quality', axis=1).columns)\nwine_importances.sort_values(ascending=False).plot(kind='bar')\n\n\n\n\ndensity is important for predicting the Good since their predictions is much less accurate if we do not use it. citric.acid is both overall less important than density but especially for prediction of Good.\n\nIn this part, we will be using the real_estate_data.csv once again. After reading the data, apply a random forest to predict price using all the other variables except No, Month and Year. Compute the RMSE and inspect the prediction quality with a graph. Note that the importance is not specific to any class here.\n\n\nR\nPython\n\n\n\n\nlibrary(dplyr)\nlibrary(magrittr)\nreal_estate_data <- read.csv(here::here(\"labs/data/real_estate_data.csv\"))\n\n# select the columns of interest\nreal_estate_data <- \n  real_estate_data %>% \n  select(-c(No, Month, Year))\n\n# once again, divide the data into training and testing sets using the function created earlier\nrestate_index <- get_split_index(real_estate_data)\nrestate_tr <- real_estate_data[restate_index == 1, ]\nrestate_te <- real_estate_data[restate_index == 2, ]\n\n# apply the RF model as a regression\nrestate_rf <- randomForest(Price~., data=restate_tr, ntree=1000, importance=TRUE)\nrestate.pred_rf<-predict(restate_rf, newdata=restate_te)\n\n# compute rmse and plot the results as well the VarImp\n(rmse <- sqrt(mean((restate_te$Price - restate.pred_rf)^2)))\nplot(restate_te$Price ~ restate.pred_rf)\nabline(0,1)\nvarImpPlot(restate_rf)\nimportance(restate_rf)\n\n\n\n\n# Load real estate dataset\nreal_estate_data = pd.read_csv(\"../data/real_estate_data.csv\")\n\nreal_estate_data = real_estate_data.drop(['No', 'Month', 'Year'], axis=1)\n\n# Split real estate dataset into train and test\ntrain_restate, test_restate = train_test_split(real_estate_data, test_size=0.25, random_state=123)\n\n# Fit a Random Forest regressor on the train set\nrestate_rf = RandomForestRegressor(n_estimators=1000, random_state=123)\nrestate_rf.fit(train_restate.drop('Price', axis=1), train_restate['Price'])\n\n# Test the model and compute the RMSE\nrestate_pred_rf = restate_rf.predict(test_restate.drop('Price', axis=1))\nrmse = np.sqrt(mean_squared_error(test_restate['Price'], restate_pred_rf))\nprint(\"RMSE:\", rmse)\n\n# Plot the prediction quality\nplt.scatter(test_restate['Price'], restate_pred_rf)\nplt.xlabel('Actual Price')\nplt.ylabel('Predicted Price')\nplt.plot([min(test_restate['Price']), max(test_restate['Price'])], [min(test_restate['Price']), max(test_restate['Price'])], color='red')\nplt.show()\n\n# Variable importance\nrestate_importances = pd.Series(restate_rf.feature_importances_, index=train_restate.drop('Price', axis=1).columns)\nrestate_importances.sort_values(ascending=False).plot(kind='bar')\n\n\n\n\nCompare this model with the one you came up with in Ex_ML_LinLogReg . Which one would you go for?"
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html#classification-1",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html#classification-1",
    "title": "Ensemble Methods",
    "section": "Classification",
    "text": "Classification\nTraining and testing the model\nWe now fit a GBM model on the wine training set and apply it to the same target variable quality. We can train the model and add\n\n\nR\nPython\n\n\n\n\nlibrary(gbm)\nset.seed(123)\nwine_gbm <- gbm(quality~., data=wine_tr, distribution=\"multinomial\", n.trees=1000, interaction.depth=4, shrinkage=0.01)\nwine.pred_gbm <- predict(wine_gbm, newdata=wine_te, n.trees=1000, type=\"response\")\nwine.pred_gbm_class <- apply(wine.pred_gbm, 1, which.max)\nlevels(wine_te$quality) <- 1:length(levels(wine_te$quality))\nconfusionMatrix(factor(wine.pred_gbm_class), wine_te$quality)\n\n\n\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Fit a Gradient Boosting classifier on the train set\nwine_gbm = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.01, max_depth=4, random_state=123)\nwine_gbm.fit(train_wine.drop('quality', axis=1), train_wine['quality'])\n\n# Test the model and compute the accuracy\nwine_pred_gbm = wine_gbm.predict(test_wine.drop('quality', axis=1))\nprint(confusion_matrix(test_wine['quality'], wine_pred_gbm))\nprint(\"Accuracy:\", accuracy_score(test_wine['quality'], wine_pred_gbm))"
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html#regression-1",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html#regression-1",
    "title": "Ensemble Methods",
    "section": "Regression",
    "text": "Regression\nIn this part, we will continue using the real_estate_data.csv. Fit a GBM model on the real estate training set to predict price using all the other variables except No, Month, and Year. Then compute the metrics and plot the predictions.\n\n\nR\nPython\n\n\n\n\nset.seed(123)\nrestate_gbm <- gbm(Price~., data=restate_tr, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.01)\nrestate.pred_gbm<-predict(restate_gbm, newdata=restate_te, n.trees=1000)\n\n# compute rmse and plot the results\n(rmse_gbm <- sqrt(mean((restate_te$Price - restate.pred_gbm)^2)))\nplot(restate_te$Price ~ restate.pred_gbm)\nabline(0,1)\n\n\n\n\n# run the code below if you have not cleared the plot yet\nplt.clf()\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n# Fit a Gradient Boosting regressor on the train set\nrestate_gbm = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01, max_depth=4, random_state=123)\nrestate_gbm.fit(train_restate.drop('Price', axis=1), train_restate['Price'])\n# Test the model and compute the RMSE\n\nrestate_pred_gbm = restate_gbm.predict(test_restate.drop('Price', axis=1))\nrmse_gbm = np.sqrt(mean_squared_error(test_restate['Price'], restate_pred_gbm))\nprint(\"RMSE:\", rmse_gbm)\n# Plot the prediction quality\n\nplt.scatter(test_restate['Price'], restate_pred_gbm)\nplt.xlabel('Actual Price')\nplt.ylabel('Predicted Price')\nplt.plot([min(test_restate['Price']), max(test_restate['Price'])], [min(test_restate['Price']), max(test_restate['Price'])], color='red')\nplt.show()\n\n\n\n\nCompare the GBM model with the Random Forest model you came up with earlier. Which one would you go for?"
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html#what-is-xgboost",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html#what-is-xgboost",
    "title": "Ensemble Methods",
    "section": "What is XGBoost?",
    "text": "What is XGBoost?\nXGBoost (Extreme Gradient Boosting) is an optimized implementation of the gradient boosting algorithm. It is designed for high performance and efficient memory usage. XGBoost improves upon the base Gradient Boosting Machine (GBM) by incorporating regularization to prevent overfitting and implementing parallel processing techniques for faster training. The algorithm also offers built-in cross-validation and early stopping to save time and resources during model training."
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html#modelling-with-xgboost",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html#modelling-with-xgboost",
    "title": "Ensemble Methods",
    "section": "Modelling with XGBoost",
    "text": "Modelling with XGBoost\nWe‚Äôll use the xgboost library in both R and python. You can see some of the hyperparameters below:\n\n\n\n\n\n\nHyperparameters of XGBoost\n\n\n\n\n\neta: Controls the learning rate, which determines the step size at each iteration while updating the model weights. Smaller values make the model more robust to overfitting but require more iterations to converge. Typical values range from 0.01 to 0.3.\n\nmax_depth: Controls the maximum depth of each tree. Deeper trees can model more complex relationships but are more prone to overfitting. Experiment with different values, keeping in mind that a shallower tree can be more interpretable and less prone to overfitting.\n\nmin_child_weight: Controls the minimum sum of instance weights needed in a child node. Increasing this value helps to prevent overfitting by making the model more conservative.\n\n\n\nYou can read more about the package in its documentation.\n\n\nR\nPython\n\n\n\n\n# Install and load the package\n# install.packages(\"xgboost\")\nlibrary(xgboost)\n\n# Prepare data for XGBoost\ndtrain <- xgb.DMatrix(data = as.matrix(restate_tr[, -ncol(restate_tr)]), label = restate_tr$Price)\ndtest <- xgb.DMatrix(data = as.matrix(restate_te[, -ncol(restate_te)]), label = restate_te$Price)\n\n# Set hyperparameters\nparams <- list(\n  objective = \"reg:squarederror\",\n  eta = 0.1,\n  max_depth = 5,\n  min_child_weight = 1,\n  subsample = 1,\n  colsample_bytree = 1\n)\n\n# Train the model\nxgb_model <- xgb.train(params, dtrain, nrounds = 1000)\n\n# Test the model and compute the RMSE\nrestate_pred_xgb <- predict(xgb_model, dtest)\nrmse_xgb <- sqrt(mean((restate_te$Price - restate_pred_xgb)^2))\nprint(paste(\"RMSE:\", rmse_xgb))\n\n\n\nWe used the python installation of xgboost from our lab setup.\n\n# Install and load the package\nimport xgboost as xgb\n\n# Prepare data for XGBoost\ndtrain = xgb.DMatrix(train_restate.drop(\"Price\", axis=1), label=train_restate[\"Price\"])\ndtest = xgb.DMatrix(test_restate.drop(\"Price\", axis=1), label=test_restate[\"Price\"])\n\n# Set hyperparameters\nparams = {\n    \"objective\": \"reg:squarederror\",\n    \"eta\": 0.1,\n    \"max_depth\": 3,\n    \"min_child_weight\": 1,\n    \"subsample\": 1,\n    \"colsample_bytree\": 1,\n}\n\n# Train the model\nxgb_model = xgb.train(params, dtrain, num_boost_round=1000)\n\n# Test the model and compute the RMSE\nrestate_pred_xgb = xgb_model.predict(dtest)\nrmse_xgb = np.sqrt(mean_squared_error(test_restate[\"Price\"], restate_pred_xgb))\nprint(\"RMSE:\", rmse_xgb)\n\nAlthough initially our GBM suffered compared to the RF, we can see that XGBoost can help improve the result (the case for the python implementation). However, random forest still outperforms all the other models.\n\n\n\nFeel free to apply XGBoost to the dataset of your choice."
  },
  {
    "objectID": "labs/07_InterpretableML/Ex_ML_VarImp.html",
    "href": "labs/07_InterpretableML/Ex_ML_VarImp.html",
    "title": "Variable Importance",
    "section": "",
    "text": "This exercise shows an example of model-agnostic variable importance for a regression problem. The dataset that we will be working with is Carseats from the ISLR library which has already been used during some of the exercises such as Ex_ML_Tree and Ex_ML_SVM. It is highly recommended that you try to implement some parts of the exercise yourself before checking the answers.\n\nLoad the data from ISLR package, then assign 90% of the data for training and the remainder for testing. Please keep in mind that we make a training split running the feature importance (instead of using the entire dataset) as we ‚Äúmay‚Äù want to re-train the model with only a fewer features rather than biasing our decision by also including the testing data.\nCreate three models including a linear regression, a regression tree and a support-vector machine.\n\nAnswer\n\n\n\nR\n\n\n\nlibrary(rpart)\nlibrary(e1071)\nlibrary(dplyr)\nlibrary(ISLR)\n\n# divide the data into training and testing sets\nset.seed(2022)\ncarseats_index <- sample(x=1:nrow(Carseats), size=0.9*nrow(Carseats), replace=FALSE)\ncarseats_tr <- Carseats[carseats_index,]\ncarseats_te <- Carseats[-carseats_index,]\n\n# define a linear regression\ncarseats_lm <- lm(Sales~., data=carseats_tr)\n\n# define a regression tree (you can also use `adabag::autoprune()`here\ncarseats_rt <- rpart(Sales~., data=carseats_tr)\n\n# define a support-vector machine\ncarseats_svm <- svm(Sales ~ ., data=carseats_tr)\n\n\n\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import fetch_openml\n\n# Use the training and test sets created in R (no easy way to get the `Carseats` data in python)\n# Convert the categorical columns to one-hot encoded ones\ncarseats_train, carseats_test = pd.get_dummies(r.carseats_tr.copy()), pd.get_dummies(r.carseats_tr.copy())\n\n# Define a linear regression\ncarseats_lr = LinearRegression()\ncarseats_lr.fit(carseats_train.drop(columns=['Sales']), carseats_train['Sales'])\n\n\n\n\nLinearRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nLinearRegression\n\n?Documentation for LinearRegressioniFitted\nLinearRegression() \n\n\n# Define a decision tree regressor\ncarseats_dtr = DecisionTreeRegressor()\ncarseats_dtr.fit(carseats_train.drop(columns=['Sales']), carseats_train['Sales'])\n\n\n\n\nDecisionTreeRegressor()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nDecisionTreeRegressor\n\n?Documentation for DecisionTreeRegressoriFitted\nDecisionTreeRegressor() \n\n\n# Define a support-vector machine\ncarseats_svr = SVR()\ncarseats_svr.fit(carseats_train.drop(columns=['Sales']), carseats_train['Sales'])\n\n\n\n\nSVR()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nSVR\n\n?Documentation for SVRiFitted\nSVR()"
  },
  {
    "objectID": "labs/07_InterpretableML/Ex_ML_VarImp.html#creating-an-explain-object",
    "href": "labs/07_InterpretableML/Ex_ML_VarImp.html#creating-an-explain-object",
    "title": "Variable Importance",
    "section": "Creating an explain object",
    "text": "Creating an explain object\nDALEX has an explain object which allows you to do various kind of explanatory analysis. Try reading about the required inputs for it by referring to its documentations (?DALEX::explain()) and also referring to the book mentioned at the beginning of this exercise (‚ÄúHands-on Machine Learning with R‚Äù). Create one explain object per model for the training data and set the inputs that you need which are model, data (data frame of features) and y (vector of observed outcomes). Also, you can give a caption to your model through the label argument.\n\n\nR\nPython\n\n\n\n\nlibrary(dplyr)\nlibrary(DALEX)\n\nx_train <- select(carseats_tr, -Sales)\ny_train <- pull(carseats_tr, Sales)\n\nexplainer_lm <- DALEX::explain(model = carseats_lm, \n                                 data = x_train, \n                                 y = y_train,\n                                 label = \"Linear Regression\")\n\nexplainer_rt <- DALEX::explain(model = carseats_rt,\n                               data = x_train,\n                               y = y_train,\n                               label = \"Regression Tree\")\n\nexplainer_svm <- DALEX::explain(model = carseats_svm,\n                                data = x_train,\n                                y = y_train,\n                                label = \"Support Vector Machine\")\n\n\n\nTo calculate the feature importances using Python, we‚Äôll be using the permutation_importance function from the sklearn library.\n\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the features (relevant to SVM)\nscaler = StandardScaler()\ncarseats_train_scaled = carseats_train.copy()\ncarseats_test_scaled = carseats_test.copy()\n\n# Calculate feature importances for each model\nimportance_lr = permutation_importance(carseats_lr, carseats_train.drop(columns=['Sales']), carseats_train['Sales'], n_repeats=10, random_state=2022)\nimportance_dtr = permutation_importance(carseats_dtr, carseats_train.drop(columns=['Sales']), carseats_train['Sales'], n_repeats=10, random_state=2022)\nimportance_svr = permutation_importance(carseats_svr, carseats_train.drop(columns=['Sales']), carseats_train['Sales'], n_repeats=10, random_state=2022)\n# Train the SVM model on scaled data\ncarseats_svr_scaled = SVR(kernel='linear')\ncarseats_svr_scaled.fit(carseats_train_scaled.drop(columns=['Sales']), carseats_train_scaled['Sales'])\n\n\n\n\nSVR(kernel='linear')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nSVR\n\n?Documentation for SVRiFitted\nSVR(kernel='linear') \n\n\nimportance_svr_scaled = permutation_importance(carseats_svr_scaled, carseats_train_scaled.drop(columns=['Sales']), carseats_train_scaled['Sales'], n_repeats=10, random_state=2022)\n\n# Print the feature importances\nimportance_df = pd.DataFrame(data={\n    'Feature': carseats_train.drop(columns=['Sales']).columns,\n    'Linear Regression': importance_lr.importances_mean,\n    'Decision Tree': importance_dtr.importances_mean,\n    'Support Vector Machine (unscaled)': importance_svr.importances_mean,\n    'Support Vector Machine (scaled)': importance_svr_scaled.importances_mean\n})\n\nprint(importance_df)\n\nYou can observe the value of scaling for SVM. The results seems to agree that Price is the most important feature."
  },
  {
    "objectID": "labs/07_InterpretableML/Ex_ML_VarImp.html#plotting-the-feature-importance",
    "href": "labs/07_InterpretableML/Ex_ML_VarImp.html#plotting-the-feature-importance",
    "title": "Variable Importance",
    "section": "Plotting the feature importance",
    "text": "Plotting the feature importance\nNow that you have created the DALEX::explain objects, we will use another function called model_parts which takes care of the feature permutation. Try reading about the function DALEX::model_parts() . The main arguments that you need to provide to the explain function are:\n\nAn explainer object (what you created above).\nB which is the number of permutations (i.e.¬†how many times you want to randomly shuffle each column).\nThe type of scores you would like it to return (raw score vs differences vs ratios) which in this case we set to ratio and you can read more the differences in the documentation.\nN argument which you can set to N=NULL which essentially asks how many samples you would like to use for calculating the variable importance, where setting it to NULL means that we use the entire training set.\nThere is also a loss_function which by default is RMSE for regression (our case) and 1-AUC for classification, by there are a few more which you can find out about by referring to the documentations (e.g.¬†through ?DALEX::loss_root_mean_square).\n\nAfter assigning model_parts to a variable, try plotting each model to see the most important variables. What do you see? Are there important features that are in common?\n\ncalculate_importance <- function(your_model_explainer, n_permutations = 10) {\n  imp <- model_parts(explainer = your_model_explainer,\n                     B = n_permutations,\n                     type = \"ratio\",\n                     N = NULL)\n  return(imp)\n}\n\nimportance_lm  <- calculate_importance(explainer_lm)\nimportance_rt  <- calculate_importance(explainer_rt)\nimportance_svm <- calculate_importance(explainer_svm)\n\nlibrary(ggplot2)\nplot(importance_lm, importance_rt, importance_svm) +\n  ggtitle(\"Mean variable-importance ratio over 10 permutations\", \"\")"
  },
  {
    "objectID": "labs/07_InterpretableML/Ex_ML_VarImp.html#svm",
    "href": "labs/07_InterpretableML/Ex_ML_VarImp.html#svm",
    "title": "Variable Importance",
    "section": "SVM",
    "text": "SVM\n\n\nR\nPython\n\n\n\nlime package does not support the SVM model from the e1071 package out of the box.You can see the list of supported models via ?model_type. There are solutions to this:\n\nRe-train your model with the caret library which we then work directly with this library (also may be good practice to build your models with caret).\n\n\n# Load the lime library\nlibrary(lime)\n\n# Create a caret model using a support vector machine\nsvm_caret_model <- caret::train(Sales ~ ., data = carseats_tr, method = \"svmLinear2\", trControl = trainControl(method = \"none\"))\n\n# Predict on a test instance\ntest_instance <- carseats_te[1:4, -1]\n\n# Create a lime explainer object for the SVM model\nlime_svm_explainer <- lime::lime(carseats_tr[, -1], \n                                 svm_caret_model)\n\n# Explain a prediction using lime\nlime_svm_explanation <- explain(test_instance, lime_svm_explainer, n_features = 10)\nplot_features(lime_svm_explanation)\n\n\nCreate custom predict_model and model_type methods for the SVM model.\n\n\n# Custom predict_model function for SVM\npredict_model.svm <- function(x, newdata, type, ...) {\n  if (type == \"raw\") {\n    res <- predict(x, newdata = newdata, ...)\n    return(data.frame(Response = res, stringsAsFactors = FALSE))\n  } else if (type == \"prob\") {\n    res <- predict(x, newdata = newdata, ...)\n    prob <- kernlab::kernel(x, newdata, j = -1)\n    return(as.data.frame(prob, check.names = FALSE))\n  }\n}\n\n# Custom model_type function for SVM\nmodel_type.svm <- function(x, ...) {\n  if (x$type == \"C-classification\") {\n    return(\"classification\")\n  } else {\n    return(\"regression\")\n  }\n}\n\n# Create a LIME explainer for the SVM model\nlime_svm_explainer <- lime(x_train, carseats_svm)\n\n# Choose a specific instance from the test set to explain\ntest_instance <- carseats_te[1:4,-1]\n\n# Generate explanations for the chosen instance\nlime_svm_explanation <- explain(test_instance, lime_svm_explainer, n_features = 10)\n\n# Visualize the explanation\nplot_features(lime_svm_explanation)\n\nYou can see the prediction plot for 4 test observations. We can see several bar charts. On the y-axis, you see the features (and their intervals), while the x-axis shows the relative strength of each feature at a given value or interval. The positive value (blue color) shows that the feature support or increases the value of the prediction, while the negative value (red color) has a negative effect or decreases the prediction value. Please note that the interpretation for each observation can be different (this explanation has been taken from this blog, which you can visit for further details).\nWe give the interpretation of the first test observation as an example. The first subplot shows that a price of less than 100 results in purchasing a higher quantity than expected. Additionally, people between the ages of 40 and 55 were most likely to buy the seat, which are people who are not too young nor too old. However, in a typical scenario, we would generally expect younger people to buy car seats, but that‚Äôs probably because of the high ages in our dataset (1st. quantile of age is around 40). If the price by the competitor (CompPrice) is also low, it‚Äôll impact the sales units badly. Once again, please note that this is specific to the first observation (i.e., the first subplot).\nThe next element is Explanation Fit. These values indicate how well LIME explains the model, similar to an R-Squared in linear regression. Here we see the explanation Fit only has values around 0.50-0.7 (50%-70%), which can be interpreted that LIME can only explain a little about our model (in some cases, like the 3rd sub-plot, this value is extremely low). You may choose not to trust the LIME output since it only has a low Explanation Fit.\n\n\nWe‚Äôll be using provide a small demonstration on how this can be achieved in python. Please note the same logic for the interpretation (and explanation) of the R version applies here, therefore, the code is shorter and there‚Äôs no further comment provided for its output.\nWe use lime package in python (already installed in the lab setup). Then we can run our model in the same way as R:\n\nfrom sklearn import svm\nfrom sklearn.pipeline import make_pipeline\nfrom lime import lime_tabular\nimport matplotlib.pyplot as plt\n\n# Assuming carseats_tr and carseats_te are already defined as pandas DataFrames\nX_train = carseats_train.drop(columns='Sales')\ny_train = carseats_train['Sales']\nX_test = carseats_test.drop(columns='Sales')\n\n# Create a support vector machine model\nsvm_caret_model = svm.LinearSVR(random_state=2022)\n\n# Train the model\nsvm_caret_model.fit(X_train, y_train)\n\n\n\n\nLinearSVR(random_state=2022)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nLinearSVR\n\n?Documentation for LinearSVRiFitted\nLinearSVR(random_state=2022) \n\n\n# Predict on a test instance\ntest_instance = X_test.iloc[0:4]\n\n# Create a lime explainer object for the SVM model\nlime_svm_explainer = lime_tabular.LimeTabularExplainer(X_train.values,\n                                                       feature_names=X_train.columns,\n                                                       class_names=['Sales'],\n                                                       mode='regression')\n\n# Explain a prediction using lime\nlime_svm_explanation = lime_svm_explainer.explain_instance(test_instance.values[0], svm_caret_model.predict, num_features=10)\n\n# Plot the features\n# plt.clf()\nlime_svm_explanation.as_pyplot_figure()\nplt.show()"
  },
  {
    "objectID": "labs/07_InterpretableML/Ex_ML_VarImp.html#bonus-xgboost",
    "href": "labs/07_InterpretableML/Ex_ML_VarImp.html#bonus-xgboost",
    "title": "Variable Importance",
    "section": "Bonus: XGBoost",
    "text": "Bonus: XGBoost\nTo give you an example for a classification problem, we can also train an XGBoost using the xgboost library:\n\nlibrary(xgboost)\n\n# Load and prepare the data\ncarseats_df <- Carseats\ncarseats_df$High <- ifelse(carseats_df$Sales <= 8, \"yes\", \"no\")\ncarseats_df$High <- as.factor(carseats_df$High)\ncarseats_df$ShelveLoc <- as.factor(carseats_df$ShelveLoc)\ncarseats_df$Urban <- as.factor(carseats_df$Urban)\ncarseats_df$US <- as.factor(carseats_df$US)\n \n# Prepare the data for xgboost (as shown in the boosting excercises)\nxgb_data <- model.matrix(High ~ ., data = carseats_df)[,-1]\nxgb_label <- as.numeric(carseats_df$High) - 1\nxgb_dmatrix <- xgb.DMatrix(data = xgb_data, label = xgb_label)\n\n# Train a gradient boosting model\nset.seed(42)\ncarseats_xgb <- xgboost(data = xgb_dmatrix, nrounds = 100, objective = \"binary:logistic\", eval_metric = \"logloss\")\n\nIdentify instances with predicted probabilities close to 1, 0, and 0.5:\n\n# LIME explanations for a gradient boosting model\nxgb_preds <- predict(carseats_xgb, xgb_dmatrix)\n\nwhich.max(xgb_preds)\nwhich.min(xgb_preds)\nwhich.min(abs(xgb_preds - 0.5))\n\nGenerate LIME explanations for the selected instances:\n\n# before making the prediction, we need to also one-hot encode the categorical variables\nto_explain <- data.frame(model.matrix(~.,data = carseats_df[c(120, 4, 60), -ncol(carseats_df)])[,-1])\n\n# we can finally run LIME on our results\ncarseats_lime_xgb <- lime(data.frame(xgb_data), carseats_xgb, bin_continuous = TRUE, quantile_bins = FALSE)\ncarseats_expl_xgb <- lime::explain(to_explain, carseats_lime_xgb, n_labels = 1, n_features = 10)\n\nVisualize the LIME explanations\n\nplot_features(carseats_expl_xgb, ncol = 2)\n\nWhat can you observe from these subplots?"
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html",
    "href": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "In this exercise, we‚Äôll use the same wine data introduced in the ensemble exercises. As noted before, all the numerical features have units. Additionally, the original objective of this dataset was to predict the wine quality from the other features (supervised learning). The data will only be used for unsupervised task here.\nFirst, load the data and scale the numerical features:\n\n\nR\nPython\n\n\n\n\nwine <- read.csv(here::here(\"labs/data/Wine.csv\"))\nrow.names(wine) <- paste(\"W\", c(1:nrow(wine)), sep=\"\") # row names are used after\nhead(wine)\nsummary(wine)\nwine[,-12] <- scale(wine[,-12]) ## scale all the features except \"quality\"\n\n\n\n\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Start fresh with all necessary imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\nimport os\n\n# Find the correct path to the data file\n# Try different possible locations\npossible_paths = [\n    'labs/data/Wine.csv',  # From project root\n    '../../data/Wine.csv',  # Relative to current file\n    '../../../data/Wine.csv',  # One level up\n    'data/Wine.csv',  # Direct in data folder\n    'Wine.csv'  # In current directory\n]\n\nwine_data = None\nfor path in possible_paths:\n    try:\n        wine_data = pd.read_csv(path)\n        print(f\"Successfully loaded data from: {path}\")\n        break\n    except FileNotFoundError:\n        continue\nif wine_data is None:\n    # If we can't find the file, create a small sample dataset for demonstration\n    print(\"Could not find Wine.csv, creating sample data for demonstration\")\n    np.random.seed(42)\n    wine_data = pd.DataFrame(\n        np.random.randn(100, 11),\n        columns=[f'feature_{i}' for i in range(11)]\n    )\n    wine_data['quality'] = np.random.randint(3, 9, size=100)\n\n# Continue with the analysis using wine_data\nwine = wine_data\nwine.index = [\"W\" + str(i) for i in range(1, len(wine)+1)]\nscaler = StandardScaler()\nwine_scaled = pd.DataFrame(\n    scaler.fit_transform(wine.iloc[:, :-1]),\n    index=wine.index,\n    columns=wine.columns[:-1]\n)\nX = wine_scaled.values\n\n# Clear previous plot (if any)\nplt.clf()\n\n# Within sum-of-squares\nwss = []\nfor k in range(1, 26):\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n    wss.append(sum(((X - centroids[labels]) ** 2).sum(axis=1)))\n\n\n\n\nAgglomerativeClustering(linkage='complete', metric='manhattan', n_clusters=25)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nAgglomerativeClustering\n\n?Documentation for AgglomerativeClusteringiFitted\nAgglomerativeClustering(linkage='complete', metric='manhattan', n_clusters=25) \n\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 26), wss, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Within sum-of-squares')\nplt.title('Elbow Method for Optimal k')\nplt.show()\n# Silhouette\nsilhouette_scores = []\nfor k in range(2, 26):\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    silhouette_scores.append(silhouette_score(X, labels))\n\n\n\n\nAgglomerativeClustering(linkage='complete', metric='manhattan', n_clusters=25)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nAgglomerativeClustering\n\n?Documentation for AgglomerativeClusteringiFitted\nAgglomerativeClustering(linkage='complete', metric='manhattan', n_clusters=25) \n\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(2, 26), silhouette_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette score')\nplt.title('Silhouette Method for Optimal k')\nplt.show()\n# Alternative metrics\nch_scores = []\ndb_scores = []\nk_range = range(2, 26)\n\nfor k in k_range:\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    ch_scores.append(calinski_harabasz_score(X, labels))\n    db_scores.append(davies_bouldin_score(X, labels))\n\n\n\n\nAgglomerativeClustering(linkage='complete', metric='manhattan', n_clusters=25)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nAgglomerativeClustering\n\n?Documentation for AgglomerativeClusteringiFitted\nAgglomerativeClustering(linkage='complete', metric='manhattan', n_clusters=25) \n\n\n# Plot Calinski-Harabasz Index\nplt.figure(figsize=(10, 6))\nplt.plot(list(k_range), ch_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Calinski-Harabasz Index (higher is better)')\nplt.title('Calinski-Harabasz Index for determining optimal k')\nplt.show()\n# Plot Davies-Bouldin Index\nplt.figure(figsize=(10, 6))\nplt.plot(list(k_range), db_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Davies-Bouldin Index (lower is better)')\nplt.title('Davies-Bouldin Index for determining optimal k')\nplt.show()\n\n\n\n\n\n\n\nWarning\n\n\n\nPlease note that all the interpretations will be based on the R outputs (like PCA and most other exercises). As usual, the python outputs may be slightly different due to differences in the implementations of the algorithms."
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#distances",
    "href": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#distances",
    "title": "Clustering",
    "section": "Distances",
    "text": "Distances\nWe apply here an agglomerative hierarchical clustering (AGNES). Only the numerical features are used here. First, we compute the distances and plot them. We use Manhattan distance below.\n\n\nR\nPython\n\n\n\n\nlibrary(reshape2) # contains the melt function\nlibrary(ggplot2)\nwine_d <- dist(wine[,-12], method = \"manhattan\") # matrix of Manhattan distances \n\nwine_melt <- melt(as.matrix(wine_d)) # create a data frame of the distances in long format\nhead(wine_melt)\n\nggplot(data = wine_melt, aes(x=Var1, y=Var2, fill=value)) + \n  geom_tile() \n\n\n\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# wine_d = pd.DataFrame(np.abs(wine.iloc[:, :-1].values[:, None] - wine.iloc[:, :-1].values), columns=wine.index, index=wine.index)\n# from scipy.spatial.distance import pdist, squareform\n\n# wine_d = pdist(wine.iloc[:, :-1], metric='cityblock')\n# wine_d = squareform(wine_d)\n# wine_d_df = pd.DataFrame(wine_d, index=wine.index, columns=wine.index)\nfrom scipy.spatial.distance import cdist\n\nwine_d = pd.DataFrame(cdist(wine.iloc[:, :-1], wine.iloc[:, :-1], metric='cityblock'), columns=wine.index, index=wine.index)\n\nwine_melt = wine_d.reset_index().melt(id_vars='index', var_name='Var1', value_name='value')\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(wine_d, cmap=\"coolwarm\", center=0)\nplt.show()\n\n\n\n\nWe can see that some wines are closer than others (darker color). However, it is not really possible to extract any information from such a graph."
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#dendrogram",
    "href": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#dendrogram",
    "title": "Clustering",
    "section": "Dendrogram",
    "text": "Dendrogram\nNow, we build a dendrogram using a complete linkage.\n\n\nR\nPython\n\n\n\n\nwine_hc <- hclust(wine_d, method = \"complete\")\nplot(wine_hc, hang=-1)\n\n\n\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nwine_linkage = linkage(wine.iloc[:, :-1], method='complete', metric='cityblock')\nplt.figure(figsize=(10, 8))\ndendrogram(wine_linkage, labels=wine.index, orientation='top', color_threshold=0, leaf_font_size=10)\nplt.show()\n\n\n\n\nWe cut the tree to 4 clusters, and represent the result. We also extract the cluster assignment of each wine.\n\n\nR\nPython\n\n\n\n\nplot(wine_hc, hang=-1)\nrect.hclust(wine_hc, k=4)\nwine_clust <- cutree(wine_hc, k=4)\nwine_clust\n\n\n\n\nfrom scipy.cluster.hierarchy import fcluster\n\nplt.figure(figsize=(10, 8))\ndendrogram(wine_linkage, labels=wine.index, orientation='top', color_threshold=80, leaf_font_size=10)\nplt.axhline(y=80, color='black', linestyle='--')\nplt.show()\nwine_clust = fcluster(wine_linkage, 4, criterion='maxclust')\nwine_clust"
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#interpretation-of-the-clusters",
    "href": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#interpretation-of-the-clusters",
    "title": "Clustering",
    "section": "Interpretation of the clusters",
    "text": "Interpretation of the clusters\nNow we analyze the clusters by looking at the distribution of the features within each cluster.\n\n\nR\nPython\n\n\n\n\nwine_comp <- data.frame(wine[,-12], Clust=factor(wine_clust), Id=row.names(wine))\nwine_df <- melt(wine_comp, id=c(\"Id\", \"Clust\"))\nhead(wine_df)\n\nggplot(wine_df, aes(y=value, group=Clust, fill=Clust)) +\n  geom_boxplot() +\n  facet_wrap(~variable, ncol=4, nrow=3)\n\n\n\n\nwine_comp = wine.iloc[:, :-1].copy()\nwine_comp['Clust'] = wine_clust\nwine_comp['Id'] = wine.index\nwine_melt = wine_comp.melt(id_vars=['Id', 'Clust'])\n\nplt.figure(figsize=(14, 10))\nsns.boxplot(x='variable', y='value', hue='Clust', data=wine_melt)\nplt.xticks(rotation=90)\nplt.legend(title='Cluster')\nplt.show()\n\n\n\n\nWe can see for example that - Cluster 4 (the smallest): small pH, large fixed.acidity and citric.acid, a large density, and a small alcohol. Also a large free.sulfur.dioxide. - Cluster 2: also has large fixed.acidity but not citric.acid. It looks like less acid than cluster 3. - Cluster 3: apparently has a large alcohol. - Etc."
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#choice-of-the-number-of-clusters",
    "href": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#choice-of-the-number-of-clusters",
    "title": "Clustering",
    "section": "Choice of the number of clusters",
    "text": "Choice of the number of clusters\nTo choose the number of cluster, we can inspect the dendrogram (judgmental approach), or we can rely on a statistics. Below, we use the within sum-of-squares, the GAP statistics, and the silhouette. It is obtained by the function fviz_nbclust in package factoextra. It uses the dendrogram with complete linkage on Manhattan distance is obtained using the function hcut with hc_method=\"complete\" and hc_metric=\"manhattan\".\n\n\nR\nPython\n\n\n\n\nlibrary(factoextra)\nfviz_nbclust(wine[,-12],\n             hcut, hc_method=\"complete\",\n             hc_metric=\"manhattan\",\n             method = \"wss\", \n             k.max = 25, verbose = FALSE)\nfviz_nbclust(wine[,-12],\n             hcut, hc_method=\"complete\",\n             hc_metric=\"manhattan\",\n             method = \"silhouette\", \n             k.max = 25, verbose = FALSE)\nfviz_nbclust(wine[,-12],\n             hcut, hc_method=\"complete\",\n             hc_metric=\"manhattan\",\n             method = \"gap\", \n             k.max = 25, verbose = FALSE)\n\n\n\nThe gap statistic is readily available in R (as seen above), but for Python we‚Äôll use more commonly available metrics in scikit-learn: - Davies-Bouldin Index (lower values indicate better clustering) - Calinski-Harabasz Index (higher values indicate better clustering)\nThese metrics serve the same purpose - helping us determine the optimal number of clusters\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\nimport os\n\n# Find the correct path to the data file\n# Try different possible locations\npossible_paths = [\n    'labs/data/Wine.csv',  # From project root\n    '../../data/Wine.csv',  # Relative to current file\n    '../../../data/Wine.csv',  # One level up\n    'data/Wine.csv',  # Direct in data folder\n    'Wine.csv'  # In current directory\n]\n\nwine_data = None\nfor path in possible_paths:\n    try:\n        wine_data = pd.read_csv(path)\n        print(f\"Successfully loaded data from: {path}\")\n        break\n    except FileNotFoundError:\n        continue\n# Continue with the analysis using wine_data\nwine = wine_data\nwine.index = [\"W\" + str(i) for i in range(1, len(wine)+1)]\nscaler = StandardScaler()\nwine_scaled = pd.DataFrame(\n    scaler.fit_transform(wine.iloc[:, :-1]),\n    index=wine.index,\n    columns=wine.columns[:-1]\n)\nX = wine_scaled.values\n\n# Clear previous plot (if any)\nplt.clf()\n\n# Within sum-of-squares (Elbow method)\nwss = []\nfor k in range(1, 26):\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n    wss.append(sum(((X - centroids[labels]) ** 2).sum(axis=1)))\n\n\n\n\nAgglomerativeClustering(linkage='complete', metric='manhattan', n_clusters=25)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nAgglomerativeClustering\n\n?Documentation for AgglomerativeClusteringiFitted\nAgglomerativeClustering(linkage='complete', metric='manhattan', n_clusters=25) \n\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 26), wss, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Within sum-of-squares')\nplt.title('Elbow Method for Optimal k')\nplt.show()\n# Silhouette method\nsilhouette_scores = []\nfor k in range(2, 26):\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    silhouette_scores.append(silhouette_score(X, labels))\n\n\n\n\nAgglomerativeClustering(linkage='complete', metric='manhattan', n_clusters=25)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nAgglomerativeClustering\n\n?Documentation for AgglomerativeClusteringiFitted\nAgglomerativeClustering(linkage='complete', metric='manhattan', n_clusters=25) \n\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(2, 26), silhouette_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette score')\nplt.title('Silhouette Method for Optimal k')\nplt.show()\n# Alternative metrics to gap statistic\nch_scores = []\ndb_scores = []\nk_range = range(2, 26)\n\nfor k in k_range:\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    ch_scores.append(calinski_harabasz_score(X, labels))\n    db_scores.append(davies_bouldin_score(X, labels))\n\n\n\n\nAgglomerativeClustering(linkage='complete', metric='manhattan', n_clusters=25)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\nAgglomerativeClustering\n\n?Documentation for AgglomerativeClusteringiFitted\nAgglomerativeClustering(linkage='complete', metric='manhattan', n_clusters=25) \n\n\n# Plot Calinski-Harabasz Index\nplt.figure(figsize=(10, 6))\nplt.plot(list(k_range), ch_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Calinski-Harabasz Index (higher is better)')\nplt.title('Calinski-Harabasz Index for determining optimal k')\nplt.show()\n# Plot Davies-Bouldin Index\nplt.figure(figsize=(10, 6))\nplt.plot(list(k_range), db_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Davies-Bouldin Index (lower is better)')\nplt.title('Davies-Bouldin Index for determining optimal k')\nplt.show()\n\n\n\n\nLike often, these methods are not easy to interpret. Globally, they choose \\(k=2\\) or \\(k=3\\)."
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html",
    "href": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "In this series of exercises, we illustrate PCA on the wine data already used for clustering. We first load the data.\n\n\nR\nPython\n\n\n\n\nwine <- read.csv(here::here(\"labs/data/Wine.csv\"))\nrow.names(wine) <- paste(\"W\", c(1:nrow(wine)), sep=\"\") \nwine[,-12] <- scale(wine[,-12])\n\n\n\n\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nwine = pd.read_csv('../../data/Wine.csv')\nwine.index = [\"W\" + str(i) for i in range(1, len(wine)+1)]\nscaler = StandardScaler()\nwine.iloc[:, :-1] = scaler.fit_transform(wine.iloc[:, :-1])\n\n\n\n\nNote that here the scaling of the variables is optional. The PCA can be applied on the correlation matrix which is equivalent to use scaled features. We could alternatively use unscaled data. The results would of course be different and dependent on the scales themselves. That choice depends on the practical application."
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#circle-of-correlations-and-interpretation",
    "href": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#circle-of-correlations-and-interpretation",
    "title": "Principal Component Analysis",
    "section": "Circle of correlations and interpretation",
    "text": "Circle of correlations and interpretation\nTo produces a circle of correlations:\n\n\nR\nPython\n\n\n\n\nfviz_pca_var(wine_pca)\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfeatures = wine.columns[:-1]\nloading_matrix = pd.DataFrame(pca.components_.T, columns=[f\"PC{i+1}\" for i in range(11)], index=features)\nloading_matrix = loading_matrix.iloc[:, :2]\n\n# Normalize the PCA loadings\nloading_matrix_normalized = loading_matrix / np.sqrt(np.sum(loading_matrix**2, axis=0))\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.scatter(loading_matrix_normalized[\"PC1\"], loading_matrix_normalized[\"PC2\"], color=\"r\", marker=\"o\", s=100)\n\nfor i, feature in enumerate(loading_matrix_normalized.index):\n    ax.arrow(0, 0, loading_matrix_normalized.loc[feature, \"PC1\"], loading_matrix_normalized.loc[feature, \"PC2\"], color=\"r\", alpha=0.8)\n    ax.text(loading_matrix_normalized.loc[feature, \"PC1\"], loading_matrix_normalized.loc[feature, \"PC2\"], feature, color=\"black\", fontsize=12)\n\nax.axhline(0, color=\"black\", linewidth=1)\nax.axvline(0, color=\"black\", linewidth=1)\nax.set_xlim(-1.1, 1.1)\nax.set_ylim(-1.1, 1.1)\ncircle = plt.Circle((0, 0), 1, color='gray', fill=False, linestyle=\"--\", linewidth=1)\nax.add_artist(circle)\n\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.set_title(\"Circle of Correlations\")\nax.grid()\nplt.show()\n\n\n\n\n\n\n\nNote\n\n\n\nPlease do note for this plot:\n\nValues on the x-axis have been normalized to be between -1 and 1. For the rest of the python plots, we‚Äôll not apply this.\nTo get the same results as R, the coordinates of PC2 should be flipped. The results appear mirrored because PCA is sensitive to the orientation of the data. The PCA algorithm calculates eigenvectors as the principal components, and eigenvectors can have either positive or negative signs. This means that the orientation of the principal components can vary depending on the implementation of PCA in different libraries. You can get the same results as R with the code below:\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Normalize the PCA loadings\nloading_matrix_normalized = loading_matrix / np.sqrt(np.sum(loading_matrix**2, axis=0))\n\n# Add this if you want the same results\nloading_matrix_normalized[\"PC2\"] = -loading_matrix_normalized[\"PC2\"]\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.scatter(loading_matrix_normalized[\"PC1\"], loading_matrix_normalized[\"PC2\"], color=\"r\", marker=\"o\", s=100)\n\nfor i, feature in enumerate(loading_matrix_normalized.index):\n    ax.arrow(0, 0, loading_matrix_normalized.loc[feature, \"PC1\"], loading_matrix_normalized.loc[feature, \"PC2\"], color=\"r\", alpha=0.8)\n    ax.text(loading_matrix_normalized.loc[feature, \"PC1\"], loading_matrix_normalized.loc[feature, \"PC2\"], feature, color=\"black\", fontsize=12)\n\nax.axhline(0, color=\"black\", linewidth=1)\nax.axvline(0, color=\"black\", linewidth=1)\nax.set_xlim(-1.1, 1.1)\nax.set_ylim(-1.1, 1.1)\ncircle = plt.Circle((0, 0), 1, color='gray', fill=False, linestyle=\"--\", linewidth=1)\nax.add_artist(circle)\n\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.set_title(\"Circle of Correlations\")\nax.grid()\nplt.show()\n\n\n\n\n\n\nThis is for the two first principal components. We see\n\nPC1 explains \\(31.4\\%\\) of the variance of the data, PC2 explains \\(15.3\\%\\). In total, \\(46.7\\%\\) of the variance of the data is explained by these two components.\nPC1 is positively correlated with density, negatively with alcohol. Which confirms that these two features are negatively correlated. It is also positively correlated with residual.sugar.\nPC2 is positively correlated with pH, negatively with fixed.acidity and, a little less, with citric.acid.\nFeatures with shorts arrows are not explained here: volatile.acidity, sulphates, etc.\n\nTo even better interpret the dimensions, we can extract the contributions of each features in the dimension. Below, for PC1.\n\n\nR\nPython\n\n\n\n\nfviz_contrib(wine_pca, choice = \"var\", axes = 1)\n\n\n\n\nexplained_variance = pca.explained_variance_ratio_\nsquare_loading_matrix = loading_matrix**2\ncontributions = square_loading_matrix * 100 / explained_variance[:2]\n\ncontributions = contributions[\"PC1\"].sort_values(ascending=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=contributions.values, y=contributions.index, palette=\"viridis\")\nplt.xlabel(\"Contribution (%)\")\nplt.ylabel(\"Features\")\nplt.title(\"Feature Contributions for PC1\")\nplt.show()\n\n\n\n\nWe recover our conclusions from the circle (for PC1)."
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#map-of-the-individual-and-biplot",
    "href": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#map-of-the-individual-and-biplot",
    "title": "Principal Component Analysis",
    "section": "Map of the individual and biplot",
    "text": "Map of the individual and biplot\nWe can represent the wines in the (PC1,PC2) map. To better interpret the map, we add on it the correlation circle: a biplot.\n\n\nR\nPython\n\n\n\n\n## fviz_pca_ind(wine_pca) ## only the individuals\nfviz_pca_biplot(wine_pca) ## biplot\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the scores for the first two principal components\nscores = wine_pca.iloc[:, :2]\n\n# Define the loadings for the first two principal components\nloadings = pca.components_.T[:, :2]\n\n# Scale the loadings by the square root of the variance\nloadings_scaled = loadings * np.sqrt(pca.explained_variance_[:2])\n\n# Calculate the scaling factor for the arrows\narrow_max = 0.9 * np.max(np.max(np.abs(scores)))\nscale_factor = arrow_max / np.max(np.abs(loadings_scaled))\n\n# Create a scatter plot of the scores\nplt.figure(figsize=(10, 8))\nplt.scatter(scores.iloc[:, 0], scores.iloc[:, 1], s=50, alpha=0.8)\n\n# Add arrows for each variable's loadings\nfor i, variable in enumerate(wine.columns[:-1]):\n    plt.arrow(0, 0, loadings_scaled[i, 0]*scale_factor, loadings_scaled[i, 1]*scale_factor, color='r', alpha=0.8)\n    plt.text(loadings_scaled[i, 0]*scale_factor, loadings_scaled[i, 1]*scale_factor, variable, color='black', fontsize=12)\n\n# Add axis labels and title\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Biplot of Wine Dataset')\n\n# Add grid lines\nplt.grid()\n\n# Show the plot\nplt.show()\n\n\n\n\nIt‚Äôs a bit difficult to see all the patterns, but for instance\n\nWine 168 has a large fixed.acidity and a low pH.\nWine 195 has a large alcohol and a low density.\netc.\n\nWhen we say ‚Äúlarge‚Äù or ‚Äúlow‚Äù, it is not in absolute value but relative to the data set, i.e., ‚Äúlarger than the average‚Äù; the average being at the center of the graph (PC1=0, PC2=0)."
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#how-many-dimensions",
    "href": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#how-many-dimensions",
    "title": "Principal Component Analysis",
    "section": "How many dimensions",
    "text": "How many dimensions\nFor graphical representation one a single graph, we need to keep only two PCs. But if we use it to reduce the dimension of our data set, or if we want to represent the data on several graphs, then we need to know how many components are needed to reach a certain level of variance. This can be achieved by looking at the eigenvalues (screeplot).\n\n\nR\nPython\n\n\n\n\nfviz_eig(wine_pca, addlabels = TRUE, ncp=11)\n\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 12), explained_variance * 100, 'o-')\nplt.xticks(range(1, 12), [f\"PC{i}\" for i in range(1, 12)])\nplt.xlabel(\"Principal Components\")\nplt.ylabel(\"Explained Variance (%)\")\nplt.title(\"Scree Plot\")\nplt.grid()\nplt.show()\n\n\n\n\nIf we want to achieve \\(75\\%\\) of representation of the data (i.e., of the variance of the data), we need 5 dimensions. This means that the three biplots below represent \\(>75\\%\\) of the data (in fact \\(84.6\\%\\)).\n\nlibrary(gridExtra)\np1 <- fviz_pca_biplot(wine_pca, axes = 1:2) \np2 <- fviz_pca_biplot(wine_pca, axes = 3:4) \np3 <- fviz_pca_biplot(wine_pca, axes = 5:6) \ngrid.arrange(p1, p2, p3, nrow = 2, ncol=2)"
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html",
    "href": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html",
    "title": "Autoencoders",
    "section": "",
    "text": "In this series of exercises, we illustrate autoencoders on the wine data already used for clustering & PCA. We first load the data.\n\n\nR\nPython\n\n\n\n\nwine <- read.csv(here::here(\"labs/data/Wine.csv\"))\nrow.names(wine) <- paste(\"W\", c(1:nrow(wine)), sep=\"\") \nwine[,-12] <- scale(wine[,-12])\n\n\n\n\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nwine = pd.read_csv('../../data/Wine.csv')\nwine.index = [\"W\" + str(i) for i in range(1, len(wine)+1)]\nscaler = StandardScaler()\nwine.iloc[:, :-1] = scaler.fit_transform(wine.iloc[:, :-1])\n\n\n\n\nNote that here the scaling of the variables is optional. Scaling the features can sometimes help with the autoencoder, especially when creating lower-dimensional representation of the data."
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#simulate-missing-values",
    "href": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#simulate-missing-values",
    "title": "Autoencoders",
    "section": "Simulate missing values",
    "text": "Simulate missing values\nThis step is done only for demonstration since the wine data does not contain missing values, and we must artificially create them and place them in a dataframe called wine_missing. Applying this technique doesn‚Äôt require this step since your dataset already should already contain the missing values.\n\n\nR\nPython\n\n\n\n\nset.seed(123)\nwine_missing <- wine\nwine_missing[sample(1:nrow(wine_missing), size = nrow(wine_missing)*0.2), sample(1:ncol(wine_missing), size = ncol(wine_missing)*0.2)] <- NA\n\n\n\n\nimport numpy as np\nimport random\nnp.random.seed(123)\n# Create a copy of the wine dataset and randomly remove 20% of the data\nwine_missing = wine.copy()\nix = [(row, col) for row in range(wine_missing.shape[0]) for col in range(wine_missing.shape[1])]\nfor row, col in random.sample(ix, int(round(.2*len(ix)))):\n    wine_missing.iat[row, col] = np.nan"
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#train-with-complete-data",
    "href": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#train-with-complete-data",
    "title": "Autoencoders",
    "section": "Train with complete data",
    "text": "Train with complete data\nNow, we can use an autoencoder to infer these missing values. To do so, we can train the autoencoder with the complete data and then use it to infer the missing values.\n\n\nR\nPython\n\n\n\n\n# Re-define the autoencoder model\ninput_layer <- layer_input(shape = input_dim)\nencoder_layer <-\n  layer_dense(units = encoding_dim, activation = 'relu')(input_layer)\ndecoder_layer <-\n  layer_dense(units = input_dim, activation = 'linear')(encoder_layer)\nautoencoder <- keras_model(input_layer, decoder_layer)\nsummary(autoencoder)\nautoencoder %>% compile(optimizer = \"adam\", loss = \"mse\")\n\n# Train the autoencoder with the complete data\nhist <- \n  autoencoder %>% fit(\n  x = as.matrix(wine[, -12]),\n  y = as.matrix(wine[, -12]),\n  epochs = 300,\n  batch_size = 32,\n  shuffle = TRUE,\n  verbose = 0, #set it as `1` if you want to see the training messages\n  validation_split = 0.2,\n  callbacks = callback_early_stopping(\n    monitor = \"val_loss\",\n    patience = 5,\n    restore_best_weights = TRUE\n  ),\n)\n\n## uncomment if you want to see the training plot\n# plot(hist)\n\n# Replace missing values with the inferred values\nwine_missing_original <- wine_missing\n\n# replace the missing values with something before being able to make a prediction on it (model cannot re-construct NA directly)\nwine_missing[is.na(wine_missing)] <- 0  # Mask the missing values as 0\n\n# Use the autoencoder to infer the missing values\npredicted_wine <- autoencoder %>% predict(as.matrix(wine_missing[, -12]))\n\n\n\n\n# to introduce early stopping\nfrom keras.callbacks import EarlyStopping\n\n# Re-define the autoencoder model\ninput_layer = Input(shape=(input_dim,))\nencoder_layer = Dense(encoding_dim, activation='relu')(input_layer)\ndecoder_layer = Dense(input_dim, activation='linear')(encoder_layer)\nautoencoder = Model(input_layer, decoder_layer)\nautoencoder.compile(optimizer='adam', loss='mse')\n\n# Train the autoencoder with the complete data\nhist = autoencoder.fit(wine.iloc[:, :-1], \n  wine.iloc[:, :-1], \n  epochs=300, \n  batch_size=32, \n  shuffle=True, \n  verbose=0, #set it as `1` if you want to see the training messages\n  validation_split=0.2, \n  callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)])\n\n# # uncomment if you want to see the training plot\n# plt.clf()\n# plt.plot(hist.history['loss'])\n# plt.plot(hist.history['val_loss'])\n# plt.title('Model loss')\n# plt.ylabel('Loss')\n# plt.xlabel('Epoch')\n# plt.legend(['Train', 'Val'], loc='upper right')\n# plt.show()\n\n# Replace missing values with the inferred values\nwine_missing_original = wine_missing.copy()\n\n# replace the missing values with something before being able to make a prediction on it (model cannot re-construct NA directly)\nwine_missing.fillna(0, inplace=True)  # Mask the missing values as 0\n\n# Use the autoencoder to infer the missing values\npredicted_wine = autoencoder.predict(wine_missing.iloc[:, :-1].values)"
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#predict-missing-values",
    "href": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#predict-missing-values",
    "title": "Autoencoders",
    "section": "Predict missing values",
    "text": "Predict missing values\nFinally, we replace the missing values in the wine dataset with the inferred values from the autoencoder.\n\n\nR\nPython\n\n\n\n\nwine_missing[is.na(wine_missing_original)] <- predicted_wine[is.na(wine_missing_original)]\n\nlibrary(dplyr)\n# see the old vs new values by filtering for the rows that contained an NA\nmissing_rows <- wine_missing_original %>% \n  mutate(row_index = row_number()) %>%\n  dplyr::filter_at(vars(dplyr::everything()),any_vars(is.na(.)))\n\n# you can see the new values that were re-constructed\nwine_missing_original[missing_rows$row_index,]\nwine_missing[missing_rows$row_index,]\n\n\n\n\n# Identify the missing values in the original dataframe\nmissing_mask = wine_missing_original.iloc[:, :-1].isna()\n\n# Replace the missing values with the predicted ones\nfor i in range(wine_missing.shape[1] - 1):  # iterate over all columns except the last one\n    wine_missing.loc[missing_mask.iloc[:, i], wine_missing.columns[i]] = predicted_wine[missing_mask.iloc[:, i], i]\n\n# see the old vs new values by filtering for the rows that contained an NA\nmissing_rows = wine_missing_original[wine_missing_original.iloc[:, :-1].isna().any(axis=1)]\n\n# you can see the new values that were re-constructed\nprint(wine_missing_original.loc[missing_rows.index, :])\nprint(wine_missing.loc[missing_rows.index, :])\n\n\n\n\nNote how the dataset wine_missing doesn‚Äôt contain any missing values. The missing values have been inferred by the autoencoder. This method can be a powerful tool to deal with missing data in machine learning projects.\n\n\n\n\n\n\nImportant\n\n\n\nThere are two essential things to note about using autoencoders for imputation:\n\nAlternative to our approach, you can train with incomplete data and mask the ‚ÄúNA‚Äù in the training data with 0s (as shown for inference). In theory, you can put anything for the mask value, for example, the mean or median value for the missing inputs, and then try to recover those (although if you scale your data, 0 may work better in practice). Once the model is trained, you can recover the value for NA while masking the NAs again with 0s (as already implemented by us). In that case, you‚Äôre pushing your model to predict 0 for the missing instances, which can sometimes be inappropriate.\nPlease note for auto-encoders to work well, you‚Äôll need a lot of observations. Additionally, you should always compare the performance of this technique against simply using mean and median values for the missing data. Another library in R commonly used for dealing with missing data is called missForest, which uses a random forest for imputation. If you need techniques to deal with missing data, feel free to check it out and make sure you understand how it works (it falls beyond the scope of this course)."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#objectives",
    "href": "labs/slides/lab_1/slide.html#objectives",
    "title": "MLBA Lab 1",
    "section": "Objectives",
    "text": "Objectives\n\nLearn about vesion-control, Git & GitHub\nUse R and Rstudio\nLearn about virtual environments in R to ensure reproducibility.\nLearn about using python üêç in (and with) R. This is useful for some ML lab sessions, and cutting-edge ML is often first implemented in python."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#git-github-for-collaboration",
    "href": "labs/slides/lab_1/slide.html#git-github-for-collaboration",
    "title": "MLBA Lab 1",
    "section": "Git & GitHub for Collaboration",
    "text": "Git & GitHub for Collaboration\n\n\n\nWhat is Git? Git is a version-control system that helps track changes, collaborate on projects, and revert to previous versions of a file.\n\nWhat is GitHub? GitHub provides a cloud-based hosting service that lets you manage Git repositories.\n\nGit vs.¬†GitHub: Git is a version-control technology to manage source code history, while GitHub is only one of the hosting service for Git repositories."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#creating-a-repository-on-github",
    "href": "labs/slides/lab_1/slide.html#creating-a-repository-on-github",
    "title": "MLBA Lab 1",
    "section": "Creating a Repository on GitHub",
    "text": "Creating a Repository on GitHub\n\n\n\nNavigate to GitHub and create a new repository.\nChoose a name and description for your repository.\nSelect whether the repository is public or private.\nClick ‚ÄúCreate repository.‚Äù\n\n\n\n\n\n\nInstall GitHub Desktop app to help you with using GitHub. Additionally, you can see our FAQ for obtaining professional accounts."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#github-workflow",
    "href": "labs/slides/lab_1/slide.html#github-workflow",
    "title": "MLBA Lab 1",
    "section": "GitHub Workflow",
    "text": "GitHub Workflow\n\n\nCloning a Repository with GitHub Desktop\n\nOpen GitHub Desktop and clone the repository to your local machine.\nThis creates a local copy of the repository for work and synchronization.\n\n\n\nCommitting Changes\n\nMake changes to your files in the project directory.\nUse GitHub Desktop to commit these changes, adding a meaningful commit message.\n\n\n\nPush and Pull Changes\n\nPush your committed changes to GitHub to share with collaborators.\nPull changes made by others to keep your local repository up to date."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#using-rstudio-projects",
    "href": "labs/slides/lab_1/slide.html#using-rstudio-projects",
    "title": "MLBA Lab 1",
    "section": "Using RStudio Projects",
    "text": "Using RStudio Projects\n\nRStudio projects simplify the management of R soure code.\nUse the here package for easy file path management within projects."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#virtual-environments-in-r-renv",
    "href": "labs/slides/lab_1/slide.html#virtual-environments-in-r-renv",
    "title": "MLBA Lab 1",
    "section": "Virtual environments in R (renv)",
    "text": "Virtual environments in R (renv)\nThe What & The Why\n\n\nrenv is a package management tool that helps you manage the packages used in an R project.\nEnsures that your project is reproducible.\nProvides a consistent environment by isolating the packages used in your project.\nSimplifies installation and setup.\nHelps you avoid compatibility issues.\nMakes it easy to share your work with others."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#virtual-environments-in-r-renv-1",
    "href": "labs/slides/lab_1/slide.html#virtual-environments-in-r-renv-1",
    "title": "MLBA Lab 1",
    "section": "Virtual environments in R (renv)",
    "text": "Virtual environments in R (renv)\nThe How\n\nCreate a new renv project with renv::init().\n\nrenv::restore() to install packages from the renv.lock file.\nUse renv::snapshot() to occasionally update your packages.\nUse renv::status() to see if the list in renv.lock needs updating."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#python-in-r-reticulate",
    "href": "labs/slides/lab_1/slide.html#python-in-r-reticulate",
    "title": "MLBA Lab 1",
    "section": "Python üêç in R (reticulate)",
    "text": "Python üêç in R (reticulate)\nSmall Motivation\n\nPython is arguably more demanded in machine learning than R.\nWidely-used language in the industry.\nPowerful libraries for data manipulation, analysis, and modeling.\nRelatively easy to pick up even for beginners to programming.\nCombining the strengths of both R and Python can enhance your skills."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#python-in-r-reticulate-1",
    "href": "labs/slides/lab_1/slide.html#python-in-r-reticulate-1",
    "title": "MLBA Lab 1",
    "section": "Python üêç in R (reticulate)",
    "text": "Python üêç in R (reticulate)\nConfiguration\n\nInstall reticulate package in R.\nUse reticulate::use_python() or reticulate::use_condaenv() to specify the location of your python environment.\n\n\nreticulate::use_condaenv(\"MLBA\")\nreticulate::py_config()\n\n\nUse reticulate::import() to import python modules in R.\n\n\npd <- import(\"pandas\")\n\n\nUse reticulate::py_run_string() to execute python code in R.\n\n\npy_run_string(\"x = 3; y = 4; print('The sum of x and y is:', x + y)\")\n\n#> The sum of x and y is: 7"
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#running-python-code-in-r",
    "href": "labs/slides/lab_1/slide.html#running-python-code-in-r",
    "title": "MLBA Lab 1",
    "section": "Running Python code in R",
    "text": "Running Python code in R\n\nTo run Python code in R, use {python} at the beginning of the code chunk.\n\nmy_dict = {'a' : 3, 'b' : 5, 'c' : 6} \n\nTo access R objects in Python, use r.OBJECT_NAME.\n\nmy_list <- list(a = 1, b = 2, c = 3)\nprint(r.my_list['b'])\n\nTo access Python objects in R, use py$OBJECT_NAME.\n\nprint(py$my_dict$b)"
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#object-casting-in-python-r",
    "href": "labs/slides/lab_1/slide.html#object-casting-in-python-r",
    "title": "MLBA Lab 1",
    "section": "Object casting in Python & R",
    "text": "Object casting in Python & R\n\nUse reticulate::r_to_py() and reticulate::py_to_r() to explicitly change between objects."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#example-plotting-with-r-python",
    "href": "labs/slides/lab_1/slide.html#example-plotting-with-r-python",
    "title": "MLBA Lab 1",
    "section": "Example: Plotting with R & Python",
    "text": "Example: Plotting with R & Python\n\n\n\nLoad some data\n\n## load mtcars dataset\ndata(mtcars)\n\n\n¬†\n\n\n\n\n\nPlotting with base R\n\n# Using base R plot\nplot(mtcars$mpg, mtcars$disp)\n\n\n\n\n\n\n\n\n\nPlotting by using python within R\n\n# Using `matplotlib`\nplt <- reticulate::import(\"matplotlib.pyplot\")\nplt$scatter(mtcars$mpg, mtcars$disp)\nplt$xlabel(\"mpg\", fontsize = 12)\nplt$ylabel(\"disp\", fontsize = 12)"
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#example-regression-in-r-python",
    "href": "labs/slides/lab_1/slide.html#example-regression-in-r-python",
    "title": "MLBA Lab 1",
    "section": "Example: Regression in R & Python",
    "text": "Example: Regression in R & Python\n\n\nLoading data\nR\nPython\nPython within R\n\n\n\n\n\n\nLoading the data R\n\n# load the data in R\ndata(iris)\n# exclude the `species` column (we\n# focus on regression here)\niris <- select(iris, -\"Species\")\nhead(iris)\n\n\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width\n#> 1          5.1         3.5          1.4         0.2\n#> 2          4.9         3.0          1.4         0.2\n#> 3          4.7         3.2          1.3         0.2\n#> 4          4.6         3.1          1.5         0.2\n#> 5          5.0         3.6          1.4         0.2\n#> 6          5.4         3.9          1.7         0.4\n\n\n\n\n\n\n\nModelling in pure R\n\n# example of running a model on iris\n# data\nr_lm <- lm(\"Sepal.Length ~. \", data = iris)\nsummary(r_lm)\n\n\n#> \n#> Call:\n#> lm(formula = \"Sepal.Length ~. \", data = iris)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.82816 -0.21989  0.01875  0.19709  0.84570 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   1.85600    0.25078   7.401 9.85e-12 ***\n#> Sepal.Width   0.65084    0.06665   9.765  < 2e-16 ***\n#> Petal.Length  0.70913    0.05672  12.502  < 2e-16 ***\n#> Petal.Width  -0.55648    0.12755  -4.363 2.41e-05 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.3145 on 146 degrees of freedom\n#> Multiple R-squared:  0.8586, Adjusted R-squared:  0.8557 \n#> F-statistic: 295.5 on 3 and 146 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\n\nModelling in pure Python\n\n# load the required libraries\nimport statsmodels.api as sm\nimport pandas as pd\n# Fit linear regression model to iris coming from R\nX = r.iris[['Sepal.Width','Petal.Length','Petal.Width']]\ny = r.iris['Sepal.Length']\nX = sm.add_constant(X)\npy_lm_fit = sm.OLS(y, X).fit()\n#print regression results\nprint(py_lm_fit.summary())\n\n\n#>                             OLS Regression Results                            \n#> ==============================================================================\n#> Dep. Variable:           Sepal.Length   R-squared:                       0.859\n#> Model:                            OLS   Adj. R-squared:                  0.856\n#> Method:                 Least Squares   F-statistic:                     295.5\n#> Date:                sam, 01 mar 2025   Prob (F-statistic):           8.59e-62\n#> Time:                        07:51:35   Log-Likelihood:                -37.321\n#> No. Observations:                 150   AIC:                             82.64\n#> Df Residuals:                     146   BIC:                             94.69\n#> Df Model:                           3                                         \n#> Covariance Type:            nonrobust                                         \n#> ================================================================================\n#>                    coef    std err          t      P>|t|      [0.025      0.975]\n#> --------------------------------------------------------------------------------\n#> const            1.8560      0.251      7.401      0.000       1.360       2.352\n#> Sepal.Width      0.6508      0.067      9.765      0.000       0.519       0.783\n#> Petal.Length     0.7091      0.057     12.502      0.000       0.597       0.821\n#> Petal.Width     -0.5565      0.128     -4.363      0.000      -0.809      -0.304\n#> ==============================================================================\n#> Omnibus:                        0.345   Durbin-Watson:                   2.060\n#> Prob(Omnibus):                  0.842   Jarque-Bera (JB):                0.504\n#> Skew:                           0.007   Prob(JB):                        0.777\n#> Kurtosis:                       2.716   Cond. No.                         54.7\n#> ==============================================================================\n#> \n#> Notes:\n#> [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\nModelling in R with Python libraries\n\n# load the library\nsm <- import(\"statsmodels.api\")\n# model the data\npy_lm <- sm$OLS(dplyr::pull(iris, \"Sepal.Length\"),\n    dplyr::select(iris, -\"Sepal.Length\"))\n# fit the data\npy_lm_fit <- py_lm$fit()\n# print the summary\nprint(py_lm_fit$summary())\n\n\n#> <class 'statsmodels.iolib.summary.Summary'>\n#> \"\"\"\n#>                                  OLS Regression Results                                \n#> =======================================================================================\n#> Dep. Variable:                      y   R-squared (uncentered):                   0.996\n#> Model:                            OLS   Adj. R-squared (uncentered):              0.996\n#> Method:                 Least Squares   F-statistic:                          1.284e+04\n#> Date:                sam, 01 mar 2025   Prob (F-statistic):                   1.33e-177\n#> Time:                        07:51:35   Log-Likelihood:                         -61.215\n#> No. Observations:                 150   AIC:                                      128.4\n#> Df Residuals:                     147   BIC:                                      137.5\n#> Df Model:                           3                                                  \n#> Covariance Type:            nonrobust                                                  \n#> ================================================================================\n#>                    coef    std err          t      P>|t|      [0.025      0.975]\n#> --------------------------------------------------------------------------------\n#> Sepal.Width      1.1211      0.024     47.658      0.000       1.075       1.168\n#> Petal.Length     0.9235      0.057     16.205      0.000       0.811       1.036\n#> Petal.Width     -0.8957      0.139     -6.439      0.000      -1.171      -0.621\n#> ==============================================================================\n#> Omnibus:                        0.421   Durbin-Watson:                   2.007\n#> Prob(Omnibus):                  0.810   Jarque-Bera (JB):                0.570\n#> Skew:                           0.026   Prob(JB):                        0.752\n#> Kurtosis:                       2.703   Cond. No.                         26.0\n#> ==============================================================================\n#> \n#> Notes:\n#> [1] R¬≤ is computed without centering (uncentered) since the model does not contain a constant.\n#> [2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n#> \"\"\""
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#conclusion",
    "href": "labs/slides/lab_1/slide.html#conclusion",
    "title": "MLBA Lab 1",
    "section": "Conclusion",
    "text": "Conclusion\n\n\nrenv helps with managing packages in R, ensuring reproducibility, and making your work easier to share.\n\nreticulate allows you to use python in R and combine the strengths of both languages.\nLearning these tools will help you become more effective in machine learning.\nLet‚Äôs get started with the lab exercises!"
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#motivation",
    "href": "labs/slides/lab_2/slide.html#motivation",
    "title": "MLBA Lab 2",
    "section": "Motivation",
    "text": "Motivation\n\nLearning about Quarto\n\nWhat are its output formats? Reports (PDF, HTML, Docx), Presentations (PPT, KEY, HTML) & websites (HTML).\n\n\nLearning how to interact with Quarto\n\nHow can it be used? It can be used with many integrated development environments (IDEs) such as Rstudio, Jupyter Notebook & VSCode.\n\n\nUsing what we have seen in class with Quarto\n\nReason(s) to use? Good application for ML course & reporting in general. It particularly benefits interactive reporting, especially when multiple programming languages are involved."
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#what-is-quarto",
    "href": "labs/slides/lab_2/slide.html#what-is-quarto",
    "title": "MLBA Lab 2",
    "section": "What is Quarto?",
    "text": "What is Quarto?\n\nQuarto¬Æ is an open-source scientific and technical publishing system built on Pandoc.\nPandoc is a versatile tool for converting documents from one format to another. It allows you to convert a document written in one markup language to another markup language, such as converting a Markdown document to HTML or LaTeX.\nThink of Quarto as R-markdown on steroids."
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#formats",
    "href": "labs/slides/lab_2/slide.html#formats",
    "title": "MLBA Lab 2",
    "section": "Formats",
    "text": "Formats\n\n\nWebsites\nBooks\nBlogs\nJournals\n\n\n\n\n\n\nnbdev.fast.ai\n\n\n\n\n\n\n\n\nPython for Data Analysis, 3E by Wes McKinney\n\n\n\n\n\n\n\n\nhttps://jollydata.blog/\n\n\n\n\n\nThe untold story of palmerpenguins by Dr.¬†Kristen Gorman, Dr.¬†Allison Horst, andDr. Alison Hill\n\n\n\n\n\n\n\nJournal of Statistical Software (JSS)\n\n\n\n\n\n\nQuarto can make very flexible websites, or books which are a a Quarto website that can be rendered to Word, ePub, etc., blogs with listings and posts and RSS feeds, Quarto has deep feature set for presentations with reveal.js optimized for scientific content, and of course, publishing for journals. There is custom format systems and the ability to flexibly adapt LaTeX templates."
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#compatibility",
    "href": "labs/slides/lab_2/slide.html#compatibility",
    "title": "MLBA Lab 2",
    "section": "Compatibility",
    "text": "Compatibility\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n..and many other editors. For more info, please see https://quarto.org/docs/get-started/"
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#presentations-pretty-code",
    "href": "labs/slides/lab_2/slide.html#presentations-pretty-code",
    "title": "MLBA Lab 2",
    "section": "Presentations: Pretty Code",
    "text": "Presentations: Pretty Code\n\nOver 20 syntax highlighting themes available\nDefault theme optimized for accessibility\n\n# Define a server for the Shiny app\nfunction(input, output) {\n  \n  # Fill in the spot we created for a plot\n  output$phonePlot <- renderPlot({\n    # Render a barplot\n  })\n}"
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#presentations-code-animations",
    "href": "labs/slides/lab_2/slide.html#presentations-code-animations",
    "title": "MLBA Lab 2",
    "section": "Presentations: Code Animations",
    "text": "Presentations: Code Animations\n\nOver 20 syntax highlighting themes available\nDefault theme optimized for accessibility\n\n# Define a server for the Shiny app\nfunction(input, output) {\n  \n  # Fill in the spot we created for a plot\n  output$phonePlot <- renderPlot({\n    # Render a barplot\n    barplot(WorldPhones[,input$region]*1000, \n            main=input$region,\n            ylab=\"Number of Telephones\",\n            xlab=\"Year\")\n  })\n}"
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#presentations-line-highlighting",
    "href": "labs/slides/lab_2/slide.html#presentations-line-highlighting",
    "title": "MLBA Lab 2",
    "section": "Presentations: Line Highlighting",
    "text": "Presentations: Line Highlighting\n\nHighlight specific lines for emphasis\nIncrementally highlight additional lines\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#presentations-executable-code",
    "href": "labs/slides/lab_2/slide.html#presentations-executable-code",
    "title": "MLBA Lab 2",
    "section": "Presentations: Executable Code",
    "text": "Presentations: Executable Code\n\nlibrary(ggplot2)\nggplot(mtcars, aes(hp, mpg, color = am)) +\n    geom_point() + geom_smooth(formula = y ~\n    x, method = \"loess\")"
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#presentations-auto-animate",
    "href": "labs/slides/lab_2/slide.html#presentations-auto-animate",
    "title": "MLBA Lab 2",
    "section": "Presentations: Auto-Animate",
    "text": "Presentations: Auto-Animate\nAutomatically animate matching elements across slides with Auto-Animate."
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#presentations-auto-animate-1",
    "href": "labs/slides/lab_2/slide.html#presentations-auto-animate-1",
    "title": "MLBA Lab 2",
    "section": "Presentations: Auto-Animate",
    "text": "Presentations: Auto-Animate\nAutomatically animate matching elements across slides with Auto-Animate."
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#references-credits",
    "href": "labs/slides/lab_2/slide.html#references-credits",
    "title": "MLBA Lab 2",
    "section": "References & Credits",
    "text": "References & Credits\n\nQuarto official website & documentation\nSlides taken from Isabella Vel√°squez from Posit (Rstudio) on this link of Intro to Quarto. For the source code, please see here.\nSlides taken from the official Posit (Rstudio) website for introducing Quarto.\nAnother nice resource is from Tom Mock (Posit/Rstudio), given as webinar in 2022. Notably, my set of slides was inspired by his introduction slides. For the entire deck of slides and source code visit this qmd file on his github.\nHuge list of resources for Quarto with many examples from mcanouil/awesome-quarto found here.\n\n\n\n\nMLBA 2024"
  },
  {
    "objectID": "lectures/01_Introduction/ML_Intro.html",
    "href": "lectures/01_Introduction/ML_Intro.html",
    "title": "Introduction to Machine Learning (ML)",
    "section": "",
    "text": "Machine Learning\n\nWhat is Machine Learning?\nMachine learning is a set of techniques using models that are trained on data to achieve forecasting or pattern discovery.\n\nUnlike programming, consisting of implementing a solution already defined by the developers, ML let the model learn by seeing data (learning from examples).\nML is a sub-field of Artificial Intelligence (AI).\nDeep Learning is a popular sub-field of of ML that uses deep neural networks, a particular type models.\n\n\n\nInformal expectations from ML\n\nProvide predictions and/or decisions (predictive analytics).\nFairly automatic and data based (not judgmental).\nThe prediction quality can be assessed (metrics).\nThe model improves with the increase/improvement of the data base.\nThe model can be interpretable.\n\n\n\nMachine Learning versus Statistics\nML is not a re-branding of statistics. Both use data and statistical models. Applications of ML are oriented toward predictions, applications of statistics are oriented toward inference (research, hypothesis assessment). E.g.,\n\nA study aims to in/validate that employees with a psychological support are less prone to burnout. This study needs observations, data analysis, and statistical tests of hypotheses. This is typical of academic work and is usually qualified as statistics (inference).\nA study aims to develop a prediction tool for the credit risk associated to a client in a bank. This study needs observations, data analysis, and model validation. It is typical of ML.\n\n\n\n\nData\n\nData types\nVarious types of data can be used in ML (non-exhaustive):\n\nTabular data: values organized in columns. The most common.\nTime series: values observed in a specific order.\nTextual data: words, set of words, texts, etc.\nImage data: e.g., \\(512\\times 512\\) pixels consisting of \\(3 \\times\\) values between 0 and 255, for a RGB color image.\nVideo: set of ordered images often going with sounds (coded with times series).\nNetworks and relational data: links and distances between units (interactions in a social networks, roads on a map, etc.).\n\nIn this course, we see ML for tabular data.\nValues organized in columns. Values can be:\n\nCategorical: can take a finite number of predefined levels.\n\nNominal: levels are unstructured. E.g., hair colors, genders, food preferences, etc.\nOrdinal: levels have a predefined order. E.g., \\(S<M<L\\), Disagree \\(<\\) Agree \\(<\\) Tot.Agree.\n\nNumerical:\n\nContinuous: numbers with decimal. E.g., revenues, age, height, etc.\nDiscrete: integers. E.g., counting people waiting in line, number of cars per household, etc.\n\n\n\n\nData types for tabular data\nSome data can be represented in different types or transformed. E.g.,\n\nCategorizing the revenue into an ordinal variable (intervals: \\(]0,10]\\), \\(]10,20]\\), etc.).\nAssign numbers to ordinal levels. E.g., Sizes \\(XS=1, \\ldots, XL=5\\).\nCreate dummy variables (see later).\n\n\n\nTabular data structure\nThe table is organized as\n\nEach row is an instance: the unit of observation.\nEach column is a feature: the variables along which the unit is measured.\n\nSynonyms:\n\nInstances, cases, observational units...\nFeatures, variables, attributes, predictors...\n\n\n\n\nLearning tasks\nIn ML, two main tasks are addressed\n\nSupervised Learning The aim is to predict an outcome of interest \\(y\\) using features \\(x\\). It includes regression and classification tasks, scoring, model selection, etc.\nUnsupervised Learning The aim is to learn the dependence patterns of several features and to find meaningful representations of these patterns. This includes clustering and dimension reduction.\n\n\nSupervised and unsupervised learning\nExamples:\n\nSupervised Learning A retailer wants to forecast the spending of a category of customers in reaction to a marketing campaign.\nUnsupervised Learning A retailer wants to group customers in different profiles according to what they buy and their personal characteristics. The profiles are not known in advance.\n\n\n\nSupervised learning tasks and the outcome\nFor supervised learning, one (or several) feature has a specific interest because it is to be predicted using the other features. It is called the outcome (syn. the response).\nTwo types of sub-tasks are distinguished depending on the outcome:\n\nClassification: the outcome is categorical.\n\nA binary outcome: binary classification\nMore than two levels: multi-class classification\n\nRegression: the outcome is numerical.\n\nNote: regression task and regression (e.g., linear model) are not the same thing. Regressions can used for the regression task but (a lot of) other models exist.\n\n\n\nModels\n\nModels\nSupervised learning has two pillars: the data and the models. A model is a trainable algorithm which takes features \\(x\\) and returns an output \\(\\hat{y}\\), a prediction of the outcome \\(y\\) corresponding to \\(x\\).\n\n\n\n\n\nSynonyms: learners, algorithms. For classification, classifiers.\nNotations: \\(f(\\cdot)\\) is the model and \\(\\hat{y} = f(x)\\) is the prediction at \\(x\\).\n\n\nModel training\nThe model is trainable. It can be trained to the available data to produce good predictions. The model, which is itself an algorithm, comes with an training algorithm that can adapt the parameters of the model to optimize a quality criterion. The training algorithm is specific to the model and the criterion. For a given model, you may have several choices (several criteria, several training algorithms, etc.).\n\n\nModel training: example\nWe want to predict \\(y\\) from features \\(x\\) with a linear regression. This model has parameters \\(\\beta\\). Training the model to the data means selecting \\(\\beta\\) from the \\((y_i,x_i)\\), \\(i=1,\\ldots,n\\). Below are two possible choice for the criterion:\n\nThe mean square of the errors (MSE). The optimal parameter \\(\\hat{\\beta}\\) is called the Ordinary Least Square (OLS) \\[\\hat{\\beta} = \\arg\\min_\\beta \\frac{1}{n}\\sum_{i=1}^n (y_i - x_i^T \\beta)^2.\\]\nThe L1-penalized MSE. The optimal parameter \\(\\hat{\\beta}\\) is the LASSO (least absolute shrinkage and selection operator) \\[\\hat{\\beta} = \\arg\\min_\\beta \\frac{1}{n}\\sum_{i=1}^n (y_i - x_i^T \\beta)^2 + \\lambda \\Vert \\beta\\Vert_1\\]\n\n\n\nModel training: optimization\nIn general, each criterion leads to a different set of parameters and a different model. No one is uniformly better than the others. E.g., the OLS has a smaller MSE on the training data (better fit) but the LASSO is more robust against overfitting. It however comes at the cost of selecting \\(\\lambda\\). Depending on the model, it can have several options for its training. It requires the minimization (resp. maximization) of a loss (resp. goodness-of-fit) function. It is always data based. How the optimization is done is part of the so-called black box aspect of machine learning models: people don‚Äôt know what is going on with the mathematics put in action when pushing the training button. Fortunately, in this course, we will study some of these optimizations for some models.\n\n\n\nMetrics\n\nMetrics\nAfter training, we have the best version of a model for the data, according to the chosen criterion. Is it the best model? Is it just doing a good job? To answer, we use metrics: the quality of the model predictions:\n\nClassification: accuracy, specificity, sensitivity, balanced accuracy, \\(F1\\), entropy, etc.\nRegression: \\(R^2\\), RMSE, etc.\n\nInterpreting/comparing these metrics allows to evaluate the model performance.A fundamental aspect of metrics is that they are only based on the predictions and the data. Thus, different models can be compared.\n\n\nModel selection\nA classical approach in ML consists of putting several models in competition. These cross-model comparisons are achieved comparing their metrics: the best model achieved the best metric. E.g., you want to build a predictor for real estate prices from observable features (\\(m^2\\), location, type, etc.). You train one linear regression with OLS, one linear regression with LASSO, one regression tree, one neural network, etc. You select the model with the lowest MSE on unseen data set aside before the training. This is possible only because (again) the metrics are only based on the predictions and the data.\n\n\nBase model\nIt is a good practice to have a base model to get a reference whether a final is worthy or not. E.g., for the prediction of the real estate price, the base model could be the linear regression with OLS since this model is common. Suppose at the end of the selection, you select the neural network. The improvement over the OLS on the MSE is \\(1\\%\\). Is it worth using a complex neural network, heavy to manipulate, not transparent, and energy consuming for such a small improvement¬†?\n\n\n\nOverfitting\nOverfitting is one of the worst enemy in ML:\n\nA model is trained on the observed data.\nThe predictions are used for future data.\n\nOverfitting arises when the model is well trained on the observed data, but has a low capacity of generalization outside this data base.\n\nOverfitting\nE.g., for the real estate example, consider the silly model:\n\nIf the apartment is already in the observed data base, the predicted price is the one in the data base.\nIf it is a new apartment, the predicted price is random.\n\nThe MSE on the data base will be close to zero since the model predicts all observed data exactly. However, this model is useless to the company that cannot use it for new apartments.\n\n\nDetect overfitting\nTo detect overfitting, data splitting strategies are applied:\n\nSplitting the data base into a training set1 and a test set. In the previous example, the silly model would achieve a perfect accuracy on the training set but a poor accuracy on the test set (because it never saw these data before): that difference reveals the overfitting.\n\nMore complex strategies exist like\n\nCross-validations\nBootstrap\n\n\n\nAvoid overfitting\nData splitting is useful to select one model among several to avoid overfitting. However, if all the models are prompt to overfit, data splitting will not help. Overfitting often results from too much complexity in the model. Complexity arises when the model has a lot of parameters. Avoiding overfitting can be obtained by constraining the model to less parameters, i.e., less complexity. When a model is trained, goodness-of-fit and complexity are taken into account. A complex model is prompt to overfit while a too simple model will be prompt to under-fit. The trained model should satisfy a compromise between these two. E.g., LASSO is a constrained OLS that penalizes the complexity by shrinking toward \\(0\\) some of the components of \\(\\beta\\).\n\n\n\nInterpretation\n\nInterpretation\nML models can be black boxes, i.e., difficult to interpret. Interpretation in ML consists in\n\nDiscover if a feature (variable) is important for the prediction of the outcome. E.g., what influences the price of an apartment2 according to the model¬†?\nDiscover how the feature is related to the outcome. E.g., Does the price increase when you move away from the city center?\n\nSeveral techniques exist (see later). Interpretation is often an aim in itself for ML (more important than the prediction task itself).\n\n\nSupervised learning steps\nWith training/test set splitting\n\n\n\n\n\n\n\n\nSkills and tools\n\nSkills\nUseful skills for ML (non-exhaustive):\n\nKnowledge: ML is an active and moving field. Stay informed!\nPrograming: success factor for a ML project as developer and/or manager.\nCommunication: not specific to ML project but the technicality of the topic makes it more complex.\n\n\n\nTools\nSeveral tools (computer programs, apps, webpages, etc.) allow to build ML solutions. In this course, we use R and Python:\n\nMost used programs. R for data exploration, low and medium-level ML, and (classical) statistics; Python for all-level ML.\nOpen, free, and well documented (help, forums, etc.).\nSerious, widespread, and under continuous development with an active community.\n\n\n\n\n\n\n\nFootnotes\n\n\nThe training set is often further split into a ‚Äútraining set‚Äù and a ‚Äúvalidation set‚Äù for the selection of the hyperparameters. This will be clarified later.‚Ü©Ô∏é\nCaution: link, association, correlation \\(\\neq\\) causality!!‚Ü©Ô∏é"
  },
  {
    "objectID": "lectures/02_DataExploration/ML_DataExplo.html",
    "href": "lectures/02_DataExploration/ML_DataExplo.html",
    "title": "Data Exploration",
    "section": "",
    "text": "In-class R script üìÑ\n\nMeta data\n\nData set information\nMeta data refers to information (data) about the data set itself. In data science, this usually includes\n\nIts origin and/or source, if possible, the first one (not only the web page from which it was retieved).\nIts dates of creation and/or retrieval.\nIts name and/or title.\nIts file type (csv, xlxs, etc.).\nIts shape and a general description of its content (each column for tabular data).\n\nE.g., see ?bank in the package liver.\n\n\n\nData Expolration\n\nEDA\nEDA stands for Exploratory Data Analysis. It is not optional! Aims:\n\nKnow the data: e.g., number of modalities for categorical variables, data length, number of missing values, etc.\nBe informed of their general behavior: modality proportions, location and dispersion of numerical variables, etc.\nDetect any outliers or special modes.\nInspect some of the data relationships.\n\nThere exist endless possibilities of EDA. A range of classical and efficient methods is presented below.\n\n\nEDA strategy\nWithout prior knowledge of the case:\n\nOne by one: univariate analysis.\nBy pairs: bivariate analysis.\nMore variables if needed and/or possible.\n\nFor univariate analysis (one variable):\nGraphical:\n\nCategorical variables: barplot and stacked barplot\nNumerical variables: boxplot, violin plot, and histogram.\n\nNumerical:\n\nCategorical variables: frequencies and proportions,\nNumerical variables: locations (mean, median, min, max), dispersions (standard deviation, IQR, range), quantiles (\\(0.25, 0.75\\)),\nBoth: number of observations, number of missing values.\n\nFor bivariate analysis (two variables):\nGraphical:\n\ncat*cat: barplots, mosaic plots\ncat*num: boxplots or histograms of num per modality of cat\nnum*num: scatterplot\n\nNumerical:\n\ncat*cat: table of frequencies or proportions\ncat*num: statistics of num per modality of cat\nnum*num: correlation\n\n\n\nExploratory tools\nFor more than two variables this mainly depends on the objective. The complexity increases fast and no universal method exists. Consider\n\ncat*cat*cat: Sankey diagram.\nnum*num*num: parallel coordinates.\nnum*num*cat: scatterplot (num*num) with colors or shapes (cat).\ncat*cat*num: can turn num into cat (intervals) or can make cat*cat = cat (combine modalities).\n\n\n\n\nData transformation\n\nNumerical variables\nMany possibilities. Consider,\n\nUnivariate: \\(x^2\\), \\(\\ln(x)\\), \\(\\ln(1+x)\\), \\(|x|\\), \\(1/x\\), ranks, etc.\nMultivariate: \\(x_1 x_2\\), \\(x_1 + x_2\\), etc.\nNum to ordinal: categorize \\(x\\) by intervals.\n\nApplying any transformation must be guided by the aim of the study (e.g., use \\(\\ln(1+x)\\) on the outcome \\(x\\geq 0\\)).\n\n\nCategorical variables\nMany possibilities. Consider,\n\nTurn ordinal into num (1, 2, 3, etc.).\nDiminish the number of modalities if too many: e.g., A, B, C covers \\(80\\%\\) of the cases. Create a category ‚Äúother‚Äù for the rest.\nCreate dummy variables.\n\n\n\nDummy variables\nML algorithms perform mathematical operations on the data. They require numbers and cannot be performed on categorical data (characters, strings, etc.). These are then represented as dummy variables.\nMost functions in R handle these representations automatically. However, it is not always the case in Python or in other computer programs. In any cases, it is important to know what is being the scene and how to represent categorical variables as dummy variables.\n\n\nDummy variable\nTransform a categorical variable into several numerical variables:\n\nOne modality is the reference (choice of the user).\nFor the other modalities: \\(0/1\\) variables, one per modality.\nValue is 1 if the modality is the same as the modality of the dummy variable, 0 otherwise.\n\nNote: one variable, called intercept1, made of 1‚Äôs, is added.\nA variable indicates a shape. It has three levels Cloud, Moon, and Star. The reference level below is Cloud. Below, 15 instances.\n\n\n\n\n\n\n\nDummy variable example\nThe same example when the reference level below is Moon.\n\n\n\n\n\n\n\n\nMissing values\nMissing value analysis is never easy and would deserve a whole course. In a quick fix, consider\n\nDetect: which variables, which code (e.g., NA, \\(-999\\), etc.).\nQuantify: how many (number and proportion) for variables and cases.\nRelate: two (or more) variables are systematically missing together?\n\nIn absence of data, lot of techniques break down. Can this be solved perfectly? No.\n\nDealing with (without) missing values\nVarious techniques, none of which is perfect:\n\nRemove the cases with at least one missing feature.\nRemove the feature with too many missing values.\nInput the missing value.\n\n\n\nRemove cases\nPro:\n\nVery simple and systematic\n\nCons:\n\nCan leave no data: e.g., one variable is \\(99\\%\\) systematically missing.\nCan bias the analysis: e.g., missing is linked to the outcome (NMAR)\n\n\n\nRemove features\nPro:\n\nif most of the missing values are concentrated in one feature, then it saves lot of cases.\n\nCons:\n\nCan leave out important information.\n\nConsider also replacing the feature by indicator ‚Äú0/1‚Äù for missing or not.\n\n\nImputation\n\nNaive: per feature, replace NA by average or median (num), or the most frequent modality (cat).\n\nPro: None.\nCons: Avoid if possible (even if it is often used).\n\nModel based: use the other features to predict the missing one (case by case).\n\nPro: Use all the available information.\nCons: Incorrectly diminish the noise in the data which enforce the correlation and give a wrong impression of certainty.\n\n\nMore advanced methods: repeated random imputation, EM algorithm, etc.\n\n\n\nOutliers\n\nDefinition\nBehind the term outlier hides lots of ideas. It can be (non-exhaustive)\n\nAn impossible value: exceeding physical limits.\nAn extreme value of a feature: possible but out of some predefined bounds.\nAn unlikely value: unlikely given the other features.\n\nThe second case is limited to numerical features. The two others can be categorical features.\n\n\nDetection\nFor the three previous cases, consider\n\nOften easy to detect from prior information. E.g., negative surface in a real estate data study, time exceeding 1 hour in a 1-hour observation study, etc.\nUse boxplot to point them: values beyond \\(1.5\\) IQR from the quartile. E.g., age of 101 years when the other largest observed is 35 years.\nComplex: use prior knowledge or a model. E.g., a room used for an RMI in a hospital for a specific diagnostic for which this room is never used, a yearly wage greater than \\(\\$1'000'000\\) for a job in manufacturing.\n\n\n\nDiscussion\nWith outliers, things are never simple and automatic removal is often a bad idea. In an ideal world, only errors should be removed. Consider,\n\nCheck the facts: how many (if only one then it could be removed safely), where, which features?\nInquiry how the data were gathered: e.g., \\(5\\%\\) of the revenues are negative. Did you consider possible that the revenue could indeed be negative in that study?\nIf set aside then document it. This should never be hidden.\nIn some situation, outliers are of interest: e.g., fraud analytics.\n\n\n\n\nFurther words\n\nFurther words\n\nAnalyze all variables systematically (univariate).\nBivariate can be too complex. Select the intersting ones (e.g., in supervised learning, outcome vs other).\nMultivariate analysis be selective.\n\nCommon fallacies:\n\nUse too many modalities in a categorical variable.\nCompute correlations between dummy variables.\nUse missing value indicator as a number (e.g., -999).\nTransformations are not neutral. They influence the final conclusion. They must be taken into account and transparent.\nThink that EDA is a one way through. It is a recursive/trial-and-error process (not unusual \\(90\\%\\) of the work‚Ä¶)\n\n\n\n\n\n\n\nFootnotes\n\n\nIn most algorithm, especially in regressions. In Neural Network, it is called the bias.‚Ü©Ô∏é"
  },
  {
    "objectID": "lectures/03_Models/030_Introduction/ML_Models_Intro.html",
    "href": "lectures/03_Models/030_Introduction/ML_Models_Intro.html",
    "title": "Introduction to Models",
    "section": "",
    "text": "Context\nIn ML, models are mainly used for supervised learning, the aim is\n\nPredict a response \\(y\\): regression if numerical, classification if categorical.\nFrom features \\(x=\\{x_1, \\ldots, x_p\\}\\): available at the moment of prediction,\nWith the best possible quality: built from the data in an optimal way.\n\nThe \\(n\\) observed features and responses are denoted \\[(y_1, x_1), \\ldots, (y_n, x_n).\\]\n\n\nElements\nIn ML, a model consists mainly of three elements:\n\nA prediction formula, taking the features \\(x\\), returning a prediction \\(\\hat{y} = f(x)\\) for \\(y\\),\nA loss function \\({\\cal L}(y,\\hat{y})\\) measuring how \"wrong\" a prediction \\(\\hat{y}\\) is for \\(y\\).\nAn algorithm which can optimize the prediction formula \\(f\\) using the observed data.\n\n\n\nThe prediction formula\nThe prediction formula is a mathematical formula (sometimes a more complex algorithm) using parameters1 \\(\\theta\\), combining them with the feature \\(x\\), returning a prediction \\[f(x;\\theta).\\] Thus, \\(\\theta\\) must be chosen carefully to obtain good predictions of \\(y\\).\n\n\nThe loss function\nThe loss function indicates how wrong is a prediction \\(\\hat{y}\\) of the corresponding \\(y\\).\nA classical example for regression is the square of the error: \\[{\\cal L}(y,\\hat{y}) = (y-\\hat{y})^2.\\] The larger \\({\\cal L}(y,\\hat{y})\\), the further \\(\\hat{y}\\) is from \\(y\\).\n\n\nThe optimal parameters\nGood parameters \\(\\theta\\) must have a low loss. We want \\({\\cal L}(y,f(x;\\theta))\\) to be small for all \\((y,x)\\). To achieve an overall quality on the whole available data base, we want \\(\\theta\\) achieving a small \\[\\bar{{\\cal L}}(\\theta) = \\frac{1}{n}\\sum_{i=1}^n {\\cal L}\\{y_i, f(x_i;\\theta)\\}.\\] Example: with the square of the error, this is \\[\\bar{{\\cal L}}(\\theta) = \\frac{1}{n}\\sum_{i=1}^n \\{y_i- f(x_i;\\theta)\\}^2.\\]\n\n\nThe optimization algorithm\nFinding the optimal \\(\\hat{\\theta}\\) is done by applying an algorithm, i.e., a procedure that finds \\[\\hat{\\theta} = \\arg\\min_{\\theta} \\bar{{\\cal L}}(\\theta).\\] The algorithm is often a sequential procedure. It builds a sequence \\(\\theta_1, \\theta_2, \\theta_3, \\ldots\\) such that \\[\\bar{{\\cal L}}(\\theta_1) > \\bar{{\\cal L}}(\\theta_2) > \\bar{{\\cal L}}(\\theta_3) > \\ldots\\] Ultimately, this should reach the minimum possible \\(\\bar{{\\cal L}}(\\theta)\\).\n\n\nMathematical considerations\n\nMore flexible model \\(f\\) provides better opportunity to minimize \\(\\bar{{\\cal L}}\\). Often, this is associated with the size of \\(\\theta\\) (number of parameters).\nThe algorithm may not reach the global minimum of \\(\\bar{{\\cal L}}\\). Most algorithms cannot guaranty such results except under theoretical assumptions.\nA probabilistic interpretation: the optimal \\(\\theta\\) is obtained by minimizing the expected loss on the population of \\((Y,X)\\) \\[E\\left[{\\cal L}\\{Y, f(X;\\theta)\\}\\right].\\] The data base is used to estimate it with an empirical mean \\[\\hat{E}\\left[{\\cal L}\\{Y, f(X;\\theta)\\}\\right] = \\frac{1}{n}\\sum_{i=1}^n {\\cal L}\\{y_i, f(x_i;\\theta)\\}.\\] This estimate is minimized in turn to find an estimate of the optimal \\(\\theta\\).\n\n\n\n\n\n\nFootnotes\n\n\nAlso called weights, especially for Neural Networks.‚Ü©Ô∏é"
  },
  {
    "objectID": "lectures/03_Models/031_LinearLogisticRegression/ML_LinLogReg.html",
    "href": "lectures/03_Models/031_LinearLogisticRegression/ML_LinLogReg.html",
    "title": "Linear & Logistic Regression",
    "section": "",
    "text": "In-class R script üìÑ\n\nLinear regression\n\nLinear regression\nThe prediction formula is \\[f(x;\\theta) = \\theta_0 + \\theta_1 x_{1} + \\ldots + \\theta_p x_{p}\\]\n\n\n\n\n\n\n\nThe MSE and the OLS\nThe loss function is the mean squared error, MSE, \\[\\bar{\\cal L}(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\{y_i - f(x_i;\\theta)\\}^2.\\] The optimal parameter under this loss is called the Ordinary Least Squares, OLS, \\[\\hat{\\theta} = \\arg\\min_{\\theta} \\frac{1}{n} \\sum_{i=1}^n \\{y_i - f(x_i;\\theta)\\}^2.\\]\n\n\nThe OLS\nTo find the OLS, the algorithm is exact and the final solution can be explicitly computed with the matrix-operation \\[\\hat{\\theta} = (X^TX)^{-1} X^T y,\\] where\n\n\\(X\\) is the so-called design matrix of size \\(n\\times (p+1)\\) and whose \\(i\\)-th row contains \\[[1,x_{i1},\\ldots,x_{ip}],\\]\n\\(y\\) is the vector of length \\(n\\) whose \\(i\\)-th element is \\(y_i\\).\n\n\n\n\nLogistic regression\n\nLogistic regression\nThe logistic regression is a model for binary classification. Prediction formula is in several steps:\n\nCompute the linear predictor \\[z(x;\\theta) = \\theta_0 + \\theta_1 x_{1} + \\cdots + \\theta_p x_{p},\\]\nCompute the probability prediction (sigmoid function): \\[p(x;\\theta) = P(Y=1 | X=x) = \\frac{\\exp\\{z(x;\\theta)\\}}{1+\\exp\\{z(x;\\theta)\\}}.\\]\nCompute the prediction of the class: \\[f(x;\\theta) = \\left\\{\n\\begin{array}{ll}\n1, & \\mbox{if } p\\geq 0.5,\\\\\n0, & \\mbox{if } p < 0.5.\n\\end{array}\n\\right.\\]\n\n\n\nThe sigmoid and the logit function\nThe logit function is the inverse of the sigmoid function and thus transforms \\(p(x;\\theta)\\) to \\(z(x;\\theta)\\). \\[z(x;\\theta) = \\log \\frac{p(x;\\theta)}{1-p(x;\\theta)}\\]\nThe loss function uses the probabilities \\(p(x;\\theta)\\) and not the final predictions. The loss is the cross-entropy also called negative log-likelihood: \\[{\\cal L}(y,p) = -y \\log p - (1-y) \\log (1-p).\\] Interpretation,\n\nIf \\(y=1\\), we want \\(p\\) to be large (close to 1). The loss is \\[{\\cal L}(1,p) = - \\log p\\] It will be small indeed if \\(p\\) is large.\nIf \\(y=0\\), we want \\(p\\) to be small (close to 0). The loss is \\[{\\cal L}(0,p) = - \\log (1-p)\\] It will be small indeed if \\(p\\) is small.\n\n\n\nEstimation\nThe overall loss is \\[\\bar{{\\cal L}}(\\theta) = -\\frac{1}{n} \\sum_{i=1}^n y_i \\log p(x_i;\\theta) + (1-y_i) \\log \\{1-p(x_i;\\theta)\\}.\\] The \\(\\log\\) in this formula can be in any base. Often,\n\nMachine learners use \\(\\log_2\\),\nStatisticians use \\(\\ln\\).\n\nThis has absolutely no consequence on the final result (all \\(\\log\\) are equivalent here). But it can bring confusion from time to time.\n\n\nOptimal parameters\nTo obtain the optimal parameters, the best algorithm is the Newton-Raphson algorithm. It requires\n\nTo compute the first and second derivatives of \\(\\bar{\\cal L}(\\theta)\\),\nTo build a sequence of \\(\\hat{\\theta}_k\\) that converges to the optimal one using these derivatives.\n\nThis algorithm is very fast and efficient. However, there is no explicit formula for \\(\\hat{\\theta}\\), unlike the OLS. The optimal \\(\\hat{\\theta}\\) is sometimes called the maximum likelihood estimator, MLE. That terminology is however less usual among machine learners than statisticians.\n\n\n\nInterpretation\n\nInterpretation\nLinear and logistic regressions are highly interpretable in that\n\nthe coefficients quantify the link between the features \\(x\\) and the outcome \\(y\\).\nthe certainty of the prediction can be quantify.\n\nFor the linear regression, coefficients are interpreted as slopes.\n\nWhen the feature \\(x_1\\) increases by \\(1\\) unit, the outcome \\(y\\) increases in average by \\(\\theta_1\\) units (same for all features \\(1,\\ldots,p\\)).\nA positive coefficient \\(\\theta_j\\) means a positive linear association between the feature \\(x_j\\) and the outcome \\(y\\).\nThe larger the coefficients, the larger the association, in absolute value. (note: pay attention to the scale!)\nFor the categorical features, the coefficients estimate the average change in the outcome \\(y\\) when the feature switches from the reference level to any other level. It is thus a contrast with the reference level.\n\nNote: one should not say that an increase in \\(x_j\\) causes an increase of the response. It is an association. The causality implies a direction and is more complex to establish.\n\n\nInterpretation of the coefficients\nFor the logistic regression, because of the sigmoid transform, the interpretation of the coefficients is more difficult than with the linear regression:\n\nWith a positive \\(\\theta_j\\), an increase of \\(x_j\\) is associated with an increase of the probability that \\(y=1\\).\nThe larger the coefficient, the larger the increase. However, it the increase is not linear and depends on the other features.\nA negative coefficient means a decrease in the probability of the positive class (\\(y=1\\)).\n\nFor linear regression, certainty can be measured by prediction intervals. In practice, the main interest relies in the prediction interval for the future value1.\nLet \\(x\\) be a set of new features, the point prediction for \\(y(x)\\) uses the estimate \\(\\hat{\\theta}\\): \\[f(x;\\hat{\\theta}) = \\hat{\\theta}_0 + \\hat{\\theta}_1 x_1 + \\cdots + \\hat{\\theta}_p x_p.\\] Now, rather than a point estimate, we want to build an interval \\([L,U]\\) such that \\[P(L \\leq y(x) \\leq U) = 1-\\alpha,\\] where \\(\\alpha\\) is usually set to \\(5\\%\\) for an interval at \\(95\\%\\). To build this interval, we rely on probabilistic assumptions of the model.\nTherefore, \\[0.95 = P(L \\leq y(x) \\leq U) = P\\left(\\frac{L-f(x;\\theta)}{\\sigma} \\leq e \\leq \\frac{U-f(x;\\theta}{\\sigma})\\right).\\] Using the normal distribution, we find that \\[L-f(x;\\theta) = -\\sigma z_{1-\\alpha/2}, \\quad U-f(x;\\theta) = \\sigma z_{1-\\alpha/2}.\\] This gives \\[L = f(x;\\theta) -\\sigma z_{1-\\alpha/2}, \\quad U = f(x;\\theta) + \\sigma z_{1-\\alpha/2}.\\] For \\(\\alpha=5\\%\\), this gives \\[(L,U) = f(x;\\theta) \\pm 1.96 \\sigma.\\]\n\n\nCertainty for linear regression\nUsing a plug-in estimate, this gives us a rough prediction interval (at \\(95\\%\\)) of \\[(\\hat{L},\\hat{U}) = f(x;\\hat{\\theta}) \\pm 1.96 s,\\] where \\(s\\) is the unbiased estimate of the standard deviation \\[s^2 = \\frac{1}{n-(p+1)} \\sum_{i=1}^n \\{y_i - f(x_i;\\hat{\\theta})\\}^2.\\] This prediction interval is however hardly used/implemented because\n\nthe estimate \\(s\\) carries uncertainty. If taken into account, the Student \\(t_{n-(p+1)}\\) distribution should be used.\nthe estimate \\(\\hat{\\theta}\\) carries uncertainty. If taken into account, the estimate of \\(s\\) should be changed to \\(s \\sqrt{1+x^T(X^TX)^{-1}x}\\).\n\nBoth adaptations widen the interval.\n\n\nCertainty for logistic regression\nThe probability provides an interpretation of the certainty the model provides on a classification. Consider:\n\n\\(\\hat{y}=1\\) with \\(\\hat{p} = 0.99\\): the prediction is certain.\n\\(\\hat{y}=1\\) with \\(\\hat{p} = 0.53\\): the prediction is uncertain.\n\nIn both cases, the predicted class is the same but the probability provides a more precise view on it: if the instance is far in the class or on the edge between the two classes. The prediction \"Good\" for a customer with a probability of \\(0.51\\) is uncertain. Alternatively, the prediction rule could set to a larger value than 0.5 to increase the certainty. Also, a model with a lot of predictions close to 0.5 is of considered as poor because of its uncertainty2.\n\n\n\nSelection of variables\n\nOccam‚Äôs Razor\nA parsimony principle:\n\"All else being equal, the simplest explanation is the best one\"\n\nIn other words, among two models with approximately the same prediction quality, choose the simplest one.\nIn practice, we remove from the model variables that do not impair too much the prediction quality.\n\nSimplifying the model is a solution to overfitting. This will be studied later in the course.\nThe AIC (Akaike Information Criterion) is \\[AIC = -2\\hat{\\ell} + 2k\\] where\n\n\\(\\hat{\\ell}\\) is the maximum log-likelihood and measure the goodness-of-fit,\n\\(k\\) is the number of parameters and measure the model complexity.\n\nMinimizing the AIC achieves a trade-off between the quality of prediction and the model complexity.\n\n\nAkaike Information Criterion\nFor linear regressions,\n\nThe number of parameters is \\(k=p+2\\) with \\(\\theta_0,\\theta_1,\\ldots,\\theta_p\\) and \\(\\sigma\\),\nThe log-likelihood part equals \\[-2\\hat{\\ell} = n\\ln 2\\pi + n \\ln \\hat{\\sigma}^2 + \\frac{1}{\\hat{\\sigma}^2}\\sum_{i=1}^n \\left\\{y_i - f(x;\\hat{\\theta})\\right\\}^2,\\] where \\(\\hat{\\sigma}^2 = (n-p-1)s^2/n\\).\n\nFor logistic regressions,\n\nThe number of parameters is \\(k=p+1\\) for \\(\\theta_0,\\theta_1,\\ldots,\\theta_p\\),\nThe log-likelihood part equals \\[-2\\hat{\\ell} = 2 \\sum_{i=1}^n y_i \\ln p(x_i;\\hat{\\theta}) + (1-y_i) \\ln \\{1-p(x_i;\\hat{\\theta})\\}.\\]\n\n\n\nVariables selection with AIC\nAutomatic variable selection using stepwise minimization of the AIC can be performed. There are\n\nBackward: start from the most complete model and try to remove variable one at a time (if it decreases the AIC)\nForward: start from the empty model and try to add one variable at a time (if it decreases the AIC).\nBoth: start to add or remove at each step.\n\nAt each step, all the models in competition are fitted. The procedure is computationally intense.\nA different approach consists of penalizing the loss function so that, during the training of the parameters, the variable selection applies directly. The most common penalties are:\n\n\\(L_1\\) penalty (LASSO) \\[\\min_\\theta \\bar{\\cal L}(\\theta) + \\lambda \\sum_{j=1}^p \\vert \\theta_j \\vert\\]\n\\(L_2\\) penalty (Ridge) \\[\\displaystyle \\min_\\theta \\bar{\\cal L}(\\theta) + \\lambda \\sum_{j=1}^p \\theta_j^2\\]\n\nUsually, \\(\\theta_0\\) is not penalized.\nThe penalty parameter \\(\\lambda \\geq 0\\):\n\nIf \\(\\lambda =0\\), then there is no penalty.\nIf \\(\\lambda \\longrightarrow \\infty\\), then \\(\\theta \\longrightarrow 0\\).\n\nFor intermediate values, some components of \\(\\theta\\) will be small, pushed toward \\(0\\). This is equivalent to variable selection: setting \\(\\theta_j=0\\) is equivalent to not including \\(x_j\\). Selection of \\(\\lambda\\) can be done with cross-validation (see later).\n\n\nVariable selection with penalization\n\n\\(L_1\\) shrink some of the \\(\\theta_j\\), set some \\(\\theta_j = 0\\), select variables.\n\\(L_2\\) shrink all the \\(\\theta_j\\)‚Äôs, avoiding extremes \\(\\theta_j\\), regularize \\(\\theta\\).\n\nElastic net combines \\(L_1\\) and \\(L_2\\): \\[\\displaystyle \\min_\\theta \\bar{\\cal L}(\\theta) + \\lambda \\left\\{\\alpha \\sum_{j=1}^p \\vert \\theta_j  \\vert + \\frac{1-\\alpha}{2}\\sum_{j=1}^p \\theta_j^2\\right\\}\\] with \\(0 \\leq \\alpha \\leq 1\\),\n\nIf \\(\\alpha=0\\), it is the ridge (\\(L_2\\))\nIf \\(\\alpha=1\\), it is the LASSO (\\(L_1\\))\n\nOften, \\(\\lambda\\) is selected by the data (cv), while \\(\\alpha\\) is set by the user.\n\n\n\n\n\n\nFootnotes\n\n\nAs opposed to prediction interval for the mean.‚Ü©Ô∏é\nA little thought on that will brings us back to the entropy.‚Ü©Ô∏é"
  },
  {
    "objectID": "lectures/03_Models/032_Trees/ML_Trees.html",
    "href": "lectures/03_Models/032_Trees/ML_Trees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "In-class R script üìÑ\n\nConcept\n\nCART\nCART stands for Classification And Regression Trees. It consists of\n\nA hierarchical set of binary rules,\nA representation in a shape of a tree.\n\nIt applies to both regression and classification tasks. The Iris data has 105 data; with the classes equally divided in 35 observations.\n\n\n\n\n\nIn this case the rule is that if Petal length \\(< 2.45\\) then Setosa, else if Petal width \\(< 1.65\\) then Versicolor, else Virginica.\n\n\nThe prediction formula\nThe set of rules including the order of the features and the thresholds at each node can be embedded into a set of parameters \\(\\theta\\).\nAfter the training, the tree contains of final nodes. We use the notation below,\n\nApplying the rules to features \\(x\\) send the instance \\((y,x)\\) to the final node that we call \\(N(x)\\).\nIn the training set \\((y_1,x_1), \\ldots, (y_n,x_n)\\), the set of instances that were sent to node \\(N\\) is called \\(I(N)\\). E.g., \\(I(N)=\\{1,3,4\\}\\) means that instances \\((y_1,x_1),(y_3,x_3),(y_4,x_4)\\) were sent to node \\(N\\).\nThe prediction for features \\(x\\) is noted \\(f(x;\\theta)\\). That prediction only depends on the node to which \\(x\\) is sent. In other words, \\[N(x) = N(x') \\quad \\Longrightarrow \\quad f(x;\\theta) = f(x';\\theta).\\]\n\n\n\nThe prediction formula: regression\nFor regression, \\(f(x;\\theta)\\), the prediction for \\(x\\), is the mean of the \\(y_i\\)‚Äôs that were sent to the node of \\(x\\) during the training.\nIn more fancy terms, \\[f(x;\\theta) = \\frac{1}{|I(N(x))|}\\sum_{i \\in I(N(x))} y_i.\\]\n\n\nThe prediction formula: classification\nFor classification,\n\nThe node \\(N(x)\\) has a predicted probability vector: probabilities one each possible class \\[p(x;\\theta) = (p_1(x;\\theta), \\ldots, p_C(x;\\theta)),\\]\nThe prediction \\(f(x;\\theta)\\) is the class that has highest probability: \\[f(x;\\theta) = \\arg\\max_{c=1,\\ldots,C} p_c(x;\\theta).\\]\n\nThe predicted probabilities \\(p(x;\\theta)\\) at node \\(N(x)\\) are the proportions of each class taken on all the instances sent to \\(N(x)\\) during the training.\nIn more fancy terms, for any class \\(c\\), \\[p_c(x;\\theta) = \\frac{1}{|I(N(x))|}\\sum_{i \\in I(N(x))} 1_{\\{y_i = c\\}}.\\]\n\n\nThe loss function: regression\nFor the regression case, the most used loss function is the MSE: \\[\\bar{\\cal L}(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\left\\{y_i - f(x_i;\\theta)\\right\\}^2\\]\nFor the classification case, the most used loss is the entropy. It is different from the one of logistic regression1. In the case of \\(C\\) classes, the entropy is \\[\\begin{aligned}\n{\\cal L}(y, p) &=& - p_1 \\log p_1 - p_2 \\log p_2 - \\cdots - p_C \\log p_C\\\\\n&=& - \\sum_{c=1}^C p_C \\log p_c.\\end{aligned}\\] A the global entropy is thus \\[\\begin{aligned}\n\\bar{\\cal L}(\\theta) &=& - \\sum_{i=1}^n\\sum_{c=1}^C p_c(x_i;\\theta) \\log p_c(x_i;\\theta).\\end{aligned}\\]\n\n\nThe loss function: classification\nOther possible choices for the loss function are:\nThe classification error: \\[{\\cal L}(y,p) = 1 - p_y.\\] If \\(y=c\\), then \\(p_c\\) should be large and \\(1-p_c\\) should be small.\nThe Gini index: \\[{\\cal L}(y,p) = p_1(1-p_1) + \\cdots + p_C(1-p_C) = \\sum_{c=1}^C p_c(1-p_c).\\] This index is smaller if \\(p\\) has one large component and the other ones small (e.g., \\((1,0,0,0)\\)). It is thus minimum when the prediction is certain. For this, it is similar to the entropy.\n\n\nThe greedy algorithm\nFinding the optimal \\(\\theta\\) cannot be obtained using a Newton-Raphson algorithm because \\(f(x;\\theta)\\) is not differentiable.\nTo obtain the optimal \\(\\hat{\\theta}\\), one should search among all possible splitting combinations. The complexity of such task is enormous and cannot be achieved even with the most powerful imaginable computers.\nAn approximate solution is obtained by the greedy algorithm.\n\n\n\nThe greedy algorithm\n\nSplitting the data space\nThe tree splits the space into rectangles2. Below with two features:\n\n\n\n\n\nEach rectangle is associated to one node and one prediction.\nThe greedy algorithm grows the tree step by step as follows:\n\nStart with no partition,\nAlong each feature \\(x_j\\), find the best split of the training set by this feature. This gives \\(p\\) splits (one for each feature).\nCompare the \\(p\\) splits and select the best one: the one diminishing the loss the most.\n\nYou obtain two nodes, \\(N_1\\) and \\(N_2\\), say. Repeat the above rule on each node until a stopping rule is reached (see later).\nExample of classification:\n\n\n\n\n\nNote: by convention \\(0\\times \\ln 0 = 0\\).\n\n\nCART: a greedy algorithm\n\n\n\n\n\netc.\n\n\nA greedy algorithm\nExample of regression:\n\n\n\n\n\n\n\nStopping rule\nThe algorithm could go on splitting the feature space forever. Several stopping rules can be used. For example in R with rpart (see ?rpart.control):\n\nminsplit: the minimum number of observations that must exist in a node in order for a split to be attempted (default is 20).\ncp: Any split that does not decrease the overall lack of fit by a factor of cp is not attempted (default to 0.1).\nminbucket: the minimum number of observations in any terminal node (default to minsplit/3).\netc.\n\nEven with these rules, once the tree is grown, it might be too large. The procedure of cutting nodes is called pruning the tree.\n\n\n\nInterpretation\n\nInterpretation\nTrees can be interpreted by reading directly on the graph the link between the features and the outcome.\n\nThe most important feature is at the top of the graph. The first split influence the other ones.\nThe final nodes should be organized in a logical way to ease the reading. E.g.,\n\n(regression) Low predictions to the left, high predictions to the right.\n(binary) predictions \"0\" to the left, and \"1\" to the right.\n\n\nInterpretation must be done with cautious. The tree structure is unstable: several trees can produce similar predictions. The greedy algorithm may just pick one at random.\n\n\n\nPruning\n\nOccam‚Äôs razor\nFor linear and logistic regressions, the more variables are in the model, the more complex that model is. For trees, model complexities can be measured by the length of the tree: a shorter tree is more simple than a longer one. To apply Occan‚Äôs razor, one needs to shorten the tree without impeding too much its prediction quality. To prune the tree consists of cutting branches while maintaining enough prediction quality.\n\n\n1-SE rule\nThere exist several ways to prune the tree. One of the most used is the 1-SE rule.\n\nBuild a long tree (using the default stopping rules)\nFor all the sub-trees compute the error of the tree\nFor the best tree (lowest error) compute the uncertainty on that error measure: the standard error SE.\nAny tree whose error is below the lowest error plus one SE is considered equivalent to the best tree.\nSelect the shortest tree among those that are equivalent to the best one.\n\n\n\nTechnical details\n\nThe SE is computed using cross-validation (see later).\nThe 1-SE rule can be replaced by other rules (like cutting at a given CP, etc.).\n\nIn practice,\n\nthe pruning is not a guarantee to obtain a good model.\nIt should be used to simplify the model and avoid overfitting.\n(empirical) it works well to build a very large tree then to prune it.\n\n\n\n\n\n\n\nFootnotes\n\n\nFor logistic regression, it is the cross-entropy.‚Ü©Ô∏é\nSeveral trees may be equivalent.‚Ü©Ô∏é"
  },
  {
    "objectID": "lectures/03_Models/033_NeuralNetworks/ML_NN.html",
    "href": "lectures/03_Models/033_NeuralNetworks/ML_NN.html",
    "title": "Neural Networks",
    "section": "",
    "text": "In-class Python script üìÑ\n\nConcept and prediction formula\n\nArtificial Neural Network\nArtificial Neural networks are models used for classification and for regression based on combining several predictions of small nodes. To illustrate the method we are applying it on the iris data set, without splitting between training and test sets.\nExample: iris data with 3 possible classes, Setosa, Virginica, and Versicolor to be predicted from 4 features Sepal.Length \\((x_1)\\), Sepal.Width \\((x_2)\\), Petal.Length \\((x_3)\\), and Petal.Width \\((x_4)\\).\n\n\nExample\nA neural network looks like this\n\n\n\n\n\n\n4 input nodes (circles)\nArrows with weights.\nOne middle layer made of 2 \\((+1)\\) nodes.\n3 output nodes: one for each class (species).\n\n\n\nParameters\nIn the context of NN, people use the term weights to talk about the parameters of the model, \\(\\theta\\).\nE.g., on the middle layer, coefficients are associated to the arrows:\n\nOne constant term (node \"1\") called the bias: \\[\\theta_{11}^{(0)}=-28.5\\]\nOne coefficient per arrows: \\[\\theta_{11}^{(1)}=-2.02, \\ldots, \\theta_{11}^{(4)}=13.21.\\]\n\n\n\nNode value\nArrows interring a node indicate a weighted sum of at the previous node values. E.g., consider the top node of the middle layer: \\[\\begin{aligned}\n\\eta_{11} &=& \\theta_{11}^{(0)} + \\theta_{11}^{(1)} x_1 + \\cdots + \\theta_{11}^{(4)} x_4\\\\\n&=& -28.5 -2.0 x_1 + \\cdots + 13.2 x_4\\end{aligned}\\] Then the sigmoid function1. function is applied to obtain the value at the top node of the middle layer: \\[\\begin{aligned}\nZ_{11} &=& \\sigma(\\eta_{11}) = \\frac{e^{\\eta_{11}}}{1+e^{\\eta_{11}}}\\end{aligned}\\] This is repeated at the second middle node: \\[\\begin{aligned}\n\\eta_{12} &=& \\theta_{12}^{(0)} + \\theta_{12}^{(1)} x_1 + \\cdots + \\theta_{12}^{(4)} x_4\\\\\n&=& -1.4 -1.0 x_1 + \\cdots - 11.7 x_4\\\\\nZ_{12} &=& \\sigma(\\eta_{12}) = \\frac{e^{\\eta_{12}}}{1+e^{\\eta_{12}}}\\end{aligned}\\]\n\n\nThe output\nThe \\(Z\\) values are now passed to the outcome nodes. E.g., at setosa: \\[\\begin{aligned}\n\\eta_{12} &=& \\theta_{12}^{(0)} + \\theta_{12}^{(1)} Z_{11} + \\theta_{12}^{(1)} Z_{21} \\\\\n&=& 0.0 + 0.0 Z_{11} + 1.0 Z_{21} \\\\\\end{aligned}\\] The calculation is repeated for each node giving \\(\\eta_{12}, \\eta_{22}, \\eta_{32}\\). Then, the value of the node is obtained applying the soft-max function: \\[\\begin{aligned}\nZ_{12} &=& \\frac{e^{\\eta_{12}}}{e^{\\eta_{12}}+e^{\\eta_{22}}+e^{\\eta_{32}}},\\\\\nZ_{22} &=& \\frac{e^{\\eta_{22}}}{e^{\\eta_{12}}+e^{\\eta_{22}}+e^{\\eta_{32}}},\\\\\nZ_{32} &=& \\frac{e^{\\eta_{32}}}{e^{\\eta_{12}}+e^{\\eta_{22}}+e^{\\eta_{32}}}.\\end{aligned}\\]\n\n\nThe prediction formula\nThe values at the output are the probabilities of each class predicted for the input \\(x\\): \\[p_c(x;\\theta) = Z_{c2}, \\quad c=1,2,3.\\] Indeed, the soft-max guaranties that \\[0 < Z_{c2} < 1, \\quad Z_{12}+Z_{22}+Z_{32} = 1.\\] The prediction for \\(x\\) is the class with maximum probability: \\[f(x;\\theta) = \\arg \\max_{c=1,\\ldots,C} p_c(x;\\theta).\\]\n\n\nThe regression case\nFor the regression, the main difference is that the output layer is made of just one node whose value is the final prediction. In the example, \\[f(x;\\theta) = \\theta_{12}^{(0)} + \\theta_{12}^{(1)} Z_{11} + \\theta_{12}^{(1)} Z_{21}\\]\n\n\n\nNeural network design\n\nNumber of nodes and layers\nBuilding NN is kind of an art. Two important parameters are\n\nThe number of middle layers called hidden layers,\nFor each hidden layer, the number of nodes.\n\nThere is no general rule for this choices. One has to try and see if the quality follows.\nEmpirical rules are\n\nHidden layers with more nodes help creating new features (new dimensions).\nHidden layers with less nodes help combining the previous features to create strong features (dimension reduction).\n\nIn the example, the sigmoid function was applied at each hidden layer node. This is not the only available choice: other functions can be used. These function are called activation functions.\nFor the hidden layers, usual choices are\n\n(so-called) Linear; meaning no function (identity). \\[g(x) = x\\]\nRectified Linear Activation ReLu \\[g(x) = \\max(0,x)\\]\nSigmoid \\[g(x) = \\frac{e^x}{1+e^x} = \\frac{1}{1+e^{-x}}\\]\n\n\n\n\n\n\nFor the output layer, choices are\n\nClassification: softmax \\[g(x_c) = \\frac{e^{x_c}}{\\sum_{j=1}^C e^{x_j}}.\\]\nRegression: same as hidden layers.\n\n\n\nActivation functions\nThere is no one good way to do it, but there are plenty of errors that can be avoided:\n\nUse non-linearity: if only the linear activation function is used in the hidden layer then the NN is a simple linear model. In particular,\n\nFor regression: if the output layer has a linear activation then the NN is equivalent to a linear regression.\nFor binary classification: if the output layer has a sigmoid activation then the NN is equivalent to a logistic regression.\n\nWatch the range of the output. E.g., if \\(y\\) is positive, then using a ReLu activation function close to the output may be good, whereas using a sigmoid on the output layer will prevent the NN to predict values larger than 1.\nMix activation functions along the hidden layers: helps too learn non-linearities.\n\n\n\n\nLoss functions\n\nLoss functions\nThe most common loss functions are\n\nFor regression, the MSE \\[\\bar{\\cal L}(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\{y_i - f(x_i;\\theta)\\}^2.\\]\nFor classification, the cross-entropy \\[\\bar{\\cal L}(\\theta) = -\\sum_{i=1}^n \\sum_{c=1}^C 1_{\\{y_i=c\\}} \\log p_c(x_i;\\theta).\\]\n\nHere, \\(\\theta\\) denotes all the NN parameters (weights).\n\n\n\nTraining algorithm\n\nGradient Descent\nThe training is done by minimizing \\(\\bar{\\cal L}(\\theta)\\), which is an intensive computation. The most used algorithm is the gradient descent:\n\nStart at a random set of weights \\(\\theta\\),\nUpdate the weights using a descent direction (i.e., a direction guaranteeing a diminution of the loss), usually the negative gradient \\[\\theta \\leftarrow \\theta - \\eta \\nabla \\bar{\\cal L}(\\theta),\\]\nIterate until convergence.\n\nAbove, \\(\\eta\\) controls the learning rate. Its choice is crucial. It varies from iterations and can be a vector, i.e., one learning rate per weight (AdaGrad, RMSProp, Adam).\n\n\nBackpropagation\nThe computation of the gradient \\(\\nabla \\bar{\\cal L}(\\theta)\\) can be very heavy on a NN. Backpropagation is a method exploiting the iterative structure of the network to compute this gradient.\n\n\nStochastic Gradient Descent\nIf \\(n\\) is large (lots of instances) \\[\\nabla \\bar{\\cal L}(\\theta)=\\sum_{i=1}^n \\nabla \\bar{\\cal L}_i(\\theta)\\] is heavy to compute. It can be approximated by a partial sum over a random subset of instances \\(S\\subseteq \\{1,\\ldots,n\\}\\) : \\[\\nabla \\bar{\\cal L}(\\theta)=\\sum_{i=1}^n \\nabla \\bar{\\cal L}_i(\\theta) \\approx \\sum_{i \\in S} \\nabla \\bar{\\cal L}_i(\\theta)\\] This is a called stochastic gradient descent (SDG).\n\n\nSDG: batches and epochs\nFor SGD, the practice is to\n\nSplit the set \\(\\{1,\\ldots,n\\}\\) randomly into \\(m\\) batches of the same size, \\(S_1,\\ldots,m\\),\nApply the gradient descent update step sequentially along the batches (in a random order).\nOne pass through all the \\(m\\) batches is called an epoch.\n\nThe choice of the size of the batch is a compromise between computation time and the quality of the gradient approximation:\n\nA large batch size (at the limit \\(n\\)) makes the gradient heavy to compute but more accurate (\\(S\\approx \\{1,\\ldots,n\\}\\)). Each epoch has few iterations but each iteration is long.\nA small batch size makes the gradient fast to compute (\\(S\\) is small) but approximate. Each epoch has a lot of short iterations.\n\n\n\n\nInterpretation\n\nInterpretation\nNN are not interpretable. They are large model combining variables along several non-linear activation function layers. Specific methods can be used (see later in the course).\n\n\n\nModel simplification\n\nModel complexity\nBy construction NN are complex models: they have a lot of weights. E.g., even a small model with \\(10\\) features, \\(2\\) hidden layers with \\((16,8)\\) nodes, and 3 classes has \\[(10+1)\\times 16 + (16+1)\\times 8 + (8+1)\\times 3 = 339\\] weights. With such a large number of parameters, the model is at risk of overfitting the training set by learning too much. One can regularize the model in a similar way to linear and logistic regressions.\n\n\nRegularization\nThe idea is to use \\(L_1\\) and/or \\(L_2\\) penalties on the loss during the training \\[\\bar{\\cal L}(\\theta) + \\lambda_1 \\sum_{j} |\\theta_j| + \\lambda_2 \\sum_{j} \\theta_j^2\\]. Again there is no simple way to set the penalty parameters \\(\\lambda_1\\) and \\(\\lambda_2\\). Note that it is possible to have different penalty parameters in different layers. Unlike regression and trees, and like SVM, this regularization can help to avoid overfitting but cannot be interpreted easily.\n\n\n\n\n\n\nFootnotes\n\n\nSee logistic regression.‚Ü©Ô∏é"
  },
  {
    "objectID": "lectures/03_Models/034_SupportVectorMachine/ML_SVM.html",
    "href": "lectures/03_Models/034_SupportVectorMachine/ML_SVM.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "In-class R script üìÑ\n\nConcept\nConsider a binary classification with two features (axes)\n\n\n\n\n\nThe 3 dashed lines represents 3 different classifiers, equally good in terms of predictions. However, line b may be preferable...\n\nConcept\nIndeed, line b has more capacity to generalize. Suppose we observe a new diamond, with line a, the model would make an incorrect prediction, even if this diamond is not especially unexpected. The symmetric situation with circles would exclude line c also \\(=>\\) line b is the most robust choice.\n\n\n\n\n\n\n\nThe margins\nLine b is better because it has a large margin \\(M\\). SVM are built on that principle: to look for a good classifier (good prediction) but also robust to new observations (large margin).\n\n\n\n\n\nThe borders can be expressed by a vector \\(w=(w_1,\\ldots,w_p)\\) and a constant \\(b\\). The central line is all the \\(x=(x_1,\\ldots,x_p)\\) such that \\[w_1x_1 + w_2x_2 + \\cdots + w_p x_p + b = 0\\] i.e.¬†\\[w^T x + b = 0.\\] The borders are such that \\(w^T x + b = -1\\) and \\(w^T x + b = 1\\). In the next slide example,\n\nthe number of features is \\(p=2\\),\n\\(w=(1,-1)\\),\n\\(b=0\\).\n\n\n\n\n\n\nFor a good separation, we generally look for a central line \\[w^TX + b = 0,\\] at the middle of the two closest points (\\(A\\) and \\(B\\) on the next slide) and such that\n\n\\(w^T X_A + b = -1\\)\n\\(w^T X_B + b = 1\\)\n\nIn addition, we want that all the red diamonds are above the upper border, and all the blue circles are below the lower border. This translates into\n\n\\(w^T X_i + b \\leq -1\\), for all \\(i\\) such that \\(Y_i\\) is \"red diamond\".\n\\(w^T X_i + b \\geq 1\\), for all \\(i\\) such that \\(Y_i\\) is \"blue circle\".\n\n\n\n\n\n\nIt can be shown that the distance between the two borders is the margin \\(M\\), and can be computed as \\[M=2/\\sqrt{w^T w}.\\] Thus, one should look for\n\nall the points are separated (previous inequalities)\nthe margin \\(M\\) is maximum or, equivalently, \\(w^Tw\\) is minimum.\n\n\n\nGeometry\nTo further simplify the notation, note that, in this perfect configuration,\n\nBlue circles (below) are such that \\(w^T x_i + b \\geq 1\\)\nRed diamonds (above) are such that \\(w^T x_i + b \\leq -1\\)\n\nThus, we assign the outcome values \\(y_i=-1\\) to red diamonds and \\(y_i=1\\) to blue circles, we have all these inequality can be written as \\[y_i (w^T x_i + b) \\geq 1, \\quad \\mbox{for all } i.\\]\n\n\nOptimization problem\nOverall, to solve the SVM, one looks for \\(w\\) for which \\(w^Tw\\) is minimum and \\[y_i (w^T x_i + b) \\geq 1, \\quad \\mbox{for all } i.\\] This can be written as the optimization problem \\[\\begin{aligned}\n\\min_{w, b} && \\frac{1}{2}w^Tw  \\\\\n\\mbox{s.t. } && y_i(w^Tx_i + b) \\geq 1, \\quad i=1,\\ldots ,n\\end{aligned}\\] This problem can be solved using quadratic program under linear constraints (see your favorite Optimization course).\nThe solution of this problem will provide a perfect separation between the 2 categories with the largest margin.\n\n\nSupport vectors\nPoints that are well inside their margins are not playing a big role. In fact, the margins (and hence the central line) are completely defined by the points on the margins, i.e.¬†those \\(i\\) such that \\[y_i(w^T x_i + b) = 1.\\] These points are called support vectors.\n\n\nNon linearly separable case\nHowever, this approach is valid only if the categories are linearly perfectly separable: there exists a line that perfectly separates the classes.\nMost cases are non-separable: no line can reach the perfect separation.\n\n\n\n\n\nUnder the perfect separation case, \\(y_i(w^Tx_i + b) \\geq 1\\), for all \\(i\\).\nIn non-separable case, some \\(i\\) are not well classified, i.e., there is a tolerance \\(z_i\\geq 0\\) such that \\[y_i(w^Tx_i + b) \\geq 1 - z_i.\\]\n\nIf \\(z_i=0\\) then the instance \\(i\\) is correctly classified,\nIf \\(z_i>0\\) then the instance \\(i\\) is misclassified.\n\nIf all the \\(z_i\\)‚Äôs are large, then all the points are misclassified. To reach a good classification, we impose a small sum of the \\(z_i\\)‚Äôs \\[\\sum_{i=1}^n z_i\\] The optimization problem is modified accordingly to \\[\\begin{aligned}\n\\label{SVM_Primal}\n\\nonumber \\min_{w, b, z} && \\frac{1}{2}w^Tw + C\\sum_{i=1}^n z_i  \\\\\n\\mbox{s.t. } && y_i(w^Tx_i + b) \\geq 1-z_i, \\quad i=1,\\ldots ,n\\\\\n\\nonumber && z_i\\geq 0, \\quad i=1,\\ldots ,n\\end{aligned}\\]\n\n\nSoft margins\nParameter \\(C\\geq 0\\) is called the cost and is fixed by the user.\nIt is a way to control the tolerance to bad classification:\n\nIf \\(C=0\\), there is no penalty on the \\(z_i\\)‚Äôs, thus they will be large, and so all the points can be misclassified.\nIf \\(C\\) is large, then most \\(z_i\\)‚Äôs will be small, thus only few misclassifications are allowed.\n\n\n\nSoft margin and support vector\nWith soft margins, the support vectors are those \\(i\\) such that \\[y_i (x_i^Tw + b) = 1-z_i.\\]\n\nIf \\(z_i=0\\), the support vectors lies exactly on the margin and thus are correctly classified.\n\nIf \\(z_i > 0\\), the support vector is away from the margin, on the wrong side, and is thus incorrectly classified.\n\nRemember that, like in the perfectly separable case, any instance \\(i\\) is correctly classified if \\(z_i=0\\), that is, \\[y_i (x_i^Tw + b) \\geq 1.\\] Therefore, a support vector can be misclassified.\n\n\nThe dual problem\nAn equivalent way of writing the optimization problem ([SVM_Primal]) is in its dual form1 \\[\\begin{aligned}\n\\label{SVM_Dual}\n\\nonumber \\max_{a} &&   \\sum_{i=1}^n a_i - \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n a_i a_j y_iy_j x_i^Tx_j\\\\\n\\mbox{s.t. } && \\sum_{i=1}^n a_i y_i = 0\\\\\n\\nonumber && 0 \\leq a_i \\leq C, \\quad i=1,\\ldots ,n\\end{aligned}\\] Support vectors are \\(i\\) such that \\(a_i>0\\).\n\nIf \\(a_i < C\\) then they are on their margin (equiv. \\(z_i=0\\))\nIf \\(a_i=C\\) then they are inside the margin (equiv. \\(z_i>0\\)).\n\n\n\nThe prediction formula\nIf the dual is more difficult to interpret, it provides a nice prediction formula. The prediction of the class of a new instance \\(x\\) is based on a decision score (below \\(\\theta\\) contains all the parameters of the SVM) \\[d(x;\\theta) = \\sum_{i=1}^n a_i y_i x_i^Tx.\\] The prediction then is \\[f(x;\\theta) = \\left\\{\n\\begin{array}{rl}\n1, & \\mbox{if } d(x;\\theta) > 0,\\\\\n-1, & \\mbox{if } d(x;\\theta) < 0.\\\\\n\\end{array}\n\\right.\\]\nThe decision has an interesting form: \\[d(x;\\theta) = \\sum_{i=1}^n a_i y_i x_i^Tx.\\] It is a weighted sum of the \\(y_i\\)‚Äôs in the training set. The weights are\n\nThe support vectors, \\(0 < a_i \\leq C\\), enter into the sum with more or less weights. The other cases (non-support vectors), have \\(a_i=0\\) do not enter into the sum.\nThe vectors such that \\(x_i^Tx\\) is large will participate more to the sum. This \\(x_i^Tx\\) is a measure of proximity (the larger, the more \\(x_i\\) and \\(x\\) are similar).\n\nIn summary, the prediction of \\(y\\) will be similar to the important support vectors \\(y_i\\) for which the features \\(x_i\\) are close to the new features \\(x\\).\n\n\nKernel-based prediction\nThe proximity \\(x_i^Tx\\) is called a kernel. In general, \\[d(x;\\theta) = \\sum_{i=1}^n a_i y_i k(x_i,x).\\] The dual ([SVM_Dual]) is written \\[\\begin{aligned}\n\\nonumber \\max_{a} &&   \\sum_{i=1}^n a_i - \\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^n a_i a_j y_iy_j k(x_i,x_j)\\\\\n\\mbox{s.t. } && \\sum_{i=1}^n a_i y_i = 0\\\\\n\\nonumber && 0 \\leq a_i \\leq C, \\quad i=1,\\ldots ,n\\end{aligned}\\]\n\n\nOther kernels\nUsual kernels are\n\nLinear \\[K(x,x') = x^Tx'\\]\nPolynomial of degree \\(q\\) \\[K(x,x') = \\left(k_0 + \\gamma x^Tx'\\right)^q\\]\nRadial basis \\[K(x,x') = \\exp\\left(-\\gamma (x-x')^T(x-x')\\right).\\]\nSigmoid \\[K(x,x') = \\tanh\\left(-\\gamma x^Tx' + k_0\\right).\\]\n\nFor each, the parameters \\(k_0\\) and \\(\\gamma\\) are set by the user.\n\n\nUnderlying features\nUsing a kernel other than \\(x^Tx'\\) can be shown to be equivalent to create new features \\(h(x)\\). New features can help in separating the cases: e.g., below a non-separable case in \\((x,y)\\) becomes separable by the new feature \\((x^2+y^2)\\):\n\n\n\n\n\n\nHowever, with the kernel, the exact form of these new features cannot be known.\n\n\n\nInterpretability\n\nInterpretability\nSupport Vector Machine models are not interpretable.\nUnlike regressions and trees, they do not provide any way to know the association between the features and the outcome.\nInterpretability must therefore rely on generic methods (see later in the course).\n\n\n\nSelection of variables\n\nModel complexity\nLike for regressions and trees, to apply the Occam‚Äôs razor, we need to be able to control the model complexity. In the case of SVM, this is done by controlling the cost \\(C\\). Reminder,\n\nIf \\(C=0\\), then all the points can be misclassified. Therefore, the model is simple.\nIf \\(C\\) is large, then only few misclassifications are allowed. Therefore, the model is complex.\n\nThe choice of \\(C\\) is made by the user and can be selected as a hyperparameter using data splitting strategies (see later in the course).\n\n\n\nGeneral cases\n\nSVM for multiclass\nSo far, we have seen only SVM for a binary classification. For the multiclass case (\\(K>2\\) classes), the principle remains the same except that several classifiers are built according to one of the following strategies:\n\nOne versus the rest: for a new \\(x\\), \\(d_k(x;\\theta)\\) is the decision function for class \\(k\\) versus all the others. The final prediction is the class that has the highest decision value, \\[f(x;\\theta) = \\arg\\max_{k=1,\\ldots,K} d_k(x;\\theta).\\]\nOne versus one: for all the \\(\\binom{k}{2}\\) unique pair of classes \\((k,k')\\), build a svm on the outcomes in these two classes. Then, the prediction is obtained by voting: the predicted class \\(f(x;\\theta)\\) is the one that has the highest number of decisions in its favor.\n\n\n\nSVM for regression\nThis topic is outside the range of this course, but SVM can be adapted to regression with a similar property of robustness.\nSimilarly to the classification case, the final prediction is of the kernel form \\[f(x;\\theta) = \\sum_{i=1}^n a_i y_i k(x_i,x).\\]\n\n\n\n\n\n\nFootnotes\n\n\nAgain see your favorite Optimization course.‚Ü©Ô∏é"
  },
  {
    "objectID": "lectures/04_Metrics/ML_Metrics.html",
    "href": "lectures/04_Metrics/ML_Metrics.html",
    "title": "Metrics",
    "section": "",
    "text": "In-class R script üìÑ\n\nConcept\n\nConcept\nA metric (score) is a numerical value measuring the performance of a model. To compare the different models, the metric only depends on the observed and predicted values or classes. There are two types of metrics: for regression and for classification.\n\n\n\n\n\n\n\n\nMetrics for classification\n\nMetrics for classifications\nMetrics for classifications can be grouped as\n\nMetrics based on the predicted classes only\nMetrics based on the predicted probabilities\nMetrics for binary classifications only\nGeneral metrics for multi-classes\n\n\n\nConfusion matrix\nThe confusion matrix reports the frequencies of \"observed\" vs \"predicted\" classes. E.g., SVM with linear kernel and cost of 1, on iris data:\n\n            Reference\nPrediction   setosa versicolor virginica\n    setosa         50          0         0\n    versicolor      0         48         2\n    virginica       0          2        48\nAll the metrics based on predicted classes can be computed from the confusion matrix.\n\n\nAccuracy\nThe accuracy is the proportion of correct predictions of the model \\[A = \\sum_i n_{ii}/\\sum_{ij} n_{ij}\\] where \\(n_{ij}\\) is the number in the confusion matrix at line \\(i\\) and column \\(j\\).\nFor the previous SVM model, the accuracy is \\[A = \\frac{50+48+48}{50+2+48+48+2} = 0.973\\]\nCohen‚Äôs kappa1 is a measure which compares the observed accuracy to the accuracy that one would expect from a random model.\nThe expected accuracy from a random model is computed as the accuracy from a table containing the expected frequencies \\(n_{ij}\\) if the model was random2. The expected accuracy is also called No Information Rate.\nWith the iris data, we have the following observations from the model:\n\n\n\n\n\nThe observed accuracy is \\(A_o = 0.973\\).\nA random model would have balanced proportions in each cell:\n\n\n\n\n\nwhere \\[0.111 = 0.333\\times 0.333.\\] The expected accuracy is \\(A_e = 0.111+0.111+0.111 = 0.333\\).\nThe expected frequencies are not of direct use. These are the expected proportions multiplied by the total amount of instances: \\[16.67 = 0.111 \\times 150.\\]\n\n\nCohen‚Äôs kappa\nBy definition, the Cohen‚Äôs kappa is \\[\\kappa = \\frac{A_o - A_e}{1-A_e}\\]\n\n\\(\\kappa = 0\\) if \\(A_o=A_e\\) when the accuracy of the model is the same as a random model (i.e., not a good model).\n\\(\\kappa = 1\\) if \\(A_o=1\\) when the accuracy of the model is 1 (i.e., a good model).\nIntermediate values are the excess of accuracy of the model over the random model, relative to the inaccuracy of the random model (\\(1-A_e\\)).\n\nIn the example, \\[\\kappa =\\frac{0.973 - 0.333}{1 - 0.333} = 0.96.\\]\nWith a binary classification task (0/1):\n\n\n\n\nPredict 1\nPredict 0\nTotal\n\n\n\n\nTrue 1\n\\(TP\\)\n\\(FN\\)\n\\(P\\)\n\n\nTrue 0\n\\(FP\\)\n\\(TN\\)\n\\(N\\)\n\n\nTotal\n\\(PP\\)\n\\(PN\\)\n\\(M\\)\n\n\n\nwhere\n\n\n\\(TP\\): \\(\\#\\) instances \\(1\\) that are predicted as \\(1\\): \"true positives\"\n\\(FP\\): \\(\\#\\) instances \\(0\\) that are predicted as \\(1\\): \"false positives\"\n\\(TN\\): \\(\\#\\) instances \\(0\\) that are predicted as \\(0\\): \"true negatives\"\n\\(FN\\): \\(\\#\\) instances \\(1\\) that are predicted as \\(0\\): \"false negatives\"\n\\(P\\): \\(\\#\\) instances \\(1\\): ‚Äúpositives‚Äù.\n\\(N\\): \\(\\#\\) instances \\(0\\): ‚Äúnegatives‚Äù.\n\\(PP\\): \\(\\#\\) instances predicted \\(1\\): ‚Äúpredicted positive‚Äù.\n\\(PN\\): \\(\\#\\) instances predicted \\(0\\): ‚Äúpredicted negative‚Äù.\nThe total number of instances is \\(M=TP+FP+TN+FN=P+N=PP+PN\\).\n\n\n\nMetrics for binary classification\nA logistic regression was fitted to a data set3. The positive class is ‚ÄúGood‚Äù. There are 700 \"Good\" and 300 \"Bad\" in the data set.\n\n        Pred\nObs    Good Bad\n    Good  686  14\n    Bad   273  27\nThen, \\[TP=686, \\, FP=273, \\, TN=27, \\, FN=14\\,\\] and \\[P=700, \\, N=300, \\, PP=959, \\, PN=41.\\]\n\n\nSensitivity\nThe sensitivity is the proportion of correct predicted positive among the true positives: \\[\\mbox{Sens}=\\frac{TP}{TP+FN}=\\frac{TP}{P}\\] It is the probability that the model recovers a positive instance: \\[Pr\\left(\\mbox{model predicts positive}|\\mbox{positive}\\right).\\] It is also referred to as recall or true positive rate. E.g., with the credit data \\[Sens=\\frac{686}{686+14}=0.98\\] The model is thus very good at recovering the positive instances.\n\n\nSpecificity\nThe specificity is the proportion of predicted negatives among the negatives: \\[\\mbox{Spec}=\\frac{TN}{TN+FP}=\\frac{TN}{N}\\] Also called True Negative Rate, it is the probability that the model recovers a negative instance: \\[Pr\\left(\\mbox{model predicts negative}|\\mbox{negative}\\right).\\] E.g., with the credit data \\[TNR=\\frac{27}{27+273}=0.09\\] The model is poor at recovering the negative instances.\n\n\nPositive Predictive Value\nThe precision also called Positive Predictive Value. It is the proportion of true positives among the predicted positive: \\[PPV=\\frac{TP}{TP+FP}=\\frac{TP}{PP}\\] The precision is the probability that the prediction is correct given that the model predicted positive: \\[Pr\\left(\\mbox{positive}|\\mbox{model predicts positive}\\right).\\] E.g., \\[PPV = \\frac{686}{273+686} = 0.715\\] Given that the model predicts a ‚Äúpositive‚Äù, the chance it is indeed a positive is 0.715.\n\n\nNegative Predictive Value\nIt is the equivalent of the precision but for the negative class. \\[NPV=\\frac{TN}{FN+TN}=\\frac{TN}{PN}\\] It is the probability that the prediction is correct given that the model predicted it as negative: \\[Pr\\left(\\mbox{negative}|\\mbox{model predicts negative}\\right).\\] E.g., \\[NPV = \\frac{27}{14+27} = 0.66\\] Given that the model predicts a ‚Äúnegative‚Äù, the chance it is indeed a negative is 0.66.\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n\n\nBalanced accuracy\nOne issue with the accuracy is that it is a global indicator that strongly depends on the number of positive and negative cases in the data set.\nFor example, with 700 positive instances out of 1000, a model always predicting ‚ÄúGood‚Äù would have an accuracy of 0.70: a large accuracy for such a useless model.\nThis is because of unbalance in the outcomes in the data4.\nThe balance accuracy is the accuracy computed as if the data were balanced. It ends up being the average between sensitivity and specificity: \\[BA = \\frac{Sens+Spec}{2}\\] E.g., \\(BA=(0.98+0.09)/2=0.535\\), a low value: the model cannot recover negative cases.\n\n\nThe \\(F_1\\) metric\nThis metrics is a (harmonic) mean of the precision and the sensitivity: \\[F_1 = 2 \\times \\frac{Prec\\times Sens}{Prec+Sens}\\] E.g., for the credit data \\(Prec = 0.715\\), \\(Sens = 0.98\\), thus \\[F_1 = 2 \\times\\frac{0.715\\times 0.98}{0.715+0.98} = 0.83.\\] The limits are\n\n\\(F_1 = 0\\) when either Prec or Sens is 0.\n\\(F_1 = 1\\) when both Prec and Sens are 1.\n\nThe larger \\(F_1\\), the better the model.\nThe Receiver Operating Characteristic curve shows the sensitivities and the specificities of a model when the threshold for the prediction varies. E.g., the sensitivity is 0.98, and the specificity is 0.09 with a threshold of \\(\\lambda=0.5\\): \\[f(x_i;\\theta)=\n\\left\\{\\begin{array}{ll}\n1 & \\mbox{if } p(x_i;\\theta) \\geq 0.5,\\\\\n0& \\mbox{if } p(x_i;\\theta) < 0.5.\n\\end{array}\n\\right.\\] What happens when the threshold \\(\\lambda\\) ranges from 0 to 1?\n\nIf \\(\\lambda=0\\), no cases are predicted as \\(0\\), thus \\(Sens=1\\) and \\(Spec=0\\).\nIf \\(\\lambda=1\\), no cases are predicted as \\(1\\), thus \\(Sens=0\\) and \\(Spec=1\\).\n\nThe ROC curve is obtained when \\(\\lambda\\) varies between 0 and 1, \\(Spec=x\\), and \\(y=Sens\\). The specificies are usually in the reverse order.\n\n\n\n\n\n\n\nROC curve and AUC\nA perfect model have large sensitivity and specificity for all \\(\\lambda\\) (except \\(0\\) and \\(1\\)). This corresponds to a single point in the upper left corner of the ROC. The diagonal in gray corresponds to the random model. The further the curve up that diagonal, the better the model. The Area Under the Curve (AUC) is the area between the diagonal and the ROC curve. The larger it is, the better the model is.E.g., \\(AUC=0.628\\). The ROC curve provides a way to tune the threshold for the prediction under unbalanced class: select the threshold with maximum balanced accuracy (Youden Index point). See later.\n\n\n\nMetrics for regression\n\nRoot Mean Squared Error (RMSE)\nThe RMSE is the square root of the mean of the squares of the errors of prediction: \\[RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\left\\{y_i - f(x_i;\\theta)\\right\\}^2},\\] The lower the RMSE, the better the model.\n\n\nMSE and MAE\nOther metrics:\n\nThe Mean Squared Error (MSE) is the RMSE without the square root \\[MSE = RMSE^2\\] It is obviously equivalent to the RMSE.\nThe Mean Absolute Error (MAE) is the average of the absolute errors: \\[MAE = \\frac{1}{n}\\sum_{i=1}^n|y_i -f(x_i;\\theta)|\\]\n\n\n\nRMSE and MAE\nLike the median to the mean, the MAE is more robust than the RMSE:\n\nOne large error only will make the RMSE large.\nThe MAE is less influenced by one error.\n\nIn practice, with:\n\nModel 1: mostly small errors except a few large ones,\nModel 2: only average errors, no large one.\n\nThen, we will probably have \\[\\begin{aligned}\nMAE(M1) &<& MAE(M2)\\\\\nRMSE(M2) &<& RMSE(M1)\\end{aligned}\\]\n\n\nThe \\(R^2\\)\nIn the ML context, the \\(R^2\\) (\\(R\\) squared) is the square of the correlation between the observations and the predictions. \\[R^2 = cor^2(y,f(x;\\theta))\\] The \\(R^2\\) is bounded between 0 and 1. The larger the \\(R^2\\), the better the model. The \\(R^2\\) is an attempt to approximate / replace the usual \\(R^2\\) of a linear regression. However, these two must not be confounded because they will often be different (though conveying the same idea).\n\n\n\nFinal words\n\nFinal words\n\nThere exist plethora of metrics with advantages and drawbacks.\nSome can be interpreted (accuracy, sensitivity, etc.), others cannot and are used only to compare models (\\(F_1\\), \\(RMSE\\), etc.).\nIt is important to have a baseline model to compare the metric (e.g., random model for classification, average for regression).\nFor regression, a scatterplot of the observations \\(y_i\\) versus the predictions \\(f(x_i;\\theta)\\) allows to analyze in more details the model quality. It is a must do!\n\n\n\n\n\n\n\nFootnotes\n\n\nOften used in psychology‚Ü©Ô∏é\nThe calculations are the same as for the chi-square test for independence.‚Ü©Ô∏é\nThe well known German credit data‚Ü©Ô∏é\nNote that the Cohen‚Äôs kappa takes this into account.‚Ü©Ô∏é"
  },
  {
    "objectID": "lectures/05_DataSplitting/ML_DataSplitting.html",
    "href": "lectures/05_DataSplitting/ML_DataSplitting.html",
    "title": "Data Splitting",
    "section": "",
    "text": "In-class R script üìÑ\n\nConcept\nA good model should have the good metric (predictions) and a good capacity to extrapolate outside the data base. Consider the following stupid but feasible model to predict an instance:\n\nif the instance is already in the data base, the predicted class is the same as the one in the data base.\nif the instance is not already in the data base, the prediction is random.\n\nThe model will have a perfect accuracy on the data base. However, it is useless for new instances.\n\nOverfitting\nThis example illustrates that it is easy to build good predictions when the data base is available: predicting the past is easy. In the context of a supervised learning, a model may be good at predicting the data base on which it is trained but not the future/other data points. In practice however, this generalization capacity is crucial. A model having very good predictions on the data base, and a poor extrapolation capacity (for new instances) is said to overfit the data. Overfitting is one of the worst enemy in machine learning: it looks like things are going well, when in fact no good new predictions can be obtained.\n\n\n\nTraining and test sets\nThe simplest data split consists of:\n\nA training set on which the models are trained;\nA test set on which predictions are tested.\n\n\nConcept\nOften,\n\nThe split is random (assignment of the instances).\nThe two sets are disjoint: no common instance, except if they are already multiple in the data base.\nCommon proportions Tr/Te are like \\(75/25\\) or \\(80/20\\).\nHowever, with very large data base, it is useless to have too large training set. With millions of data points, often a \\(10\\%\\) training set is enough (if the data are representative and rich enough).\n\n\n\nApparent and test metrics\n\nOn the training set, the metrics (accuracies, etc.) are called apparent metrics: often too optimistic.\nThe test set they are called test metrics: close to the one expected when the model will be in use on new data.\n\nTo detect overfitting, for a positive metric (e.g., accuracy),\n\nIf \\(Metric(Tr) >> Metric(Te)\\): overfitting.\nIf \\(Metric(Tr) \\approx Metric(Te)\\): no sign of overfitting.\n\nA data set with 1000 instances,\n\nSplit at random:\n\ntraining set of 750 instances,\nTest set of 250 instances.\n\nA logistic regression is fitted to the training set.\nThe features of the test set are given to the model to make the test set predictions. These are compared to the correct classes in the test set.\nThe apparent followed by test confusion matrices:\n\n            Reference\nPrediction   Bad Good\n        Bad   30   19\n        Good 195  506\n\n            Reference\nPrediction   Bad Good\n        Bad    6    7\n        Good  69  168\nThe apparent and test accuracies are \\[A_{tr} = \\frac{30+506}{750} = 0.715, \\quad\nA_{te} = \\frac{6+168}{250} = 0.696.\\] The accuracy on the test set is closer to the one that can be expected if the model was used on new instances (e.g., in production).\nHere, the test accuracy is lower than the apparent accuracy. This is a usual sign of over-fitting1.\n\n\n\nValidation set\n\nValidation set\nWhen the training set is split further into a training set2 and a validation set.\n\nSeveral models are trained on this smaller training set\nThe best model is selected based on the metrics obtained on the validation set.\nThe test metric of best model is computed to have a final evaluation of what this metric will be on new data.\n\nThis avoids information leakage: selecting the best model on the test set may cause overfitting of this test set...\nValidation set is also used for cross-validation, hyperparameter selection, early stopping, etc. See below.\n\n\nIllustration\n\n\n\n\n\n\n\n\nRepeated splits\n\nRepeated splits\nOne single training/validation/test split taken at random can be unstable or impossible to replicate: the randomness was badly balanced or too much dependence on this specific split.\nRepeated splits can be used to address this issue. They are under two forms:\n\nCross-validation (CV).\nBootstrap.\n\n\nSplit the data set into non-overlapping \\(K\\) subsets,\nFor each \\(k=1,\\ldots,K\\),\n\nUse subset \\(k\\) as the validation set, and the rest of the data set as the training set,\nTrain the model on the training set\nCompute the predictions on the validation set\nCompute and record the metric \\(S_k\\)\n\n\nThis provides \\(K\\) metrics: \\(S_1, \\ldots, S_K\\). The final CV metric is their average: \\[\\hat{S} = \\frac{1}{K}\\sum_{k=1}^K S_k.\\] In addition, these metrics are used to estimate the standard deviation3 of \\(\\hat{S}\\).\n\n\n\n\n\n\n\nExample\nA 10-fold CV is made. The accuracies of are logistic regression in each fold are \\[\\begin{array}{|c|c|c|c|c|c|c|c|c|c|}\n\\hline\nS_1 & S_2 & S_3 & S_4 & S_5 & S_6 & S_7 & S_8 & S_9 & S_{10}\\\\\n\\hline\n0.63 & 0.71 & 0.73 & 0.73 & 0.76 & 0.64 & 0.71 & 0.75 & 0.78 & 0.69\\\\\n\\hline\n\\end{array}\\] The CV accuracy (standard deviation) is \\(0.713\\) \\((0.049)\\).\nOn the same data split, another model gives CV accuracy of 0.654 (0.053).\nThe logistic regression should be preferred here.\n\n\n\\(K\\)-fold CV\nHow to choose \\(K\\)?\n\nNo universal good choice.\nChoice of \\(K\\) is a trade-off between\n\ncomputational capacity: large \\(K\\) gives more iterations and larger training sets, thus longer computation.\nnumber of validation metrics: large \\(K\\) gives more \\(S_k\\) so \\(\\hat{S}\\) is well estimated.\nnumber of data to train the model: large \\(K\\) gives large training set thus better estimated models.\nnumber of data to estimate the validation metric: large \\(K\\) gives small validation set, thus \\(S_k\\) is not well estimated.\n\nIn practice, use \\(K=5\\) or \\(K=10\\).\n\nA special case is \\(K=n\\), known as leave-one-out (LOO).\n\n\n\n\nBootstrap: Re-sampling\nIn context of ML, bootstrap stands for re-sampling with replacement. It is an alternative to CV. The differences between CV and bootstrap are\n\nfor CV, the whole data base is split once. Training and validation sets do not have common instances.\nFor bootstrap, the training and validation sets vary in size and composition at each step.\n\nThe fact that instances can be repeated in the training set can create a bias. A weighted estimate tackles this issue: the 632-rule.\n\n\nBootstrap: Procedure\nIn practice, one bootstrap step consists of\n\nSelect at random a training set by sampling from the original data base, with replacements, a training set of the same size.\nThe instances in the original data base that are not in the generated training set constitute the validation set.\nCompute the metric on the validation set after having trained the model with the training set: out-of-bag metric, \\(\\hat{S}_{oob}\\).\nCompute also the prediction of the training set itself: apparent metric, \\(\\hat{S}_{app}\\).\nEstimate the metric using the 632-rule: \\[\\hat{S} = 0.632 \\times \\hat{S}_{oob} + 0.368 \\times \\hat{S}_{app}.\\]\n\n\n\nOne bootstrap step\n\n\n\n\n\n\n\nBootstrap estimate\nThe previous bootstrap step is repeated a large number of times; typically 500 to 10‚Äô000, depending on the time it takes (computation power). The final estimate is the average of each \\(\\hat{S}\\). Standard deviation can be also computed.\n\n\n\nBootstrap samples\nHow many bootstrap samples (\\(R\\)) should be used?\n\nThere is no theoretical number but people consider that it should be at least large enough so that all the instances of the data base are at least once selected in a validation set.\nIn general, a large number, depending on the computational power and the data set size.\n\n\n\nPractical use\n\nWith small data base, prefer bootstrap.\nWith large data base, prefer CV.\nCV can be repeated: several CV pattern are simulated independently and aggregated at the end.\nThese procedures can be computed in parallel for better performance.\n\n\n\n\nSolving the overfitting problem\n\nSolving overfitting\nComparing apparent and test metrics allows to detect overfitting only. How can we improve the situation? A model that overfits the training data set is too flexible/complex. We can use the methods seen for each model to try to simplify them:\n\nLinear and logistic regression: variable selection and/or use \\(L_1\\) and \\(L_2\\) penalization.\nTrees: prune the tree.\nSVM: tune the cost parameter and/or the parameters of the kernel.\nNN: use \\(L_1\\) and \\(L_2\\) penalization and/or simplify the structure of the network.\n\n\n\nHyperparameters\nSome of these methods require to set parameters: penalization parameter, cost parameter, others. These are called hyperparameters.Hyperparameters are structural parameters or \"pseudo-parameters\" of the models. An informal definition is \"whatever cannot be optimized by the training algorithm\". To set these parameters is called tuning.\n\n\nTuning hyperparameters\nThe main technique consists of direct comparison (brute force):\n\nCreate a list of possible hyperparameter values.\nTrain all the models from the list.\nSelect the best in class according to the validation metric.\n\nThis approach can be done on\n\nA simple split: training and validation\nA repeated split: bootstrap or CV.\n\nThe choice of the hyperparameters must not be done on the test: a part of the information contained in the test set may leaks to the model choice and thus to the training. Avoid any information leakage: the test set must not be seen by the model, at any stage.\n\n\n\nSolving the unbalance problem\n\"Unbalanced classes\" refers to data set where the classes frequencies are unbalanced. E.g., doctor visit data have 4 times more ‚ÄúNo‚Äù than ‚ÄúYes‚Äù.\n\nNo   Yes \n4141 1049\nDuring the training, the model favors the predictions of the ‚ÄúNo‚Äù to reach a better accuracy.\n\nUnbalanced classes\nE.g., the logistic regression, gives:\nConfusion Matrix and Statistics\n[...]\n                Accuracy : 0.8091          \n[...]                                          \n            Sensitivity : 0.9771          \n            Specificity : 0.1435          \n[...]\n        Balanced Accuracy : 0.5603\nThis problem is detected with\n\nlow balanced accuracy\nlow sensitivity or specificity\n\nOne approach to solve this issue is to tune the probability threshold \\(\\lambda\\): \\[f(x_i;\\theta) = \\left\\{\n\\begin{array}{ll}\n1, & \\mbox{if } p(x_i;\\theta) > \\lambda,\\\\\n0, & \\mbox{if } p(x_i;\\theta) \\leq \\lambda.\n\\end{array}\n\\right.\\] To maximize the balanced accuracy, find the threshold \\(\\lambda\\) that maximizes the Youden‚Äôs J statistics: \\[J(\\lambda) = \\mbox{Spec}(\\lambda) + \\mbox{Sens}(\\lambda) - 1.\\] Note: this optimization is always at at cost of a lower accuracy.\n\n\nTuning probability threshold\nExample after optimization:\nConfusion Matrix and Statistics\n[...]                     \n                Accuracy : 0.6991          \n[...]\n            Sensitivity : 0.7186          \n            Specificity : 0.6220          \n[...]\n        Balanced Accuracy : 0.6703  \nSensitivity decreased, specificity increased. Overall BA increased (and accuracy decreased).\nThe probability threshold \\(\\lambda\\) is an hyperparameter. Use the splitting data method to avoid overfitting.\nAnother approach consists of balancing the classes in the training set. Two possible approaches:\n\nSub-sampling: Remove cases in the training set,\nRe-sampling: Add cases in the training set.\n\n\n\n\n\n\nIn details\n\nSub-sampling: take all the ‚ÄúYes‚Äù and a random subset of ‚ÄúNo‚Äù of the same size.\nRe-sampling: take all the ‚ÄúNo‚Äù and add replicates of the ‚ÄúYes‚Äù, chosen at random, until there are as many ‚ÄúYes‚Äù as ‚ÄúNo‚Äù.\n\nVariations are possible to increase or decrease the weight of each class, to take into account for the proportions.\nNote: the test set must NOT be balanced.\n\n\nClass balancing\nExamples, with sub-sampling\n                Accuracy : 0.7049          \n[...]\n            Sensitivity : 0.7331          \n            Specificity : 0.5933          \n[...]\n        Balanced Accuracy : 0.6632          \nand with re-sampling\n                Accuracy : 0.7097       \n[...]\n            Sensitivity : 0.7391          \n            Specificity : 0.5933          \n[...]\n        Balanced Accuracy : 0.6662          \nThe results are similar to tuning of \\(\\lambda\\).\n\n\nDiscussion on class balancing\n\nCase imbalance is a common problem in real life data (fraud detection, rare events, etc.).\nApplied on the data, class balancing is a possible solution to improve the fit.\nUnlike tuning of the probability threshold, class balancing can be apply to no-probability models and multi-class problems.\nThere is no guarantee. The final performances must be inspected.\nThe test set must remain unbalanced to be representative of future data.\nIf you use CV or bootstrap, the validation set must remain unbalanced (often not the case in built-in methods).\n\n\n\n\n\n\n\nFootnotes\n\n\nMild though.‚Ü©Ô∏é\nThere is no specific name to distinguish between the first ‚Äúbig‚Äù training set, and the second one, that is smaller.‚Ü©Ô∏é\nTechnically, this is the method used in 1-SE rule for trees.‚Ü©Ô∏é"
  },
  {
    "objectID": "lectures/06_Ensembles/ML_Ensemble.html",
    "href": "lectures/06_Ensembles/ML_Ensemble.html",
    "title": "Ensemble Methods",
    "section": "",
    "text": "In-class R script üìÑ\n\nEnsemble and bagging\n\nEnsemble\nUntil now we have tried to train the best learner among several candidates to solve a given task. On the contrary, an ensemble learner is made of several learners (so called base learners or sub-learners) that are combined for the prediction. Base learners are often simple learners, coming from an homogenous family of learners (e.g.¬†regression trees, regressions, etc.).\nBagging stands for Bootstrap AGGregatING. It is a way to build an ensemble learner from several versions of the same learner, trained on bootstrapped versions of the same training set:\n\nDraw several random samples (with replacement) from the training data set,\nFor each sample, construct a separate model and separate predictions for your test set.\nAveraged these predictions to create a final prediction value.\n\nThe objective of the intensive randomization is to built many small uncorrelated learners rather than one unique big learner.\n\n\n\n\n\nDoes it work and why? (in theory...)\n\nOverfitting: no model is trained on the whole training set. Thus, no model can overfit it.\nAccuracy: each resampled data set is similar to the training set. Thus, it can produce the same accuracy as the one trained on the whole training set.\n\nThus, in theory, there is no reason why each model has a lower accuracy than a model that would be trained on the whole training set. In addition, there is no reason why they, together, could overfit the training set since no model can overfit it. In practice, bagging is efficient when the features are unstable and very influential on the model (e.g., outliers). The properties of bagging come at the cost of extra complexity:\n\nA large number of models must be kept in memory to make the final prediction.\nEven if each model are interpretable (e.g., regressions), the final model is not interpretable.\n\nHowever, unlike with a single model, the increase in complexity is not at the cost of over-fitting.\n\n\nBagging\nWhen doesn‚Äôt it work / how to improve it?\n\nIf, bad luck, all the sub-models are similar \\(\\rightarrow\\) highly correlated.\nThe final model is no more than one of the sub-models.\nIt may overfit the training set and be poor in prediction (highly variable).\n\nThis may happen when the resampling has little effect. It is important that the sub-models are uncorrelated (or even independent) \\(\\rightarrow\\) random forest.\n\n\n\nRandom forest\nRandom forest is a popular ensemble method consisting of building lots of trees whose predictions will be averaged to produce one final prediction. In practice:\n\nDraw at random \\(M\\) subsets from the training set: same size as the original one, with replacement.\nOn each subset, grow a small tree with: at each split trial, a small amount of features is drawn at random: you have \\(M\\) small trees.\nThe RF prediction is the average of the \\(M\\) trees for regression, or the vote majority for classification.\n\n\nPoint 1. is bagging. Point 2 is an additional mixing technique to enhance sub-model non-correlation.\n\nWhy does that work?\nThe reasons are the same as for bagging. Furthermore, with the additional mixing of the features when constructing the trees:\n\neach tree remains unbiased (it has the same chance to get the best features).\nthe trees are even less correlated than with bagging alone. This brings more stability.\n\nThere exist several variants of RF. Below, some default settings of randomForest function:\n\nIf the re-sampling is with replacement, then the subset is of the same size as the training set.\nIf the re-sampling is without replacement, then the subset is of size 0.632 of the training set .\nThe number of features at each split is \\(\\sqrt{p}\\) for classification and \\(p/3\\) for regression.\n\n\n\nVariants\nMore hyperparameters of randomForest:\n\nNumber of trees: \\({\\tt ntrees} = 500\\)\nNumber of variables: \\({\\tt mtry}\\) is the number of features selected at random; \\(\\sqrt{p}\\) for classification and \\(p/3\\) for regression.\nSample size: sampsize same as the data set (training set) when replace=TRUE, and 0.632 of sampsize when replace=FALSE.\nMaximum tree size: maxnodes as large as possible, can be set.\nMaximum node size: nodesize 1 for classification and 5 for regression.\n\nThese hyperparameters can be tuned.\n\n\nFinal words\n\nRandom forest is an ensemble method that combines bagging and feature mixing.\nIt is often a good learner: good fit, not prompt to overfitting.\nIt may be not better than a single model (e.g., a tree).\nIt is not interpretable (no tree, coefficients, etc.).\n\n\n\n\nBoosting\n\nConcept\nUnlike bagging that creates models in parallel, boosting is an ensemble technique that creates models sequentially.\n\n\\(T\\) models \\(f_t\\) are trained one after the other, \\(t=1,\\ldots,T\\),\nThe final boosting model \\(F_T(x)\\) aggregates all the models \\[F_T(x) = \\sum_{t=1}^T \\alpha_t f_t(x),\\]\nAt each step \\(k\\), the next model \\(f_{k+1}\\) is trained by giving more importance to the instance that are incorrectly classified by the current boosting model \\(F_k=\\sum_{t=1}^k \\alpha_t f_t\\).\nThe model weight \\(\\alpha_t\\) is associated with the overall quality of \\(f_t\\) (the better the quality, the larger the weight).\n\n\n\nVariations\nThere exist lots of versions of the boosting:\n\nUsually, \\(f_t\\) is a weak learner: a simple learner, easy to train. It must be possible to assign a weight to each instance. E.g., a 1-split decision tree.\nIn theory, one could boost any model.\nThe number of models, \\(T\\) is arbitrary.\nThe computation of the instance weights at each step may vary.\nThe computation of the model weight at each step may vary.\nThere exist several formula of the total error of the model.\n\n\n\nAdaboost\nOne of the first boosting model1.\n\nBinary classification: \\(y\\) is \\(-1\\) or \\(1\\).\nExponential loss: \\[L(F_k) = \\sum_{i=1}^N e^{-y_i F_k(x_i)}\\]\nNext model: \\(f_{k+1}\\) minimize \\(\\sum_i 1_{\\{y_i \\neq f_{k+1}(x_i)\\}} e^{-y_i F_{k}(x_i)}\\).\nModel weight: \\[\\alpha_{k+1} = \\frac{1}{2} \\ln \\left(\\frac{\\sum_i 1_{\\{y_i \\neq f_{k+1}(x_i)\\}} e^{-y_i F_{k}(x_i)}}{\\sum_i 1_{\\{y_i = f_{k+1}(x_i)\\}} e^{-y_i F_{k}(x_i)}}\\right)\\]\n\n\n\nGradient boosting: a descent method\n\nCurrent apparent loss (general) \\[L(F_k) = \\sum_{i=1}^N L(y_i, F_k(x_i)).\\]\nCompute the pseudo-residual (gradient) \\[g_k = -\\frac{\\partial L(F_k)}{\\partial F_k} = -\\left\\{L_2(y_i, F_k(x_i))\\right\\}_{i=1,N},\\] where \\(L_2\\) is the derivative of \\(L\\) wrt its second argument.\nTrain the new weak learner \\(f_{k+1}\\) on \\(g_k\\) (i.e., on \\(\\{x_i, g_{k,i}\\}\\)).\nFind the model weight \\[\\alpha_{k+1} = \\arg\\min_{\\alpha} \\sum_{i=1}^N L(y_i, F_k(x_i) + \\alpha f_{k+1}(x_i)).\\]\nUpdate \\(F_{k+1} = F_k + \\alpha_{k+1} f_{k+1}\\).\n\n\n\nRegularization\nOverfitting can occur quickly with boosting. One way to prevent it is to use regularization: \\[F_{k+1} = F_{k} + \\nu \\alpha_{k+1} f_{k+1},\\] where \\(\\nu\\) is a learning rate, fixed by the user.\n\nIf \\(\\nu = 1\\) then there is no regularization,\nIf \\(\\nu = 0\\) then the model never learns.\n\nTo some extent this is equivalent to choose a low \\(T\\) (total number of models) to avoid overfitting.\nThe learning rate can be tuned.\n\n\nStochastic gradient boosting\nThe evaluation of the gradient can be costly (and useless) if \\(N\\) is large. One can act by random batches of data. At each step, only a proportion \\(0 < p \\leq 1\\) of the training set is used. The batch set is selected at random. Using random batches can also diminish the risk of overfitting since it takes more time for the model to see all the data set.\n\n\n\n\n\n\n\nBagging vs boosting\n\n\n\n\n\n\n\n\n\n\n\nFootnotes\n\n\nFreund, Y and Schapire, R E (1997). \"A decision-theoretic generalization of on-line learning and an application to boosting\". Journal of Computer and System Sciences. 55: 119-139.‚Ü©Ô∏é"
  },
  {
    "objectID": "lectures/07_InterpretableML/ML_Interp.html",
    "href": "lectures/07_InterpretableML/ML_Interp.html",
    "title": "Interpretable ML",
    "section": "",
    "text": "In-class R script üìÑ\n\nConcept\nIn ML, some models are interpretable (e.g., regression, CART) and some are not (e.g., SVM, NN). So called interpretable machine learning is a set of techniques aiming at\n\nDiscover / rank variables or features by their importance on the predicted outcome,\nAssociate variation of the important features with a direction of the outcome (positive vs negative association).\n\n\n\nVariable importance\n\nVariable importance\nVariable importance is a method that provides a measure of the importance of each feature for the model prediction. The variables importance for a trained model can be evaluated for one variable by:\n\nIn the test set, shuffle the instances on one variable,\nMake the predictions of this new test set,\nCompute the quality metric (RMSE, Accuracy, ...),\nCompare it to the quality metric on the original test set.\nIf the variable is important, giving to the model an incorrect values of this variable should decrease the quality metric,\n\nThis is repeated for each variable.\n\n\nIllustration\n\n\n\n\n\nIf predictions using the right-hand side data are the same as the left-hand side ones, then Var2 is not important. With the iris data and a CART, trained on \\(80\\%\\) of the data (test set at \\(20\\%\\)). The tree is\n\n\n\n\n\nWe see directly that Petal length and Petal width are the only two important features (except in case of missing data).\nTo measure this, in the test set, we shuffle Petal length. The accuracy of the shuffled test set is much lower than the original one (left: original test set; right: modified test set). This confirms that Petal length is essential for a good prediction of the species by the model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the other hand, the Sepal length does not appear on the graph. And, indeed, it is not important for the prediction as shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor tree, the importance of a variable can be read to some extend directly on the graph (if it is not too large). For a model like SVM, it is not. But we can still make the same analysis by shuffling the instances for a given feature (top: original; left: Petal length; right: Sepal length).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeveral versions\nThe variable importance presented previously is called model-agnostic: it is independent of the model type.\nThere exist several ways of it:\n\nModification of the variable: shuffling is one possibility. Perturbation is another (add a random noise). Simulation from another distribution.\nModification of the score: accuracy is one possibility. Specificity/Sensitivity/Bal. accuracy, Entropy... The importance can be linked to the prediction of one level only.\nRegression: use RMSE, MAE, etc.\n\nThere exist also model-specific approaches that are specific to the model that is used.\nIn R,\n\niml allows to compute model-agnostic variable importance, with any loss function.\ncaret allows to compute mainly model-specific variable importance. Also model-agnostic type (limited choice of loss functions).\n\nVI on SVM estimated on cross-entropy, repeated 100 times.\nlibrary(iml)\niris.iml <- Predictor$new(iris.svm, data = iris.te[,-5], y = iris.te[,5])\niris.imp <- FeatureImp$new(iris.iml, loss = \"ce\", n.repetitions = 100)\nplot(iris.imp)\n\n\n\n\n\n\n\nExample: iml\nFor each feature: The difference in entropy obtained between the original data set and the shuffled data set is computed. This is repeated 100 times. These differences are averaged and shown on the plot.\nVI on SVM estimated on AUC.\nlibrary(caret)\ntrctrl <- trainControl(method = \"repeatedcv\", repeats= 3, number=5)\niris.caret <- caret::train(form=Species ~., data = iris.tr, \n                    method = \"svmLinear\", \n                    trControl=trctrl)\nplot(varImp(iris.caret))\n\n\n\n\n\n\n\nExample: caret and VarImp\nSince no specific method was developped for SVM, a filter-based method is implemented. From the help of VarImp:\nFor classification, [...] the area under the ROC curve [...] is used \nas the measure of variable importance. For multi-class outcomes, the problem \nis decomposed into all pair-wise problems and the area under the curve is\ncalculated for each class pair (i.e class 1 vs. class 2, class 2 vs. \nclass 3 etc.). For a specific class, the maximum area under the \ncurve across the relevant pair-wise AUC's is used as the variable importance \nmeasure.\nEven for one model, there may exist lots of implementations. Below, we give one example: VarImp for CART. CART is recognized as Recursive Partitioning (from rpart). From the help of VarImp:\nThe reduction in the loss function (e.g. mean squared error) attributed \nto each variable at each split is tabulated and the sum is returned. \n\nAt each split, the reduction of loss due to the best split on each variable is extracted.\nThese loss reductions are summed.\n\n(No resampling or measure of error on the VI estimate)\n\n\nModel-specific VI\n\n\n\n\n\nWhatever implementation you use, VI aims at the same objective: quantify the importance of variables for the predictions of the model. Some limitations are:\n\nInstability: the estimation of the importance can be unstable due to the random perturbation. Resampling can help (i.e., repeat the shufling several times).\nInteractions: sometimes it is the combination of two features that makes a good prediction (typ. trees). Variable importance cannot see this.\n\nThe variable importance is only seen with the eyes of the model. If the model is doing a poor job (e.g., low accuracy) then the variable importance analysis is of low quality.\n\n\n\nPartial Dependence Plots\nVariable important allows to inspect how much a variable is important in the construction of the prediction by a model.\nPartial dependence plots (PDP) show in which direction is the association between a feature \\(x\\) and the prediction of \\(y\\).\n\nPartial Dependence Plot\nMathematically, for the feature \\(x_s\\), let \\(f(x_s, x_{-s})\\) be the prediction of \\(y\\) by the model \\(f\\), then the PD-function of \\(X_s\\) at \\(x_s\\) is \\[F_{X_s}(x_s) = \\int f(x_s, x_{-s}) p(x_{-s}) dx_{-s} = E[f(x_s, X_{-s})],\\] where \\(p(x_{-s})\\) is the distribution of \\(x_{-s}\\). \\(F_{X_s}(x_s)\\) is the expected value over \\(x_{-s}\\) of the prediction when \\(x_s\\) is fixed (not conditional).\nThe estimation of the expectation above is obtained by averaging on the training set: \\[\\hat{F}_{X_s}(x_s) = \\frac{1}{N}\\sum_{i=1}^N f(x_s, x_{-s}^{(i)}).\\]\n\n\nEstimation\n\n\n\n\n\n\n\nInterpretation\n\nLeft: PDP increases. The prediction increases in average when the feature increases ; it is a positive association.\nRight: PDP is stable. The prediction does not change in average when the feature value changes; there is no association.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDP allows to explore the link between response and any feature with the eyes of the model.\nPDP can be also used to see the feature importance with the amplitude of the graph (the larger the more important): \\[\\vert \\max_{x_s} F_{X_s}(x_s) - \\min F_{X_s}(x_s) \\vert.\\] It is richer than VI but also longer to run.\nPDP can be made multivariate with \\(X_s\\) being several features (usually, max. 2).\nBy averaging, PDP ignores any interaction between \\(x_S\\) and \\(x_{-S}\\).\n\n\n\n\nLIME\n\nLIME\nLIME stands for Local interpretable model-agnostic explanations. The paradigm is that a large ML model can be locally approximated by an interpretable (smaller) model, a surrogate model.\n\nSelect a instance of interest.\nBuild instances close to it and predict them.\nFit a surrogate model.\nInterpret it coefficients.\n\n\n\nIllustration in 2 dimensions\nOn iris data, a random forest predicts setosa/virginica/versicolor from length and width (sum of the petal/sepal features). It is globally complex (left graph). Select a point of interest (blue circle). Around it (right plot), a logistic regression could be used. If you fit it on the black dots, you will find a positive association between large width and \"being green\", and no association with length.\n\n\n\n\n\n\n\nTechnical details\nThere is no unique way to build/interpret surrogate models. Often,\n\nSurrogate models are linear models with variable selection.\nThe number of variables is fixed by the user.\nCategorical features are transformed to a dummy variable: 1 if it is the same as the point of interest, 0 otherwise.\nNumerical features are binned and dummy variables are created: 1 if it is the same bin as the point of interest, 0 otherwise.\nWeights are assigned to sampled data according to their proximity with the point of interest (Gower‚Äôs).\n\n\n\nExample\nWith the bank data (see R code of examples). Three cases: Label 0 with high prob., label 1 with high prob., Label 1 with prob. \\(\\approx 50\\%\\).\n\n\n\n\n\n\n\nInterpretation for duration: no local behavior discovered\n\nAround Case 1: Duration being lower than 759 supports a label 0. This means that a low duration supports label 0 and, consequently, a larger duration would support label 1.\nAround Case 2: Duration being greater than 759 (lower than 1514) supports label 1. This means that a large duration supports label 1 and, consequently, a lower duration would support label 0.\nAround case 3: same as Case 2.\n\nRegarding duration, case 1 and case 2 coincide. They also coincide with the general previous findings that duration is an important factor, with prob. of label 1 increasing with it. The exploration of this two cases did not reveal any local behavior linked to duration. A similar analysis can be done for contact, housing, etc.\n\n\nInterpretation for campaign: local behavior\n\nAround Case 1: Campaign being low supports label 0.\nAround Case 2: Campaign being low contradicts label 1. This is in line with Case 1.\nAround Case 3: Campaign being low supports label 1. This is in contradiction with Cases 1 and 2.\n\nThis example shows that a global behavior (campaign is positively associated with label 1) can change locally. The fact that Case 3 has a probability around 0.5 makes it interesting. Limitation: here the effect of Campaign is so small that a good explanation is that it has almost no effect everywhere. That was just a toy example.\n\n\nDiscussion\n\nThe choice of the point of interest can be anything, even non-observed instances. Average, extreme, and change points are often of interest.\nThis method interprets locally the link between the features and the response, again, with the eyes of the model.\nThe surrogate models (as well as the global model) cannot support rigorous causal analysis. We can discover only association here.\nLike often, this method can be unstable (implementation, choice of the model, etc.). Try several combinations and be cautious with conclusions."
  },
  {
    "objectID": "lectures/08_UnsupervisedLearning/080_Introduction/ML_UnsupIntro.html",
    "href": "lectures/08_UnsupervisedLearning/080_Introduction/ML_UnsupIntro.html",
    "title": "Introduction to Unsupervised Learning",
    "section": "",
    "text": "Supervised vs unsupervised learning\nSupervised learning aims to predict an outcome \\(y\\) from features \\(x\\). The quality of the model can be inspected by comparing the predictions to the true outcomes. Unsupervised learning aims to analyze the link between the features. There is no ‚Äúsupervisor‚Äù. Unsupervised methods can be separated in two main ones:\n\nClustering: group instances according to their similarities across the features.\nDimension reduction: link the features according to their similarities across the instances, find commonalities, and combine the features in fewer dimensions."
  },
  {
    "objectID": "lectures/08_UnsupervisedLearning/081_Clustering/ML_Clustering.html",
    "href": "lectures/08_UnsupervisedLearning/081_Clustering/ML_Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "In-class R script üìÑ\n\nConcepts\n\nClusters\nA cluster is a group of instances that share some common characteristics measured by the features. They are similar. Unlike classification (supervised learning) where groups are already defined, clustering aims at finding groups that do not exist a priori.\n\n\nHierarchical clustering and partitioning\nTwo different approaches:\n\nHierarchical clustering: methods based on a dissimilarity matrix (hierarchical clustering).\nPartitioning: methods based on the value of the features (K-Means, PAM).\n\n\n\n\nHierarchical clustering\n\nConcept\nHierarchical clustering algorithms\n\nTake a dissimilarity matrix as input: the features are used to compute dissimilarities between instances. The feature values are not by the algorithm strickly speaking.\nCan be of two types: agglomerative (e.g., AGNES) or divisive (not treated).\n\nIn agglomerative algorithm, once two instances are clustered, they cannot be separated.\n\n\nAGNES\nAccronym for AGglomerative NESting: a sequential algorithm:\n\nTo start each instance is a cluster in itself.\nMerge the two closest clusters.\nRepeat 2 until only one cluster remains.\n\nTo merge two clusters (2), we need a linkage method. To link two clusters, we need dissimilarities.\n\n\nDissimilarity\nA dissimilarity between two instances with features \\(x\\) and \\(x'\\) is a function \\(d(\\cdot, \\cdot)\\) such that\n\nNon-negativity: \\(d(x,x')\\geq 0\\),\nNullity: \\(d(x,x') = 0\\) iff \\(x=x'\\),\nSymmetry: \\(d(x,x') = d(x',x)\\),\n\nIf further it satisfies the triangle inequality, \\[d(x, x') + d(x', x'') \\geq d(x, x''),\\] then it is called a distance\n\n\nFor numerical features\nA general distance: Minkowski, \\(q \\geq 1\\) \\[d(x,x') = \\left(\\sum_{j=1}^p |x_j - x'_j|^q\\right)^{1/q}\\] Special cases:\n\nEuclidean for \\(q=2\\) (the most used), \\[d(x,x') = \\sqrt{\\sum_{j=1}^p (x_{j} - x'_{j})^2}\\]\nManhattan for \\(q=1\\), \\[d(x,x') = \\sum_{j=1}^p \\vert x_j - x'_j \\vert\\]\nMaximum distance \\(q=\\infty\\), \\[d(x,x') = \\max_{j=1,\\ldots p} \\vert x_j - x'_j\\vert\\]\n\n\n\nNumerical features\nExample on the iris data. The Euclidean distance between instances 1 and 2 is \\(0.54\\)\n> iris[1:2,-5]\n    Sepal.Length Sepal.Width Petal.Length Petal.Width \n1          5.1         3.5          1.4         0.2 \n2          4.9         3.0          1.4         0.2 \n> dist(iris[1:2, -5], p=2)\n    0.5385165\nIn detail, \\[\\sqrt{(5.1-4.9)^2+(3.5-3.0)^2+(1.4-1.4)^2+(0.2-0.2)^2}=0.54\\]\n\n\nCategorical and mixed features types\nWhen categorical features are present in the data set, the Gower index can be used. It combines dissimilarities to build a global one: \\[d_{ij} = \\frac{\\sum_{k=1}^p w_k d_{ij,k}}{\\sum_{k=1}^p w_k}\\] is the dissimilarity between instances \\(i\\) and \\(j\\) over the \\(p\\) features. The weight \\(w_k\\) are always 1, except if \\(x_ik\\) or \\(x_jk\\) is missing, in which case it is \\(0\\). The dissimilarity \\(d_{ij,k}\\) depends on the type of the feature \\(x_k\\):\n\nNumerical: \\[d_{ij,k} = |x_{ik} - x_{jk}|/r_k,\\] where \\(r_k\\) is the range of variable \\(k\\),\nCategorical: \\(d_{ij,k} = 1\\) if \\(x_{ik}\\neq x_{jk}\\), \\(d_{ij,k} = 0\\) if \\(x_{ik}= x_{jk}\\).\n\n\n\nMixed features types\nExample:\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Color    Species\n1          5.1         3.5          1.4         0.2    mauve     setosa\n2          4.9         3.0          1.4         0.2   violet     setosa\n3          4.7         3.2          1.3         0.2   violet  virginica\nThe ranges are respectively \\(3.6,2.4,5.9,2.4\\) (from the full data set). \\[\\begin{aligned}\nd_{12} &=& \\frac{1}{6}\\left(\\frac{|5.1-4.9|}{3.6} + \\frac{|3.5-3.0|}{2.4} + \\frac{|1.4-1.4|}{5.9} + \\frac{|0.2-0.2|}{2.4} + 1 + 0\\right) = 0.2106.\\\\\nd_{13} &=& 0.3755\\\\\nd_{23} &=& 0.1926\\end{aligned}\\]\n\n\nLinkage\nOnce the matrix of dissimilarities between all the instances is computed, we use linkage to merge the two clusters:\n\nSingle linkage: For all pairs of clusters \\(A\\) and \\(B\\), compute the smallest dissimilarity between each pair of cluster members, \\[d(A,B) = \\min \\{d(x,x'): x\\in A,\\; x' \\in B\\}\\] Link \\(A\\) and \\(B\\) with the smallest \\(d(A,B)\\).\nComplete linkage: For all pairs of clusters \\(A\\) and \\(B\\), compute the largest dissimilarity between each pair of cluster members, \\[d(A,B) = \\max \\{d(x, x'): x\\in A,\\; x' \\in B\\}\\] Link \\(A\\) and \\(B\\) with the smallest \\(d(A,B)\\).\nWard‚Äôs method: merges the two clusters \\(A\\) and \\(B\\) such that, after the merging, the total within variance is minimum1.\n\n\n\nDendrogram\nThe dendrogram is a graphical representation of the hierarchical clustering. It shows which clusters are grouped at each step \\(k\\), together with the dissimilarity on the \\(y\\)-axis ate which they have been clustered.\nItalian cities: Bari, Firenze, Napoli, Roma, Torino.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource: https://home.deib.polimi.it/matteucc/Clustering/tutorial_html/hierarchical.html\nAGNES is agglomerating because it starts from individual points and run up to a unique cluster, merging clusters one after the other. It is called \"hierarchical\" because when an instance is attached to a group, it cannot be separated from it anymore.\nThere also exist divisive hierarchical clustering, DIANA, for DIvise ANAlysis. It starts from one unique cluster with all the instances, and splits clusters one after the other, going down to one instance per cluster. Divise clustering is considered complex because at each step there exist a lot of possibilities to split a cluster.\n\n\n\nPartitioning methods\n\nPartitioning methods\nUnlike the hierarchical clustering, partitioning methods apply for a given number of clusters. From an initial assignment of instances to \\(k\\) clusters, the method allocates the instances to clusters such that a given criterion is optimized. Two popular methods are\n\nThe K-means.\nThe Partitioning Around the Medoid (PAM).\n\nFor \\(k\\) clusters:\n\nThe initial clustering is random: assign each instance to one cluster at random.\nCompute the centers of the clusters.\nEach instance is then re-assigned to the cluster with the closest center.\nStep 2. and 3. are repeated until a convergence criterion is satisfied.\n\nThe K-means is suitable for numerical features only. The Euclidean distance is often used. Below is an illustration on simulated data, with \\(k=2\\), and the data (\\(p=2\\))\n\n\n\n\n\nFirst step, each instance is randomly assigned (circles and triangles). The centers are computed (dots).\n\n\n\n\n\nNext step, instances are re-assigned to the closest center. Then, new centers are computed. The clustering is already finished.\n\n\n\n\n\n\n\nK-means\nIllustration for \\(k=3\\) clusters.\n\n\n\n\n\n\n\nPartitioning Around the Medoid\nPAM extends the K-mean to features of mixed types (numerical and categorical) by extending the definition of the center of the cluster to a medoid. The medoid of cluster is the most representative instance belonging to it. The medoid of a cluster \\(A\\) is the instance with smallest sum of dissimilarities with the other instances in the cluster \\(A\\): \\[m_A = \\arg \\min_{x \\in A} \\sum_{y\\in A; y\\neq x} d_{xy}\\] Unlike for the K-means, where the center is computed by averaging the features values (and thus may not be an instance of the data base), the medoid is an instance of its cluster. Therefore, PAM can be used for numerical and categorical features.\nWith the Italian cities, consider the cluster {BA, NA, RM}. Then the medoƒ±Ãàd is NA. Indeed, the sum of dissimilarities are\n\nBA: \\(d_{BA,NA}+d_{BA,RM} = 255+412=667\\)\nNA: \\(d_{NA,BA}+d_{NA,RM} = 255+219=474\\)\nRM: \\(d_{RM,BA}+d_{RM,NA} = 212+219=631\\)\n\nThus NA is the most representative city of this cluster. Note that the medoƒ±Ãàd of cluster {MI, TO} is either MI or TO (at random).\n\n\n\nNumber of clusters\n\nChoice or selection?\nThe number of clusters is often an arbitrary choice guided by the domain and the business application. This approach should be preferred if possible. It is also possible to select of the number of clusters with an index. There are a lot of such index2. In this course, we see\n\nThe total within-cluster sum of squares (TWSS),\nThe silhouette.\n\nIn practice, the index is computed for a clustering with \\(1\\), \\(2\\), \\(3\\), etc., clusters and the best choice is made.\n\n\nTotal within-cluster sum of squares\nThe sum of squares within cluster \\(C\\), whose center (mean or medoid) is \\(x_C\\), is the \\[\\quad W_C=\\sum_{i \\in C} \\Vert x_i - x_C\\Vert^2 = \\sum_{i \\in C}\\sum_{j=1}^p (x_{ij} - x_{Cj})^2.\\] The TWSS is the sum of all \\(W_k\\) over the \\(K\\) clusters \\(C_1,\\ldots,C_K\\), \\[\\sum_{k=1}^K W_{C_k}\\] The Each time a cluster is created (\\(K \\rightarrow K+1\\)), TWSS diminishes. The number of clusters is selected at an elbow on the graph. Below is a plot of clustering on data with TWSS (dendrogram with complete linkage):\n\n\n\n\n\n\\(k=3\\) is a good choice here.\nThe silhouette measures how \"well\" an instance is within its cluster. Consider an instance \\(x\\) assigned to cluster \\(C\\). Let \\(d(x,A)\\) be the dissimilarity from \\(x\\) to any cluster \\(A\\): the average dissimilarity between \\(x\\) and all the instances in cluster \\(A\\): \\[d(x,A) = \\frac{1}{|A|} \\sum_{y\\in A} d_{xy}.\\] In particular, \\(d(x, C)\\) is the dissimilarity from \\(x\\) to its cluster. A small \\(d(x,C)\\) indicates a good clustering of \\(x\\) within its cluster \\(C\\).\nNow, consider \\(d(x,\\bar{C})\\), the smallest average dissimilarities from \\(x\\) to all the clusters to which it is not assigned (all but \\(C\\)): \\[d(x,\\bar{C}) = \\min_{A\\neq C} d(x, A)\\] This measures how \\(x\\) is separated from the other clusters. A large \\(d(x,\\bar{C})\\) indicates a good separation of \\(x\\) from all the other clusters (i.e., the ones in which \\(x\\) is not).\n\n\nSilhouette\nThe silhouette of \\(x\\) combines\n\n\\(d(x, C)\\), how well \\(x\\) is integrated to its cluster \\(C\\), and\n\\(d(x,\\bar{C})\\), how well \\(x\\) is separated from the other clusters.\n\nThe silhouette of \\(x\\) is \\[s_x=\\frac{d(x,\\bar{C})-d(x,C)}{\\max\\left\\{d(x,\\bar{C}),d(x,C)\\right\\}}\\] The denominator ensures that \\(-1\\leq s_x\\leq 1\\). The larger \\(s_x\\), the better the clustering of \\(x\\) in \\(C\\).\n\n\nAverage silhouette\nThe average silhouette of all instances is taken as a measure of the goodness of the clustering: \\[s = \\frac{1}{n}\\sum_{i=1}^n s_{x_i}.\\] The larger the average silhouette \\(s\\), the better the clustering overall. The number of clusters \\(k\\) should maximize the average silhouette. The average silhouette should not be interpreted for itself. Now, we show a graph of clustering on data with average silhouette:\n\n\n\n\n\n\\(k=8\\) is a good choice here.\nA detailed analysis of the clustering can be obtained using the silhouette plot shows: the silhouettes of all the instances.\n\n\n\n\n\nClusters 1 and 3 are well formed, Clusters 2 and 3 are less homogeneous.\n\n\nSilhouette plot\nPolicemen - PAM \\(k=4\\)\n\n\n\n\n\n\n\nDivisive hierarchical clustering\nAGNES is \"agglomerating\" because it starts from individual points and run up to a unique cluster, merging clusters one after the other. It is called \"hierarchical\" because when an instance is attached to a group, it cannot be separated from it anymore. \"Divisive\" hierarchical clustering also exists: DIANA for DIvise ANAlysis. It starts from one unique cluster with all the instances, and splits clusters one after the other, going down to one instance per cluster.\n\n\n\nInterpretation\n\nInterpretation\nWhat are the features behind: what feature/combination of features makes that an individual belong to a cluster. A usual method consists of representing each feature against the clusters.\nExample: 170 Swiss policemen recorded their emotional state in time using a questionnaire with ordinal scales (1 to 6). The features are\n\nmood,\nstress,\nemotion,\npressure,\nsocial support,\nsituation familiarity,\nmaterial support.\n\nThere is nothing to predict: the interest is to discover patterns in the data. Below is the scatter plots of the features (jittering points for more readability).\n\n\n\n\n\n4 clusters of policemen were formed using AGNES. They can be analyzed by the feature values within each cluster.\n\n\n\n\n\n\n\nExample\nSome possible conclusions are\n\nPolicemen in Cluster 3 have large mood, stress, and emotion values.\nPressure characterizes cluster 1 (low) versus clusters 2 to 4 (medium).\nMood and stress are associated because clusters 1 to 4 have the same levels of these two features.\n\n\n\n\n\n\n\nFootnotes\n\n\nOther criteria are possible. Several algorithms exists (e.g., Ward 1963, Murtagh and Legendre 2014).‚Ü©Ô∏é\nThere are 30 index available in the function NbClust in R.‚Ü©Ô∏é"
  },
  {
    "objectID": "lectures/08_UnsupervisedLearning/082_DimensionReduction/ML_DimRed.html",
    "href": "lectures/08_UnsupervisedLearning/082_DimensionReduction/ML_DimRed.html",
    "title": "Dimension Reduction",
    "section": "",
    "text": "In-class R script üìÑ\n\nDimension reduction\nFor unsupervised learning, data representation and simplification is an important task. A natural objective is to represent the data on a graph: simplify \\(p\\) features to 2 dimensions and plot on \\(x\\)- and \\(y\\)-axes.\nExample: iris data has \\(4\\) features (species not used). The data cannot be represented on a single graph.\n\n\n\n\n\nObservation: Petal.Length and Petal.Width are highly correlated: if we know one, we can predict the other one.\nTo simplify the representation, we do not need these two features: replace them by their average Petal.WL = (Petal.W+Petal.L)/2:\n\n\n\n\n\nSame observation for Petal.WL and Sepal.Length...\nIn summary, two correlated features can be replaced by one combination without keeping the information.\nThe resulting representation shows as much variance as possible: theinformation is kept.\nThis is the principle of Principal Component Analysis (PCA).\n\n\nPrincipal Components Analysis\n\nPCA\nThe PCA is a method that looks for dimensions that are linear combinations of the \\(p\\) features: \\[a_1 x_1 + a_2 x_2 + \\cdots + a_p x_p.\\] These linear combinations are called the principal components. There are \\(p\\) principal components.\nTo find the first component (i.e.¬†coefficients \\(a\\)), one should maximize the variance along it. That is, \\(a\\) should be the solution to \\[\\begin{aligned}\n\\max_a &&Var (a_1 x_{1} + \\cdots + a_p x_{p})\\\\\ns.t.&& \\sum_j a_j^2 = 1\\end{aligned}\\] where the variance is computed on the data set. The constraint on \\(a\\) is here because \\(a\\) is only indicating a direction and should therefore be scaled to 1. As an example, let‚Äôs look at a toy case where we only have 2 features \\(x_1\\) and \\(x_2\\) showed below. We want to represent the data in one dimension, that is, along a line (dimension reduction). The dashed line shows more variability of the data and is a better principal component than the solid line.\n\n\n\n\n\n\n\nPCA: the first component\nWhen extended to \\(p\\geq 2\\) features, the principle remains the same. For a choice of \\(a\\) with \\(\\sum a_j^2=1\\),\n\n\\(z_i = a_1 x_{i1} + \\cdots + a_p x_{ip}\\) are computed on the data set (\\(i=1,\\ldots,n\\)),\nthe variance of the \\(z_i\\) is computed.\n\nThen this variance is maximized by changing \\(a\\). The maximum value gives \\(a^{(1)}\\), the first principle component.\n\n\nPCA: the second component, and third, etc.\nThe second component \\(a^{(2)}\\) is obtained with the same principle and the extra constraint to be orthogonal to the first one: \\[\\sum_{j=1}^p a^{(1)}_j a^{(2)}_j = 0.\\] The procedure is repeated until \\(p\\) principal components are found (each one is orthogonal to all the previous ones).\nNote: the result can be easily obtained by computing the spectral decomposition of the variance matrix of the data.\n\n\nPCA: scaling\nBefore computing the PCA, the data can be standardized. Like any standardization, this is a choice of the user, which depends strongly on the application and on the scale (units) of the data. When the data are first standardized, the spectral decomposition is made on the correlation matrix of the data.\n\n\nPCA: projection\nFor each PC \\(j\\), we have the corresponding projections of the data \\(x_i\\) \\[z_i^{(j)} = a^{(j)}_1 x_{i1} + \\cdots + a^{(j)}_p x_{ip}.\\] We thus have a new data set \\(z\\) whose column are the projection of the features \\(x\\) on the PCA‚Äôs. These new features \\(z_1, \\ldots, z_p\\) can be used\n\nfor data representation (dimension reduction),\nto describe the dependence between the features (factor analysis).\n\n\n\nPCA: variance proportion\nBy construction, the first principal component \\(z_1\\) has larger variance than \\(z_2\\) and so on. Also, by construction, the total of the variance is preserved: \\[\\sum_{j=1}^p var(x_j) = \\sum_{j=1}^p var(z_j).\\] The proportion of the total variance explained by the PC \\(z_j\\) is thus \\[var(z_j)/\\sum_{j'=1}^p var(z_{j'}).\\] It represents how well the data are represented on the component \\(z_j\\).\n\n\n\n\n\n\n\nPCA: example\n> iris.pca <- PCA(iris[,-5], graph=FALSE) \n> summary(iris.pca)\n\nCall:\nPCA(X = iris[, -5], graph = FALSE) \n\n\nEigenvalues\n                        Dim.1   Dim.2   Dim.3   Dim.4\nVariance               2.918   0.914   0.147   0.021\n% of var.             72.962  22.851   3.669   0.518\nCumulative % of var.  72.962  95.813  99.482 100.000\nTogether, PC1 and PC2 explain \\(95.8\\%\\) of the variation of the data. The scatter plot matrix shows that it is a good representation of the data with only two dimensions (PC\\(_1\\), PC\\(_2\\)).\nThe correlation between the PC and the features \\(x\\) can be computed to see how is PC is correlated to each features.\n\n\n\n\n\n\n\nPCA: the circle of correlations\nWe see that\n\nPC\\(_1\\) is positively and strongly correlated with petal length, petal width, and sepal length. This component summarizes these 3 features: the larger PC\\(_1\\), the larger these 3 features.\nPC\\(_2\\) is (negatively) correlated with sepal width. The larger PC\\(_2\\) the smaller this feature.\nPC\\(_1\\) explains \\(73\\%\\) of the total data variation. PC\\(_2\\) explains \\(23\\%\\) of it.\n\nWith one graph, we know\n\nwhich features are correlated/independent\nhow to summarize the data into two dimensions.\n\n\n\nPCA: the \\(cos^2\\)\nThe circle of correlations relates the dimensions and the features. Another view is the \\(cos^2\\) graph. It is interpreted as the quality of the representation of the feature by the dimension. Of course this is intimately related to the correlations.\n\n\n\n\n\nThe biplot shows a map of the individuals in the dimensions and adds the circle of correlations.\n\n\n\n\n\n\n\nPCA: the biplot\nFor example, we can conclude that\n\ninstance 61 has a sepal width smaller than the average (large PC\\(_2\\)) and an average PC\\(_1\\) (which indicates an average petal length, width and sepal length).\ninstance 119 has an average sepal width but a large PC\\(_1\\), i.e. petal length and width and sepal length.\nTwo clusters can be observed and are well separated by PC\\(_1\\) (in fact these clusters correspond to species here).\n\n\n\nPCA: number of components\nHere, two components explain more than \\(95\\%\\) of the data variability. Sometimes, more components are needed. One way to set the number of components is to target the proportion of explained variance: often between \\(75\\%\\) and \\(95\\%\\).\nIf the features are independent, this number is likely to be large. If the features are correlated, this number will be smaller. The screeplot may help.\n\n\nPCA: the screeplot\n\n\n\n\n\n\n\nPCA and machine learning\nIn the context of machine learning, PCA is often used\n\nTo inspect the data, find/explain clusters, find dependence between the features. PCA can be used for EDA.\nTo diminish the number of features when there are too many: dimension reduction \\(=>\\) only keep few first PC.\n\n\n\nCategorical data\nPCA can only be performed on numerical features. When categorical features are also included in the analysis,\n\nfor ordinal data, quick and dirty solution: modalities can be mapped to numbers (\\(1,2,\\ldots\\)) respecting their order,\nfor nominal data: there is no correct solution; especially replacing by numbers is incorrect.\n\nWith only categorical data, (Multiple) Correspondence Analysis is a solution. And for mixed data type (categorical and numerical), Factor Analysis of Mixed Data (FAMD) is a solution. However, they are not adapted to large data set.\n\n\n\nAuto-encoders\n\nPrinciple\nPCA is a \"linear\" technique, based on the explanation of the correlation between the features. Auto-encoder are neural network. The idea is to train a model that recovers an instance with an intermediate layer of lower dimension than the number of features.\n\n\n\n\n\n\n\n\nencoder + decoder = autoencoder\n\nThe input and the output (response) are the same instance (\\(X\\)), of dimension \\(p\\).\nThe intermediate layer (at least one) is of dimension \\(k<p\\).\nThe model is trained to recover \\(X\\) ate the end.\n\nIf, after traning, the model can recover \\(X\\) from \\(X\\), then it can recover \\(X\\) from its image in the intermediate layer \\(h(X)\\). Thus, only \\(p\\) dimensions would be needed to recover \\(X\\).\nThus,\n\nthe left part of the NN encodes \\(X\\) in its lower-dimension version \\(h(X)\\)\nthe right part of the NN decodes \\(h(X)\\) in an output \\(g(h(X))\\), hopefully close to the original image \\(X\\).\n\nThe better this final image the better the encoding/decoding: \\[g(h(X)) \\stackrel{?}{\\approx} X.\\]\n\n\nAutoencoder vs PCA\nIn ML, often autoencoder are used to\n\nReduce the noise in the data (smoothing)\nReduce the memory needed (compressing)\nRepresent the data (dimension reduction)\n\nUnlike PCA, autoencoder do not provide interpretation of the dimensions, which dimension is the most important, etc. On the other hand, autoencoders can produce better final representation of the data: the recovery of \\(X\\) with \\(k\\) components is better than with PCA. Like PCA make two-dimensional plots to discover pattern. Below is autoencoder with 3-node intermediate-layer:\n\n\n\n\n\n\n\nInterpretability\nRelate each component of \\(h(X)\\) to each component of \\(X\\) using variable importance (or another technique). Below, Node 1 to 3 (left to right)."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Books\nThere are no mandatory readings for this course, except our MLBA website; however, here are some of the books that inspired our course. Except for one book, the rest are all freely available:\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition. Springer Science & Business Media. Available at: Trevor Hastie‚Äôs website\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Springer Science & Business Media. Available at: statlearning.com\nKuhn, M. and Johnson, K. (2013) Applied Predictive Modeling. Springer Science & Business Media. Available at1: Springer\n\n\nBoehmke, B. and Greenwell, B. (2020) Hands-On Machine Learning with R. Taylor & Francis. Available at: HOML (retrieved the 2023-01-10)\nMolnar C. (2023) Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. Available at: Interpretable Machine Learning (retrieved the 2023-08-10)\n\n\n\nLicense\nThis course is provided under ¬†which is the Creative Commons Attribution-ShareAlike 4.0 International license.\n\n\nAcknowledgements\nThe design of the MLBA course website was adapted and inspired from the following sources:\n\nDr.¬†Mine √áetinkaya-Rundel STA 101 website\nDr.¬†Tomas Mas√°k, Dr.¬†Linda Mhalla and Charles Dufour MATH-517 website\n\n\n\n\n\n\nFootnotes\n\n\nThis is the only book that is not freely available and must be purchased.‚Ü©Ô∏é"
  },
  {
    "objectID": "resources/beginners_r.html",
    "href": "resources/beginners_r.html",
    "title": "Beginners in R",
    "section": "",
    "text": "R and RStudio installation\nüé¨ UNIL course Quantitative Methods for Management (QMM)\n\n\nIntroduction to R\nüé¨ Intro to R by the the QMM course\n\n\nswirl\n</> R package for learning to code directly within the program\n\n\nOther resources\nüîó http://www.r-tutor.com/r-introduction\nüìÑ https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf\nüìñ https://intro2r.com/"
  },
  {
    "objectID": "resources/cheatsheets.html",
    "href": "resources/cheatsheets.html",
    "title": "Coding Cheatsheets & Books",
    "section": "",
    "text": "Python\nYou can find an extensive cheatsheet for Python in addition to a book for learning python:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\nThe following cheatsheets come from https://posit.co/resources/cheatsheets. We haven‚Äôt covered every function and functionality listed on them, but you might still find them useful as references."
  },
  {
    "objectID": "resources/data_acquisition/data_sources.html",
    "href": "resources/data_acquisition/data_sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "Below are some links to data platform that can help you find datasets for your projects:\n\nPopular platforms\n\nGoogle Dataset Search\nData is Plural: Google Sheet with 1000+ datasets\nGithub Awesome Public Datasets\nOpen Data Inception - 2600+ Open Data Portals Around the World\ndata.world\nSubreddit r/datasets\nCORGIS Datasets Project\n\n\n\nGovernments / NGOs websites.\n\nSwitzerland\n\nOffice F√©d√©ral de la Statistique\nSwiss Open Data\n\nEurope\n\nEuropean Data Portal\n\nUnited States\n\nNYC Open Data\nThe General Social Survey\nUS Bureau of Labor Statistics\nUS Department of Education\nFederal Reserve Economic Data\nUS Department of Agriculture\n\nGlobal\n\nWorld Bank Open Data\nUN Data and other UNSD databases\n\nMonthly Bulletin of Statistics Online\nSDG Indicators\nUN Comtrade Database\n\n\n\n\n\nOther platforms\n\nNews API\nKaggle\n10 great sites with free datasets\nShift Data Portal Data about energy consumption and climate\n\n\n\nAcknowledgements\nCredits to dsfba_2021 for providing these data sources."
  },
  {
    "objectID": "resources/data_acquisition/web_scraping_api.html",
    "href": "resources/data_acquisition/web_scraping_api.html",
    "title": "Web scraping & APIs",
    "section": "",
    "text": "Navigating the digital age means unlocking the treasure trove of data available online. Web scraping and APIs aren‚Äôt just technical skills; they‚Äôre your keys to a universe of data for any project you can imagine. Think about the ease of analyzing trends, financial markets, or even sports‚Äîall through data you gather and analyze yourself.\nIn this section, Ilia walks us through the essentials of harnessing web data, offering a powerful alternative for those looking to source unique datasets for their projects. Knowing these techniques empowers you to find and utilize data that sparks your curiosity and fuels your research. Let‚Äôs dive in and discover how these tools can transform your approach to data collection."
  },
  {
    "objectID": "resources/data_acquisition/web_scraping_api.html#using-css",
    "href": "resources/data_acquisition/web_scraping_api.html#using-css",
    "title": "Web scraping & APIs",
    "section": "Using CSS",
    "text": "Using CSS\nIn this pratice, we learn how to use the rvest package to extract information from the famous IMDB (Internet Movie Database) site of the 50 most popular movies (https://www.imdb.com/search/title/?groups=top_250&sort=user_rating). The page was saved (downloaded) and is also available in the data/ folder. Alternatively, you can directly work on the link. However, bear in mind that thr structure of online websites can change in time, therefore, the code below might need adjustments (i.e., change in tags).\nFirst, we load the page.\n\nlibrary(rvest)\nlibrary(magick)\nlibrary(tidyverse)\nlibrary(flextable)\nlibrary(pdftools)\nlibrary(tesseract)\n\n\n# local file (html)\nimdb.html <- read_html(\"data/IMDb _Top 250.html\") \n# or alternatively use the link\n# imdb.html <- read_html(\"https://www.imdb.com/search/title/?groups=top_250&sort=user_rating\") # webpage\n\nNow, we identify the positions of the titles. On the web page (opened preferably with Chrome) right-click on a title and select ‚ÄúInspect‚Äù. The tag corresponding to the titles appears on the developer window (partially reproduced below).\n\n<div class=\"lister-item-content\">\n<h3 class=\"lister-item-header\">\n    <span class=\"lister-item-index unbold text-primary\">1.</span>\n    <a href=\"https://www.imdb.com/title/tt0111161/?ref_=adv_li_tt\">The Shawshank Redemption</a>\n    <span class=\"lister-item-year text-muted unbold\">(1994)</span>\n</h3>\n[...]\n</div>\n\nLooking above, the title (‚ÄúThe Shawshank Redemption‚Äù) is under the div tag with class=\"lister-item-content\", then the sub-tag h3 within it then the tag a within it. The html_nodes function can target this tag. The ‚Äúdot‚Äù after div indicates the class value. It actually targets all such tags.\n\ntitles <- imdb.html %>%\n  html_nodes(\"div.lister-item-content h3 a\") \nhead(titles) \n\nThe results are cleaned from the html code (i.e., only the texts remain) using html_text2 function.\n\ntitles <- html_text2(titles)\nhead(titles)\n\nAnother way would have been to use the fact that the targeted h3 tags have a class value. Modify the previous code to extract tags a within h3 with class value ‚Äúlister-item-header‚Äù.\nAnswer\n\n\n\ntitles <- imdb.html %>% \n  html_nodes(\"h3.lister-item-header a\") %>% \n  html_text2()\ntitles\n\n\nNow, repeat that approach for the year and the run time. You may use the function substr to extract the year from the text.\nAnswer\n\n\nFor the years:\n\n## Extract the years\nyears <- imdb.html %>% \n  html_nodes(\"div.lister-item-content h3 .lister-item-year\") %>%\n  html_text2()\nyears <- as.numeric(substr(years,\n                           start = 2,\n                           stop = 5))\n# take only characters 2 to 5 corresponding to the year\n##years <- as.numeric(gsub(\"[^0-9.-]\", \"\", years)) # an alternative: keep only the numbers in a string\n\nFor the run times, first they are extracted in the format ‚Äú120 min‚Äù. Then, the run time is split by space which gives ‚Äú120‚Äù and ‚Äúmin‚Äù. The unlist command casts this to a vector. Then we take one element every two (corresponding to the minutes).\n\nruntimes <- imdb.html %>%\n  html_nodes(\"span.runtime\") %>%\n  html_text2()\nruntimes <- as.numeric(\n  unlist(\n    strsplit(runtimes, \" \"))[seq(from = 1, by = 2, len = 50)]) \n# by space, \n\n\nSuppose that now we want to extract the description. In this case, there is no unique class value identifying the field (see the html code). However, one can note that it is the 4th paragraph (element) within a div tag with a useful class value. To access the k-th paragraph you can use p.nth-child(k) starting from the correct hierarchical position. For example, p:nth-child(2) extract the 2-nd paragraph.\nFor the 4-th paragraph (i.e., the wanted description), a possible code is thus\n\ndesc <- imdb.html %>% \n  html_nodes(\"div.lister-item-content p:nth-child(4)\") %>% \n  html_text2()\nhead(desc)\n\nTo finish, we build a data frame containing this information (tibble format below).\n\nimdb.top.50 <- tibble(data.frame(\n  Titles = titles,\n  Years = years,\n  RunTimes = runtimes,\n Desc = desc))\nimdb.top.50 %>% \n  head() %>% \n  flextable() %>% \n  autofit()"
  },
  {
    "objectID": "resources/data_acquisition/web_scraping_api.html#xpath",
    "href": "resources/data_acquisition/web_scraping_api.html#xpath",
    "title": "Web scraping & APIs",
    "section": "XPath",
    "text": "XPath\nIn the previous part, we used the CSS to identify the tags. We now use an alternative: the XPath. The Xpath is preferably used when we want to extract a specific text. For example, we want to extract the description of the first description: right-click and select inspect. Then right-click the corresponding code line, and select ‚ÄúCopy xpath‚Äù. Pass this, to the xpath parameter of html_nodes like below:\n\ndesc1 <- imdb.html %>% \n  html_nodes(xpath=\"//*[@id='main']/div/div[3]/div/div[1]/div[3]/p[2]/text()\") %>% \n  html_text2()\ndesc1\n\n\n\n\n\n\n\nNote\n\n\n\nIn the xpath, you must turn the quotes around main to simple quotes.\n\n\nThis is convenient when you want to extract a particular text. You can also use the Selector Gadget from Chrome to extract multiple Xpath."
  }
]