[
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "There are no mandatory readings for this course, except our MLBA website; however, here are some of the books that inspired our course. Except for one book, the rest are all freely available:\n\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition. Springer Science & Business Media. Available at: Technically-oriented PDF Collection; GitHub repo\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning: with Applications in R. Springer Science & Business Media. Available at: statlearning.com\nKuhn, M. and Johnson, K. (2013) Applied Predictive Modeling. Springer Science & Business Media. Available at1: Springer\n\n\nBoehmke, B. and Greenwell, B. (2020) Hands-On Machine Learning with R. Taylor & Francis. Available at: HOML (retrieved the 2023-01-10)\nMolnar C. (2023) Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. Available at: Interpretable Machine Learning (retrieved the 2023-08-10)",
    "crumbs": [
      "Course information",
      "References"
    ]
  },
  {
    "objectID": "references.html#footnotes",
    "href": "references.html#footnotes",
    "title": "References",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is the only book that is not freely available and must be purchased.↩︎",
    "crumbs": [
      "Course information",
      "References"
    ]
  },
  {
    "objectID": "lectures/02_DataExploration/ML_DataExplo.html",
    "href": "lectures/02_DataExploration/ML_DataExplo.html",
    "title": "Data Exploration",
    "section": "",
    "text": "In-class R script 📄",
    "crumbs": [
      "Lectures",
      "Data Exploration"
    ]
  },
  {
    "objectID": "lectures/02_DataExploration/ML_DataExplo.html#footnotes",
    "href": "lectures/02_DataExploration/ML_DataExplo.html#footnotes",
    "title": "Data Exploration",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn most algorithm, especially in regressions. In Neural Network, it is called the bias.↩︎",
    "crumbs": [
      "Lectures",
      "Data Exploration"
    ]
  },
  {
    "objectID": "lectures/07_InterpretableML/ML_Interp.html",
    "href": "lectures/07_InterpretableML/ML_Interp.html",
    "title": "Interpretable ML",
    "section": "",
    "text": "In-class R script 📄\n\nConcept\nIn ML, some models are interpretable (e.g., regression, CART) and some are not (e.g., SVM, NN). So called interpretable machine learning is a set of techniques aiming at\n\nDiscover / rank variables or features by their importance on the predicted outcome,\nAssociate variation of the important features with a direction of the outcome (positive vs negative association).\n\n\n\nVariable importance\n\nVariable importance\nVariable importance is a method that provides a measure of the importance of each feature for the model prediction. The variables importance for a trained model can be evaluated for one variable by:\n\nIn the test set, shuffle the instances on one variable,\nMake the predictions of this new test set,\nCompute the quality metric (RMSE, Accuracy, ...),\nCompare it to the quality metric on the original test set.\nIf the variable is important, giving to the model an incorrect values of this variable should decrease the quality metric,\n\nThis is repeated for each variable.\n\n\nIllustration\n\n\n\n\n\nIf predictions using the right-hand side data are the same as the left-hand side ones, then Var2 is not important. With the iris data and a CART, trained on \\(80\\%\\) of the data (test set at \\(20\\%\\)). The tree is\n\n\n\n\n\nWe see directly that Petal length and Petal width are the only two important features (except in case of missing data).\nTo measure this, in the test set, we shuffle Petal length. The accuracy of the shuffled test set is much lower than the original one (left: original test set; right: modified test set). This confirms that Petal length is essential for a good prediction of the species by the model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn the other hand, the Sepal length does not appear on the graph. And, indeed, it is not important for the prediction as shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor tree, the importance of a variable can be read to some extend directly on the graph (if it is not too large). For a model like SVM, it is not. But we can still make the same analysis by shuffling the instances for a given feature (top: original; left: Petal length; right: Sepal length).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeveral versions\nThe variable importance presented previously is called model-agnostic: it is independent of the model type.\nThere exist several ways of it:\n\nModification of the variable: shuffling is one possibility. Perturbation is another (add a random noise). Simulation from another distribution.\nModification of the score: accuracy is one possibility. Specificity/Sensitivity/Bal. accuracy, Entropy... The importance can be linked to the prediction of one level only.\nRegression: use RMSE, MAE, etc.\n\nThere exist also model-specific approaches that are specific to the model that is used.\nIn R,\n\niml allows to compute model-agnostic variable importance, with any loss function.\ncaret allows to compute mainly model-specific variable importance. Also model-agnostic type (limited choice of loss functions).\n\nVI on SVM estimated on cross-entropy, repeated 100 times.\nlibrary(iml)\niris.iml &lt;- Predictor$new(iris.svm, data = iris.te[,-5], y = iris.te[,5])\niris.imp &lt;- FeatureImp$new(iris.iml, loss = \"ce\", n.repetitions = 100)\nplot(iris.imp)\n\n\n\n\n\n\n\nExample: iml\nFor each feature: The difference in entropy obtained between the original data set and the shuffled data set is computed. This is repeated 100 times. These differences are averaged and shown on the plot.\nVI on SVM estimated on AUC.\nlibrary(caret)\ntrctrl &lt;- trainControl(method = \"repeatedcv\", repeats= 3, number=5)\niris.caret &lt;- caret::train(form=Species ~., data = iris.tr, \n                    method = \"svmLinear\", \n                    trControl=trctrl)\nplot(varImp(iris.caret))\n\n\n\n\n\n\n\nExample: caret and VarImp\nSince no specific method was developped for SVM, a filter-based method is implemented. From the help of VarImp:\nFor classification, [...] the area under the ROC curve [...] is used \nas the measure of variable importance. For multi-class outcomes, the problem \nis decomposed into all pair-wise problems and the area under the curve is\ncalculated for each class pair (i.e class 1 vs. class 2, class 2 vs. \nclass 3 etc.). For a specific class, the maximum area under the \ncurve across the relevant pair-wise AUC's is used as the variable importance \nmeasure.\nEven for one model, there may exist lots of implementations. Below, we give one example: VarImp for CART. CART is recognized as Recursive Partitioning (from rpart). From the help of VarImp:\nThe reduction in the loss function (e.g. mean squared error) attributed \nto each variable at each split is tabulated and the sum is returned. \n\nAt each split, the reduction of loss due to the best split on each variable is extracted.\nThese loss reductions are summed.\n\n(No resampling or measure of error on the VI estimate)\n\n\nModel-specific VI\n\n\n\n\n\nWhatever implementation you use, VI aims at the same objective: quantify the importance of variables for the predictions of the model. Some limitations are:\n\nInstability: the estimation of the importance can be unstable due to the random perturbation. Resampling can help (i.e., repeat the shufling several times).\nInteractions: sometimes it is the combination of two features that makes a good prediction (typ. trees). Variable importance cannot see this.\n\nThe variable importance is only seen with the eyes of the model. If the model is doing a poor job (e.g., low accuracy) then the variable importance analysis is of low quality.\n\n\n\nPartial Dependence Plots\nVariable important allows to inspect how much a variable is important in the construction of the prediction by a model.\nPartial dependence plots (PDP) show in which direction is the association between a feature \\(x\\) and the prediction of \\(y\\).\n\nPartial Dependence Plot\nMathematically, for the feature \\(x_s\\), let \\(f(x_s, x_{-s})\\) be the prediction of \\(y\\) by the model \\(f\\), then the PD-function of \\(X_s\\) at \\(x_s\\) is \\[F_{X_s}(x_s) = \\int f(x_s, x_{-s}) p(x_{-s}) dx_{-s} = E[f(x_s, X_{-s})],\\] where \\(p(x_{-s})\\) is the distribution of \\(x_{-s}\\). \\(F_{X_s}(x_s)\\) is the expected value over \\(x_{-s}\\) of the prediction when \\(x_s\\) is fixed (not conditional).\nThe estimation of the expectation above is obtained by averaging on the training set: \\[\\hat{F}_{X_s}(x_s) = \\frac{1}{N}\\sum_{i=1}^N f(x_s, x_{-s}^{(i)}).\\]\n\n\nEstimation\n\n\n\n\n\n\n\nInterpretation\n\nLeft: PDP increases. The prediction increases in average when the feature increases ; it is a positive association.\nRight: PDP is stable. The prediction does not change in average when the feature value changes; there is no association.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPDP allows to explore the link between response and any feature with the eyes of the model.\nPDP can be also used to see the feature importance with the amplitude of the graph (the larger the more important): \\[\\vert \\max_{x_s} F_{X_s}(x_s) - \\min F_{X_s}(x_s) \\vert.\\] It is richer than VI but also longer to run.\nPDP can be made multivariate with \\(X_s\\) being several features (usually, max. 2).\nBy averaging, PDP ignores any interaction between \\(x_S\\) and \\(x_{-S}\\).\n\n\n\n\nLIME\n\nLIME\nLIME stands for Local interpretable model-agnostic explanations. The paradigm is that a large ML model can be locally approximated by an interpretable (smaller) model, a surrogate model.\n\nSelect a instance of interest.\nBuild instances close to it and predict them.\nFit a surrogate model.\nInterpret it coefficients.\n\n\n\nIllustration in 2 dimensions\nOn iris data, a random forest predicts setosa/virginica/versicolor from length and width (sum of the petal/sepal features). It is globally complex (left graph). Select a point of interest (blue circle). Around it (right plot), a logistic regression could be used. If you fit it on the black dots, you will find a positive association between large width and \"being green\", and no association with length.\n\n\n\n\n\n\n\nTechnical details\nThere is no unique way to build/interpret surrogate models. Often,\n\nSurrogate models are linear models with variable selection.\nThe number of variables is fixed by the user.\nCategorical features are transformed to a dummy variable: 1 if it is the same as the point of interest, 0 otherwise.\nNumerical features are binned and dummy variables are created: 1 if it is the same bin as the point of interest, 0 otherwise.\nWeights are assigned to sampled data according to their proximity with the point of interest (Gower’s).\n\n\n\nExample\nWith the bank data (see R code of examples). Three cases: Label 0 with high prob., label 1 with high prob., Label 1 with prob. \\(\\approx 50\\%\\).\n\n\n\n\n\n\n\nInterpretation for duration: no local behavior discovered\n\nAround Case 1: Duration being lower than 759 supports a label 0. This means that a low duration supports label 0 and, consequently, a larger duration would support label 1.\nAround Case 2: Duration being greater than 759 (lower than 1514) supports label 1. This means that a large duration supports label 1 and, consequently, a lower duration would support label 0.\nAround case 3: same as Case 2.\n\nRegarding duration, case 1 and case 2 coincide. They also coincide with the general previous findings that duration is an important factor, with prob. of label 1 increasing with it. The exploration of this two cases did not reveal any local behavior linked to duration. A similar analysis can be done for contact, housing, etc.\n\n\nInterpretation for campaign: local behavior\n\nAround Case 1: Campaign being low supports label 0.\nAround Case 2: Campaign being low contradicts label 1. This is in line with Case 1.\nAround Case 3: Campaign being low supports label 1. This is in contradiction with Cases 1 and 2.\n\nThis example shows that a global behavior (campaign is positively associated with label 1) can change locally. The fact that Case 3 has a probability around 0.5 makes it interesting. Limitation: here the effect of Campaign is so small that a good explanation is that it has almost no effect everywhere. That was just a toy example.\n\n\nDiscussion\n\nThe choice of the point of interest can be anything, even non-observed instances. Average, extreme, and change points are often of interest.\nThis method interprets locally the link between the features and the response, again, with the eyes of the model.\nThe surrogate models (as well as the global model) cannot support rigorous causal analysis. We can discover only association here.\nLike often, this method can be unstable (implementation, choice of the model, etc.). Try several combinations and be cautious with conclusions.",
    "crumbs": [
      "Lectures",
      "Interpretable ML"
    ]
  },
  {
    "objectID": "lectures/03_Models/030_Introduction/ML_Models_Intro.html",
    "href": "lectures/03_Models/030_Introduction/ML_Models_Intro.html",
    "title": "Introduction to Models",
    "section": "",
    "text": "In ML, models are mainly used for supervised learning, the aim is\n\nPredict a response \\(y\\): regression if numerical, classification if categorical.\nFrom features \\(x=\\{x_1, \\ldots, x_p\\}\\): available at the moment of prediction,\nWith the best possible quality: built from the data in an optimal way.\n\nThe \\(n\\) observed features and responses are denoted \\[(y_1, x_1), \\ldots, (y_n, x_n).\\]",
    "crumbs": [
      "Lectures",
      "Intro to Models"
    ]
  },
  {
    "objectID": "lectures/03_Models/030_Introduction/ML_Models_Intro.html#footnotes",
    "href": "lectures/03_Models/030_Introduction/ML_Models_Intro.html#footnotes",
    "title": "Introduction to Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlso called weights, especially for Neural Networks.↩︎",
    "crumbs": [
      "Lectures",
      "Intro to Models"
    ]
  },
  {
    "objectID": "lectures/03_Models/033_NeuralNetworks/ML_NN.html",
    "href": "lectures/03_Models/033_NeuralNetworks/ML_NN.html",
    "title": "Neural Networks",
    "section": "",
    "text": "In-class Python script 📄",
    "crumbs": [
      "Lectures",
      "Neural Networks"
    ]
  },
  {
    "objectID": "lectures/03_Models/033_NeuralNetworks/ML_NN.html#footnotes",
    "href": "lectures/03_Models/033_NeuralNetworks/ML_NN.html#footnotes",
    "title": "Neural Networks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee logistic regression.↩︎",
    "crumbs": [
      "Lectures",
      "Neural Networks"
    ]
  },
  {
    "objectID": "lectures/06_Ensembles/ML_Ensemble.html",
    "href": "lectures/06_Ensembles/ML_Ensemble.html",
    "title": "Ensemble Methods",
    "section": "",
    "text": "In-class R script 📄",
    "crumbs": [
      "Lectures",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "lectures/06_Ensembles/ML_Ensemble.html#footnotes",
    "href": "lectures/06_Ensembles/ML_Ensemble.html#footnotes",
    "title": "Ensemble Methods",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFreund, Y and Schapire, R E (1997). \"A decision-theoretic generalization of on-line learning and an application to boosting\". Journal of Computer and System Sciences. 55: 119-139.↩︎",
    "crumbs": [
      "Lectures",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "lectures/08_UnsupervisedLearning/082_DimensionReduction/ML_DimRed.html",
    "href": "lectures/08_UnsupervisedLearning/082_DimensionReduction/ML_DimRed.html",
    "title": "Dimension Reduction",
    "section": "",
    "text": "In-class R script 📄\n\nDimension reduction\nFor unsupervised learning, data representation and simplification is an important task. A natural objective is to represent the data on a graph: simplify \\(p\\) features to 2 dimensions and plot on \\(x\\)- and \\(y\\)-axes.\nExample: iris data has \\(4\\) features (species not used). The data cannot be represented on a single graph.\n\n\n\n\n\nObservation: Petal.Length and Petal.Width are highly correlated: if we know one, we can predict the other one.\nTo simplify the representation, we do not need these two features: replace them by their average Petal.WL = (Petal.W+Petal.L)/2:\n\n\n\n\n\nSame observation for Petal.WL and Sepal.Length...\nIn summary, two correlated features can be replaced by one combination without keeping the information.\nThe resulting representation shows as much variance as possible: theinformation is kept.\nThis is the principle of Principal Component Analysis (PCA).\n\n\nPrincipal Components Analysis\n\nPCA\nThe PCA is a method that looks for dimensions that are linear combinations of the \\(p\\) features: \\[a_1 x_1 + a_2 x_2 + \\cdots + a_p x_p.\\] These linear combinations are called the principal components. There are \\(p\\) principal components.\nTo find the first component (i.e. coefficients \\(a\\)), one should maximize the variance along it. That is, \\(a\\) should be the solution to \\[\\begin{aligned}\n\\max_a &&Var (a_1 x_{1} + \\cdots + a_p x_{p})\\\\\ns.t.&& \\sum_j a_j^2 = 1\\end{aligned}\\] where the variance is computed on the data set. The constraint on \\(a\\) is here because \\(a\\) is only indicating a direction and should therefore be scaled to 1. As an example, let’s look at a toy case where we only have 2 features \\(x_1\\) and \\(x_2\\) showed below. We want to represent the data in one dimension, that is, along a line (dimension reduction). The dashed line shows more variability of the data and is a better principal component than the solid line.\n\n\n\n\n\n\n\nPCA: the first component\nWhen extended to \\(p\\geq 2\\) features, the principle remains the same. For a choice of \\(a\\) with \\(\\sum a_j^2=1\\),\n\n\\(z_i = a_1 x_{i1} + \\cdots + a_p x_{ip}\\) are computed on the data set (\\(i=1,\\ldots,n\\)),\nthe variance of the \\(z_i\\) is computed.\n\nThen this variance is maximized by changing \\(a\\). The maximum value gives \\(a^{(1)}\\), the first principle component.\n\n\nPCA: the second component, and third, etc.\nThe second component \\(a^{(2)}\\) is obtained with the same principle and the extra constraint to be orthogonal to the first one: \\[\\sum_{j=1}^p a^{(1)}_j a^{(2)}_j = 0.\\] The procedure is repeated until \\(p\\) principal components are found (each one is orthogonal to all the previous ones).\nNote: the result can be easily obtained by computing the spectral decomposition of the variance matrix of the data.\n\n\nPCA: scaling\nBefore computing the PCA, the data can be standardized. Like any standardization, this is a choice of the user, which depends strongly on the application and on the scale (units) of the data. When the data are first standardized, the spectral decomposition is made on the correlation matrix of the data.\n\n\nPCA: projection\nFor each PC \\(j\\), we have the corresponding projections of the data \\(x_i\\) \\[z_i^{(j)} = a^{(j)}_1 x_{i1} + \\cdots + a^{(j)}_p x_{ip}.\\] We thus have a new data set \\(z\\) whose column are the projection of the features \\(x\\) on the PCA’s. These new features \\(z_1, \\ldots, z_p\\) can be used\n\nfor data representation (dimension reduction),\nto describe the dependence between the features (factor analysis).\n\n\n\nPCA: variance proportion\nBy construction, the first principal component \\(z_1\\) has larger variance than \\(z_2\\) and so on. Also, by construction, the total of the variance is preserved: \\[\\sum_{j=1}^p var(x_j) = \\sum_{j=1}^p var(z_j).\\] The proportion of the total variance explained by the PC \\(z_j\\) is thus \\[var(z_j)/\\sum_{j'=1}^p var(z_{j'}).\\] It represents how well the data are represented on the component \\(z_j\\).\n\n\n\n\n\n\n\nPCA: example\n&gt; iris.pca &lt;- PCA(iris[,-5], graph=FALSE) \n&gt; summary(iris.pca)\n\nCall:\nPCA(X = iris[, -5], graph = FALSE) \n\n\nEigenvalues\n                        Dim.1   Dim.2   Dim.3   Dim.4\nVariance               2.918   0.914   0.147   0.021\n% of var.             72.962  22.851   3.669   0.518\nCumulative % of var.  72.962  95.813  99.482 100.000\nTogether, PC1 and PC2 explain \\(95.8\\%\\) of the variation of the data. The scatter plot matrix shows that it is a good representation of the data with only two dimensions (PC\\(_1\\), PC\\(_2\\)).\nThe correlation between the PC and the features \\(x\\) can be computed to see how is PC is correlated to each features.\n\n\n\n\n\n\n\nPCA: the circle of correlations\nWe see that\n\nPC\\(_1\\) is positively and strongly correlated with petal length, petal width, and sepal length. This component summarizes these 3 features: the larger PC\\(_1\\), the larger these 3 features.\nPC\\(_2\\) is (negatively) correlated with sepal width. The larger PC\\(_2\\) the smaller this feature.\nPC\\(_1\\) explains \\(73\\%\\) of the total data variation. PC\\(_2\\) explains \\(23\\%\\) of it.\n\nWith one graph, we know\n\nwhich features are correlated/independent\nhow to summarize the data into two dimensions.\n\n\n\nPCA: the \\(cos^2\\)\nThe circle of correlations relates the dimensions and the features. Another view is the \\(cos^2\\) graph. It is interpreted as the quality of the representation of the feature by the dimension. Of course this is intimately related to the correlations.\n\n\n\n\n\nThe biplot shows a map of the individuals in the dimensions and adds the circle of correlations.\n\n\n\n\n\n\n\nPCA: the biplot\nFor example, we can conclude that\n\ninstance 61 has a sepal width smaller than the average (large PC\\(_2\\)) and an average PC\\(_1\\) (which indicates an average petal length, width and sepal length).\ninstance 119 has an average sepal width but a large PC\\(_1\\), i.e. petal length and width and sepal length.\nTwo clusters can be observed and are well separated by PC\\(_1\\) (in fact these clusters correspond to species here).\n\n\n\nPCA: number of components\nHere, two components explain more than \\(95\\%\\) of the data variability. Sometimes, more components are needed. One way to set the number of components is to target the proportion of explained variance: often between \\(75\\%\\) and \\(95\\%\\).\nIf the features are independent, this number is likely to be large. If the features are correlated, this number will be smaller. The screeplot may help.\n\n\nPCA: the screeplot\n\n\n\n\n\n\n\nPCA and machine learning\nIn the context of machine learning, PCA is often used\n\nTo inspect the data, find/explain clusters, find dependence between the features. PCA can be used for EDA.\nTo diminish the number of features when there are too many: dimension reduction \\(=&gt;\\) only keep few first PC.\n\n\n\nCategorical data\nPCA can only be performed on numerical features. When categorical features are also included in the analysis,\n\nfor ordinal data, quick and dirty solution: modalities can be mapped to numbers (\\(1,2,\\ldots\\)) respecting their order,\nfor nominal data: there is no correct solution; especially replacing by numbers is incorrect.\n\nWith only categorical data, (Multiple) Correspondence Analysis is a solution. And for mixed data type (categorical and numerical), Factor Analysis of Mixed Data (FAMD) is a solution. However, they are not adapted to large data set.\n\n\n\nAuto-encoders\n\nPrinciple\nPCA is a \"linear\" technique, based on the explanation of the correlation between the features. Auto-encoder are neural network. The idea is to train a model that recovers an instance with an intermediate layer of lower dimension than the number of features.\n\n\n\n\n\n\n\n\nencoder + decoder = autoencoder\n\nThe input and the output (response) are the same instance (\\(X\\)), of dimension \\(p\\).\nThe intermediate layer (at least one) is of dimension \\(k&lt;p\\).\nThe model is trained to recover \\(X\\) ate the end.\n\nIf, after traning, the model can recover \\(X\\) from \\(X\\), then it can recover \\(X\\) from its image in the intermediate layer \\(h(X)\\). Thus, only \\(p\\) dimensions would be needed to recover \\(X\\).\nThus,\n\nthe left part of the NN encodes \\(X\\) in its lower-dimension version \\(h(X)\\)\nthe right part of the NN decodes \\(h(X)\\) in an output \\(g(h(X))\\), hopefully close to the original image \\(X\\).\n\nThe better this final image the better the encoding/decoding: \\[g(h(X)) \\stackrel{?}{\\approx} X.\\]\n\n\nAutoencoder vs PCA\nIn ML, often autoencoder are used to\n\nReduce the noise in the data (smoothing)\nReduce the memory needed (compressing)\nRepresent the data (dimension reduction)\n\nUnlike PCA, autoencoder do not provide interpretation of the dimensions, which dimension is the most important, etc. On the other hand, autoencoders can produce better final representation of the data: the recovery of \\(X\\) with \\(k\\) components is better than with PCA. Like PCA make two-dimensional plots to discover pattern. Below is autoencoder with 3-node intermediate-layer:\n\n\n\n\n\n\n\nInterpretability\nRelate each component of \\(h(X)\\) to each component of \\(X\\) using variable importance (or another technique). Below, Node 1 to 3 (left to right).",
    "crumbs": [
      "Lectures",
      "Dimension Reduction"
    ]
  },
  {
    "objectID": "lectures/01_Introduction/ML_Intro.html",
    "href": "lectures/01_Introduction/ML_Intro.html",
    "title": "Introduction to Machine Learning (ML)",
    "section": "",
    "text": "Machine learning is a set of techniques using models that are trained on data to achieve forecasting or pattern discovery.\n\nUnlike programming, consisting of implementing a solution already defined by the developers, ML let the model learn by seeing data (learning from examples).\nML is a sub-field of Artificial Intelligence (AI).\nDeep Learning is a popular sub-field of of ML that uses deep neural networks, a particular type models.\n\n\n\n\n\nProvide predictions and/or decisions (predictive analytics).\nFairly automatic and data based (not judgmental).\nThe prediction quality can be assessed (metrics).\nThe model improves with the increase/improvement of the data base.\nThe model can be interpretable.\n\n\n\n\nML is not a re-branding of statistics. Both use data and statistical models. Applications of ML are oriented toward predictions, applications of statistics are oriented toward inference (research, hypothesis assessment). E.g.,\n\nA study aims to in/validate that employees with a psychological support are less prone to burnout. This study needs observations, data analysis, and statistical tests of hypotheses. This is typical of academic work and is usually qualified as statistics (inference).\nA study aims to develop a prediction tool for the credit risk associated to a client in a bank. This study needs observations, data analysis, and model validation. It is typical of ML.",
    "crumbs": [
      "Lectures",
      "Intro to ML"
    ]
  },
  {
    "objectID": "lectures/01_Introduction/ML_Intro.html#footnotes",
    "href": "lectures/01_Introduction/ML_Intro.html#footnotes",
    "title": "Introduction to Machine Learning (ML)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe training set is often further split into a “training set” and a “validation set” for the selection of the hyperparameters. This will be clarified later.↩︎\nCaution: link, association, correlation \\(\\neq\\) causality!!↩︎",
    "crumbs": [
      "Lectures",
      "Intro to ML"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html",
    "title": "",
    "section": "",
    "text": "Code\nHow can I render the following?— title: “Model scoring”",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#data",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#data",
    "title": "",
    "section": "Data",
    "text": "Data\nThe data set is the one used in the series on linear regressions.\n\nlibrary(readr)\nreal_estate_data &lt;- read.csv(here::here(\"labs/data/real_estate_data.csv\"))\n\nThen we split the data in a training and a test set (0.8/0.2). For this, we use the createDataPartition function of the caret package.\n\nlibrary(caret)\nset.seed(234)\nindex_tr &lt;- createDataPartition(y = real_estate_data$Price, p= 0.8, list = FALSE)\ndf_tr &lt;- real_estate_data[index_tr,]\ndf_te &lt;- real_estate_data[-index_tr,]",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#models",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#models",
    "title": "",
    "section": "Models",
    "text": "Models\nWe will compare a linear regression, a regression tree and a 3-NN (KNN).\n\nRPython\n\n\n\nlibrary(rpart)\nest_lm &lt;- lm(Price~TransDate+HouseAge+Dist+\n               NumStores+Lat+Long, data=df_tr)\nest_rt &lt;- rpart(Price~TransDate+HouseAge+Dist+\n                      NumStores+Lat+Long, data=df_tr)\nest_knn &lt;- knnreg(Price~TransDate+HouseAge+Dist+\n                      NumStores+Lat+Long, data=df_tr, k = 3)\n\n\n\n\n# Load the course python environment as usual with a r code chunks.\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\n\n# Fit the models: linear regression, regression tree, and KNN\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Define predictors and target variable\npredictors = ['TransDate', 'HouseAge', 'Dist', 'NumStores', 'Lat', 'Long']\ntarget = 'Price'\n\n# Fit models\nest_lm = LinearRegression().fit(r.df_tr[predictors], r.df_tr[target])\nest_rt = DecisionTreeRegressor(random_state=234).fit(r.df_tr[predictors], r.df_tr[target])\nest_knn = KNeighborsRegressor(n_neighbors=3).fit(r.df_tr[predictors], r.df_tr[target])",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#r-squared",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#r-squared",
    "title": "",
    "section": "R-squared",
    "text": "R-squared\n\nRPython\n\n\nWe now compute the R2 for each model using the a defined function.\n\nR2 = function(y_predict, y_actual){\n  cor(y_actual,y_predict)^2\n}\n\nR2(predict(est_lm, newdata = df_te), df_te$Price)\nR2(predict(est_rt, newdata = df_te), df_te$Price)\nR2(predict(est_knn, newdata = df_te), df_te$Price)\n\nJust for the exercise, we can compute it by hand (square of the correlation)\n\ncor(predict(est_lm, newdata = df_te), df_te$Price)^2\n\n\n\n\n# Same thing as the R code\nfrom sklearn.metrics import r2_score\nimport numpy as np\n\n# Only to demonostrate which argument goes where (different from `caret::R2`)\nprint(r2_score(y_true = r.df_te[target], y_pred = est_lm.predict(r.df_te[predictors])))\nprint(r2_score(r.df_te[target], est_rt.predict(r.df_te[predictors])))\nprint(r2_score(r.df_te[target], est_knn.predict(r.df_te[predictors])))\n\n# Computing it by hand gives us the same result as R\nnp.corrcoef(est_lm.predict(r.df_te[predictors]), r.df_te[target])[0][1]**2\n\nTo understand why the results are different in R2 from our defined function in R vs. sklearn.metrics.r2_score() in Python, see this post on stackoverflow. If you want to recieve the same results in both, you can try computing the R2 not by correlation but by the formula \\(1 - \\frac{SSR}{SST}\\) where \\(SSR\\) is the sum of squared residuals and \\(SST\\) is the total sum of squares.\nAdditionally, please note that the performance of the tree is highly dependent on the seed, so setting a different seed can lead to different results.",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#rmse",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#rmse",
    "title": "",
    "section": "RMSE",
    "text": "RMSE\nNow, we compute the RMSE.\n\nRPython\n\n\n\nRMSE(predict(est_lm, newdata = df_te), df_te$Price)\nRMSE(predict(est_rt, newdata = df_te), df_te$Price)\nRMSE(predict(est_knn, newdata = df_te), df_te$Price)\n\nThe formula would be:\n\nsqrt(mean((predict(est_lm, newdata = df_te)-df_te$Price)^2))\n\n\n\n\nfrom sklearn.metrics import mean_squared_error, root_mean_squared_error\nimport numpy as np\n\nprint(root_mean_squared_error(r.df_te[target], est_lm.predict(r.df_te[predictors])))\n# alternatively in the older version of `sklearn`, you had to run the code below\n# print(np.sqrt(mean_squared_error(r.df_te[target], est_lm.predict(r.df_te[predictors]))))\nprint(root_mean_squared_error(r.df_te[target], est_rt.predict(r.df_te[predictors])))\nprint(root_mean_squared_error(r.df_te[target], est_knn.predict(r.df_te[predictors])))",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#mae",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#mae",
    "title": "",
    "section": "MAE",
    "text": "MAE\nNow, we compute the MAE.\n\nRPython\n\n\n\nMAE(predict(est_lm, newdata = df_te), df_te$Price)\nMAE(predict(est_rt, newdata = df_te), df_te$Price)\nMAE(predict(est_knn, newdata = df_te), df_te$Price)\n\nThe formula would be:\n\nmean(abs(predict(est_lm, newdata = df_te)-df_te$Price))\n\n\n\n\n# Compute MAE for each model\nfrom sklearn.metrics import mean_absolute_error\n\nprint(mean_absolute_error(r.df_te[target], est_lm.predict(r.df_te[predictors])))\nprint(mean_absolute_error(r.df_te[target], est_rt.predict(r.df_te[predictors])))\nprint(mean_absolute_error(r.df_te[target], est_knn.predict(r.df_te[predictors])))",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#best-model",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#best-model",
    "title": "",
    "section": "Best model",
    "text": "Best model\nThese three measures agree on the fact that the regression tree is the best model. To inspect further the predictions, we use scatterplots:\n\nRPython\n\n\n\npar(mfrow=c(2,2))\nplot(df_te$Price ~ predict(est_lm, newdata = df_te), xlab=\"Prediction\", \n     ylab=\"Observed prices\", main=\"Lin. Reg.\")\nabline(0,1)\nplot(df_te$Price ~ predict(est_rt, newdata = df_te), xlab=\"Prediction\", \n     ylab=\"Observed prices\", main=\"Lin. Reg.\")\nabline(0,1)\nplot(df_te$Price ~ predict(est_knn, newdata = df_te), xlab=\"Prediction\", \n     ylab=\"Observed prices\", main=\"Lin. Reg.\")\nabline(0,1)\npar(mfrow=c(1,1))\n\n\n\n\n# visualize also in Python\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\n\nplt.subplot(221)\nplt.scatter(est_lm.predict(r.df_te[predictors]), r.df_te[target], alpha=0.5)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Observed prices\")\nplt.title(\"Lin. Reg.\")\nplt.plot(r.df_te[target], r.df_te[target], color='red')\n\nplt.subplot(222)\nplt.scatter(est_rt.predict(r.df_te[predictors]), r.df_te[target], alpha=0.5)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Observed prices\")\nplt.title(\"Regression Tree\")\nplt.plot(r.df_te[target], r.df_te[target], color='red')\n\nplt.subplot(223)\nplt.scatter(est_knn.predict(r.df_te[predictors]), r.df_te[target], alpha=0.5)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Observed prices\")\nplt.title(\"KNN\")\nplt.plot(r.df_te[target], r.df_te[target], color='red')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe scatterplots are in line with the conclusion that KNN is the best, even though it is not easy to declare from a plot. We can in addition see that the regression tree (RT) has made more error on the larger prices.",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#data-1",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#data-1",
    "title": "",
    "section": "Data",
    "text": "Data\nThe data set is the visit data (already used in previous exercises). For simplicity, we turn the outcome (visits) into factor. Like before, that are also split into a training and a test set.\n\nDocVis &lt;- read.csv(here::here(\"labs/data/DocVis.csv\"))\nDocVis$visits &lt;- as.factor(DocVis$visits)\n\nlibrary(caret)\nset.seed(346)\nindex_tr &lt;- createDataPartition(y = DocVis$visits, p= 0.8, list = FALSE)\ndf_tr &lt;- DocVis[index_tr,]\ndf_te &lt;- DocVis[-index_tr,]",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#models-1",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#models-1",
    "title": "",
    "section": "Models",
    "text": "Models\nWe will compare a logistic regression, a classification tree (pruned) and a SVM with radial basis (cost and gamma tuned).\n\nRPython\n\n\nNote that the code for tuning the SVM is provided below in comments because of the time it takes to run. The final parameters have been selected accordingly. Also, the SVM fit includes the argument probability=TRUE to allow the calculations of predicted probabilities later.\n\nlibrary(e1071)\nlibrary(adabag)\n\n## Logistic regression\nDoc_lr &lt;- glm(visits~., data=df_tr, family=\"binomial\")\nDoc_lr &lt;- step(Doc_lr)\n\n## Classification tree \nDoc_ct &lt;- autoprune(visits~., data=df_tr)\n\n## SVM radial basis\n# grid_radial &lt;- expand.grid(sigma = c(0.0001, 0.001, 0.01, 0.1),\n#                           C = c(0.1, 1, 10, 100, 1000))\n# trctrl &lt;- trainControl(method = \"cv\", number=10)\n# set.seed(143)\n# Doc_svm &lt;- train(visits ~., data = df_tr, method = \"svmRadial\",\n#                          trControl=trctrl,\n#                          tuneGrid = grid_radial)\nDoc_svm &lt;- svm(visits~., data=df_tr, gamma=0.001, cost=1000, probability=TRUE)\n\n\n\n\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\n# We first put the data in a nice format by one-hot encoding the categorical variables\nX_train = pd.get_dummies(r.df_tr.drop('visits', axis=1))\ny_train = r.df_tr['visits']\nX_test = pd.get_dummies(r.df_te.drop('visits', axis=1))\ny_test = r.df_te['visits']\n\n## Logistic regression\ndoc_lr = LogisticRegression()\ndoc_lr.fit(X_train, y_train)\n\n## Classification tree\ndoc_ct = DecisionTreeClassifier(random_state=123)\ndoc_ct.fit(X_train, y_train)\n\n## SVM radial basis\ndoc_svm = SVC(kernel='rbf', gamma=0.001, C=1000, probability=True, random_state=123)\ndoc_svm.fit(X_train, y_train)",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#predictions",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#predictions",
    "title": "",
    "section": "Predictions",
    "text": "Predictions\nWe now compute the predicted probabilities and the predictions of all the models.\n\nRPython\n\n\nNote that, for SVM, we need to extract the attribute “probabilities” from the predicted object. This can be done with the attr function.\n\n## Logistic regression\nDoc_lr_prob &lt;- predict(Doc_lr, newdata=df_te, type=\"response\")\nDoc_lr_pred &lt;- ifelse(Doc_lr_prob&gt;0.5,\"Yes\",\"No\")\n\n## Classification tree \nDoc_ct_prob &lt;- predict(Doc_ct, newdata=df_te, type=\"prob\")\nDoc_ct_pred &lt;- predict(Doc_ct, newdata=df_te, type=\"class\")\n\n## SVM radial basis\nlibrary(dplyr)\nDoc_svm_prob &lt;- predict(Doc_svm, newdata=df_te, probability=TRUE) %&gt;% attr(\"probabilities\")\nDoc_svm_pred &lt;- predict(Doc_svm, newdata=df_te, type=\"class\")\n\n\n\n\n## Logistic regression\n## the second column represents the `no` values, to make sure of that, you can run `doc_lr.classes_`\ndoc_lr_prob = doc_lr.predict_proba(X_test)[:,1]\ndoc_lr_pred = np.where(doc_lr_prob&gt;0.5, \"Yes\", \"No\")\n\n## Classification tree\ndoc_ct_prob = doc_ct.predict_proba(X_test)[:,1]\ndoc_ct_pred = doc_ct.predict(X_test)\n\n## SVM radial basis\ndoc_svm_prob = doc_svm.predict_proba(X_test)[:,1]\ndoc_svm_pred = doc_svm.predict(X_test)",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#confusion-matrices-prediction-based-measures",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#confusion-matrices-prediction-based-measures",
    "title": "",
    "section": "Confusion matrices & prediction-based measures",
    "text": "Confusion matrices & prediction-based measures\n\nRPython\n\n\nThe confusionMatrix function provides all the accuracy measures that we want.\n\nconfusionMatrix(data=as.factor(Doc_lr_pred), reference = df_te$visits)\nconfusionMatrix(data=as.factor(Doc_ct_pred), reference = df_te$visits)\nconfusionMatrix(data=as.factor(Doc_svm_pred), reference = df_te$visits)\n\n\n\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, cohen_kappa_score\n\n## Logistic regression\nprint(confusion_matrix(y_test, doc_lr_pred))\nprint(f\"Accuracy: {accuracy_score(y_test, doc_lr_pred):.3f}\")\nprint(f\"Kappa: {cohen_kappa_score(y_test, doc_lr_pred):.3f}\")\nprint(f\"Balanced accuracy: {balanced_accuracy_score(y_test, doc_lr_pred):.3f}\")\n\n## Classification tree\nprint(confusion_matrix(y_test, doc_ct_pred))\nprint(f\"Accuracy: {accuracy_score(y_test, doc_ct_pred):.3f}\")\nprint(f\"Kappa: {cohen_kappa_score(y_test, doc_ct_pred):.3f}\")\nprint(f\"Balanced accuracy: {balanced_accuracy_score(y_test, doc_ct_pred):.3f}\")\n\n## SVM radial basis\nprint(confusion_matrix(y_test, doc_svm_pred))\nprint(f\"Accuracy: {accuracy_score(y_test, doc_svm_pred):.3f}\")\nprint(f\"Kappa: {cohen_kappa_score(y_test, doc_svm_pred):.3f}\")\nprint(f\"Balanced accuracy: {balanced_accuracy_score(y_test, doc_svm_pred):.3f}\")\n\nDifferent results for the tree and CSV due to randomness, but even with that, SVM remains the best model in terms of accuracy.\n\n\n\nThe conclusion may be different from one measure to another\n\nAccuracy: the SVM reaches the highest accuracy\nKappa: the CT is the highest.\nBalanced accuracy: the CT is the highest.\netc.\n\nLooking at the confusion matrix, we see that the data is highly unbalanced (many more “No” than “Yes”). Therefore, measures like balanced accuracy and kappa are interesting because they take this characteristics into account. This shows that the CT is probably better than the SVM because it reaches a better balance between predicting “Yes” and “No”.\nBy looking at the sensitivity and specificity (!! here the positive class is “No”), we see that the best model to recover the “No” is the logistic regression (largest sensitivity) and the best model to recover the “Yes” is the classification tree (largest specificity).",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#probability-based-measures",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#probability-based-measures",
    "title": "",
    "section": "Probability-based measures",
    "text": "Probability-based measures\n\nRPython\n\n\nTo compute the AUC (area under the ROC curve) we can use the caret::twoClassSummary function. The use of this function can be tricky. Its argument should be a data frame with columns (names are fixed):\n\n“obs”: the observed classes\n“pred”: the predicted classes\ntwo columns with names being the levels of the classes, here “Yes” and “No”, containing the predicted probabilities.\n\n\ndf_pred_lr &lt;- data.frame(obs=df_te$visits,\n                         Yes=Doc_lr_prob,\n                         No=1-Doc_lr_prob,\n                         pred=as.factor(Doc_lr_pred))\nhead(df_pred_lr)\n\ndf_pred_ct &lt;- data.frame(obs=df_te$visits,\n                         Doc_ct_prob,\n                         pred=as.factor(Doc_ct_pred))\nhead(df_pred_ct)\ndf_pred_svm &lt;- data.frame(obs=df_te$visits,\n                          Doc_svm_prob,\n                          pred=as.factor(Doc_svm_pred))\nhead(df_pred_svm)\n\nThen we pass these objects to the function, and levels of the classes to be predicted (for the function to be able to recover them in the data frame). The function compute the AUC by default (under the name ROC_.. not very wise) as well as sensitivity and specificity (that we already have).\n\ntwoClassSummary(df_pred_lr, lev = levels(df_pred_lr$obs))\ntwoClassSummary(df_pred_ct, lev = levels(df_pred_lr$obs))\ntwoClassSummary(df_pred_svm, lev = levels(df_pred_lr$obs))\n\nThis brings us another view: the logistic regression has the highest AUC. This shows that varying the prediction threshold provides a good potential of improving the specificity and the sensitivity (in fine, the balanced accuracy).\nNow we compute the entropy using the mnLogLoss function (entropy is also called log-loss).\n\nmnLogLoss(df_pred_lr, lev = levels(df_pred_lr$obs))\nmnLogLoss(df_pred_ct, lev = levels(df_pred_lr$obs))\nmnLogLoss(df_pred_svm, lev = levels(df_pred_lr$obs))\n\nHere again, the entropy selects the logistic regression as the best model, though close to classification tree and SVM.\n\n\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\n## Logistic regression\nprint(f\"AUC: {roc_auc_score(y_test, doc_lr_prob):.3f}\")\n\n## Classification tree\nprint(f\"AUC: {roc_auc_score(y_test, doc_ct_prob):.3f}\")\n\n## SVM radial basis\nprint(f\"AUC: {roc_auc_score(y_test, doc_svm_prob):.3f}\")\n\n# Now we compute the entropy using the `log_loss` function (entropy is also called *log-loss*).\n\nfrom sklearn.metrics import log_loss\n\n## Logistic regression\nprint(f\"Log-loss: {log_loss(y_test, doc_lr_prob):.3f}\")\n\n## Classification tree\nprint(f\"Log-loss: {log_loss(y_test, doc_ct_prob):.3f}\")\n\n## SVM radial basis\nprint(f\"Log-loss: {log_loss(y_test, doc_svm_prob):.3f}\")",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#roc-curve-prob-threshold-tuning",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#roc-curve-prob-threshold-tuning",
    "title": "",
    "section": "ROC curve & prob threshold tuning",
    "text": "ROC curve & prob threshold tuning\n\nRPython\n\n\nTo go deeper in the analysis, we now produce the ROC curve of each model using the roc function of the proc package.\n\nlibrary(pROC)\nROC_lr &lt;- roc(obs ~ Yes, data=df_pred_lr)\nROC_ct &lt;- roc(obs ~ Yes, data=df_pred_ct)\nROC_svm &lt;- roc(obs ~ Yes, data=df_pred_svm)\n\nplot(ROC_lr, print.thres=\"best\")\nplot(ROC_ct, print.thres=\"best\", add=TRUE)\nplot(ROC_svm, print.thres=\"best\", add=TRUE)\n\nThe plotting function provides an “optimal” threshold that reaches the best trade-off between sensitivity and specificity (according to some criterion). We see that there is room to improve this trade-off.\nNow, to tune this threshold, we need to do it on the training set to avoid overfitting. To do this, we just repeat the previous calculations (predictions) on the training set. To simplify, we only do this on the logistic regression (note that you can try on the other models; you may find that logistic regression is the best one).\n\nDoc_lr_prob_tr &lt;- predict(Doc_lr, newdata=df_tr, type=\"response\")\ndf_pred_lr_tr &lt;- data.frame(obs=df_tr$visits,\n                            Yes=Doc_lr_prob_tr)\nROC_lr_tr &lt;- roc(obs ~ Yes, data=df_pred_lr_tr)\nplot(ROC_lr_tr, print.thres=\"best\")\n\nThe best threshold is 0.193. Now let us compute the confusion table with this threshold.\n\nDoc_lr_pred_opt &lt;- ifelse(Doc_lr_prob&gt;0.193,\"Yes\",\"No\")\nconfusionMatrix(data=as.factor(Doc_lr_pred_opt), reference = df_te$visits)\n\nWe now have a model with an accuracy of circa \\(70\\%\\) but with a balanced accuracy of \\(67\\%\\). Far from perfect, this is still an interesting improvement compare to the CT \\(62\\%\\). The specificity and sensitivity are now respectively \\(62\\%\\) and \\(72\\%\\). The specificity in particular made a huge improvement (from around \\(29\\%\\) at best - by CT - to \\(62\\%\\) - by log. reg).\nIf the aim is to predict both “Yes” and “No”, this last model (log. reg. with tuned threshold) is the best one to use.\n\n\n\n## Logistic regression\n## We need to turn back our results into binary values to be plotted\ndoc_lr_prob_dict = {'Yes': 1, 'No': 0}\ny_test_binary = np.array([doc_lr_prob_dict[x] for x in y_test])\nfpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test_binary, doc_lr_prob)\nplt.plot(fpr_lr, tpr_lr, label=\"Logistic Regression\")\n\n## Classification tree\ndoc_ct_prob_dict = {'Yes': 1, 'No': 0}\ny_test_binary = np.array([doc_ct_prob_dict[x] for x in y_test])\nfpr_ct, tpr_ct, thresholds_ct = roc_curve(y_test_binary, doc_ct_prob)\nplt.plot(fpr_ct, tpr_ct, label=\"Classification Tree\")\n\n## SVM radial basis\ndoc_svm_prob_dict = {'Yes': 1, 'No': 0}\ny_test_binary = np.array([doc_svm_prob_dict[x] for x in y_test])\nfpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test_binary, doc_svm_prob)\n\n# Clear the last plot (if any)\n# plt.clf()\n\nplt.plot(fpr_svm, tpr_svm, label=\"SVM Radial Basis\")\n# Plot the ROC curve\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Classifier\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()\n\nWe can then plot the results in the similar way to R:\n\ndoc_lr_prob_tr = doc_lr.predict_proba(X_train)[:,1]\ndoc_lr_prob_tr_dict = {'Yes': 1, 'No': 0}\ny_train_binary = np.array([doc_lr_prob_tr_dict[x] for x in y_train])\nfpr_lr_tr, tpr_lr_tr, thresholds_lr_tr = roc_curve(y_train_binary, doc_lr_prob_tr)\noptimal_idx = np.argmax(tpr_lr_tr - fpr_lr_tr)\noptimal_threshold = thresholds_lr_tr[optimal_idx]\nprint(f\"Optimal threshold: {optimal_threshold:.3f}\")\n\nFinally, we print the confusion matrix again:\n\ndoc_lr_pred_opt = np.where(doc_lr_prob &gt; optimal_threshold, \"Yes\", \"No\")\nprint(confusion_matrix(y_test, doc_lr_pred_opt))\n\nThe logistic regression produced with R was better.",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#classification",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#classification",
    "title": "",
    "section": "Classification",
    "text": "Classification\nRepeat the analysis on the German credit data. Put several models in competition. Tune them and try to optimize their threshold. Select the best one and analyze its performance.",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#regression",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#regression",
    "title": "",
    "section": "Regression",
    "text": "Regression\nRepeat the analysis on the nursing cost data. Put several models in competition. Tune them and select the best one. Analyze its performance using a scatterplot.",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html",
    "href": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html",
    "title": "Models: Support Vector Machine",
    "section": "",
    "text": "In this exercise, we apply the Support Vector Machines (SVM) to the classification problem of the data set Carseats from the package ISLR (already used with CART).\nSVM are often difficult to interpret. They are typically what we call “a black box” model, that is, a model which provides predictions without understanding what is behind (except if you have followed the course…) and how the features influences these predictions. Therefore, and because we do not want to get bored, we will also use the caret::train() function to make our first real machine learning application. Note that this application can be done also with the models that have been seen previously.\n\n\nThe lines below are just a repetition/reminder of the CART series to have the data ready. The only difference is the cast of SalesHigh into factors rather than characters because the SVM functions require it.\nTo proceed we first have to build the data (below is the) Install the package ISLR in order to access the data set Carseats. Use ?Carseats to read its description.\nTo apply a classification of the sales, we first create a categorical outcome SaleHigh which equals “Yes” if Sales &gt; 7.5 and “No” otherwise. Then we create a data frame MyCarseats containing SaleHigh and all the features of Carseats except Sales. Finally, split MyCarseats into a training and a test set (2/3 vs 1/3). Below we call them df_tr and df_te.\n\nlibrary(ISLR)\nlibrary(dplyr)\nMyCarseats &lt;- Carseats %&gt;% mutate(SaleHigh=ifelse(Sales &gt; 7.5, \"Yes\", \"No\"))\nMyCarseats &lt;- MyCarseats %&gt;% select(-Sales)\nMyCarseats$SaleHigh &lt;- as.factor(MyCarseats$SaleHigh)\n\nset.seed(123) # for reproducibility \nindex_tr &lt;- sample(x=1:nrow(MyCarseats), size=0.8*nrow(MyCarseats), replace=FALSE)\ndf_tr &lt;- MyCarseats[index_tr,]\ndf_te &lt;- MyCarseats[-index_tr,]\n\n\n\n\n\nRPython\n\n\nThe e1071::svm() function of the e1071 package allows to fit SVM to the data with several possible kernels. Below, it is the linear kernel. We fit a linear kernel and check the predictions on the test set.\n\nlibrary(e1071)\nset.seed(123)\ncarseats_svm &lt;- svm(SaleHigh ~ ., data=df_tr, kernel=\"linear\")\ncarseats_svm\ncarseats_svm_pred &lt;- predict(carseats_svm, newdata = df_te)\n\ntable(Pred=carseats_svm_pred, obs=df_te$SaleHigh)\n\nTo obtain a better insight about the prediction quality, we will use the accuracy measure. It is simply the proportion of correct predictions. This can be conveniently obtained (and much more) from the function caret::confusionMatrix() of the library caret. In the parameters, data are the predictions, and reference are the observations.\n\nlibrary(caret)\nconfusionMatrix(data=carseats_svm_pred, reference = df_te$SaleHigh )\n\nWe should only focus on the accuracy for now (the other measures will be studied later). In our run, it was ≈\\(86\\%\\). That number is obtained using the default parameter, cost \\(C=1\\).\n\n\n\n# The usual loading of our environment\nlibrary(reticulate)\nuse_condaenv(\"MLBA\", required = TRUE)\n\nSimilar to the CART exercises, we use the df_train and df_te created in R to carry out our SVM training. Once again, we continue using the sklearn library. First, we copy our dataset from the R variable to a python variable. Then we use LabelEncoder() from sklearn.preprocessing to convert categorical data into a numeric form, which many sklearn machine learning algorithms require. The encoding assigns a unique numerical value to each categorical value, which can sometimes help the performance. In the case of caret::train(), the function handles this transformation automatically. Also, we standardize the data in the case of python for faster computations using StandardScaler, because of the same numerical stability mentioned for CART. After this, we divide the training and test sets into predictors (e.g., X_train) and the outcome (e.g., y_train) and initialize our linear kernel SVM to fit the model to the data.\nWe will use classification_report and accuracy_score from sklearn.metrics to get more information on the performance. (you could also use confusion_matrix from the same module.)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import svm\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n\n# We copy the training and testing datasets from R to Python.\n# Use r.df_tr to access R objects from Python\ncarset_tr_py = r.df_tr.copy()\ncarset_te_py = r.df_te.copy()\n\n# We encode categorical variables as numeric, which is necessary for SVM.\nle = LabelEncoder()\ncat_vars = [\"ShelveLoc\", \"Urban\", \"US\", \"SaleHigh\"]\nfor var in cat_vars:\n    # Fit the LabelEncoder to the training set and transform the training and testing sets\n    # We need to use the same encoder for both sets to ensure consistency\n    carset_tr_py[var] = le.fit_transform(carset_tr_py[var])\n    carset_te_py[var] = le.transform(carset_te_py[var])\n\n# Split the data into training and testing sets\nX_train, y_train = carset_tr_py.drop(columns=[\"SaleHigh\"]), carset_tr_py[\"SaleHigh\"]\nX_test, y_test = carset_te_py.drop(columns=[\"SaleHigh\"]), carset_te_py[\"SaleHigh\"]\n\n# Standardize only the continuous variables\ncont_vars = [\"CompPrice\", \"Income\", \"Advertising\", \"Population\", \"Price\", \"Age\", \"Education\"]\nscaler = StandardScaler()\nX_train[cont_vars] = scaler.fit_transform(X_train[cont_vars])\nX_test[cont_vars] = scaler.transform(X_test[cont_vars])\n\n# To speed up the operation you can also transform the inputs\n# X_train = X_train.to_numpy()\n# y_train = y_train.to_numpy()\n\n# Set random seed for reproducibility\nnp.random.seed(123)\n\n# Initialize a linear SVM model\ncarseats_svm_py = svm.SVC(kernel=\"linear\")\n\n# Fit the SVM model to the training data\ncarseats_svm_py.fit(X_train, y_train)\n\n# Print the model parameters\nprint(carseats_svm_py)\n\n# Make predictions on the test set\ncarseats_svm_pred_py = carseats_svm_py.predict(X_test)\n\n# Print a confusion matrix\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_py, rownames=['True'], colnames=['Predicted']))\n# Alternatively, print a confusion matrix using `sklearn.metrics`\n# print(confusion_matrix(y_test, carseats_svm_pred_py))\n\n# Compute metrics\nreport_linear_py = classification_report(y_test, carseats_svm_pred_py, target_names=['No', 'Yes'])\naccuracy_linear_py = accuracy_score(y_test, carseats_svm_pred_py)\n\nprint(report_linear_py)\nprint(\"Overall accuracy:\", accuracy_linear_py)\n\nThis performance is worse than our model in R and requires more tuning (due to differences in default values and implementations of the functions).\n\n\n\n\n\n\n\nRPython\n\n\nWe try now with a radial basis kernel (the default).\n\nset.seed(123)\ncarseats_rb &lt;- svm(SaleHigh ~ ., data=df_tr, kernel=\"radial\")\ncarseats_rb\ncarseats_rb_pred &lt;- predict(carseats_rb, newdata = df_te)\nconfusionMatrix(data=carseats_rb_pred, reference = df_te$SaleHigh )\n\nThe accuracy is now ≈\\(81\\%\\). This shows how important that choice can be. In the same vein, we are now relying on the default parameters of the function. For the cost \\(C\\) it is \\(1\\), for the parameter gamma of the kernel, it is 1/(data dimension) (see ?svm). The train function allows us to make a better selection.\n\n\nThis is same as the linear kernel, and we just need to change the kernel parameter value from linear to rbf. Here’s the radial version:\n\nnp.random.seed(123) # once again, for reproducibility\n\n# Use a radial kernel in the SVM classifier\ncarseats_svm_rb_py = svm.SVC(kernel=\"rbf\")\ncarseats_svm_rb_py.fit(X_train, y_train)\nprint(carseats_svm_rb_py)\n\n# Predict the values with the radial approach\ncarseats_svm_pred_rb_py = carseats_svm_rb_py.predict(X_test)\n\n# Compute metrics\nreport_radial_py = classification_report(y_test, carseats_svm_pred_rb_py, target_names=['No', 'Yes'])\naccuracy_radial_py = accuracy_score(y_test, carseats_svm_pred_rb_py)\n\n# Print metrics\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_rb_py, rownames=['True'], colnames=['Predicted']))\nprint(report_radial_py)\nprint(\"Overall accuracy:\", accuracy_radial_py)\n\n\n\n\nIn both cases (R & python), with the default parameters, the linear kernel seems to do better than the radial one.\n\n\n\n\nRPython\n\n\nIn R, caret uses various libraries to run the svm models (check for yourself by searching for support vector machine here). For instance, calling svmLinear or svmRadial uses the library kernlab, and the kernlab::ksvm() function.\nThe C hyperparameter (from kernlab::ksvm()) accounts for the cost argument and controls the trade-off between allowing misclassifications in the training set and finding a decision boundary that generalizes well to new data. A larger cost value leads to a smaller margin and a more complex model that may overfit the data. On the other hand, a smaller cost value leads to a larger margin and a simpler model that may underfit the data.\nSimilarly to EX_ML_NN, to select the good hyperparameters, we build a search grid and fit the model with each possible value in the grid. Then, the best model is chosen among all the combinations of the hyperparameters.\nAs a reminder, the train function from caret. Has:\n\na formula.\na dataset.\na method (i.e. the model which in this case is SVM with linear kernel).\na training control procedure.\n\n\n\n\ntrctrl &lt;- trainControl(method = \"cv\", number=10)\nset.seed(143)\nsvm_Linear &lt;- train(SaleHigh ~., data = df_tr, method = \"svmLinear\",\n                    trControl=trctrl)\nsvm_Linear\n\nFor now, the validation accuracy is very high (≈\\(89\\%\\)). This is normal since this accuracy is computed on the training set.\nWe now supply a grid of values for the cost that we want to try and pass to the argugmenttuneGrid. Be patient, it may take time.\n\ngrid &lt;- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))\ngrid\nset.seed(143)\nsvm_Linear_Grid &lt;- train(SaleHigh ~., data = df_tr, method = \"svmLinear\",\n                           trControl=trctrl,\n                           tuneGrid = grid)\nsvm_Linear_Grid\nplot(svm_Linear_Grid)\nsvm_Linear_Grid$bestTune\n\nWe see that setting the cost to 1 provides the best model. The accuracy apparently reaches a plateau at this value. This is same as our cost parameter in section 1.2.\n\n\n\nThe sigma hyperparameter (also from kernlab::ksvm()) controls the width of the radial basis function kernel, which is used to transform the input data into a higher-dimensional feature space. A larger value of sigma corresponds to a narrower kernel and a more complex model, while a smaller value corresponds to a wider kernel and a simpler model.\nWe repeat the procedure for SVM with a radial basis kernel. Here, there are two parameters ( sigma and C) to tune. The grid choice is rather arbitrary (often the result of trials and errors), and very few general useful guidelines exist. The code below may take a few minutes to run.\n\ngrid_radial &lt;- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),\n                           C = c(1, 10, 100, 500, 1000))\ngrid_radial\nset.seed(143)\nsvm_Radial_Grid &lt;- train(SaleHigh ~., data = df_tr, method = \"svmRadial\",\n                           trControl=trctrl,\n                           tuneGrid = grid_radial)\nsvm_Radial_Grid\nplot(svm_Radial_Grid)\nsvm_Radial_Grid$bestTune\n\nThe optimal model from this search is with sigma = 0.01 and C=100.\n\n\n\nWe can use GridSearchCV from sklearn.model_selection to achieve the same goal in python and set the argument for cross-validation (cv to achieve the same results) and tune both types of kernels at once. sklearn.svm.SVC() does contain the two arguments for the C and sigma but the relationship is slightly different and we’ll explain below. Also, please note in this approach, the linear kernel by default ignores the sigma values.\nAs mentioned earlier, in sklearn.svm.SVC(), the equivalent parameter to cost is also C, which is straightforward. Still, the equivalent parameter to sigma is a bit trickier (called gamma in sklearn.svm.SVC()), which also controls the width of the radial basis function kernel. However, the relationship between gamma and sigma differs, and the two parameters cannot be directly compared. In particular, gamma is defined as the inverse of the width of the kernel, i.e., gamma = 1/(2 * sigma**2).\nFor simplicity’s sake, we’ll directly use the gamma with the same values as sigma; however, you can always run 'gamma': [1/(2*sigma**2) for sigma in [0.01, 0.02, 0.05, 0.1]] to get similar values (although you would want to round as this division results in non-terminating repeating decimal numbers).\nThe code will take a few minutes to run (longer than the R version).\n\nfrom sklearn.model_selection import GridSearchCV\n\nnp.random.seed(123) # for reproducibility\n\n# Define the grid of hyperparameters to search over\nparam_grid = {'C': [1, 10, 100, 500, 1000],\n              'gamma': [0.01, 0.02, 0.05, 0.1],\n              # 'gamma': [round(1/(2*sigma**2),2) for sigma in [0.01, 0.02, 0.05, 0.1]],\n              'kernel': ['linear', 'rbf']}\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(svm.SVC(), param_grid, cv=10, scoring='accuracy', n_jobs=1) # you can also set n_jobs = -1 to use all the cores and obtain the results faster (but unfortunately atm it only works on Mac/Linux and not Windows OS with `reticulate`)\n# for more info, please see https://github.com/rstudio/reticulate/issues/1346\ngrid_search.fit(X_train, y_train)\n\nprint(\"Best hyperparameters:\", grid_search.best_params_)\nprint(\"Best score:\", grid_search.best_score_)\n\nNote that the accuracy is on the training set. The best parameters are returned bybest_grid_param. We will use a new plotting library for seeing this evolution called seaborn, which offers some great visualization tools.\nWe use the installed seaborn package from our Setup which allows for grouping our hyperparameters and displaying them with a heatmap.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming `results` is your DataFrame from `grid_search.cv_results_`\nresults = pd.DataFrame(grid_search.cv_results_)\n\n# Before performing the pivot or groupby operation, drop the 'params' column (it's a dictionary)\nresults = results.drop(columns=['params'])\n\n# Now, you can safely group by 'param_C', 'param_gamma', and 'param_kernel' to calculate mean test scores\ngrouped_results = results.pivot_table(index=['param_C', 'param_gamma'], columns='param_kernel', values='mean_test_score')\n\nplt.clf()\n\n# Create the heatmap\nax = sns.heatmap(grouped_results, annot=True, fmt='.3f', cmap='viridis', cbar=False)\nax.invert_yaxis()  # invert the y-axis to match your preference\n\nplt.xlabel('Kernel')\nplt.ylabel('C-Gamma')\nplt.show()\n\nWe can see that rbf kernel benefits from changes in C & gamma, however, for the linear, we’re always using the same gamma of 0.01 so for this kernel, the only changes are coming from C parameter.\n\n\n\n\n\n\n\nRPython\n\n\nAfter finding the best hyperparameters, it is often good practice to re-train the model with the best hyperparameters on the entire training set before evaluating everything on the test set. We do not need to re-train the model with the entire dataset for the linear SVM, as the best cost matched those used in section 1.2. We re-train the final model with the entire training set using optimal hyperparameters for the radial basis kernel.\n\ncarseats_rb_tuned &lt;- svm(SaleHigh ~ .,data = df_tr,\n                         kernel = \"radial\", gamma = svm_Radial_Grid$bestTune$sigma,\n                         cost = svm_Radial_Grid$bestTune$C)\ncarseats_rb_tuned_pred &lt;- predict(carseats_rb_tuned, newdata = df_te)\nconfusionMatrix(data=carseats_rb_tuned_pred, reference = df_te$SaleHigh)\n\nOverall, if we compare all the models, we see that the linear kernel SVM with cost of 1 looks like the best model. We already saw that it provides a \\(86\\%\\) accuracy on the test set. This is what can be expected in the future from that model.\n\n\n\n# re-train the model with best hyperparameters\nsvm_best_py = svm.SVC(**grid_search.best_params_)\nsvm_best_py.fit(X_train, y_train)\n\n# predict on test dataset\ncarseats_svm_pred_py = svm_best_py.predict(X_test)\n\n# print confusion matrix and accuracy score\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_py, rownames=['True'], colnames=['Predicted']))\nprint(classification_report(y_test, carseats_svm_pred_py, target_names=['No', 'Yes']))\nprint('Accuracy Score:', accuracy_score(y_test, carseats_svm_pred_py))\n\nThe results are similar to the R outcome for the linear models (except differences in the confusion matrix).\n\n\n\n\n\n\nRepeat the analysis on the German credit data (german.csv). Since dataset is much larger than MyCarSeats, the tuning procedure may be longer. For this reason, just limit to a linear SVM model for the tuning with limited range for the grid search.",
    "crumbs": [
      "Labs",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#prepare-the-data",
    "href": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#prepare-the-data",
    "title": "Models: Support Vector Machine",
    "section": "",
    "text": "The lines below are just a repetition/reminder of the CART series to have the data ready. The only difference is the cast of SalesHigh into factors rather than characters because the SVM functions require it.\nTo proceed we first have to build the data (below is the) Install the package ISLR in order to access the data set Carseats. Use ?Carseats to read its description.\nTo apply a classification of the sales, we first create a categorical outcome SaleHigh which equals “Yes” if Sales &gt; 7.5 and “No” otherwise. Then we create a data frame MyCarseats containing SaleHigh and all the features of Carseats except Sales. Finally, split MyCarseats into a training and a test set (2/3 vs 1/3). Below we call them df_tr and df_te.\n\nlibrary(ISLR)\nlibrary(dplyr)\nMyCarseats &lt;- Carseats %&gt;% mutate(SaleHigh=ifelse(Sales &gt; 7.5, \"Yes\", \"No\"))\nMyCarseats &lt;- MyCarseats %&gt;% select(-Sales)\nMyCarseats$SaleHigh &lt;- as.factor(MyCarseats$SaleHigh)\n\nset.seed(123) # for reproducibility \nindex_tr &lt;- sample(x=1:nrow(MyCarseats), size=0.8*nrow(MyCarseats), replace=FALSE)\ndf_tr &lt;- MyCarseats[index_tr,]\ndf_te &lt;- MyCarseats[-index_tr,]",
    "crumbs": [
      "Labs",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#linear_svm",
    "href": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#linear_svm",
    "title": "Models: Support Vector Machine",
    "section": "",
    "text": "RPython\n\n\nThe e1071::svm() function of the e1071 package allows to fit SVM to the data with several possible kernels. Below, it is the linear kernel. We fit a linear kernel and check the predictions on the test set.\n\nlibrary(e1071)\nset.seed(123)\ncarseats_svm &lt;- svm(SaleHigh ~ ., data=df_tr, kernel=\"linear\")\ncarseats_svm\ncarseats_svm_pred &lt;- predict(carseats_svm, newdata = df_te)\n\ntable(Pred=carseats_svm_pred, obs=df_te$SaleHigh)\n\nTo obtain a better insight about the prediction quality, we will use the accuracy measure. It is simply the proportion of correct predictions. This can be conveniently obtained (and much more) from the function caret::confusionMatrix() of the library caret. In the parameters, data are the predictions, and reference are the observations.\n\nlibrary(caret)\nconfusionMatrix(data=carseats_svm_pred, reference = df_te$SaleHigh )\n\nWe should only focus on the accuracy for now (the other measures will be studied later). In our run, it was ≈\\(86\\%\\). That number is obtained using the default parameter, cost \\(C=1\\).\n\n\n\n# The usual loading of our environment\nlibrary(reticulate)\nuse_condaenv(\"MLBA\", required = TRUE)\n\nSimilar to the CART exercises, we use the df_train and df_te created in R to carry out our SVM training. Once again, we continue using the sklearn library. First, we copy our dataset from the R variable to a python variable. Then we use LabelEncoder() from sklearn.preprocessing to convert categorical data into a numeric form, which many sklearn machine learning algorithms require. The encoding assigns a unique numerical value to each categorical value, which can sometimes help the performance. In the case of caret::train(), the function handles this transformation automatically. Also, we standardize the data in the case of python for faster computations using StandardScaler, because of the same numerical stability mentioned for CART. After this, we divide the training and test sets into predictors (e.g., X_train) and the outcome (e.g., y_train) and initialize our linear kernel SVM to fit the model to the data.\nWe will use classification_report and accuracy_score from sklearn.metrics to get more information on the performance. (you could also use confusion_matrix from the same module.)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import svm\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n\n# We copy the training and testing datasets from R to Python.\n# Use r.df_tr to access R objects from Python\ncarset_tr_py = r.df_tr.copy()\ncarset_te_py = r.df_te.copy()\n\n# We encode categorical variables as numeric, which is necessary for SVM.\nle = LabelEncoder()\ncat_vars = [\"ShelveLoc\", \"Urban\", \"US\", \"SaleHigh\"]\nfor var in cat_vars:\n    # Fit the LabelEncoder to the training set and transform the training and testing sets\n    # We need to use the same encoder for both sets to ensure consistency\n    carset_tr_py[var] = le.fit_transform(carset_tr_py[var])\n    carset_te_py[var] = le.transform(carset_te_py[var])\n\n# Split the data into training and testing sets\nX_train, y_train = carset_tr_py.drop(columns=[\"SaleHigh\"]), carset_tr_py[\"SaleHigh\"]\nX_test, y_test = carset_te_py.drop(columns=[\"SaleHigh\"]), carset_te_py[\"SaleHigh\"]\n\n# Standardize only the continuous variables\ncont_vars = [\"CompPrice\", \"Income\", \"Advertising\", \"Population\", \"Price\", \"Age\", \"Education\"]\nscaler = StandardScaler()\nX_train[cont_vars] = scaler.fit_transform(X_train[cont_vars])\nX_test[cont_vars] = scaler.transform(X_test[cont_vars])\n\n# To speed up the operation you can also transform the inputs\n# X_train = X_train.to_numpy()\n# y_train = y_train.to_numpy()\n\n# Set random seed for reproducibility\nnp.random.seed(123)\n\n# Initialize a linear SVM model\ncarseats_svm_py = svm.SVC(kernel=\"linear\")\n\n# Fit the SVM model to the training data\ncarseats_svm_py.fit(X_train, y_train)\n\n# Print the model parameters\nprint(carseats_svm_py)\n\n# Make predictions on the test set\ncarseats_svm_pred_py = carseats_svm_py.predict(X_test)\n\n# Print a confusion matrix\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_py, rownames=['True'], colnames=['Predicted']))\n# Alternatively, print a confusion matrix using `sklearn.metrics`\n# print(confusion_matrix(y_test, carseats_svm_pred_py))\n\n# Compute metrics\nreport_linear_py = classification_report(y_test, carseats_svm_pred_py, target_names=['No', 'Yes'])\naccuracy_linear_py = accuracy_score(y_test, carseats_svm_pred_py)\n\nprint(report_linear_py)\nprint(\"Overall accuracy:\", accuracy_linear_py)\n\nThis performance is worse than our model in R and requires more tuning (due to differences in default values and implementations of the functions).",
    "crumbs": [
      "Labs",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#radial-basis-svm",
    "href": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#radial-basis-svm",
    "title": "Models: Support Vector Machine",
    "section": "",
    "text": "RPython\n\n\nWe try now with a radial basis kernel (the default).\n\nset.seed(123)\ncarseats_rb &lt;- svm(SaleHigh ~ ., data=df_tr, kernel=\"radial\")\ncarseats_rb\ncarseats_rb_pred &lt;- predict(carseats_rb, newdata = df_te)\nconfusionMatrix(data=carseats_rb_pred, reference = df_te$SaleHigh )\n\nThe accuracy is now ≈\\(81\\%\\). This shows how important that choice can be. In the same vein, we are now relying on the default parameters of the function. For the cost \\(C\\) it is \\(1\\), for the parameter gamma of the kernel, it is 1/(data dimension) (see ?svm). The train function allows us to make a better selection.\n\n\nThis is same as the linear kernel, and we just need to change the kernel parameter value from linear to rbf. Here’s the radial version:\n\nnp.random.seed(123) # once again, for reproducibility\n\n# Use a radial kernel in the SVM classifier\ncarseats_svm_rb_py = svm.SVC(kernel=\"rbf\")\ncarseats_svm_rb_py.fit(X_train, y_train)\nprint(carseats_svm_rb_py)\n\n# Predict the values with the radial approach\ncarseats_svm_pred_rb_py = carseats_svm_rb_py.predict(X_test)\n\n# Compute metrics\nreport_radial_py = classification_report(y_test, carseats_svm_pred_rb_py, target_names=['No', 'Yes'])\naccuracy_radial_py = accuracy_score(y_test, carseats_svm_pred_rb_py)\n\n# Print metrics\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_rb_py, rownames=['True'], colnames=['Predicted']))\nprint(report_radial_py)\nprint(\"Overall accuracy:\", accuracy_radial_py)\n\n\n\n\nIn both cases (R & python), with the default parameters, the linear kernel seems to do better than the radial one.",
    "crumbs": [
      "Labs",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#tuning-the-hyperparameter",
    "href": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#tuning-the-hyperparameter",
    "title": "Models: Support Vector Machine",
    "section": "",
    "text": "RPython\n\n\nIn R, caret uses various libraries to run the svm models (check for yourself by searching for support vector machine here). For instance, calling svmLinear or svmRadial uses the library kernlab, and the kernlab::ksvm() function.\nThe C hyperparameter (from kernlab::ksvm()) accounts for the cost argument and controls the trade-off between allowing misclassifications in the training set and finding a decision boundary that generalizes well to new data. A larger cost value leads to a smaller margin and a more complex model that may overfit the data. On the other hand, a smaller cost value leads to a larger margin and a simpler model that may underfit the data.\nSimilarly to EX_ML_NN, to select the good hyperparameters, we build a search grid and fit the model with each possible value in the grid. Then, the best model is chosen among all the combinations of the hyperparameters.\nAs a reminder, the train function from caret. Has:\n\na formula.\na dataset.\na method (i.e. the model which in this case is SVM with linear kernel).\na training control procedure.\n\n\n\n\ntrctrl &lt;- trainControl(method = \"cv\", number=10)\nset.seed(143)\nsvm_Linear &lt;- train(SaleHigh ~., data = df_tr, method = \"svmLinear\",\n                    trControl=trctrl)\nsvm_Linear\n\nFor now, the validation accuracy is very high (≈\\(89\\%\\)). This is normal since this accuracy is computed on the training set.\nWe now supply a grid of values for the cost that we want to try and pass to the argugmenttuneGrid. Be patient, it may take time.\n\ngrid &lt;- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))\ngrid\nset.seed(143)\nsvm_Linear_Grid &lt;- train(SaleHigh ~., data = df_tr, method = \"svmLinear\",\n                           trControl=trctrl,\n                           tuneGrid = grid)\nsvm_Linear_Grid\nplot(svm_Linear_Grid)\nsvm_Linear_Grid$bestTune\n\nWe see that setting the cost to 1 provides the best model. The accuracy apparently reaches a plateau at this value. This is same as our cost parameter in section 1.2.\n\n\n\nThe sigma hyperparameter (also from kernlab::ksvm()) controls the width of the radial basis function kernel, which is used to transform the input data into a higher-dimensional feature space. A larger value of sigma corresponds to a narrower kernel and a more complex model, while a smaller value corresponds to a wider kernel and a simpler model.\nWe repeat the procedure for SVM with a radial basis kernel. Here, there are two parameters ( sigma and C) to tune. The grid choice is rather arbitrary (often the result of trials and errors), and very few general useful guidelines exist. The code below may take a few minutes to run.\n\ngrid_radial &lt;- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),\n                           C = c(1, 10, 100, 500, 1000))\ngrid_radial\nset.seed(143)\nsvm_Radial_Grid &lt;- train(SaleHigh ~., data = df_tr, method = \"svmRadial\",\n                           trControl=trctrl,\n                           tuneGrid = grid_radial)\nsvm_Radial_Grid\nplot(svm_Radial_Grid)\nsvm_Radial_Grid$bestTune\n\nThe optimal model from this search is with sigma = 0.01 and C=100.\n\n\n\nWe can use GridSearchCV from sklearn.model_selection to achieve the same goal in python and set the argument for cross-validation (cv to achieve the same results) and tune both types of kernels at once. sklearn.svm.SVC() does contain the two arguments for the C and sigma but the relationship is slightly different and we’ll explain below. Also, please note in this approach, the linear kernel by default ignores the sigma values.\nAs mentioned earlier, in sklearn.svm.SVC(), the equivalent parameter to cost is also C, which is straightforward. Still, the equivalent parameter to sigma is a bit trickier (called gamma in sklearn.svm.SVC()), which also controls the width of the radial basis function kernel. However, the relationship between gamma and sigma differs, and the two parameters cannot be directly compared. In particular, gamma is defined as the inverse of the width of the kernel, i.e., gamma = 1/(2 * sigma**2).\nFor simplicity’s sake, we’ll directly use the gamma with the same values as sigma; however, you can always run 'gamma': [1/(2*sigma**2) for sigma in [0.01, 0.02, 0.05, 0.1]] to get similar values (although you would want to round as this division results in non-terminating repeating decimal numbers).\nThe code will take a few minutes to run (longer than the R version).\n\nfrom sklearn.model_selection import GridSearchCV\n\nnp.random.seed(123) # for reproducibility\n\n# Define the grid of hyperparameters to search over\nparam_grid = {'C': [1, 10, 100, 500, 1000],\n              'gamma': [0.01, 0.02, 0.05, 0.1],\n              # 'gamma': [round(1/(2*sigma**2),2) for sigma in [0.01, 0.02, 0.05, 0.1]],\n              'kernel': ['linear', 'rbf']}\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(svm.SVC(), param_grid, cv=10, scoring='accuracy', n_jobs=1) # you can also set n_jobs = -1 to use all the cores and obtain the results faster (but unfortunately atm it only works on Mac/Linux and not Windows OS with `reticulate`)\n# for more info, please see https://github.com/rstudio/reticulate/issues/1346\ngrid_search.fit(X_train, y_train)\n\nprint(\"Best hyperparameters:\", grid_search.best_params_)\nprint(\"Best score:\", grid_search.best_score_)\n\nNote that the accuracy is on the training set. The best parameters are returned bybest_grid_param. We will use a new plotting library for seeing this evolution called seaborn, which offers some great visualization tools.\nWe use the installed seaborn package from our Setup which allows for grouping our hyperparameters and displaying them with a heatmap.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming `results` is your DataFrame from `grid_search.cv_results_`\nresults = pd.DataFrame(grid_search.cv_results_)\n\n# Before performing the pivot or groupby operation, drop the 'params' column (it's a dictionary)\nresults = results.drop(columns=['params'])\n\n# Now, you can safely group by 'param_C', 'param_gamma', and 'param_kernel' to calculate mean test scores\ngrouped_results = results.pivot_table(index=['param_C', 'param_gamma'], columns='param_kernel', values='mean_test_score')\n\nplt.clf()\n\n# Create the heatmap\nax = sns.heatmap(grouped_results, annot=True, fmt='.3f', cmap='viridis', cbar=False)\nax.invert_yaxis()  # invert the y-axis to match your preference\n\nplt.xlabel('Kernel')\nplt.ylabel('C-Gamma')\nplt.show()\n\nWe can see that rbf kernel benefits from changes in C & gamma, however, for the linear, we’re always using the same gamma of 0.01 so for this kernel, the only changes are coming from C parameter.",
    "crumbs": [
      "Labs",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#best-model",
    "href": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#best-model",
    "title": "Models: Support Vector Machine",
    "section": "",
    "text": "RPython\n\n\nAfter finding the best hyperparameters, it is often good practice to re-train the model with the best hyperparameters on the entire training set before evaluating everything on the test set. We do not need to re-train the model with the entire dataset for the linear SVM, as the best cost matched those used in section 1.2. We re-train the final model with the entire training set using optimal hyperparameters for the radial basis kernel.\n\ncarseats_rb_tuned &lt;- svm(SaleHigh ~ .,data = df_tr,\n                         kernel = \"radial\", gamma = svm_Radial_Grid$bestTune$sigma,\n                         cost = svm_Radial_Grid$bestTune$C)\ncarseats_rb_tuned_pred &lt;- predict(carseats_rb_tuned, newdata = df_te)\nconfusionMatrix(data=carseats_rb_tuned_pred, reference = df_te$SaleHigh)\n\nOverall, if we compare all the models, we see that the linear kernel SVM with cost of 1 looks like the best model. We already saw that it provides a \\(86\\%\\) accuracy on the test set. This is what can be expected in the future from that model.\n\n\n\n# re-train the model with best hyperparameters\nsvm_best_py = svm.SVC(**grid_search.best_params_)\nsvm_best_py.fit(X_train, y_train)\n\n# predict on test dataset\ncarseats_svm_pred_py = svm_best_py.predict(X_test)\n\n# print confusion matrix and accuracy score\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_py, rownames=['True'], colnames=['Predicted']))\nprint(classification_report(y_test, carseats_svm_pred_py, target_names=['No', 'Yes']))\nprint('Accuracy Score:', accuracy_score(y_test, carseats_svm_pred_py))\n\nThe results are similar to the R outcome for the linear models (except differences in the confusion matrix).",
    "crumbs": [
      "Labs",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#your-turn",
    "href": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#your-turn",
    "title": "Models: Support Vector Machine",
    "section": "",
    "text": "Repeat the analysis on the German credit data (german.csv). Since dataset is much larger than MyCarSeats, the tuning procedure may be longer. For this reason, just limit to a linear SVM model for the tuning with limited range for the grid search.",
    "crumbs": [
      "Labs",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "labs/03_Models/035_KNN/Ex_ML_KNN.html",
    "href": "labs/03_Models/035_KNN/Ex_ML_KNN.html",
    "title": "Models: K-Nearest Neighbors",
    "section": "",
    "text": "K-NN: a gentle introduction\nIn this first part, we apply the K-NN to the iris data set with examples in both R & python. In both cases, we first load the iris built-in data set found dplyr.\n\nlibrary(dplyr)\niris\nstr(iris)\n\n\nRPython\n\n\nIn R, we can model iris using the knn3 function from the caret package (named knn3 because knn is already taken by another package). This function is limited to the Euclidean distance with numerical features.\nWe first make the prediction using a 2-NN (with Euclidean distance).\n\nlibrary(caret)\nmod_knn &lt;- knn3(data=iris, Species~., k=2) ## build the K-NN model\npredict(mod_knn, newdata = iris, type=\"class\")\n\nNow, we want to know if the predictions are good. To do this, we must ensure that the model is not overfitting the data. For this, we must split the data into training and test sets (these concepts will be seen in detail in a future course). To do so, we randomly take 75% of the iris data that will be the training set, and the remaining 25% are the test set.\n\nset.seed(123) ## for replication purpose\n\nindex_tr &lt;- sample(1:nrow(iris), replace=FALSE, size=0.75*nrow(iris))\nindex_tr ## the index of the rows of iris that will be in the training set\n\niris_tr &lt;- iris[index_tr,] ## the training set\niris_te &lt;- iris[-index_tr,] ## the test set\n\nNow we can use the 2-NN to predict the test set using the training set. Note that the model is fitted on the training set, and the predictions are computed on the test set.\n\nmod_knn &lt;- knn3(data=iris_tr, Species~., k=2)\niris_te_pred &lt;- predict(mod_knn, newdata = iris_te, type=\"class\")\n\nTo compare the predictions above and the true species (the one in the test set), we can build a table. It is called a confusion matrix (again, this will be explained in detail later on).\n\ntable(Pred=iris_te_pred, Observed=iris_te[,5])\n\nThe prediction is almost perfect. It is so good that it is pointless to try to improve the prediction by changing K at that point. However, just to illustrate, below we use K=3.\n\nmod_knn &lt;- knn3(data=iris_tr, Species~., k=3)\niris_te_pred &lt;- predict(mod_knn, newdata = iris_te, type=\"class\")\n\ntable(Pred=iris_te_pred, Observed=iris_te[,5])\n\nNote that, in th formula “Species~.”, the dot means “all the other variables”. It is equivalent (but shorter) to “Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width”\n\n\nBefore running the python, we must ensure we’re using the MLBA conda (virtual) environment created during setup.Rmd. As we already installed some of the packages, it should suffice for this part of the exercise:\n\n# make sure we're using the right environment\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\npy_config()\n\nFirst, let’s load the iris dataset and print its structure. You can access the r data object by using r.iris or only the pure python data as shown below (you don’t need the chunk below):\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\niris_df['species'] = iris.target_names[iris.target]\nprint(iris_df)\nprint(iris_df.info())\nNow, in python, we will also apply K-NN to the iris data set using the KNeighborsClassifier function from the sklearn package in python, where we set K=2. This function can be applied to both numerical and categorical features. We then make predictions with this python model.\n\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# call iris data from r\ny = r.iris[[\"Species\"]]  # select column \"Species\"\nX = r.iris.drop(columns=[\"Species\"])  # drop column \"Species\"\n\n# if you imported the data directly in python, you can instead run the commands below\n# y = iris.target\n# X = iris.data\n\nk = 2\nknn_model = KNeighborsClassifier(n_neighbors=k)\nknn_model.fit(X, y)\n\npred = knn_model.predict(X)\nprint(pred)\n\nWe will split the data again into training and test sets. We will randomly select 75% of the data for the training set and the remaining 25% for the test set. To achieve this, we will use numpy, most often used for efficient numerical computing. This split of training and test sets will only be done once in the exercises as we already created the same objects in R. Nevertheless, this is one way of dividing data into training and test sets in python:\n\nimport numpy as np\n\nnp.random.seed(123)\nindex_tr = np.random.choice(range(len(r.iris)), size=int(0.75*len(r.iris)), replace=False)\nindex_te = np.setdiff1d(range(len(r.iris)), index_tr)\n\niris_tr = r.iris.iloc[index_tr, :]\niris_te = r.iris.iloc[index_te, :]\n\nNow, we can fit the K-NN model to the training set and make predictions on the test set.\n\nknn_model = KNeighborsClassifier(n_neighbors=k)\nknn_model.fit(iris_tr.iloc[:, :-1], iris_tr.iloc[:, -1])\n\npred = knn_model.predict(iris_te.iloc[:, :-1])\nprint(pred)\n\nTo evaluate the performance of our model, we can construct a confusion matrix.\n\nfrom sklearn.metrics import confusion_matrix\n\n# for the confusion matrix in python, we need to specify the column names\nlabels = r.iris['Species'].unique()\n\n# create the confusion matrix with the labels as column names\nconf_mat = confusion_matrix(iris_te.iloc[:, -1], pred, labels=labels)\nconf_mat_df = pd.DataFrame(conf_mat, columns=labels, index=labels)\n\n# print the confusion matrix\nprint(conf_mat_df)\n\nThe prediction is almost perfect (as was the case in R), so it is not necessary to try to improve the prediction by changing k at this point. However, just for illustration, we can repeat the process with K=3.\n\nk = 3\n\nknn_model = KNeighborsClassifier(n_neighbors=k)\nknn_model.fit(iris_tr.iloc[:, :-1], iris_tr.iloc[:, -1])\n\npred = knn_model.predict(iris_te.iloc[:, :-1])\nconf_mat = confusion_matrix(iris_te.iloc[:, -1], pred, labels=labels)\nprint(pd.DataFrame(conf_mat, index=labels, columns=labels))\n\n\n\n\n\n\n\nWhy results are different in Python vs R\n\n\n\nPlease note that the training and test sets we used in python are not the same ones used in R due to a different generator for the seed (i.e., set.seed(123) is not generating the same numbers as np.random.seed(123)). Therefore, different observations were taken for the test set in the two languages. If you want to see the same performance, you can also run in both languages with the same dataset. For instance, running the code below would give you the same results:\n```{r}\nmod_knn_py &lt;- knn3(data=py$iris_tr, Species~., k=2)\niris_te_pred_py &lt;- predict(mod_knn_py, newdata = py$iris_te, type=\"class\")\ntable(Pred=iris_te_pred_py, Observed=py$iris_te[,5])\n```\n\n\n\n\n\n\n\nK-NN: regression\nIn the case of a regression task, the prediction is obtained by averaging the K nearest neighbors. To illustrate this, we will use the imports.85 data set. The aim is to predict the price using only the numerical features (categorical features are illustrated later).\nBelow, we identify the numerical columns (don’t forget to load the data first!). The new data frame is named tmp (temporary…). Then, we remove all the rows containing an NA.\n\nimports_85 &lt;- read.csv(here::here(\"labs/data/imports-85.data\"), \n                       header=FALSE, na.strings=\"?\")\n\nnames = c(\"symboling\",\"normalized.losses\",\"make\",\"fuel.type\",\n         \"aspiration\",\"num.of.doors\",\"body.style\",\"drive.wheels\",\n         \"engine.location\",\"wheel.base\",\"length\",\"width\",\n         \"height\",\"curb.weight\",\"engine.type\",\"num.of.cylinders\",\n         \"engine.size\",\"fuel.system\",\"bore\",\"stroke\",\n         \"compression.ratio\",\"horsepower\",\"peak.rpm\",\"city.mpg\",\n         \"highway.mpg\",\"price\")\n\nnames(imports_85) = names\n\ntmp &lt;- imports_85 %&gt;% select(where(is.numeric))\ntmp &lt;- filter(tmp, complete.cases(tmp))\ntmp %&gt;% head()\n\nNow, we make a 75%-25% split for our training and testing.\n\nset.seed(123)\nindex_tr &lt;- sample(1:nrow(tmp), size=0.75*nrow(tmp), replace = FALSE)\ntmp_tr &lt;- tmp[index_tr,]\ntmp_te &lt;- tmp[-index_tr,]\n\n\nRPython\n\n\nWe now fit the knnreg function to fit the model in R.\n\nmod_knn &lt;- knnreg(data=tmp_tr, price~., k=3)\ntmp_pred &lt;- predict(mod_knn, newdata=tmp_te)\n\nWe now graphically compare the real prices in the test set with the predicted ones (the diagonal line shows the equality between the prediction and the real price).\n\ntmp_te %&gt;% mutate(pred=tmp_pred) %&gt;%\nggplot(aes(x=price, y=pred)) + \n  geom_point() + geom_abline(slope=1, intercept = 0)\n\nIt looks like there is still room for improvement. Check for yourself if this can be improved by changing K.\n\n\nAs you already learnt how to import the data in R, and divided into training and test sets, we will use that directly (which also allows for better comparisons between modelling in R vs python).\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsRegressor\n\nX_tr = r.tmp_tr.drop(columns=[\"price\"])\ny_tr = r.tmp_tr[\"price\"]\nmod_knn_py = KNeighborsRegressor(n_neighbors=3)\nmod_knn_py.fit(X_tr, y_tr)\n\nX_te = r.tmp_te.drop(columns=[\"price\"])\ny_te = r.tmp_te[\"price\"]\ntmp_pred = mod_knn_py.predict(X_te)\n\nWe do the same graphical comparison in python, however, we will use matplotlib to demonstrate the results.\n\nimport matplotlib.pyplot as plt\n\n# we have to make a copy otherwise `r.temp_te` is not directly modified (i.e. we can't add a column)\ntmp_test_plt = r.tmp_te.copy()\ntmp_test_plt[\"pred\"]= tmp_pred\nfig, ax = plt.subplots()\nax.plot(tmp_test_plt[\"price\"], tmp_test_plt[\"pred\"], '.', color='black')\nax.plot(tmp_test_plt[\"price\"], tmp_test_plt[\"price\"], '-', color='blue')\nax.set_xlabel(\"Price\")\nax.set_ylabel(\"Predicted price\")\nplt.show()\n\nWe obtained the same results as R.\n\n\n\n\n\nK-NN: mixture of feature types\nIn this part, we illustrate how to incorporate categorical variables with K-NN. As a reminder, to use categorical variables, the easiest way is probably to cast them to dummy variables.\nThe code below identifies the categorical columns (i.e., non numerical), then create the dummies using and add them to the data frame. In addition, the numerical variables are standardized.\nThere are several difficulties in the code below.\n\nThe price (the outcome) is first set aside because we do not want to standardize it.\nTwo brand names (variable make) contain “-”, namely “alfa-romeo” and “mercedes-benz”. After the creation of the dummy variables, these are used as titles. However, knnreg does not support “-” in the variable names. They thus have to be removed first (in the code below they are turned to “_”).\n\n\n## tmp is imports.85 without incomplete cases and without price\nimp_comp &lt;- imports_85 %&gt;% filter(complete.cases(imports_85))\ny &lt;- imp_comp$price\nimp_comp &lt;- imp_comp %&gt;% select(!price)\n\n# we will use the standardize function shown by Marc-Olivier during the course (was called `my_fun`)\n# normalize num variable: (x - min(x))/(max(x) - min(x))\nmy_normalize &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n# additionally, you can use the `scale()` function which centers and scales variables\n\n## the numerical variables in tmp are standardized\nimp_num &lt;- imp_comp %&gt;% select(where(is.numeric)) %&gt;% my_normalize() %&gt;% as.data.frame()\n\n## the dummy variables of the numerical features of tmp are created\nlibrary(fastDummies)\nimp_dumm &lt;- imp_comp %&gt;% select(!where(is.numeric)) %&gt;% \n  dummy_cols(remove_first_dummy = FALSE, remove_selected_columns = TRUE)\n\n## These dummy variables are added to tmp\nimp_dat &lt;- data.frame(imp_num, imp_dumm)\n\n## The names of the two problematic brands are changed\nnames(imp_dat)[c(23)] &lt;- c(\"make_mercedes_benz\")\n\n## The raw price is added to tmp\nimp_dat$price &lt;- y\n\nAt this stage, tmp contains all the variables that we want. We once again divide our data into training and test sets.\n\nset.seed(123)\nindex_tr &lt;- sample(1:nrow(imp_dat), size=0.75*nrow(imp_dat), replace = FALSE)\nimp_tr &lt;- imp_dat[index_tr,]\nimp_te &lt;- imp_dat[-index_tr,]\n\n\nRPython\n\n\nWe can now run our 3-NN directly, like in the previous exercise.\n\nmod_knn &lt;- knnreg(data=imp_tr, price~., k=3)\nimp_pred &lt;- predict(mod_knn, newdata=imp_te)\nimp_te %&gt;% mutate(pred=imp_pred) %&gt;%\n  ggplot(aes(x=price, y=pred)) + \n  geom_point() + geom_abline(slope=1, intercept = 0)\n\nHere, like always, it is difficult to compare the scales of the scatterplot, and thus to tell if the quality of that 3-NN is better or worst than the previous one. The issue of model scoring will be studied later in the course. For now, it is just about being able to mix categorical and numeric variables in a K-NN.\n\n\nThe same approach in Python with the same.\n\nX_tr = r.imp_tr.drop(columns=[\"price\"])\ny_tr = r.imp_tr[\"price\"]\nmod_knn_py = KNeighborsRegressor(n_neighbors=3)\nmod_knn_py.fit(X_tr, y_tr)\n\nX_te = r.imp_te.drop(columns=[\"price\"])\ny_te = r.imp_te[\"price\"]\ntmp_pred = mod_knn_py.predict(X_te)\n\ntmp_test_plt = r.imp_te.copy()\ntmp_test_plt[\"pred\"]= tmp_pred\nfig, ax = plt.subplots()\nax.plot(tmp_test_plt[\"price\"], tmp_test_plt[\"pred\"], '.', color='black')\nax.plot(tmp_test_plt[\"price\"], tmp_test_plt[\"price\"], '-', color='blue')\nax.set_xlabel(\"Price\")\nax.set_ylabel(\"Predicted price\")\nplt.show()\n\n\n\n\n\n\nAnalysis of nursing home data\nNow it is your turn. Develop a K-NN model to predict the cost using the other variables. Inspect the quality of the prediction using a training set and a test set."
  },
  {
    "objectID": "labs/03_Models/032_Trees/Ex_ML_Tree.html",
    "href": "labs/03_Models/032_Trees/Ex_ML_Tree.html",
    "title": "Models: CART",
    "section": "",
    "text": "In this exercise, the classification tree method is used to analyze the data set Carseats from the package ISLR. The exercise took some inspiration from this video.\n\n\nFirst, install the package ISLR in order to access the data set Carseats. Use ?Carseats to read its description. To apply a classification of the sales, we first create a categorical outcome SaleHigh which equals “Yes” if Sales &gt; 7.5 and “No” otherwise. Then we create a data frame MyCarseats containing SaleHigh and all the features of Carseats except Sales. Finally, split MyCarseats into a training and a test set (2/3 vs 1/3). Below we call them df_tr and df_te.\n\nlibrary(ISLR)\nlibrary(dplyr)\nMyCarseats &lt;- Carseats %&gt;% mutate(SaleHigh=ifelse(Sales &gt; 7.5, \"Yes\", \"No\"))\nMyCarseats &lt;- MyCarseats %&gt;% select(-Sales)\n\nset.seed(123) # for reproducibility \nindex_tr &lt;- sample(x=1:nrow(MyCarseats), size=2/3*nrow(MyCarseats), replace=FALSE)\ndf_tr &lt;- MyCarseats[index_tr,]\ndf_te &lt;- MyCarseats[-index_tr,]\n\nNote: this is not the point of this exercise but remember that in a real situation the first step of the data analysis would be an EDA.\n\n\n\n\nRPython\n\n\nThe rpart function in the package rpart can be used to fit a classification tree with the same type of formulas as naive_bayes. It can then be plotted using the function rpart.plot of the package rpart.plot.\n\nlibrary(rpart)\nlibrary(rpart.plot)\nset.seed(1234)\ncarseats_tree &lt;- rpart(SaleHigh ~ ., data=df_tr)\nrpart.plot(carseats_tree)\n\n\n\n\n# Load MLBA environment\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\nWe use the df_train and df_te created in R to carry out our CART training. For this, we will use the sklearn library. First, we copy our dataset from the R variable to a python variable. Then we use any encoder (e.g. OneHotEncoder, LabelEncoder, OrdinalEncoder) from sklearn.preprocessing to convert categorical data into a numeric form, which many sklearn machine learning algorithms require. In this case, we use the OneHoteEncoder which is similar to making dummy variables and turn each level of the category into a column. Also, we standardize the data in the case of python for faster computations using StandardScaler, which also helps to bring numerical stability and improve the results. After this, we divide the training and test sets into predictors (e.g., X_train) and the outcome (e.g., y_train) and initialize our classification tree and fit the model to the data.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n# We copy the training and testing datasets from R to Python.\ncarset_tr_py = r.df_tr.copy()\ncarset_te_py= r.df_te.copy()\n\n# We encode categorical variables as numeric, which is necessary for CART.\nle = OneHotEncoder()\ncat_vars = [\"ShelveLoc\", \"Urban\", \"US\", \"SaleHigh\"]\nfor var in cat_vars:\n    # Fit the LabelEncoder to the training set and transform the training and testing sets\n    # We need to use the same encoder for both sets to ensure consistency\n    carset_tr_py_encoded = le.fit_transform(carset_tr_py[[var]].values.reshape(-1, 1))\n    carset_te_py_encoded = le.transform(carset_te_py[[var]].values.reshape(-1, 1))\n    carset_tr_py[var] = carset_tr_py_encoded.toarray()\n    carset_te_py[var] = carset_te_py_encoded.toarray()\n\n# Split the data into training and testing sets\nX_train, y_train = carset_tr_py.drop(columns=[\"SaleHigh\"]), carset_tr_py[\"SaleHigh\"]\nX_test, y_test = carset_te_py.drop(columns=[\"SaleHigh\"]), carset_te_py[\"SaleHigh\"]\n\n# Standardize only the continuous variables\ncont_vars = [\"CompPrice\", \"Income\", \"Advertising\", \"Population\", \"Price\", \"Age\", \"Education\"]\nscaler = StandardScaler()\nX_train[cont_vars] = scaler.fit_transform(X_train[cont_vars])\nX_test[cont_vars] = scaler.transform(X_test[cont_vars])\n\n# To speed up the operation you can also transform the inputs\n# X_train = X_train.to_numpy()\n# y_train = y_train.to_numpy()\n\nWe can now train and plot our decision tree using DecisionTreeClassifier and plot_tree functions.\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nimport matplotlib.pyplot as plt\n\n# we clear any previous figures\nplt.clf()\n\nnp.random.seed(1234)\ncarseats_tree = DecisionTreeClassifier().fit(X_train, y_train)\nplt.figure(figsize=(20,10))\nplot_tree(carseats_tree,filled=True, feature_names=X_train.columns, label = 'root',fontsize=5)\nplt.savefig('tree_high_dpi', dpi=300)\n# for a better quality, save the image and load it again\n#plt.show()\n\nAs the image dimensions are not always great for matplotlib plots in Rstudio, we saved the image and now load it again below using an R chunk.\n\nknitr::include_graphics(\"tree_high_dpi.png\")\n\n\n\n\n\n\n\n\nRPython\n\n\nThe analysis of the tree complexity can be obtained using function plotcp.\n\nplotcp(carseats_tree)\n\nFrom the graph, we can identify that, according to the 1-SE rule, the tree with 8 nodes is equivalent to the tree with 12 nodes. This 8-nodes tree should be preferred.\nTo prune the tree (i.e., extract the tree with 8 nodes), we can use the function prune with argument cp. The cp of the tree can be read on the bottom x-axis of the plotcp. The argument in prune should be set to any value between the cp of 8-nodes tree (0.031) and the 11-node tree (0.019). Here 0.025 is OK.\n\ncarseats_tree_prune &lt;- prune(carseats_tree, cp=0.025)\nrpart.plot(carseats_tree_prune)\n\nImportant note: The CP evaluation relies on a cross-validation procedure, which uses random number generation. This is why we have set the seed to some value (1234). You may not find the same result with another seed or if you do not set it. In any case, just be coherent with your results and prune the tree accordingly.\nLet the computer do the work for you: Pruning using the 1-SE rule can be automatically obtained with the function autoprune in package adabag. Note that, because of the randomness involved in the CP evaluation, you may not find exactly the same result as the one obtained by hand. Use set.seed to make your result reproducible.\n\nlibrary(adabag)\nset.seed(123455)\nrpart.plot(autoprune(SaleHigh ~ ., data=df_tr))\n\n\n\nThe rpart.plot package in R provides a convenient plotcp() function to plot the complexity parameter table for a decision tree fit with rpart(). Unfortunately, scikit-learn doesn’t provide an equivalent function out of the box. However, you can still calculate the complexity parameter values for your decision tree and plot them using matplotlib.\n\n# Get the paths of the leaf nodes for the Car Seats decision tree using cost complexity pruning\npath = carseats_tree.cost_complexity_pruning_path(X_train, y_train)\n# Extract the effective alphas and total impurities of the leaf nodes from the path object\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n# Create a plot to visualize the relationship between effective alphas and total impurities\nfig, ax = plt.subplots()\nax.plot(ccp_alphas, impurities, marker='o', linestyle=\"-\")\nax.set_xlabel(\"Effective alpha\")\nax.set_ylabel(\"Total impurity of leaves\")\nax.set_title(\"Total Impurity vs Effective alpha for Car Seats dataset\")\nax.invert_xaxis()\nplt.show()\n\nThis plot only shows the training set results, which doesn’t tell us about over-fitting. A better approach is to compute accuracy (a different metric than rpart) on a second test set, called the validation set used solely for finding the ideal hyperparameters in machine learning. Once we choose the best hyperparameters, we re-train the model one final time with those parameters and compare everything on the test set (but we should no longer change our models based on the test set). We will learn more about this validation set during the upcoming lectures. Here we’ll use the two functions cross_val_score and KFold from the sklearn.module_selection sub-module. We will use ten folds to find the ideal alpha (equivalent to cp from rpart::rpart()). This is not using the 1-SE rule but proposes a good alternative.\n\nfrom sklearn.model_selection import cross_val_score, KFold\n\ndef plotcp(X_train, y_train, random_state=123):\n    # Create a decision tree classifier\n    clf = DecisionTreeClassifier(random_state=random_state)\n\n    # Calculate the cross-validation scores for different values of alpha\n    path = clf.cost_complexity_pruning_path(X_train, y_train)\n    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n    # Perform cross-validation for each alpha\n    kfold = KFold(n_splits=10, shuffle=True, random_state=random_state)\n    scores = []\n    for ccp_alpha in ccp_alphas:\n        clf = DecisionTreeClassifier(random_state=random_state, ccp_alpha=ccp_alpha)\n        score = cross_val_score(clf, X_train, y_train, cv=kfold, scoring='accuracy')\n        scores.append(np.mean(score))\n\n    # Plot the cross-validation scores vs alpha\n    fig, ax = plt.subplots()\n    ax.plot(ccp_alphas, scores, marker='o', linestyle=\"-\")\n    ax.set_xlabel(\"ccp_alpha\")\n    ax.set_ylabel(\"Cross-validation score (accuracy)\")\n    ax.set_title(\"Pruning Complexity Parameter (ccp) vs Cross-validation Score\")\n    ax.invert_xaxis()\n    plt.show()\n\nplotcp(X_train, y_train)\n\nWe can see that the best validation scores are obtained around a ccp_alpha of 0.008. Similar to cp from rpart, in scikit-learn, the parameter controls the complexity of a classification tree by setting a penalty on the number of leaf nodes. A higher value of results in a simpler tree with fewer splits and more nodes being pruned. More specifically, alpha is the regularization parameter used for controlling the cost complexity of the tree. The cost complexity is the sum of the misclassification cost and the complexity cost of the tree. The complexity cost is proportional to the number of terminal nodes (leaves) in the tree. A higher value of alpha thus means that the model is less likely to overfit the training data and more likely to generalize better to new, unseen data. However, setting alpha too high can result in underfitting and poor model performance on both the training and test data. The optimal alpha value depends on the specific dataset and the problem being solved and can be determined through cross-validation or other model selection techniques, as demonstrated above.\nOnce you find the ideal alpha, you can specify it with the ccp_alpha argument in DecisionTreeClassifier(). Here we will take ccp_alpha as 0.008 for simplicity.\n\n# Create a decision tree classifier with a ccp_alpha of 0.025\ncarseats_tree_prune = DecisionTreeClassifier(random_state=123, ccp_alpha=0.008)\n\n# Fit the model to the data\ncarseats_tree_prune.fit(X_train, y_train)\n\n# You can again plot the figure with\nplt.clf()\nplt.figure(figsize=(12,10))\nplot_tree(carseats_tree_prune,filled=True, feature_names=X_train.columns, label = 'root',fontsize=5)\nplt.savefig('tree_pruned_high_dpi', dpi=200)\nplt.close()\n\n\nknitr::include_graphics(\"tree_pruned_high_dpi.png\")\n\nThis model is a lot simpler compared to the first tree made using python (where ccp_alpha was 0 by default).\nFor automatically pruning the tree, unlike the adabag::autoprune() function in R’s adabag package, scikit-learn does not have a built-in function for automatic pruning of decision trees. Instead, you can use cross-validation to determine the optimal tree depth and use that to prune the tree.\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Set up the decision tree classifier\ntree = DecisionTreeClassifier()\n\n# Set up the grid search parameters\nparam_grid = {'max_depth': range(1, 11)}\n\n# Run grid search cross-validation to find optimal tree depth\ngrid_search = GridSearchCV(tree, param_grid=param_grid, cv=10)\ngrid_search.fit(X_train, y_train)\n\n# Use the best estimator to fit and prune the tree\npruned_tree = grid_search.best_estimator_\n\nplt.clf()\nfig, ax = plt.subplots(figsize=(15, 10))\nplot_tree(pruned_tree, ax=ax, feature_names=X_train.columns)\nplt.show()\n# you can choose to re-train the model once again with this new parameter\n\n\n\n\n\n\n\nPython approach vs adabag::autoprune()\n\n\n\nThe technique above does not explicitly use the 1-SE rule for pruning the decision tree. Instead, it uses cross-validation to find the optimal tree depth based on the mean test score across all folds. According to the documentation of adabag::autoprune(), “The cross validation estimation of the error (xerror) has a random component. To avoid this randomness the 1-SE rule (or 1-SD rule) selects the simplest model with a xerror equal or less than the minimum xerror plus the standard deviation of the minimum xerror.” If you’re interested in the 1-SE, see the adapted python code for it below.\n\n\nImplementing GridSearchCV with 1-SE rule\n\n\n# Set up the decision tree classifier\ntree = DecisionTreeClassifier()\n\n# Set up the grid search parameters\nparam_grid = {'max_depth': range(1, 11)}\n\n# Run grid search cross-validation to find optimal tree depth\ngrid_search = GridSearchCV(tree, param_grid=param_grid, cv=10)\ngrid_search.fit(X_train, y_train)\n\n# Calculate the mean and standard error of test scores for each tree depth\nmean_scores = grid_search.cv_results_['mean_test_score']\nstd_scores = grid_search.cv_results_['std_test_score'] / np.sqrt(10)\n\n# Find the optimal depth using the 1-SE rule\noptimal_depth = grid_search.best_params_['max_depth']\noptimal_score = mean_scores[optimal_depth - 1]\nse = std_scores[optimal_depth - 1]\nbest_depth = optimal_depth\nfor depth in range(optimal_depth - 1, -1, -1):\n    score = mean_scores[depth]\n    if score + se &lt; optimal_score:\n        break\n    else:\n        best_depth = depth + 1\n\n# Use the best estimator to fit and prune the tree\npruned_tree = DecisionTreeClassifier(max_depth=best_depth)\npruned_tree.fit(X_train, y_train)\n\nThe results are the same as not using 1-SE.\n\n\n\n\n\n\n\n\n\nFirst, use the R plot to determine what is the prediction of the first instance of MyCarseats.\n\n\nAnswer\n\n\nMyCarseats[1,]\n\nFollow Left, Left, Left =&gt; The predicted answer is “No”.\n\n\nRPython\n\n\nThe function predict can be used build the predictions of the test set (use option type=\"class\"). This is similar to the previously seen models: the predict function used on an object of class .rpart (created by the function rpart), in fact, calls the function predict.rpart which is adapted to the model.\n\npred &lt;- predict(carseats_tree_prune, newdata=df_te, type=\"class\")\ntable(Pred=pred, Obs=df_te$SaleHigh)\n\nNote that, like most categorical models, you may ask for the probabilities instead of the classes by setting type=\"prob\".\n\npredict(carseats_tree_prune, newdata=df_te, type=\"prob\")\n\n\n\nTo print a confusion matrix of the predicted labels versus the true labels, we can use the crosstab() function from the pandas library. We pass the predicted and true labels as arguments to crosstab(), and use the rownames and colnames parameters to label the rows and columns of the table, respectively.\nFinally, we use the predict_proba() method of the decision tree classifier to predict the class probabilities for the test data. The predicted probabilities are stored in the y_probs variable, which is a NumPy array with shape (n_samples, n_classes).\n\n# Predict the class labels for the test data with the python implementation\ny_pred = carseats_tree_prune.predict(X_test)\n\n# Print a confusion matrix of the predicted labels versus the true labels\nprint(pd.crosstab(index=y_pred, columns=y_test, rownames=['Pred'], colnames=['Obs']))\n\nThis tree is worse than the R implementation, probably due to differences in other default values that we did not tune for.\n\n# Predict the class probabilities for the test data\ny_probs = carseats_tree_prune.predict_proba(X_test)\nprint(pd.DataFrame(y_probs))\n\n\n\n\n\n\n\nBy looking at the tree, interpret the most important features for the Sales: the highest in the tree.\nNote that, for the level Good of the ShelveLoc variable, only the Price drives the Sales (according to the tree). Otherwise, it is a subtle mixture between Price and CompPrice.",
    "crumbs": [
      "Labs",
      "Decision Trees"
    ]
  },
  {
    "objectID": "labs/03_Models/032_Trees/Ex_ML_Tree.html#data-preparation",
    "href": "labs/03_Models/032_Trees/Ex_ML_Tree.html#data-preparation",
    "title": "Models: CART",
    "section": "",
    "text": "First, install the package ISLR in order to access the data set Carseats. Use ?Carseats to read its description. To apply a classification of the sales, we first create a categorical outcome SaleHigh which equals “Yes” if Sales &gt; 7.5 and “No” otherwise. Then we create a data frame MyCarseats containing SaleHigh and all the features of Carseats except Sales. Finally, split MyCarseats into a training and a test set (2/3 vs 1/3). Below we call them df_tr and df_te.\n\nlibrary(ISLR)\nlibrary(dplyr)\nMyCarseats &lt;- Carseats %&gt;% mutate(SaleHigh=ifelse(Sales &gt; 7.5, \"Yes\", \"No\"))\nMyCarseats &lt;- MyCarseats %&gt;% select(-Sales)\n\nset.seed(123) # for reproducibility \nindex_tr &lt;- sample(x=1:nrow(MyCarseats), size=2/3*nrow(MyCarseats), replace=FALSE)\ndf_tr &lt;- MyCarseats[index_tr,]\ndf_te &lt;- MyCarseats[-index_tr,]\n\nNote: this is not the point of this exercise but remember that in a real situation the first step of the data analysis would be an EDA.",
    "crumbs": [
      "Labs",
      "Decision Trees"
    ]
  },
  {
    "objectID": "labs/03_Models/032_Trees/Ex_ML_Tree.html#fit-plot",
    "href": "labs/03_Models/032_Trees/Ex_ML_Tree.html#fit-plot",
    "title": "Models: CART",
    "section": "",
    "text": "RPython\n\n\nThe rpart function in the package rpart can be used to fit a classification tree with the same type of formulas as naive_bayes. It can then be plotted using the function rpart.plot of the package rpart.plot.\n\nlibrary(rpart)\nlibrary(rpart.plot)\nset.seed(1234)\ncarseats_tree &lt;- rpart(SaleHigh ~ ., data=df_tr)\nrpart.plot(carseats_tree)\n\n\n\n\n# Load MLBA environment\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\nWe use the df_train and df_te created in R to carry out our CART training. For this, we will use the sklearn library. First, we copy our dataset from the R variable to a python variable. Then we use any encoder (e.g. OneHotEncoder, LabelEncoder, OrdinalEncoder) from sklearn.preprocessing to convert categorical data into a numeric form, which many sklearn machine learning algorithms require. In this case, we use the OneHoteEncoder which is similar to making dummy variables and turn each level of the category into a column. Also, we standardize the data in the case of python for faster computations using StandardScaler, which also helps to bring numerical stability and improve the results. After this, we divide the training and test sets into predictors (e.g., X_train) and the outcome (e.g., y_train) and initialize our classification tree and fit the model to the data.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n# We copy the training and testing datasets from R to Python.\ncarset_tr_py = r.df_tr.copy()\ncarset_te_py= r.df_te.copy()\n\n# We encode categorical variables as numeric, which is necessary for CART.\nle = OneHotEncoder()\ncat_vars = [\"ShelveLoc\", \"Urban\", \"US\", \"SaleHigh\"]\nfor var in cat_vars:\n    # Fit the LabelEncoder to the training set and transform the training and testing sets\n    # We need to use the same encoder for both sets to ensure consistency\n    carset_tr_py_encoded = le.fit_transform(carset_tr_py[[var]].values.reshape(-1, 1))\n    carset_te_py_encoded = le.transform(carset_te_py[[var]].values.reshape(-1, 1))\n    carset_tr_py[var] = carset_tr_py_encoded.toarray()\n    carset_te_py[var] = carset_te_py_encoded.toarray()\n\n# Split the data into training and testing sets\nX_train, y_train = carset_tr_py.drop(columns=[\"SaleHigh\"]), carset_tr_py[\"SaleHigh\"]\nX_test, y_test = carset_te_py.drop(columns=[\"SaleHigh\"]), carset_te_py[\"SaleHigh\"]\n\n# Standardize only the continuous variables\ncont_vars = [\"CompPrice\", \"Income\", \"Advertising\", \"Population\", \"Price\", \"Age\", \"Education\"]\nscaler = StandardScaler()\nX_train[cont_vars] = scaler.fit_transform(X_train[cont_vars])\nX_test[cont_vars] = scaler.transform(X_test[cont_vars])\n\n# To speed up the operation you can also transform the inputs\n# X_train = X_train.to_numpy()\n# y_train = y_train.to_numpy()\n\nWe can now train and plot our decision tree using DecisionTreeClassifier and plot_tree functions.\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nimport matplotlib.pyplot as plt\n\n# we clear any previous figures\nplt.clf()\n\nnp.random.seed(1234)\ncarseats_tree = DecisionTreeClassifier().fit(X_train, y_train)\nplt.figure(figsize=(20,10))\nplot_tree(carseats_tree,filled=True, feature_names=X_train.columns, label = 'root',fontsize=5)\nplt.savefig('tree_high_dpi', dpi=300)\n# for a better quality, save the image and load it again\n#plt.show()\n\nAs the image dimensions are not always great for matplotlib plots in Rstudio, we saved the image and now load it again below using an R chunk.\n\nknitr::include_graphics(\"tree_high_dpi.png\")",
    "crumbs": [
      "Labs",
      "Decision Trees"
    ]
  },
  {
    "objectID": "labs/03_Models/032_Trees/Ex_ML_Tree.html#pruning",
    "href": "labs/03_Models/032_Trees/Ex_ML_Tree.html#pruning",
    "title": "Models: CART",
    "section": "",
    "text": "RPython\n\n\nThe analysis of the tree complexity can be obtained using function plotcp.\n\nplotcp(carseats_tree)\n\nFrom the graph, we can identify that, according to the 1-SE rule, the tree with 8 nodes is equivalent to the tree with 12 nodes. This 8-nodes tree should be preferred.\nTo prune the tree (i.e., extract the tree with 8 nodes), we can use the function prune with argument cp. The cp of the tree can be read on the bottom x-axis of the plotcp. The argument in prune should be set to any value between the cp of 8-nodes tree (0.031) and the 11-node tree (0.019). Here 0.025 is OK.\n\ncarseats_tree_prune &lt;- prune(carseats_tree, cp=0.025)\nrpart.plot(carseats_tree_prune)\n\nImportant note: The CP evaluation relies on a cross-validation procedure, which uses random number generation. This is why we have set the seed to some value (1234). You may not find the same result with another seed or if you do not set it. In any case, just be coherent with your results and prune the tree accordingly.\nLet the computer do the work for you: Pruning using the 1-SE rule can be automatically obtained with the function autoprune in package adabag. Note that, because of the randomness involved in the CP evaluation, you may not find exactly the same result as the one obtained by hand. Use set.seed to make your result reproducible.\n\nlibrary(adabag)\nset.seed(123455)\nrpart.plot(autoprune(SaleHigh ~ ., data=df_tr))\n\n\n\nThe rpart.plot package in R provides a convenient plotcp() function to plot the complexity parameter table for a decision tree fit with rpart(). Unfortunately, scikit-learn doesn’t provide an equivalent function out of the box. However, you can still calculate the complexity parameter values for your decision tree and plot them using matplotlib.\n\n# Get the paths of the leaf nodes for the Car Seats decision tree using cost complexity pruning\npath = carseats_tree.cost_complexity_pruning_path(X_train, y_train)\n# Extract the effective alphas and total impurities of the leaf nodes from the path object\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n# Create a plot to visualize the relationship between effective alphas and total impurities\nfig, ax = plt.subplots()\nax.plot(ccp_alphas, impurities, marker='o', linestyle=\"-\")\nax.set_xlabel(\"Effective alpha\")\nax.set_ylabel(\"Total impurity of leaves\")\nax.set_title(\"Total Impurity vs Effective alpha for Car Seats dataset\")\nax.invert_xaxis()\nplt.show()\n\nThis plot only shows the training set results, which doesn’t tell us about over-fitting. A better approach is to compute accuracy (a different metric than rpart) on a second test set, called the validation set used solely for finding the ideal hyperparameters in machine learning. Once we choose the best hyperparameters, we re-train the model one final time with those parameters and compare everything on the test set (but we should no longer change our models based on the test set). We will learn more about this validation set during the upcoming lectures. Here we’ll use the two functions cross_val_score and KFold from the sklearn.module_selection sub-module. We will use ten folds to find the ideal alpha (equivalent to cp from rpart::rpart()). This is not using the 1-SE rule but proposes a good alternative.\n\nfrom sklearn.model_selection import cross_val_score, KFold\n\ndef plotcp(X_train, y_train, random_state=123):\n    # Create a decision tree classifier\n    clf = DecisionTreeClassifier(random_state=random_state)\n\n    # Calculate the cross-validation scores for different values of alpha\n    path = clf.cost_complexity_pruning_path(X_train, y_train)\n    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n    # Perform cross-validation for each alpha\n    kfold = KFold(n_splits=10, shuffle=True, random_state=random_state)\n    scores = []\n    for ccp_alpha in ccp_alphas:\n        clf = DecisionTreeClassifier(random_state=random_state, ccp_alpha=ccp_alpha)\n        score = cross_val_score(clf, X_train, y_train, cv=kfold, scoring='accuracy')\n        scores.append(np.mean(score))\n\n    # Plot the cross-validation scores vs alpha\n    fig, ax = plt.subplots()\n    ax.plot(ccp_alphas, scores, marker='o', linestyle=\"-\")\n    ax.set_xlabel(\"ccp_alpha\")\n    ax.set_ylabel(\"Cross-validation score (accuracy)\")\n    ax.set_title(\"Pruning Complexity Parameter (ccp) vs Cross-validation Score\")\n    ax.invert_xaxis()\n    plt.show()\n\nplotcp(X_train, y_train)\n\nWe can see that the best validation scores are obtained around a ccp_alpha of 0.008. Similar to cp from rpart, in scikit-learn, the parameter controls the complexity of a classification tree by setting a penalty on the number of leaf nodes. A higher value of results in a simpler tree with fewer splits and more nodes being pruned. More specifically, alpha is the regularization parameter used for controlling the cost complexity of the tree. The cost complexity is the sum of the misclassification cost and the complexity cost of the tree. The complexity cost is proportional to the number of terminal nodes (leaves) in the tree. A higher value of alpha thus means that the model is less likely to overfit the training data and more likely to generalize better to new, unseen data. However, setting alpha too high can result in underfitting and poor model performance on both the training and test data. The optimal alpha value depends on the specific dataset and the problem being solved and can be determined through cross-validation or other model selection techniques, as demonstrated above.\nOnce you find the ideal alpha, you can specify it with the ccp_alpha argument in DecisionTreeClassifier(). Here we will take ccp_alpha as 0.008 for simplicity.\n\n# Create a decision tree classifier with a ccp_alpha of 0.025\ncarseats_tree_prune = DecisionTreeClassifier(random_state=123, ccp_alpha=0.008)\n\n# Fit the model to the data\ncarseats_tree_prune.fit(X_train, y_train)\n\n# You can again plot the figure with\nplt.clf()\nplt.figure(figsize=(12,10))\nplot_tree(carseats_tree_prune,filled=True, feature_names=X_train.columns, label = 'root',fontsize=5)\nplt.savefig('tree_pruned_high_dpi', dpi=200)\nplt.close()\n\n\nknitr::include_graphics(\"tree_pruned_high_dpi.png\")\n\nThis model is a lot simpler compared to the first tree made using python (where ccp_alpha was 0 by default).\nFor automatically pruning the tree, unlike the adabag::autoprune() function in R’s adabag package, scikit-learn does not have a built-in function for automatic pruning of decision trees. Instead, you can use cross-validation to determine the optimal tree depth and use that to prune the tree.\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Set up the decision tree classifier\ntree = DecisionTreeClassifier()\n\n# Set up the grid search parameters\nparam_grid = {'max_depth': range(1, 11)}\n\n# Run grid search cross-validation to find optimal tree depth\ngrid_search = GridSearchCV(tree, param_grid=param_grid, cv=10)\ngrid_search.fit(X_train, y_train)\n\n# Use the best estimator to fit and prune the tree\npruned_tree = grid_search.best_estimator_\n\nplt.clf()\nfig, ax = plt.subplots(figsize=(15, 10))\nplot_tree(pruned_tree, ax=ax, feature_names=X_train.columns)\nplt.show()\n# you can choose to re-train the model once again with this new parameter\n\n\n\n\n\n\n\nPython approach vs adabag::autoprune()\n\n\n\nThe technique above does not explicitly use the 1-SE rule for pruning the decision tree. Instead, it uses cross-validation to find the optimal tree depth based on the mean test score across all folds. According to the documentation of adabag::autoprune(), “The cross validation estimation of the error (xerror) has a random component. To avoid this randomness the 1-SE rule (or 1-SD rule) selects the simplest model with a xerror equal or less than the minimum xerror plus the standard deviation of the minimum xerror.” If you’re interested in the 1-SE, see the adapted python code for it below.\n\n\nImplementing GridSearchCV with 1-SE rule\n\n\n# Set up the decision tree classifier\ntree = DecisionTreeClassifier()\n\n# Set up the grid search parameters\nparam_grid = {'max_depth': range(1, 11)}\n\n# Run grid search cross-validation to find optimal tree depth\ngrid_search = GridSearchCV(tree, param_grid=param_grid, cv=10)\ngrid_search.fit(X_train, y_train)\n\n# Calculate the mean and standard error of test scores for each tree depth\nmean_scores = grid_search.cv_results_['mean_test_score']\nstd_scores = grid_search.cv_results_['std_test_score'] / np.sqrt(10)\n\n# Find the optimal depth using the 1-SE rule\noptimal_depth = grid_search.best_params_['max_depth']\noptimal_score = mean_scores[optimal_depth - 1]\nse = std_scores[optimal_depth - 1]\nbest_depth = optimal_depth\nfor depth in range(optimal_depth - 1, -1, -1):\n    score = mean_scores[depth]\n    if score + se &lt; optimal_score:\n        break\n    else:\n        best_depth = depth + 1\n\n# Use the best estimator to fit and prune the tree\npruned_tree = DecisionTreeClassifier(max_depth=best_depth)\npruned_tree.fit(X_train, y_train)\n\nThe results are the same as not using 1-SE.",
    "crumbs": [
      "Labs",
      "Decision Trees"
    ]
  },
  {
    "objectID": "labs/03_Models/032_Trees/Ex_ML_Tree.html#predictions",
    "href": "labs/03_Models/032_Trees/Ex_ML_Tree.html#predictions",
    "title": "Models: CART",
    "section": "",
    "text": "First, use the R plot to determine what is the prediction of the first instance of MyCarseats.\n\n\nAnswer\n\n\nMyCarseats[1,]\n\nFollow Left, Left, Left =&gt; The predicted answer is “No”.\n\n\nRPython\n\n\nThe function predict can be used build the predictions of the test set (use option type=\"class\"). This is similar to the previously seen models: the predict function used on an object of class .rpart (created by the function rpart), in fact, calls the function predict.rpart which is adapted to the model.\n\npred &lt;- predict(carseats_tree_prune, newdata=df_te, type=\"class\")\ntable(Pred=pred, Obs=df_te$SaleHigh)\n\nNote that, like most categorical models, you may ask for the probabilities instead of the classes by setting type=\"prob\".\n\npredict(carseats_tree_prune, newdata=df_te, type=\"prob\")\n\n\n\nTo print a confusion matrix of the predicted labels versus the true labels, we can use the crosstab() function from the pandas library. We pass the predicted and true labels as arguments to crosstab(), and use the rownames and colnames parameters to label the rows and columns of the table, respectively.\nFinally, we use the predict_proba() method of the decision tree classifier to predict the class probabilities for the test data. The predicted probabilities are stored in the y_probs variable, which is a NumPy array with shape (n_samples, n_classes).\n\n# Predict the class labels for the test data with the python implementation\ny_pred = carseats_tree_prune.predict(X_test)\n\n# Print a confusion matrix of the predicted labels versus the true labels\nprint(pd.crosstab(index=y_pred, columns=y_test, rownames=['Pred'], colnames=['Obs']))\n\nThis tree is worse than the R implementation, probably due to differences in other default values that we did not tune for.\n\n# Predict the class probabilities for the test data\ny_probs = carseats_tree_prune.predict_proba(X_test)\nprint(pd.DataFrame(y_probs))",
    "crumbs": [
      "Labs",
      "Decision Trees"
    ]
  },
  {
    "objectID": "labs/03_Models/032_Trees/Ex_ML_Tree.html#interpretation",
    "href": "labs/03_Models/032_Trees/Ex_ML_Tree.html#interpretation",
    "title": "Models: CART",
    "section": "",
    "text": "By looking at the tree, interpret the most important features for the Sales: the highest in the tree.\nNote that, for the level Good of the ShelveLoc variable, only the Price drives the Sales (according to the tree). Otherwise, it is a subtle mixture between Price and CompPrice.",
    "crumbs": [
      "Labs",
      "Decision Trees"
    ]
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#motivation",
    "href": "labs/slides/lab_2/slide.html#motivation",
    "title": "MLBA Lab 1 - Introduction to Quarto",
    "section": "Motivation",
    "text": "Motivation\n\nLearning about Quarto\n\nWhat are its output formats? Reports (PDF, HTML, Docx), Presentations (PPT, KEY, HTML) & websites (HTML).\n\nLearning how to interact with Quarto\n\nHow can it be used? It can be used with many integrated development environments (IDEs) such as Rstudio, Jupyter Notebook & VSCode.\n\nUsing what we have seen in class with Quarto\n\nReason(s) to use? Good application for ML course & reporting in general. It particularly benefits interactive reporting, especially when multiple programming languages are involved."
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#what-is-quarto",
    "href": "labs/slides/lab_2/slide.html#what-is-quarto",
    "title": "MLBA Lab 1 - Introduction to Quarto",
    "section": "What is Quarto?",
    "text": "What is Quarto?\n\nQuarto® is an open-source scientific and technical publishing system built on Pandoc.\nPandoc is a versatile tool for converting documents from one format to another. It allows you to convert a document written in one markup language to another markup language, such as converting a Markdown document to HTML or LaTeX.\nThink of Quarto as R-markdown on steroids."
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#formats",
    "href": "labs/slides/lab_2/slide.html#formats",
    "title": "MLBA Lab 1 - Introduction to Quarto",
    "section": "Formats",
    "text": "Formats\n\nWebsitesBooksBlogsJournals\n\n\n\n\n\n\nnbdev.fast.ai\n\n\n\n\n\n\n\n\n\nPython for Data Analysis, 3E by Wes McKinney\n\n\n\n\n\n\n\n\n\nhttps://jollydata.blog/\n\n\n\n\n\n\n\n\n\nJournal of Statistical Software (JSS)\n\n\n\n\n\n\n\nQuarto can make very flexible websites, or books which are a a Quarto website that can be rendered to Word, ePub, etc., blogs with listings and posts and RSS feeds, Quarto has deep feature set for presentations with reveal.js optimized for scientific content, and of course, publishing for journals. There is custom format systems and the ability to flexibly adapt LaTeX templates."
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#compatibility",
    "href": "labs/slides/lab_2/slide.html#compatibility",
    "title": "MLBA Lab 1 - Introduction to Quarto",
    "section": "Compatibility",
    "text": "Compatibility\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n..and many other editors. For more info, please see https://quarto.org/docs/get-started/"
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#presentations-pretty-code",
    "href": "labs/slides/lab_2/slide.html#presentations-pretty-code",
    "title": "MLBA Lab 1 - Introduction to Quarto",
    "section": "Presentations: Pretty Code",
    "text": "Presentations: Pretty Code\n\nOver 20 syntax highlighting themes available\nDefault theme optimized for accessibility\n\n# Define a server for the Shiny app\nfunction(input, output) {\n  \n  # Fill in the spot we created for a plot\n  output$phonePlot &lt;- renderPlot({\n    # Render a barplot\n  })\n}"
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#presentations-code-animations",
    "href": "labs/slides/lab_2/slide.html#presentations-code-animations",
    "title": "MLBA Lab 1 - Introduction to Quarto",
    "section": "Presentations: Code Animations",
    "text": "Presentations: Code Animations\n\nOver 20 syntax highlighting themes available\nDefault theme optimized for accessibility\n\n# Define a server for the Shiny app\nfunction(input, output) {\n  \n  # Fill in the spot we created for a plot\n  output$phonePlot &lt;- renderPlot({\n    # Render a barplot\n    barplot(WorldPhones[,input$region]*1000, \n            main=input$region,\n            ylab=\"Number of Telephones\",\n            xlab=\"Year\")\n  })\n}"
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#presentations-line-highlighting",
    "href": "labs/slides/lab_2/slide.html#presentations-line-highlighting",
    "title": "MLBA Lab 1 - Introduction to Quarto",
    "section": "Presentations: Line Highlighting",
    "text": "Presentations: Line Highlighting\n\nHighlight specific lines for emphasis\nIncrementally highlight additional lines\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#presentations-executable-code",
    "href": "labs/slides/lab_2/slide.html#presentations-executable-code",
    "title": "MLBA Lab 1 - Introduction to Quarto",
    "section": "Presentations: Executable Code",
    "text": "Presentations: Executable Code\n\nlibrary(ggplot2)\nggplot(mtcars, aes(hp, mpg, color = am)) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, method = \"loess\")"
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#presentations-auto-animate",
    "href": "labs/slides/lab_2/slide.html#presentations-auto-animate",
    "title": "MLBA Lab 1 - Introduction to Quarto",
    "section": "Presentations: Auto-Animate",
    "text": "Presentations: Auto-Animate\nAutomatically animate matching elements across slides with Auto-Animate."
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#presentations-auto-animate-1",
    "href": "labs/slides/lab_2/slide.html#presentations-auto-animate-1",
    "title": "MLBA Lab 1 - Introduction to Quarto",
    "section": "Presentations: Auto-Animate",
    "text": "Presentations: Auto-Animate\nAutomatically animate matching elements across slides with Auto-Animate."
  },
  {
    "objectID": "labs/slides/lab_2/slide.html#references-credits",
    "href": "labs/slides/lab_2/slide.html#references-credits",
    "title": "MLBA Lab 1 - Introduction to Quarto",
    "section": "References & Credits",
    "text": "References & Credits\n\nQuarto official website & documentation\nSlides taken from Isabella Velásquez from Posit (Rstudio) on this link of Intro to Quarto. For the source code, please see here.\nSlides taken from the official Posit (Rstudio) website for introducing Quarto.\nAnother nice resource is from Tom Mock (Posit/Rstudio), given as webinar in 2022. Notably, my set of slides was inspired by his introduction slides. For the entire deck of slides and source code visit this qmd file on his github.\nHuge list of resources for Quarto with many examples from mcanouil/awesome-quarto found here."
  },
  {
    "objectID": "labs/00_lab/setup.html",
    "href": "labs/00_lab/setup.html",
    "title": "General Instructions",
    "section": "",
    "text": "Objectives\n\n\n\nThis setup tutorial is optional for those who are interested in the following:\n\nLearning about (and using) virtual environments in R to ensure reproducibility (benefits explained below).\nLearning about using python 🐍 in (and with) R. This is useful for some ML lab sessions, and cutting-edge ML is often first implemented in python.",
    "crumbs": [
      "Labs",
      "Setup"
    ]
  },
  {
    "objectID": "labs/00_lab/setup.html#the-what-the-why",
    "href": "labs/00_lab/setup.html#the-what-the-why",
    "title": "General Instructions",
    "section": "The What & The Why",
    "text": "The What & The Why\nAs you start working on machine learning in R, you’ll be using various R packages that provide different functionalities. As you might know, managing multiple packages and their dependencies can be a daunting task, especially when you’re new to programming.\nThis is where renv comes in handy. renv is a package management tool that helps you manage the packages used in an R project, making it easier to handle dependencies and ensuring that your project is reproducible. Here are some specific reasons why renv is particularly useful for machine learning lab sessions:\n\nConsistent environment: When you work on machine learning lab sessions, you’ll often be working with complex models that use multiple R packages. It’s crucial to ensure that all the packages used in your project are compatible with each other. With renv, you can create a consistent environment by isolating the packages used in your project and making sure they work well together.\nEasy installation and setup: renv makes it easy to set up a new R project with the required packages. Once you’ve created an renv project, you can easily install all the required packages with a single command. This saves you time and ensures that you have all the necessary packages for your machine learning lab sessions.\nReproducibility: Reproducibility is critical in machine learning lab sessions. With renv, you can easily share your code and the exact packages used in your project with your peers or instructors, making it easy for them to reproduce your results.\n\nIn summary, renv is an essential tool for managing packages in R and ensuring that your machine learning lab sessions are efficient and reproducible. It helps you avoid compatibility issues, simplifies installation and setup, and makes it easy to share your work with others. In python, you have similar tools such as virtualenv, venv and conda. We hope you find renv useful as you begin your journey into machine learning with R!\n\n# Check if renv is installed\nif (!require(\"renv\")) {\n  # Install renv if it is not already installed\n  install.packages(\"renv\")\n}",
    "crumbs": [
      "Labs",
      "Setup"
    ]
  },
  {
    "objectID": "labs/00_lab/setup.html#the-how",
    "href": "labs/00_lab/setup.html#the-how",
    "title": "General Instructions",
    "section": "The How",
    "text": "The How\nTo create a new renv project, first set your working directory to the location where you want to create the project, and then run the command renv::init() which has to be executed only once. We recommend creating a main folder for all your ML-related exercises and running the command at the top directory. Running the initialization will create a bunch of auxiliary files, such as renv.lock to keep track of the packages that you’re using and all their versions. If you move to another computer or something goes wrong with your packages, you can always re-install the packages listed in your renv.lock to the previous versions using renv::restore(). To occasionally update your packages, you can always use renv::snapshot(); however, every time that you start a new R session, if renv detects any changes, it will automatically ask you to run renv::status() to see if the list in renv.lock needs updating.\n\n# Check if renv is already initialized\nif (!file.exists(\"renv.lock\")) {\n  # Initialize renv project\n  renv::init()\n\n  # Restore packages\n  renv::restore()\n}",
    "crumbs": [
      "Labs",
      "Setup"
    ]
  },
  {
    "objectID": "labs/00_lab/setup.html#small-motivation",
    "href": "labs/00_lab/setup.html#small-motivation",
    "title": "General Instructions",
    "section": "Small Motivation",
    "text": "Small Motivation\nMost careers after HEC require data literacy and data-driven insights to solve business problems. To that end, R is a potent tool; however, in the realm of machine learning, Python is arguably more demanded and, therefore, can be a good tool in your toolbox. Additionally, python is a widely-used language in the industry and offers powerful libraries for data manipulation, analysis, and modeling. Furthermore, you don’t need to be a technical person to learn python as there are many resources available online, and it’s a language that is relatively easy to pick up, even for absolute beginners to programming. Combining the strengths of both R and Python can enhance your workflow and improve your ability to work with data. To understand how R and Python compare, you are highly encouraged to watch this video by IBM on R vs Python.",
    "crumbs": [
      "Labs",
      "Setup"
    ]
  },
  {
    "objectID": "labs/00_lab/setup.html#configuration",
    "href": "labs/00_lab/setup.html#configuration",
    "title": "General Instructions",
    "section": "Configuration",
    "text": "Configuration\nYou need to follow a few steps to ensure that everything runs smoothly. There are several solutions for running the python part of the exercises. Our preferred method is to run python in R using the reticulate package as recently there has been a smooth integration between the two languages, especially if your IDE of choice (integrated development environment) is Rstudio. This library provides a comprehensive set of tools for interoperability (i.e., exchanging languages) between python and R. You can either do this in your own central python installation or use a virtual environment where we’ll install the desired python packages. In python, virtual environments are similar to those of R (i.e. renv), allowing you to create isolated python installations, making it easier to manage different projects, and avoiding compatibility issues between dependencies. This setup usually works well, but if there are any individual issues, do not hesitate to contact me (Ilia). With that said, let’s get started with python!\nFirst, load all the corresponding libraries to install and run python:\n\nlibrary(reticulate)\nlibrary(tidyverse)\n\nTo install python in R, we will use miniconda, a smaller version of Anaconda package manager. If you don’t have Miniconda or Anaconda, first run reticulate::install_miniconda(), which will automatically create a virtual environment for you.\n\n# reticulate::install_miniconda() # if you got an error, you could also try `install_python()` or installing conda seperately on your OS\n\nNow, we will create our virtual environment with the command reticulate::conda_create(). We will then make sure our rstudio is using this correct conda environment by enforcing it via reticulate::use_condaenv().\n\n# assign the right virtual environment for the exercises\nenv_name &lt;- \"MLBA\"\n\n# if the virtual enviroment does not already exist, only then create then\nif (!env_name %in% reticulate::conda_list()$name) {\n  reticulate::conda_create(env_name, pip = TRUE) # we use pip for installations\n}\n\n# Print the conda list\nprint(reticulate::conda_list())\n\n# make sure we're using the right environment\nreticulate::use_condaenv(env_name)\n\n# if you preferred, you can also use your own version of python with `use_python()`\n# you can see all the versions of your path here and which one has been assigned\nreticulate::py_config()\n\n# Check if python is setup properly now\nreticulate::py_available()\n\nIf you have made it here so far and see the name MLBA in your python path, you have successfully installed/configured python and setup the virtual environment(s)🥳.\n\nBonus: Using Rstudio to select Python path\nIn case you wanted to use the Rstudio interface, you can always go to Tools &gt; Project Options (or Global Options if you’re not using renv) and then select the version inside the Python field as shown below.",
    "crumbs": [
      "Labs",
      "Setup"
    ]
  },
  {
    "objectID": "labs/00_lab/setup.html#python-very-brief-overview",
    "href": "labs/00_lab/setup.html#python-very-brief-overview",
    "title": "General Instructions",
    "section": "Python (very brief) overview",
    "text": "Python (very brief) overview\nWe do not teach the principles of python programming during this course. With that said, if you already have a background in R from your previous courses (e.g., DSFBA, QMM), we provide a few links to help you get started. If you’re new to python, feel free to continue reading; otherwise, skip to the section Calling Python in R.\nIf you would like to see a crash course in python on different data structures and types, check out this video on Python for Data Science [Crash Course] (you can skip the installation part). You may notice that much of the syntax is similar to R.\n\nPython data-oriented libraries\nPython provides a range of libraries for data manipulation, analysis, and modeling, including Pandas (similar to tibble+dplyr), which offers easy-to-use data structures for working with tabular data, and NumPy (dplyr+data.table), which provides powerful tools for array manipulation, linear algebra, and Fourier analysis. For data visualization, python offers Matplotlib (similar to plot() in base R). For machine learning, Scikit-learn (similar to the caret package in R which we will be introduced later) provides a wide range of tools for classification, regression, clustering, and dimensionality reduction, while Tensorflow & Keras (both used for deep learning and neural networks) are also popular libraries in this space which are available in both R and Python. These libraries are just a few examples of the many tools available in python that can help you work with data and build machine learning models. If you’re interested to learn about these libraries and how to manipulate Pandas dataframes, you can check out this other video on Data Analysis with Python - Full Course for Beginners (Numpy, Pandas, Matplotlib, Seaborn) (it’s slightly long).\n\n\nAssigning variables in Python vs. R\nFor those of you new to python, there’s an important difference between R and Python when assignment variables. In Python, when you assign a variable to another variable, you are creating a reference to the same object in memory, so any changes made to one variable will be reflected in the other variable as well. This is demonostrated in the example below:\n\na = [1, 2, 3]\nb = a\nprint(a, b)\nb[0] = 4 # we only change `b`\nprint(a, b) # the variable `a` also changed unlike how R treats the variable\n\n\n\n\n\n\n\nIndexing in R vs. Python\n\n\n\nAnother difference between R & Python is that the index (first value of any object) starts from 0 (python) rather than 1 (R). So if you have a list, the first element is python is starting at element 0.\n\n\nTo create a separate copy of the object, you need to use the .copy() method (python way of saying a function).\n\na = [1, 2, 3]\nb = a.copy() # create a separate copy of the list\nprint(a, b)\nb[0] = 4 # modify only `b`\nprint(a, b) # variable `a` does not change\n\nIn R, on the other hand, assignment creates a copy by default, so you don’t need to use a .copy() method. Any changes you make to one variable will not affect the other variable, because they are separate copies of the same object.",
    "crumbs": [
      "Labs",
      "Setup"
    ]
  },
  {
    "objectID": "labs/00_lab/setup.html#use-python-in-r",
    "href": "labs/00_lab/setup.html#use-python-in-r",
    "title": "General Instructions",
    "section": "Calling Python in R",
    "text": "Calling Python in R\n\nInstalling Python libraries\nYou can install any python package in R using the reticulate::py_install() command. This is similar to calling install.packages() in R. You may have heard of CRAN in R, a central repository for all R packages. In Python, the equivalent of CRAN is PyPI (Python Package Index). If you want the latest version of the packages, it is recommended to install packages using pip. To do so, you must set the argument pip=TRUE inside reticulate::py_install(), as demonstrated below.\nWe will install all the packages we need for this particular setup. This should be go smoothly with the following command:\n\n# Install python package into virtual environment\nreticulate::py_install(c(\"jupyter\", \"pandas\", \"matplotlib\",\"statsmodels\",\"scikit-learn\", \"seaborn\", \"mlxtend\", \"lime\", \"mkl-service\", \"xgboost\",\"scikit-learn-extra\"), envname = \"MLBA\", pip=TRUE)\n\n+ /usr/bin/python3.12 -m venv /home/runner/.virtualenvs/MLBA\n\n\n+ /home/runner/.virtualenvs/MLBA/bin/python -m pip install --upgrade pip wheel setuptools\n\n\n+ /home/runner/.virtualenvs/MLBA/bin/python -m pip install --upgrade --no-user jupyter pandas matplotlib statsmodels scikit-learn seaborn mlxtend lime mkl-service xgboost scikit-learn-extra\n\n\n\n\n\n\n\n\nmkl-service for Mac Users\n\n\n\nPlease note that the mkl-service python package from above is specific to windows/linux users and is not needed for Mac users. In case of any errors (especially on M1 chips), feel free to remove it.\n\n\nLet’s checked if the package is installed successfully.\n\n# import package that is used for dealing with data.frames in Python (equivalent of tibble+dplyr)\npd &lt;- reticulate::import(\"pandas\")\n# import the package for plotting in python\nplt &lt;- reticulate::import(\"matplotlib.pyplot\")\n# import the library which we will use for linear regression\nsm &lt;- reticulate::import(\"statsmodels.api\")\n\nNo error messages? Then installation was successful!\n\n\nCalling Python and R objects interchangeably\nAll the data types in R vs. Python are explained below (image from reticulate’s home page). If you need to explicitly change between objects in R and Python (usually R handles that automatically) to get the objects from the image above, you can use reticulate::r_to_py() and reticulate::py_to_r() (e.g., R dataframes to pandas dataframes).\n\nTo run objects from python in R, you have to use $ to access their elements. For instance, when you want to load a python library, you can use reticulate::import() function and then assign it to a variable with the name of your choice as we did in the previous part. If that library contains a function (in python called module or sub-module), it always follows the format LIBRARY$FUNCTION(). We can see an example of that below:\n\n# Using R\n## load mtcars dataset\ndata(mtcars)\n\n## plot it using base R plot function (or ggplot)\nplot(mtcars$mpg, mtcars$disp)\n\n# Using Python\n# plot it using matplotlib in python (or another python library for plots)\nplt$scatter(mtcars$mpg,mtcars$disp)\nplt$xlabel('mpg', fontsize = 12)\nplt$ylabel('disp', fontsize = 12)\n\n# save the figure and then include in the Rmd\nplt$savefig(\"pyplot.png\")\nknitr::include_graphics(\"pyplot.png\")\n# alternatively, when not knitting, you can uncomment and run the two following lines\n# instead of save the figure\n# plt$show() # you always have to call this for the plot to be made\n# plt$clf() #this means clear figure\n\nNow, this is using R inside Python, but if you wanted to do it the other way around, that’s also possible by using a dot . instead of $. If you’re running python in markdown, you can replace {r...} at the beginning of the code chunk with {python...}, and it’ll run python code. Additionally, if you would like to run a script instead (interactively or the whole script), you can do it by going to file &gt; New File &gt; Python Script in your Rstudio, and then running any part of your python script starts the interactive python session using reticulate::repl_python(). Alternatively, you can call the repl_python() to start the interactive session.\n```{python}\n# Your python code goes here\n```\nWe will now create a code chunk that purely runs Python and accesses the objects object with r.OBJECT_NAME:\n\n# we access mtcars dataset from R\nprint(r.mtcars.head())\n\nfrom sklearn import datasets\nimport pandas as pd\n\n# we open iris data from `sklearn` python package\niris = datasets.load_iris()\niris_data = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n\n# Get the head of the DataFrame\nprint(iris_data.head())\n\nWe can do the same thing in R by calling py$OBJECT_NAME by using reticulate::py$OBJECT_NAME:\n\n# plotting the iris data from python\nplot(py$iris_data)",
    "crumbs": [
      "Labs",
      "Setup"
    ]
  },
  {
    "objectID": "labs/00_lab/setup.html#modelling-in-r-python",
    "href": "labs/00_lab/setup.html#modelling-in-r-python",
    "title": "General Instructions",
    "section": "Modelling in R & Python",
    "text": "Modelling in R & Python\nLet’s take a simple use case of making a regression in R and Python so you can see how the two languages compare:\n\n# remove the spaces and `(cm)` from the column names\nnames(py$iris_data) &lt;- gsub(' ', '_', names(py$iris_data))\nnames(py$iris_data) &lt;- gsub('_\\\\(cm\\\\)', '', names(py$iris_data))\n\n# example of running a model on iris data\nr_lm &lt;- lm(\"sepal_length ~. \", data = py$iris_data)\nsummary(r_lm)\n\nWithin R, you can still use python to run the same linear regression with the statsmodels python library.\n\n# example of runnning lm model in python -&gt; firstly, process the data\n# specify your dependent variable and independent variables\ny_iris = select(py$iris_data, \"sepal_length\")\nx_iris = select(py$iris_data, -\"sepal_length\")\n\n# for python approach, we need to add a constant to predictor variables\nx_iris = sm$add_constant(x_iris)\n\n# create a linear regression model and fit it to the data\npy_lm = sm$OLS(y_iris, x_iris)$fit()\n\n# get the model summary\nprint(py_lm$summary())\n\nAs you can see, the results are the same (those of statsmodels have been rounded). This could have also been done purely in python, as demonstrated below. Please note that here we have brought the iris data from python to R, then manipulated it in R (with the select() operation), and now we have two options to use these objects in pure python (just one is needed):\n\nWe can simply call the objects from R by r.x_iris and r.y_axis.\nWe can do the same select() operation in python with it’s own syntax on the original python iris data:\n\n\npy_y_iris = iris_data[\"sepal_length\"]\npy_x_iris = iris_data.drop(\"sepal_length\", axis=1)\n\nNeedless to say that in this case, the most efficient approach is the first one as demonstrated below:\n\n# load the sm library with it's alias\nimport statsmodels.api as sm\n\n# add the constant again\nx_iris = sm.add_constant(r.x_iris)\n\n# create a linear regression model and fit it to the data\npy_lm = sm.OLS(r.y_iris, r.x_iris).fit()\n\n# get the model summary\nprint(py_lm.summary())\n\nThe outcome is the same as calling python within R.",
    "crumbs": [
      "Labs",
      "Setup"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html",
    "href": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "In this series of exercises, we illustrate PCA on the wine data already used for clustering. We first load the data.\n\nRPython\n\n\n\nwine &lt;- read.csv(here::here(\"labs/data/Wine.csv\"))\nrow.names(wine) &lt;- paste(\"W\", c(1:nrow(wine)), sep=\"\") \nwine[,-12] &lt;- scale(wine[,-12])\n\n\n\n\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nwine = pd.read_csv('../../data/Wine.csv')\nwine.index = [\"W\" + str(i) for i in range(1, len(wine)+1)]\nscaler = StandardScaler()\nwine.iloc[:, :-1] = scaler.fit_transform(wine.iloc[:, :-1])\n\n\n\n\nNote that here the scaling of the variables is optional. The PCA can be applied on the correlation matrix which is equivalent to use scaled features. We could alternatively use unscaled data. The results would of course be different and dependent on the scales themselves. That choice depends on the practical application.",
    "crumbs": [
      "Labs",
      "PCA"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#circle-of-correlations-and-interpretation",
    "href": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#circle-of-correlations-and-interpretation",
    "title": "Principal Component Analysis",
    "section": "Circle of correlations and interpretation",
    "text": "Circle of correlations and interpretation\nTo produces a circle of correlations:\n\nRPython\n\n\n\nfviz_pca_var(wine_pca)\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfeatures = wine.columns[:-1]\nloading_matrix = pd.DataFrame(pca.components_.T, columns=[f\"PC{i+1}\" for i in range(11)], index=features)\nloading_matrix = loading_matrix.iloc[:, :2]\n\n# Normalize the PCA loadings\nloading_matrix_normalized = loading_matrix / np.sqrt(np.sum(loading_matrix**2, axis=0))\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.scatter(loading_matrix_normalized[\"PC1\"], loading_matrix_normalized[\"PC2\"], color=\"r\", marker=\"o\", s=100)\n\nfor i, feature in enumerate(loading_matrix_normalized.index):\n    ax.arrow(0, 0, loading_matrix_normalized.loc[feature, \"PC1\"], loading_matrix_normalized.loc[feature, \"PC2\"], color=\"r\", alpha=0.8)\n    ax.text(loading_matrix_normalized.loc[feature, \"PC1\"], loading_matrix_normalized.loc[feature, \"PC2\"], feature, color=\"black\", fontsize=12)\n\nax.axhline(0, color=\"black\", linewidth=1)\nax.axvline(0, color=\"black\", linewidth=1)\nax.set_xlim(-1.1, 1.1)\nax.set_ylim(-1.1, 1.1)\n\ncircle = plt.Circle((0, 0), 1, color='gray', fill=False, linestyle=\"--\", linewidth=1)\nax.add_artist(circle)\n\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.set_title(\"Circle of Correlations\")\nax.grid()\nplt.show()\n\n\n\n\n\n\n\nNote\n\n\n\nPlease do note for this plot:\n\nValues on the x-axis have been normalized to be between -1 and 1. For the rest of the python plots, we’ll not apply this.\nTo get the same results as R, the coordinates of PC2 should be flipped. The results appear mirrored because PCA is sensitive to the orientation of the data. The PCA algorithm calculates eigenvectors as the principal components, and eigenvectors can have either positive or negative signs. This means that the orientation of the principal components can vary depending on the implementation of PCA in different libraries. You can get the same results as R with the code below:\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Normalize the PCA loadings\nloading_matrix_normalized = loading_matrix / np.sqrt(np.sum(loading_matrix**2, axis=0))\n\n# Add this if you want the same results\nloading_matrix_normalized[\"PC2\"] = -loading_matrix_normalized[\"PC2\"]\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.scatter(loading_matrix_normalized[\"PC1\"], loading_matrix_normalized[\"PC2\"], color=\"r\", marker=\"o\", s=100)\n\nfor i, feature in enumerate(loading_matrix_normalized.index):\n    ax.arrow(0, 0, loading_matrix_normalized.loc[feature, \"PC1\"], loading_matrix_normalized.loc[feature, \"PC2\"], color=\"r\", alpha=0.8)\n    ax.text(loading_matrix_normalized.loc[feature, \"PC1\"], loading_matrix_normalized.loc[feature, \"PC2\"], feature, color=\"black\", fontsize=12)\n\nax.axhline(0, color=\"black\", linewidth=1)\nax.axvline(0, color=\"black\", linewidth=1)\nax.set_xlim(-1.1, 1.1)\nax.set_ylim(-1.1, 1.1)\n\ncircle = plt.Circle((0, 0), 1, color='gray', fill=False, linestyle=\"--\", linewidth=1)\nax.add_artist(circle)\n\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.set_title(\"Circle of Correlations\")\nax.grid()\nplt.show()\n\n\n\n\n\n\nThis is for the two first principal components. We see\n\nPC1 explains \\(31.4\\%\\) of the variance of the data, PC2 explains \\(15.3\\%\\). In total, \\(46.7\\%\\) of the variance of the data is explained by these two components.\nPC1 is positively correlated with density, negatively with alcohol. Which confirms that these two features are negatively correlated. It is also positively correlated with residual.sugar.\nPC2 is positively correlated with pH, negatively with fixed.acidity and, a little less, with citric.acid.\nFeatures with shorts arrows are not explained here: volatile.acidity, sulphates, etc.\n\nTo even better interpret the dimensions, we can extract the contributions of each features in the dimension. Below, for PC1.\n\nRPython\n\n\n\nfviz_contrib(wine_pca, choice = \"var\", axes = 1)\n\n\n\n\nexplained_variance = pca.explained_variance_ratio_\nsquare_loading_matrix = loading_matrix**2\ncontributions = square_loading_matrix * 100 / explained_variance[:2]\n\ncontributions = contributions[\"PC1\"].sort_values(ascending=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=contributions.values, y=contributions.index, palette=\"viridis\")\nplt.xlabel(\"Contribution (%)\")\nplt.ylabel(\"Features\")\nplt.title(\"Feature Contributions for PC1\")\nplt.show()\n\n\n\n\nWe recover our conclusions from the circle (for PC1).",
    "crumbs": [
      "Labs",
      "PCA"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#map-of-the-individual-and-biplot",
    "href": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#map-of-the-individual-and-biplot",
    "title": "Principal Component Analysis",
    "section": "Map of the individual and biplot",
    "text": "Map of the individual and biplot\nWe can represent the wines in the (PC1,PC2) map. To better interpret the map, we add on it the correlation circle: a biplot.\n\nRPython\n\n\n\n## fviz_pca_ind(wine_pca) ## only the individuals\nfviz_pca_biplot(wine_pca) ## biplot\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the scores for the first two principal components\nscores = wine_pca.iloc[:, :2]\n\n# Define the loadings for the first two principal components\nloadings = pca.components_.T[:, :2]\n\n# Scale the loadings by the square root of the variance\nloadings_scaled = loadings * np.sqrt(pca.explained_variance_[:2])\n\n# Calculate the scaling factor for the arrows\narrow_max = 0.9 * np.max(np.max(np.abs(scores)))\nscale_factor = arrow_max / np.max(np.abs(loadings_scaled))\n\n# Create a scatter plot of the scores\nplt.figure(figsize=(10, 8))\nplt.scatter(scores.iloc[:, 0], scores.iloc[:, 1], s=50, alpha=0.8)\n\n# Add arrows for each variable's loadings\nfor i, variable in enumerate(wine.columns[:-1]):\n    plt.arrow(0, 0, loadings_scaled[i, 0]*scale_factor, loadings_scaled[i, 1]*scale_factor, color='r', alpha=0.8)\n    plt.text(loadings_scaled[i, 0]*scale_factor, loadings_scaled[i, 1]*scale_factor, variable, color='black', fontsize=12)\n\n# Add axis labels and title\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Biplot of Wine Dataset')\n\n# Add grid lines\nplt.grid()\n\n# Show the plot\nplt.show()\n\n\n\n\nIt’s a bit difficult to see all the patterns, but for instance\n\nWine 168 has a large fixed.acidity and a low pH.\nWine 195 has a large alcohol and a low density.\netc.\n\nWhen we say “large” or “low”, it is not in absolute value but relative to the data set, i.e., “larger than the average”; the average being at the center of the graph (PC1=0, PC2=0).",
    "crumbs": [
      "Labs",
      "PCA"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#how-many-dimensions",
    "href": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#how-many-dimensions",
    "title": "Principal Component Analysis",
    "section": "How many dimensions",
    "text": "How many dimensions\nFor graphical representation one a single graph, we need to keep only two PCs. But if we use it to reduce the dimension of our data set, or if we want to represent the data on several graphs, then we need to know how many components are needed to reach a certain level of variance. This can be achieved by looking at the eigenvalues (screeplot).\n\nRPython\n\n\n\nfviz_eig(wine_pca, addlabels = TRUE, ncp=11)\n\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 12), explained_variance * 100, 'o-')\nplt.xticks(range(1, 12), [f\"PC{i}\" for i in range(1, 12)])\nplt.xlabel(\"Principal Components\")\nplt.ylabel(\"Explained Variance (%)\")\nplt.title(\"Scree Plot\")\nplt.grid()\nplt.show()\n\n\n\n\nIf we want to achieve \\(75\\%\\) of representation of the data (i.e., of the variance of the data), we need 5 dimensions. This means that the three biplots below represent \\(&gt;75\\%\\) of the data (in fact \\(84.6\\%\\)).\n\nlibrary(gridExtra)\np1 &lt;- fviz_pca_biplot(wine_pca, axes = 1:2) \np2 &lt;- fviz_pca_biplot(wine_pca, axes = 3:4) \np3 &lt;- fviz_pca_biplot(wine_pca, axes = 5:6) \ngrid.arrange(p1, p2, p3, nrow = 2, ncol=2)",
    "crumbs": [
      "Labs",
      "PCA"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Important dates (more info during the class)\n\n\n\n\nMonday the 5th of May: Written exam (in-class)\nWednesday the 21th of May: Project report deadline\nSunday the 25th of May: Slides submission deadline\nMonday the 26th of May: Presentations of the projects\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nLecture Slide\nLecture Note\nLab\n\n\n\n\n1\nMon, Feb 17\nIntroduction + EDA\n\n\n🧑🏻‍🏫 Introduction\n\n🧑🏻‍🏫 EDA\n\n\n📄 Introduction\n\n📄 EDA\n\n\n\n\n2\nMon, Feb 24\nModels(regression & classification)\n\n\n🧑🏻‍🏫 Intro to Models\n\n🧑🏻‍🏫 Linear & Logistic Models\n🤖 ML_LinLogReg.R\n🧑🏻‍🏫 Trees\n🤖 ML_Trees.R\n\n\n📄 Intro to Models\n\n📄 Linear & Logistic Models\n📄 Trees\n\n\n\n\n3\nMon, Mar 3\nLab 1\n\n\n\n\n\n\n📝 Lab 1 Setup: Slides\n\n📝 Lab 1 Intro to Quarto: Slides\n💻 Setup\n💻 LinLogReg\n💻 Tree\n\n\n4\nMon, Mar 10\nMetrics(& Overfitting detection)\n\n\n🧑🏻‍🏫 Neural Networks\n\n🧑🏻‍🏫 Support Vector Machines\n🧑🏻‍🏫 Metrics\n\n\n📄 Neural Networks\n\n📄 Support Vector Machines\n📄 Metrics\n\n\n\n\n5\nMon, Mar 17\nLab 2\n\n\n\n\n\n\n💻 NN\n\n💻 SVM\n💻 Scoring\n\n\n6\nMon, Mar 24\nData splitting + Ensemble methods\n\n\n🧑🏻‍🏫 Data Splitting\n\n🧑🏻‍🏫 Ensemble Methods\n\n\n📄 Data Splitting\n\n📄 Ensemble Methods\n\n\n\n\n\n\nTue, Apr 1\nno lecture 😢\n\n\n\n\n\n\n\n\n7\nMon, Apr 7\nInterpretable ML + Unsupervised Learning\n\n\n🧑🏻‍🏫 Interpretable ML\n\n🧑🏻‍🏫 Intro to Unsupervised Learning\n🧑🏻‍🏫 Clustering\n🧑🏻‍🏫 Dimension Reduction\n\n\n📄 Interpretable ML\n\n📄 Intro to Unsupervised Learning\n📄 Clustering\n📄 Dimension Reduction\n\n\n\n\n8\nMon, Apr 14\nLab 3\n\n\n\n\n\n\n💻 Splitting\n\n💻 Ensemble\n💻 VarImp\n💻 Clustering\n\n\n9\nMon, Apr 21\nEaster(i.e., no lecture) 😊\n\n\n\n\n\n\n\n\n10\nMon, Apr 28\nLab 4 + revisions\n\n\n\n\n\n\n💻 PCA\n\n💻 Autoencoders\n\n\n11\nMon, May 5\nWritten exam(on site) ❗\n\n\n\n\n\n\n\n\n12\nMon, May 12\nWorking session(projects)\n\n\n\n\n\n\n\n\n13\nMon, May 19\nWorking session(projects)\n\n\n\n\n\n\n\n\n\n\n1 Wed, May 21\nProject report deadline(moodle) ❗\n\n\n\n\n\n\n\n\n\n\n1 Sun, May 25\nProject presentation deadline(moodle) ❗\n\n\n\n\n\n\n\n\n14\nMon, May 26\nFinal presentations(on site) ❗\n\n\n\n\n\n\n\n\n\n1 📩 Report & Slides submission at 23:59\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLecture Slides vs Lecture Notes\n\n\n\nThe most up to the date content of the course are found in the lecture slides. The lecture notes are only there to assist you, and may contain outdated (but still correct) information. Therefore, always refer to the lecture slides for the most accurate and up to date content.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe content of the schedule may be adapted (faster or slower).",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  },
  {
    "objectID": "resources/data_acquisition/web_scraping_api.html",
    "href": "resources/data_acquisition/web_scraping_api.html",
    "title": "Web scraping & APIs",
    "section": "",
    "text": "Navigating the digital age means unlocking the treasure trove of data available online. Web scraping and APIs aren’t just technical skills; they’re your keys to a universe of data for any project you can imagine. Think about the ease of analyzing trends, financial markets, or even sports—all through data you gather and analyze yourself.\nIn this section, Ilia walks us through the essentials of harnessing web data, offering a powerful alternative for those looking to source unique datasets for their projects. Knowing these techniques empowers you to find and utilize data that sparks your curiosity and fuels your research. Let’s dive in and discover how these tools can transform your approach to data collection.",
    "crumbs": [
      "Resources",
      "Web Scraping"
    ]
  },
  {
    "objectID": "resources/data_acquisition/web_scraping_api.html#using-css",
    "href": "resources/data_acquisition/web_scraping_api.html#using-css",
    "title": "Web scraping & APIs",
    "section": "Using CSS",
    "text": "Using CSS\nIn this pratice, we learn how to use the rvest package to extract information from the famous IMDB (Internet Movie Database) site of the 50 most popular movies (https://www.imdb.com/search/title/?groups=top_250&sort=user_rating). The page was saved (downloaded) and is also available in the data/ folder. Alternatively, you can directly work on the link. However, bear in mind that thr structure of online websites can change in time, therefore, the code below might need adjustments (i.e., change in tags).\nFirst, we load the page.\n\nlibrary(rvest)\nlibrary(magick)\nlibrary(tidyverse)\nlibrary(flextable)\nlibrary(pdftools)\nlibrary(tesseract)\n\n\n# local file (html)\nimdb.html &lt;- read_html(\"data/IMDb _Top 250.html\") \n# or alternatively use the link\n# imdb.html &lt;- read_html(\"https://www.imdb.com/search/title/?groups=top_250&sort=user_rating\") # webpage\n\nNow, we identify the positions of the titles. On the web page (opened preferably with Chrome) right-click on a title and select “Inspect”. The tag corresponding to the titles appears on the developer window (partially reproduced below).\n\n&lt;div class=\"lister-item-content\"&gt;\n&lt;h3 class=\"lister-item-header\"&gt;\n    &lt;span class=\"lister-item-index unbold text-primary\"&gt;1.&lt;/span&gt;\n    &lt;a href=\"https://www.imdb.com/title/tt0111161/?ref_=adv_li_tt\"&gt;The Shawshank Redemption&lt;/a&gt;\n    &lt;span class=\"lister-item-year text-muted unbold\"&gt;(1994)&lt;/span&gt;\n&lt;/h3&gt;\n[...]\n&lt;/div&gt;\n\nLooking above, the title (“The Shawshank Redemption”) is under the div tag with class=\"lister-item-content\", then the sub-tag h3 within it then the tag a within it. The html_nodes function can target this tag. The “dot” after div indicates the class value. It actually targets all such tags.\n\ntitles &lt;- imdb.html %&gt;%\n  html_nodes(\"div.lister-item-content h3 a\") \nhead(titles) \n\nThe results are cleaned from the html code (i.e., only the texts remain) using html_text2 function.\n\ntitles &lt;- html_text2(titles)\nhead(titles)\n\nAnother way would have been to use the fact that the targeted h3 tags have a class value. Modify the previous code to extract tags a within h3 with class value “lister-item-header”.\n\n\nAnswer\n\n\n\ntitles &lt;- imdb.html %&gt;% \n  html_nodes(\"h3.lister-item-header a\") %&gt;% \n  html_text2()\ntitles\n\n\n\nNow, repeat that approach for the year and the run time. You may use the function substr to extract the year from the text.\n\n\nAnswer\n\n\nFor the years:\n\n## Extract the years\nyears &lt;- imdb.html %&gt;% \n  html_nodes(\"div.lister-item-content h3 .lister-item-year\") %&gt;%\n  html_text2()\nyears &lt;- as.numeric(substr(years,\n                           start = 2,\n                           stop = 5))\n# take only characters 2 to 5 corresponding to the year\n##years &lt;- as.numeric(gsub(\"[^0-9.-]\", \"\", years)) # an alternative: keep only the numbers in a string\n\nFor the run times, first they are extracted in the format “120 min”. Then, the run time is split by space which gives “120” and “min”. The unlist command casts this to a vector. Then we take one element every two (corresponding to the minutes).\n\nruntimes &lt;- imdb.html %&gt;%\n  html_nodes(\"span.runtime\") %&gt;%\n  html_text2()\nruntimes &lt;- as.numeric(\n  unlist(\n    strsplit(runtimes, \" \"))[seq(from = 1, by = 2, len = 50)]) \n# by space, \n\n\n\nSuppose that now we want to extract the description. In this case, there is no unique class value identifying the field (see the html code). However, one can note that it is the 4th paragraph (element) within a div tag with a useful class value. To access the k-th paragraph you can use p.nth-child(k) starting from the correct hierarchical position. For example, p:nth-child(2) extract the 2-nd paragraph.\nFor the 4-th paragraph (i.e., the wanted description), a possible code is thus\n\ndesc &lt;- imdb.html %&gt;% \n  html_nodes(\"div.lister-item-content p:nth-child(4)\") %&gt;% \n  html_text2()\nhead(desc)\n\nTo finish, we build a data frame containing this information (tibble format below).\n\nimdb.top.50 &lt;- tibble(data.frame(\n  Titles = titles,\n  Years = years,\n  RunTimes = runtimes,\n Desc = desc))\nimdb.top.50 %&gt;% \n  head() %&gt;% \n  flextable() %&gt;% \n  autofit()",
    "crumbs": [
      "Resources",
      "Web Scraping"
    ]
  },
  {
    "objectID": "resources/data_acquisition/web_scraping_api.html#xpath",
    "href": "resources/data_acquisition/web_scraping_api.html#xpath",
    "title": "Web scraping & APIs",
    "section": "XPath",
    "text": "XPath\nIn the previous part, we used the CSS to identify the tags. We now use an alternative: the XPath. The Xpath is preferably used when we want to extract a specific text. For example, we want to extract the description of the first description: right-click and select inspect. Then right-click the corresponding code line, and select “Copy xpath”. Pass this, to the xpath parameter of html_nodes like below:\n\ndesc1 &lt;- imdb.html %&gt;% \n  html_nodes(xpath=\"//*[@id='main']/div/div[3]/div/div[1]/div[3]/p[2]/text()\") %&gt;% \n  html_text2()\ndesc1\n\n\n\n\n\n\n\nNote\n\n\n\nIn the xpath, you must turn the quotes around main to simple quotes.\n\n\nThis is convenient when you want to extract a particular text. You can also use the Selector Gadget from Chrome to extract multiple Xpath.",
    "crumbs": [
      "Resources",
      "Web Scraping"
    ]
  },
  {
    "objectID": "resources/cheatsheets.html",
    "href": "resources/cheatsheets.html",
    "title": "Coding Cheatsheets & Books",
    "section": "",
    "text": "Python\nYou can find an extensive cheatsheet for Python in addition to a book for learning python:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\nThe following cheatsheets come from https://posit.co/resources/cheatsheets. We haven’t covered every function and functionality listed on them, but you might still find them useful as references.",
    "crumbs": [
      "Resources",
      "Coding Cheatsheets"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning in Business Analytics 2024-2025",
    "section": "",
    "text": "🧑🏻‍🏫 Teaching staff: Dr. Marc-Olivier Boldi (Lecturer) & Ilia Azizi (TA).\n🕛 Time: Mondays 8:30-12:00, starting on Monday the 17th of February 2025.\n🏫 Room: Anthropole 3021 (no live broadcasting or recording).\n\n📖 Content\nThis course presents several machine learning techniques in business and management contexts. The list of topics is meant to cover mainly supervised methods of classification and prediction although unsupervised methods are also seen. Below is a tentative lists of topics. It will be adapted according to the pace of the class:\n\nSupervised learning models: regressions, trees, support vector machine, neural networks.\nData splitting: training/test sets, cross-validation, bootstrap.\nMetrics: MSE, Accuracy, …\nEnsemble methods: bagging, random forests, boosting.\nUnsupervised learning: clustering, PCA, FAMD, Auto-Encoder.\nInterpretable machine learning: variale importance, partial dependence plots, LIME.\n\nExercises and theory are equally crucial for the success of the class. For the labs, we use primarily R, with equivalent code in Python.\n\n\n📝 Evaluation\n\nPartial written exam: organized during the semester.\nApplied project: individual or in group (depending on the number of participants to the course).\n\nOne report (incl. supplementary material, codes, etc.)\nOne final presentation will be organized during the semester or during the exam session (depending on the number of participants).\n\n\nFinal grade = (0.3 x exam) + (0.3 x presentation) + (0.4 x report)\n\n\n\n\n\n\nProject member contributions\n\n\n\nThe project grade is a group grade. However, if the contribution of the members of the groups to the project is unbalanced, an individual adaptation of the grade will be made, e.g., absence of one of the members of the group during the presentation (in catch-up, a subsequent and adapted presentation of the absent member may be required).\n\n\nFurther directives and guidelines are provided in the Assessments section.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "assessments/Report_Guideline.html",
    "href": "assessments/Report_Guideline.html",
    "title": "Report Guidelines",
    "section": "",
    "text": "For your report, you can use the Quarto template we provide here. Please note that the Personal Information And Non-Plagiarism Statement indicated below must be integrated into the final report during your submission."
  },
  {
    "objectID": "assessments/Report_Guideline.html#why-do-you-write-a-technical-report",
    "href": "assessments/Report_Guideline.html#why-do-you-write-a-technical-report",
    "title": "Report Guidelines",
    "section": "Why Do You Write A (Technical) Report?",
    "text": "Why Do You Write A (Technical) Report?\nThe aim of writing a report is to communicate results from a study to someone who did not participate to it.\nThe report should be relevant, easy to read and convincing. Advices:\n\nTake some time to ask yourself what the aims of your report are, and to whom it is addressed.\nAsk yourself which messages you report should convey and which is the best way to convey it."
  },
  {
    "objectID": "assessments/Report_Guideline.html#level-of-details",
    "href": "assessments/Report_Guideline.html#level-of-details",
    "title": "Report Guidelines",
    "section": "Level Of Details",
    "text": "Level Of Details\nIncluding all the minutes of your research is NOT interesting. Relevance and reproducibility are the key words here. On the one hand, the report should include all the needed details for the reader to be able to repeat on his own the relevant part of the study. On the other hand, the report is not a course about a given subject. In particular, it is not expected that you include in the report all the theory about a method that is used. Of course, it is not easy to find the good balance, which anyway probably varies from one reader to another. Often, this contradiction can be solved by referencing to clear and accessible sources (paper, book chapter, webpage, etc.)."
  },
  {
    "objectID": "assessments/Report_Guideline.html#the-structure",
    "href": "assessments/Report_Guideline.html#the-structure",
    "title": "Report Guidelines",
    "section": "The Structure",
    "text": "The Structure\nThere are several ways to write a report. The following structure can be used and adapted according to the specificity of the study.\n\nA. Title Page (Mandatory)\nOne separate page. It should include\n\nTitle: large format. It should be short AND informative. Never more than a sentence.\nLogo: UNIL/HEC at least; optional: logo of the company\nDate of edition\nCourse name and the year\nGroup name\n\n\n\nB. Personal Information And Non-Plagiarism Statement (Mandatory)\nOne separated page containing\n\nFull names of all the group members and their email address\nThe following text hand-signed by all the authors. This project was written by us and in our own words, except for quotations from published and unpublished sources, which are clearly indicated and acknowledged as such. We are conscious that the incorporation of material from other works or a paraphrase of such material without acknowledgement will be treated as plagiarism, subject to the custom and usage of the subject, according to the University Regulations. The source of any picture, map or other illustration is also indicated, as is the source, published or unpublished, of any material not resulting from our own research.\n\n\n\nC. Summary Or Abstract\nUsually one page, it is limited to one-and-a-half-page maximum. The summary states very succinctly and clearly the issue, including a data brief description, the methods used, the main results, and the main conclusions. This summary allows someone to know, without to read your work, what has been done, why and what are the results and implications. Advises for writing it:\n\nWrite it at the very end the work, after the report body is written and clean.\nTake each section (Introduction, etc.) and summarize it in one, two, three, sentences, one paragraph or so. Use the needed space. State the important results.\nOnce finished, read it again. Cut/shorten long and useless words (e.g. “has been” -&gt; “was”, “therefore” -&gt; “hence”, etc.).\nIf it is still too long, cut some part by order of importance.\nThe summary is not a teaser to read the report. It contains the results. It should summary them. It should not be only inviting the reader to read the report but propose an alternative for readers who do not have time to read it all.\nTypically, “We have made an analysis of the data in this report about the influential factors on the sales. We decided to use an ARIMA model to make the forecasts.” It contains a piece of information but is quite frustrating. More useful: “Using an ARIMA model, the [what] sales were found influenced by the [what] factors. Such model achieved an RMSE of [what] on forecasting a [what] test set.”.\n\n\n\nD. Table Of Content (Toc), Table Of Figures (Tof) And Table Of Tables (Tot)\nToC is mandatory. ToF and ToT are nice to have especially for very long reports, or when these elements are central to the report. ToC must be up to date. You gain generating it automatically (easy with tools like word, LateX, Rmarkdown, etc.). ToF and ToT should also be generated automatically. This implies that figures and tables are correctly inserted with a caption.\n\n\nE. Body Of The Report\n\n1. Introduction\nIn brief, the introduction explains why the theme is interesting, what is done in the study and what it will bring. After reading it, the reader should have a broad view on the study, why it is relevant, and how it is organized. The purpose of the introduction is not to detail all theoretical aspects. Introduction is a section that should contain\n\nThe context and background: course, company name, business context.\nAim of the investigation: major terms should be defined, the question of research (more generally the issue), why it is of interest and relevant in that context.\nDescription of the data and the general material provided and how it was made available (and/or collected, if it is relevant). Only in broad terms however, the data will be further described in a following section. Typically, the origin/source of the data (the company, webpage, etc.), the type of files (Excel files, etc.), and what it contains in broad terms (e.g. “a file containing weekly sales with the factors of interest including in particular the promotion characteristics”).\n\nThe method that is used, in broad terms, no details needed at this point. E.g. “Model based machine learning will help us quantifying the important factors on the sales”.\n\nAn outlook: a short paragraph indicating from now what will be treated in each following sections/chapters. E.g. “in Section 3, we describe the data. Section 4 is dedicated to the presentation of the text mining methods…”\n\n\n\n2. Data Description\nThe data description section may be optional. It is typical of report for project oriented toward statistical and machine learning. The data description section can be included as a subsection of the introduction or be a whole section by itself, depending on the complexity and the size of the database. If relevant, it should contain\n\nDescription of the data file format (xlsx, csv, text, video, etc.)\nThe features or variables: type, units, the range (e.g. the time, numerical, in weeks from January 1, 2012 to December 31, 2015), their coding (numerical, the levels for categorical, etc.), etc.\nThe instances: customers, company, products, subjects, etc.\nMissing data pattern: if there are missing data, if they are specific to some features, etc.\nAny modification to the initial data: aggregation, imputation in replacement of missing data, recoding of levels, etc.\nIf only a subset was used, it should be mentioned and explained; e.g. inclusion criteria. Note that if inclusion criteria do not exist and the inclusion was an arbitrary choice, it should be stated as such. One should not try to invent unreal justifications.\n\nIn some cases, data structure can be more complex like images, videos, etc. This should be adapted accordingly. Any unknown element should be mentioned as such. Overall, this description can be quite short or very long, depending on the format of the data. For example, one xlsx file with 10 features and 50’000 instances representing customer behaviors is in general quite easy to describe. If you have 25 files with different formats from different stores representing these customers, with features and instances only partially matching, then it may be more difficult to deal with. Typically, the first and simple case would be included into the introduction as a subsection while the more complex case would deserve a whole data description section.\n\n\n3. Methods\nA very important section describing in detail all the methods that are used in the report. This section explains how the study was conducted. It aims at giving relevant information and tools for others to be able to reproduce the study. It explains the global strategy of the study, and describes each tool used to achieve this goal. For a comprehensive and structured presentation, often it is important to give the general structure of the study, and then to drill down each step. The individual tools (e.g. machine learning methods, etc.) should be described in a concise and relevant way. This section is not a course. For example, one can give a brief definition and the important properties (important for the study), then refer to an external source for the details. In addition, the description should be balanced. Do not spend three pages on explaining the boxplots when a predictive model of random forest is explained in one-half page. It should describe and duly cite\n\nStatistical and machine learning methods to analyze the data\nSoftware, computer programs, function packages (typically for R).\nThe sequence of analysis, the reason of each method.\n\nHowever,\n\nIt does not include methods that are not used, even if you spent a lot of time on it. Except maybe if the fact that a particular method is not useful is of interest for the report.\nIt is not a course about a method. It should contain all the relevant pieces of information, but it should also be stopped at some points. Clear references and well-chosen are of major importance here.\n\n\n\n4. Results\nObviously, this section presents the results: application of the methods to the data. It presents all the results (even negative ones) and only the results (no more data or method descriptions). This section should be very well structured especially if there are lot of results. Use subsections and clear references. In addition, use the appendix. For example, if 30 time series were analyzed and the corresponding 30 figures produced. Then only the most interesting ones should be included in the results section. The other ones should be reported in the appendix (or supplementary material) and appropriately cited. The interpretation can be included at this point but should be limited to technical aspects. Keep discussions for the next section.\n\n\n5. Recommendations And Discussion\nIn this section, the interpretations of the results are given:\n\nSummary of what was observed from the results\nImplications for the business (link with the context and the initial objectives of the study).\nLimitations (this may be the subject of a separated section). An opening is often expected here: “What next?” “What can be done for improvement”, etc.?\n\n\n\n6. References\nAll external sources should be cited. The APA system is a good and safe one. One can use the Reference tool of Word. For more details, see http://www.apastyle.org/manual/index.aspx.\n\n\n7. Appendix\nThe appendix section is not a trash in which one puts all what was done but not integrated in the body of the report. All the elements in the appendix must be cited in the text. Appendix can contain graphs, texts, tables, codes, etc. For example, in the body of the report: “Fig. 1 presents the analysis of the times of the process A […]. The 29 other processes are represented in Fig. 40 to 68 in Appendix B.”\n\n\n\nF. Supplementary Material\nIn this section, you mention and briefly describe all the external documents given with the report. For example, a code file name “OurSuperRCode.R” attached to the report should be mentioned here."
  },
  {
    "objectID": "assessments/Report_Guideline.html#further-remarks",
    "href": "assessments/Report_Guideline.html#further-remarks",
    "title": "Report Guidelines",
    "section": "Further Remarks",
    "text": "Further Remarks\n\nGeneral Format And Style\n\nFont: Times New Roman, 11pts\nPage format: A4\nText justification: even on both side\nMargins: 2.5cm\nMax. 40 pages for the body of the report (i.e. not including title, toc, introduction, reference, appendix, sup. mat.). It is not an expected value. It is an upper bound.\n\n\n\nText editor\nUsing Word is probably the simplest choice, but \\(LaTeX\\) is also welcomed (see Overleaf for easy and free usage) Freely available alternatives exist like LibreOffice. It should be noted that Quarto and Rmarkdown could also be a useful tool. However, be mindful of mixing research (i.e. working R codes, running complex R code or procedure) and the writing of the report. Often, one loses more time and encounter lots of difficulties. Nevertheless, Quarto is very valuable for writing reproducible reports. In any cases, pay attention to use cache in order to avoid running again and again very heavy computations each time you want to compile the report.\n\n\nMaths and formulae\nIf you need to edit equations or formulae, you may gain using an equation editor like MathWord. Free alternatives exist like LibreOffice.\n\n\nNumbers, figures and other remarks\nUsually, in the text, numbers zero (0) to ten (10) are written in letters, then 11, 12, etc. This not for reference like “in Table 1” should never be written “in Table one”. Figures, tables and sections referenced in the text are names an should have a capital letter. One should write “in Table 1”, “in Figure 3 shown below”, “Section 3.2 shows […]”. It is not the case when you refer to an object like “the next table”, “in the following section we can see […]”, etc.\nDo not use don’t (except when quoting speeches for example). For a better scientific writing, check out this blog."
  },
  {
    "objectID": "assessments/Presentation_Guidelines.html",
    "href": "assessments/Presentation_Guidelines.html",
    "title": "Presentation Guidelines",
    "section": "",
    "text": "A typical plan:\n\nTitle, date and team\nAgenda\nContext (Problem Description)\nLiterature Review\nMethodology\nResults\nConclusion and discussion\nReferences if needed\n“Thank you + Any Question” slide\n\nMaterial:\n\n1-3 minutes per slide (e.g., around 10 to 20 slides for a 30-minutes presentation with only slides).\nKeep additional slides hidden (typical technical ones, extra results, etc.) and show them in case of questions/discussion.\nConsider also live demo, showing codes, etc.\n\nOrganization:\n\nIn group, define clearly who is going to explain what.\n\nMake fluent transition (useless to start over again the context when the speaker change).\n\nWalk away from these guidelines if they are not adapted in your case. Also, feel free to let your originality speaks.\n\nThe further guidelines below are based on the following link:\n\nKnow Your Audience\n\nAdapt your speech to your audience. If people think often about adapting the technical content, it is equally important to adapt the context. Including a deep explanation of a context to an audience who knows it can be as counter-productive and time consuming as not explaining this context when it is due.\n\nTell audience members up front why they should care and what is in it for them Explain the issue and why it is important.\nConvey your excitement. If any… but do not overplay this.\nBuild a story:\n\nExplain the context\nFrame the problem\nProvide highlights: some parts in the analysis/project were striking and determinant. Find and explain these key points.\nConclude by summing up key points and open to new area, problems, etc.\n\nKeep it simple Depending on your audience, some terms may be difficult to understand. If you use difficult terms, you will not impress the audience, you will annoy them.\nPrepare, prepare, prepare:\n\nSet the stage (equipment ready, have a charger in case, have your headphone ready, etc.)\nGet ready (concentrate and calm down, breathe, visualize)\n\nSpeak up, talk to the audience (not to the screen)\nStart and finish on time. Nothing worse than a late presentation. If you need more time, mention it during the presentation and apologize, before to end of the presentation.\nDo not drift at the end The end of your presentation should be clear. Do not start a new conversation.\n\nFor the slides/support materials:\n\nLess Is More. Though, it is difficult to gives a general rule, a regular slide (not the title or the “Thank you. Any Question?” slide) is 1 to 3 minutes. Thus, a 30 minutes presentation cannot have more than 30 slides, and it is already very long. If you are afraid to miss something, have some extra slides with the details that you may show if anyone has a question about it.\nCreate sections Pretty much like the report. It is especially important for group presentation that must be well structured.\nAvoid clutter A slide should be light and easy to read. It is a support for your speech. It is not to be read in detail neither by you nor by the audience.\nMake it readable It is useless to bold or to capitalize every word. Use one appropriate and coherent font. An incomprehensible slide does not become comprehensible or joyful if written in colors.\nUse visual Consider using image, figures, etc. “A drawing is worth a thousand words”. Of course, this is not an objective. Always choose the best way to convey information. If an image is only here to be “nice”, prefer words.\nCheck your spelling We all make typos, especially when it is not our mother tongue. But having too many misspelled words on 20 slides with few words will make you lose credibility. Use correctors, re-read and re-read, ask for help, etc.",
    "crumbs": [
      "Assessments",
      "Project",
      "Presentation Guidelines"
    ]
  },
  {
    "objectID": "assessments/Exam.html",
    "href": "assessments/Exam.html",
    "title": "Exam",
    "section": "",
    "text": "Exam questions with the solutions 📄\nAnswer booklet 📄\nAbove is a recent past exam. The course content varies from year to year, as well as its focus. Therefore, it must not be considered as a mock exam but as an example of the type of questions that can be asked. Furthermore, the answer booklet is an example of the file on which you will solve the exam. During the exam, a lot of flexibility is accepted in its use (copy-paste, include images, adapt answer space, etc.) as long as the presentation of the solutions follows the question orders.",
    "crumbs": [
      "Assessments",
      "Exam"
    ]
  },
  {
    "objectID": "assessments/Project_Directives.html",
    "href": "assessments/Project_Directives.html",
    "title": "Project Directives",
    "section": "",
    "text": "The project grade accounts for 70% of the final grade: 30% for the presentation and 40% for the report. The remaining 30% are for the exam. Grades are from 1 to 6 (Absent=0), rounded to the closest 0.1. The final grade (40% report + 30% presentation + 30% exam) is rounded to the closest 0.5.\n\n\nThe size of groups is 3 students. You may now register via our moodle page. Because we do not know the final number of participants yet, we may create groups of 4 by the end of the grouping process. By default, only groups of 3 can register for the moment.\n\n\n\n\n\n\n\n\n\nYour work must match the structure of analysis and of reporting similar to a scientific paper using machine learning. To guide you, 10 scientific papers have been selected as examples here 📁. Note that these papers are not about developing new machine learning techniques but rather about predictions in some domain using machine learning techniques.\nFor this work, you must find an interesting case (scientific domain of application), make an extensive literature review about the use of machine learning in this context, find appropriate data set(s), define appropriate methodologies of analysis, apply them, and report the results in a scientific paper\nThe paper must contain\n\nAn abstract: summary of the paper including the main results.\nAn introduction: context and objective of the study.\nA literature review: state of the art, previous works, and appropriate citations.\nA methodology exposure: methods and models that are used in the paper.\nResults: all the interesting results under the shape of tables, figures, interpretations.\nConclusion: limitations, discussion, and outlook subsections.\nAppendix: extra elements from the paper body.\nReferences: in an appropriate style (e.g., APA).\n\nThe project must cover methods seen in class such as:\n\nCleaning of the data, creation of new features if needed.\nExploratory Data Analysis.\nSupervised learning analysis: several models, appropriate metrics, a tuning of hyperparameters, data splitting strategies, interpretation of the model.\nUnsupervised analysis: clustering and/or dimension reduction.\n\n\n\n\n\nReport: a pdf, between 10 to 20 pages (not including appendix or supplementary material)\nPresentation: the slides of your presentation.\n\n\n\n\nYou must meet the following deadlines:\n\nProject report: Wednesday the 21st of May 2025 at 23h59\nPresentation slide: Sunday the 25th of May 2025 at 23h59\n\nNot meeting the deadline (presentation and/or report) penalizes the grade by 0.1 per started hour of delay. No maximum penalty for the project report and presentation slide.\n\n\n\nPresentations will be organized on site on Monday the 26th of May 2025 in a 15+10-minutes format (presentation + questions). If the number of groups is large, then either the presentation time will be reduced, or, if not possible, another oral presentation session will be organized later (possibly during the exam session).",
    "crumbs": [
      "Assessments",
      "Project",
      "Project Directives"
    ]
  },
  {
    "objectID": "assessments/Project_Directives.html#machine-learning-2025-project-guidelines",
    "href": "assessments/Project_Directives.html#machine-learning-2025-project-guidelines",
    "title": "Project Directives",
    "section": "",
    "text": "The project grade accounts for 70% of the final grade: 30% for the presentation and 40% for the report. The remaining 30% are for the exam. Grades are from 1 to 6 (Absent=0), rounded to the closest 0.1. The final grade (40% report + 30% presentation + 30% exam) is rounded to the closest 0.5.\n\n\nThe size of groups is 3 students. You may now register via our moodle page. Because we do not know the final number of participants yet, we may create groups of 4 by the end of the grouping process. By default, only groups of 3 can register for the moment.\n\n\n\n\n\n\n\n\n\nYour work must match the structure of analysis and of reporting similar to a scientific paper using machine learning. To guide you, 10 scientific papers have been selected as examples here 📁. Note that these papers are not about developing new machine learning techniques but rather about predictions in some domain using machine learning techniques.\nFor this work, you must find an interesting case (scientific domain of application), make an extensive literature review about the use of machine learning in this context, find appropriate data set(s), define appropriate methodologies of analysis, apply them, and report the results in a scientific paper\nThe paper must contain\n\nAn abstract: summary of the paper including the main results.\nAn introduction: context and objective of the study.\nA literature review: state of the art, previous works, and appropriate citations.\nA methodology exposure: methods and models that are used in the paper.\nResults: all the interesting results under the shape of tables, figures, interpretations.\nConclusion: limitations, discussion, and outlook subsections.\nAppendix: extra elements from the paper body.\nReferences: in an appropriate style (e.g., APA).\n\nThe project must cover methods seen in class such as:\n\nCleaning of the data, creation of new features if needed.\nExploratory Data Analysis.\nSupervised learning analysis: several models, appropriate metrics, a tuning of hyperparameters, data splitting strategies, interpretation of the model.\nUnsupervised analysis: clustering and/or dimension reduction.\n\n\n\n\n\nReport: a pdf, between 10 to 20 pages (not including appendix or supplementary material)\nPresentation: the slides of your presentation.\n\n\n\n\nYou must meet the following deadlines:\n\nProject report: Wednesday the 21st of May 2025 at 23h59\nPresentation slide: Sunday the 25th of May 2025 at 23h59\n\nNot meeting the deadline (presentation and/or report) penalizes the grade by 0.1 per started hour of delay. No maximum penalty for the project report and presentation slide.\n\n\n\nPresentations will be organized on site on Monday the 26th of May 2025 in a 15+10-minutes format (presentation + questions). If the number of groups is large, then either the presentation time will be reduced, or, if not possible, another oral presentation session will be organized later (possibly during the exam session).",
    "crumbs": [
      "Assessments",
      "Project",
      "Project Directives"
    ]
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Tip\n\n\n\nIf you do not find the answer to your question here, you can use the search bar on the top left to search the entire site. If you are still unable to find the answer, you can ask your question during the exercise hours. If you found the solution and think it would be useful to other students, you can report an issue (see on the right) with the questions and the solution, and we will add it to the FAQ.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "faq.html#organizational-issues",
    "href": "faq.html#organizational-issues",
    "title": "FAQ",
    "section": "Organizational issues",
    "text": "Organizational issues\n\nDo we use moodle?\nWe will be using moodle to collect the assessments, and any communication with you. On the other hand, we will use this website to upload the content for the course. You are encouraged to continuously check the website for any material update.\n\n\nWhat is the difference between Pdf and Qmd of the lectures?\nThey contain the same material, except that format-wise, Qmd files are more accessible to navigate and go through when preparing for the exam.\n\n\nHow are the lab sessions organized?\nThe lab exercises on different topics can be followed at your own pace, hence why all the materials are available on this website. You may go slower or faster through the content and then show up for support during one of the four physical lab sessions to ask your questions.\n\n\nWhich programming language will we be using?\nYou can use either R or Python. We will be providing code examples and support in both languages.\n\n\nWhat software do I need to install?\nYou need a text editor or an Integrated Development Environment (IDE). RStudio or VS Code are both good options. We also recommend the use of git and version control.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "faq.html#technical-issues",
    "href": "faq.html#technical-issues",
    "title": "FAQ",
    "section": "Technical issues",
    "text": "Technical issues\n\nHow do I ask a question or report an issue or suggestion regarding the content?\nFollow these steps to propose changes/suggestions to correct the material. First, start off by making a GitHub account:\n\nCreate a GitHub account at https://github.com. You can use whatever username you like, but here is some helpful username advice.1. With your UNIL email address, you can also get the GitHub Student Developer Pack\nGo to https://github.com/do-unil/mlba/issues/new/choose and select either “Suggestion” or “Typo”.\n\nIf Suggestion, describe in detail your suggestion, giving as much information and justification as possible.\nIf Typo, fill in the blanks to provide as much information as possible for the typo to be fixed.\n\nWhen done, hit Submit. \nBefore opening an issue, you must the existing ones at https://github.com/do-unil/mlba/issues to make sure you’re not reporting something someone else has already reported.\nIssues can refer to the text of the lecture or the labs.\n\n\n\nI am lost with Github, what should I do?\nGitHub has possibly the best tutorials and documentation of any software out there. You can find the Hello world there. Most of the time googling your issue will lead you to the right place.\n\n\n\n\n\n\nAdvice for you\n\n\n\nGitHub is super useful for your projects, and working in teams. Ilia will provide you with a tutorial on how to use it.\n\n\n\n\nI have bugs in my code/ something doesn’t work/ I don’t know how to do something, what should I do?\nFirst google your issue. 90 times out of 100 this will solve your issues. Another 9% can be resolved by reading the documentation of the software you are using. For the very last percent you can ask your question during the exercises hours.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "faq.html#footnotes",
    "href": "faq.html#footnotes",
    "title": "FAQ",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGitHub is a code hosting platform for collaboration and version control. You do not need not need to learn about GitHub for this class, however it is also where our open-source textbook is hosted so we’re asking you to make an account on GitHub to be able to provide feedback on the book.↩︎",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "resources/data_acquisition/data_sources.html",
    "href": "resources/data_acquisition/data_sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "Below are some links to data platform that can help you find datasets for your projects:\n\nPopular platforms\n\nGoogle Dataset Search\nData is Plural: Google Sheet with 1000+ datasets\nGithub Awesome Public Datasets\nOpen Data Inception - 2600+ Open Data Portals Around the World\ndata.world\nSubreddit r/datasets\nCORGIS Datasets Project\n\n\n\nGovernments / NGOs websites.\n\nSwitzerland\n\nOffice Fédéral de la Statistique\nSwiss Open Data\n\nEurope\n\nEuropean Data Portal\n\nUnited States\n\nNYC Open Data\nThe General Social Survey\nUS Bureau of Labor Statistics\nUS Department of Education\nFederal Reserve Economic Data\nUS Department of Agriculture\n\nGlobal\n\nWorld Bank Open Data\nUN Data and other UNSD databases\n\nMonthly Bulletin of Statistics Online\nSDG Indicators\nUN Comtrade Database\n\n\n\n\n\nOther platforms\n\nNews API\nKaggle\n10 great sites with free datasets\nShift Data Portal Data about energy consumption and climate\n\n\n\nAcknowledgements\nCredits to dsfba_2021 for providing these data sources.",
    "crumbs": [
      "Resources",
      "Data Sources"
    ]
  },
  {
    "objectID": "resources/beginners_r.html",
    "href": "resources/beginners_r.html",
    "title": "Beginners in R",
    "section": "",
    "text": "R and RStudio installation\n🎬 UNIL course Quantitative Methods for Management (QMM)\n\n\nIntroduction to R\n🎬 Intro to R by the the QMM course\n\n\nswirl\n&lt;/&gt; R package for learning to code directly within the program\n\n\nOther resources\n🔗 http://www.r-tutor.com/r-introduction\n📄 https://cran.r-project.org/doc/manuals/r-release/R-intro.pdf\n📖 https://intro2r.com/",
    "crumbs": [
      "Resources",
      "Beginners in R"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html",
    "href": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "In this exercise, we’ll use the same wine data introduced in the ensemble exercises. As noted before, all the numerical features have units. Additionally, the original objective of this dataset was to predict the wine quality from the other features (supervised learning). The data will only be used for unsupervised task here.\nFirst, load the data and scale the numerical features:\n\nRPython\n\n\n\nwine &lt;- read.csv(here::here(\"labs/data/Wine.csv\"))\nrow.names(wine) &lt;- paste(\"W\", c(1:nrow(wine)), sep=\"\") # row names are used after\nhead(wine)\nsummary(wine)\nwine[,-12] &lt;- scale(wine[,-12]) ## scale all the features except \"quality\"\n\n\n\n\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Start fresh with all necessary imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\nimport os\n\n# Find the correct path to the data file\n# Try different possible locations\npossible_paths = [\n    'labs/data/Wine.csv',  # From project root\n    '../../data/Wine.csv',  # Relative to current file\n    '../../../data/Wine.csv',  # One level up\n    'data/Wine.csv',  # Direct in data folder\n    'Wine.csv'  # In current directory\n]\n\nwine_data = None\nfor path in possible_paths:\n    try:\n        wine_data = pd.read_csv(path)\n        print(f\"Successfully loaded data from: {path}\")\n        break\n    except FileNotFoundError:\n        continue\n\nif wine_data is None:\n    # If we can't find the file, create a small sample dataset for demonstration\n    print(\"Could not find Wine.csv, creating sample data for demonstration\")\n    np.random.seed(42)\n    wine_data = pd.DataFrame(\n        np.random.randn(100, 11),\n        columns=[f'feature_{i}' for i in range(11)]\n    )\n    wine_data['quality'] = np.random.randint(3, 9, size=100)\n\n# Continue with the analysis using wine_data\nwine = wine_data\nwine.index = [\"W\" + str(i) for i in range(1, len(wine)+1)]\nscaler = StandardScaler()\nwine_scaled = pd.DataFrame(\n    scaler.fit_transform(wine.iloc[:, :-1]),\n    index=wine.index,\n    columns=wine.columns[:-1]\n)\nX = wine_scaled.values\n\n# Clear previous plot (if any)\nplt.clf()\n\n# Within sum-of-squares\nwss = []\nfor k in range(1, 26):\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n    wss.append(sum(((X - centroids[labels]) ** 2).sum(axis=1)))\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 26), wss, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Within sum-of-squares')\nplt.title('Elbow Method for Optimal k')\nplt.show()\n\n# Silhouette\nsilhouette_scores = []\nfor k in range(2, 26):\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    silhouette_scores.append(silhouette_score(X, labels))\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(2, 26), silhouette_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette score')\nplt.title('Silhouette Method for Optimal k')\nplt.show()\n\n# Alternative metrics\nch_scores = []\ndb_scores = []\nk_range = range(2, 26)\n\nfor k in k_range:\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    ch_scores.append(calinski_harabasz_score(X, labels))\n    db_scores.append(davies_bouldin_score(X, labels))\n\n# Plot Calinski-Harabasz Index\nplt.figure(figsize=(10, 6))\nplt.plot(list(k_range), ch_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Calinski-Harabasz Index (higher is better)')\nplt.title('Calinski-Harabasz Index for determining optimal k')\nplt.show()\n\n# Plot Davies-Bouldin Index\nplt.figure(figsize=(10, 6))\nplt.plot(list(k_range), db_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Davies-Bouldin Index (lower is better)')\nplt.title('Davies-Bouldin Index for determining optimal k')\nplt.show()\n\n\n\n\n\n\n\nWarning\n\n\n\nPlease note that all the interpretations will be based on the R outputs (like PCA and most other exercises). As usual, the python outputs may be slightly different due to differences in the implementations of the algorithms.",
    "crumbs": [
      "Labs",
      "Clustering"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#distances",
    "href": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#distances",
    "title": "Clustering",
    "section": "Distances",
    "text": "Distances\nWe apply here an agglomerative hierarchical clustering (AGNES). Only the numerical features are used here. First, we compute the distances and plot them. We use Manhattan distance below.\n\nRPython\n\n\n\nlibrary(reshape2) # contains the melt function\nlibrary(ggplot2)\nwine_d &lt;- dist(wine[,-12], method = \"manhattan\") # matrix of Manhattan distances \n\nwine_melt &lt;- melt(as.matrix(wine_d)) # create a data frame of the distances in long format\nhead(wine_melt)\n\nggplot(data = wine_melt, aes(x=Var1, y=Var2, fill=value)) + \n  geom_tile() \n\n\n\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# wine_d = pd.DataFrame(np.abs(wine.iloc[:, :-1].values[:, None] - wine.iloc[:, :-1].values), columns=wine.index, index=wine.index)\n# from scipy.spatial.distance import pdist, squareform\n\n# wine_d = pdist(wine.iloc[:, :-1], metric='cityblock')\n# wine_d = squareform(wine_d)\n# wine_d_df = pd.DataFrame(wine_d, index=wine.index, columns=wine.index)\nfrom scipy.spatial.distance import cdist\n\nwine_d = pd.DataFrame(cdist(wine.iloc[:, :-1], wine.iloc[:, :-1], metric='cityblock'), columns=wine.index, index=wine.index)\n\nwine_melt = wine_d.reset_index().melt(id_vars='index', var_name='Var1', value_name='value')\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(wine_d, cmap=\"coolwarm\", center=0)\nplt.show()\n\n\n\n\nWe can see that some wines are closer than others (darker color). However, it is not really possible to extract any information from such a graph.",
    "crumbs": [
      "Labs",
      "Clustering"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#dendrogram",
    "href": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#dendrogram",
    "title": "Clustering",
    "section": "Dendrogram",
    "text": "Dendrogram\nNow, we build a dendrogram using a complete linkage.\n\nRPython\n\n\n\nwine_hc &lt;- hclust(wine_d, method = \"complete\")\nplot(wine_hc, hang=-1)\n\n\n\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nwine_linkage = linkage(wine.iloc[:, :-1], method='complete', metric='cityblock')\nplt.figure(figsize=(10, 8))\ndendrogram(wine_linkage, labels=wine.index, orientation='top', color_threshold=0, leaf_font_size=10)\nplt.show()\n\n\n\n\nWe cut the tree to 4 clusters, and represent the result. We also extract the cluster assignment of each wine.\n\nRPython\n\n\n\nplot(wine_hc, hang=-1)\nrect.hclust(wine_hc, k=4)\nwine_clust &lt;- cutree(wine_hc, k=4)\nwine_clust\n\n\n\n\nfrom scipy.cluster.hierarchy import fcluster\n\nplt.figure(figsize=(10, 8))\ndendrogram(wine_linkage, labels=wine.index, orientation='top', color_threshold=80, leaf_font_size=10)\nplt.axhline(y=80, color='black', linestyle='--')\nplt.show()\n\nwine_clust = fcluster(wine_linkage, 4, criterion='maxclust')\nwine_clust",
    "crumbs": [
      "Labs",
      "Clustering"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#interpretation-of-the-clusters",
    "href": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#interpretation-of-the-clusters",
    "title": "Clustering",
    "section": "Interpretation of the clusters",
    "text": "Interpretation of the clusters\nNow we analyze the clusters by looking at the distribution of the features within each cluster.\n\nRPython\n\n\n\nwine_comp &lt;- data.frame(wine[,-12], Clust=factor(wine_clust), Id=row.names(wine))\nwine_df &lt;- melt(wine_comp, id=c(\"Id\", \"Clust\"))\nhead(wine_df)\n\nggplot(wine_df, aes(y=value, group=Clust, fill=Clust)) +\n  geom_boxplot() +\n  facet_wrap(~variable, ncol=4, nrow=3)\n\n\n\n\nwine_comp = wine.iloc[:, :-1].copy()\nwine_comp['Clust'] = wine_clust\nwine_comp['Id'] = wine.index\nwine_melt = wine_comp.melt(id_vars=['Id', 'Clust'])\n\nplt.figure(figsize=(14, 10))\nsns.boxplot(x='variable', y='value', hue='Clust', data=wine_melt)\nplt.xticks(rotation=90)\nplt.legend(title='Cluster')\nplt.show()\n\n\n\n\nWe can see for example that - Cluster 4 (the smallest): small pH, large fixed.acidity and citric.acid, a large density, and a small alcohol. Also a large free.sulfur.dioxide. - Cluster 2: also has large fixed.acidity but not citric.acid. It looks like less acid than cluster 3. - Cluster 3: apparently has a large alcohol. - Etc.",
    "crumbs": [
      "Labs",
      "Clustering"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#choice-of-the-number-of-clusters",
    "href": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#choice-of-the-number-of-clusters",
    "title": "Clustering",
    "section": "Choice of the number of clusters",
    "text": "Choice of the number of clusters\nTo choose the number of cluster, we can inspect the dendrogram (judgmental approach), or we can rely on a statistics. Below, we use the within sum-of-squares, the GAP statistics, and the silhouette. It is obtained by the function fviz_nbclust in package factoextra. It uses the dendrogram with complete linkage on Manhattan distance is obtained using the function hcut with hc_method=\"complete\" and hc_metric=\"manhattan\".\n\nRPython\n\n\n\nlibrary(factoextra)\nfviz_nbclust(wine[,-12],\n             hcut, hc_method=\"complete\",\n             hc_metric=\"manhattan\",\n             method = \"wss\", \n             k.max = 25, verbose = FALSE)\nfviz_nbclust(wine[,-12],\n             hcut, hc_method=\"complete\",\n             hc_metric=\"manhattan\",\n             method = \"silhouette\", \n             k.max = 25, verbose = FALSE)\nfviz_nbclust(wine[,-12],\n             hcut, hc_method=\"complete\",\n             hc_metric=\"manhattan\",\n             method = \"gap\", \n             k.max = 25, verbose = FALSE)\n\n\n\nThe gap statistic is readily available in R (as seen above), but for Python we’ll use more commonly available metrics in scikit-learn: - Davies-Bouldin Index (lower values indicate better clustering) - Calinski-Harabasz Index (higher values indicate better clustering)\nThese metrics serve the same purpose - helping us determine the optimal number of clusters\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import calinski_harabasz_score, davies_bouldin_score\nimport os\n\n# Find the correct path to the data file\n# Try different possible locations\npossible_paths = [\n    'labs/data/Wine.csv',  # From project root\n    '../../data/Wine.csv',  # Relative to current file\n    '../../../data/Wine.csv',  # One level up\n    'data/Wine.csv',  # Direct in data folder\n    'Wine.csv'  # In current directory\n]\n\nwine_data = None\nfor path in possible_paths:\n    try:\n        wine_data = pd.read_csv(path)\n        print(f\"Successfully loaded data from: {path}\")\n        break\n    except FileNotFoundError:\n        continue\n\n# Continue with the analysis using wine_data\nwine = wine_data\nwine.index = [\"W\" + str(i) for i in range(1, len(wine)+1)]\nscaler = StandardScaler()\nwine_scaled = pd.DataFrame(\n    scaler.fit_transform(wine.iloc[:, :-1]),\n    index=wine.index,\n    columns=wine.columns[:-1]\n)\nX = wine_scaled.values\n\n# Clear previous plot (if any)\nplt.clf()\n\n# Within sum-of-squares (Elbow method)\nwss = []\nfor k in range(1, 26):\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n    wss.append(sum(((X - centroids[labels]) ** 2).sum(axis=1)))\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 26), wss, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Within sum-of-squares')\nplt.title('Elbow Method for Optimal k')\nplt.show()\n\n# Silhouette method\nsilhouette_scores = []\nfor k in range(2, 26):\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    silhouette_scores.append(silhouette_score(X, labels))\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(2, 26), silhouette_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette score')\nplt.title('Silhouette Method for Optimal k')\nplt.show()\n\n# Alternative metrics to gap statistic\nch_scores = []\ndb_scores = []\nk_range = range(2, 26)\n\nfor k in k_range:\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    ch_scores.append(calinski_harabasz_score(X, labels))\n    db_scores.append(davies_bouldin_score(X, labels))\n\n# Plot Calinski-Harabasz Index\nplt.figure(figsize=(10, 6))\nplt.plot(list(k_range), ch_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Calinski-Harabasz Index (higher is better)')\nplt.title('Calinski-Harabasz Index for determining optimal k')\nplt.show()\n\n# Plot Davies-Bouldin Index\nplt.figure(figsize=(10, 6))\nplt.plot(list(k_range), db_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Davies-Bouldin Index (lower is better)')\nplt.title('Davies-Bouldin Index for determining optimal k')\nplt.show()\n\n\n\n\nLike often, these methods are not easy to interpret. Globally, they choose \\(k=2\\) or \\(k=3\\).",
    "crumbs": [
      "Labs",
      "Clustering"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html",
    "href": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html",
    "title": "Autoencoders",
    "section": "",
    "text": "In this series of exercises, we illustrate autoencoders on the wine data already used for clustering & PCA. We first load the data.\n\nRPython\n\n\n\nwine &lt;- read.csv(here::here(\"labs/data/Wine.csv\"))\nrow.names(wine) &lt;- paste(\"W\", c(1:nrow(wine)), sep=\"\") \nwine[,-12] &lt;- scale(wine[,-12])\n\n\n\n\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nwine = pd.read_csv('../../data/Wine.csv')\nwine.index = [\"W\" + str(i) for i in range(1, len(wine)+1)]\nscaler = StandardScaler()\nwine.iloc[:, :-1] = scaler.fit_transform(wine.iloc[:, :-1])\n\n\n\n\nNote that here the scaling of the variables is optional. Scaling the features can sometimes help with the autoencoder, especially when creating lower-dimensional representation of the data.",
    "crumbs": [
      "Labs",
      "Autoencoders"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#simulate-missing-values",
    "href": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#simulate-missing-values",
    "title": "Autoencoders",
    "section": "Simulate missing values",
    "text": "Simulate missing values\nThis step is done only for demonstration since the wine data does not contain missing values, and we must artificially create them and place them in a dataframe called wine_missing. Applying this technique doesn’t require this step since your dataset already should already contain the missing values.\n\nRPython\n\n\n\nset.seed(123)\nwine_missing &lt;- wine\nwine_missing[sample(1:nrow(wine_missing), size = nrow(wine_missing)*0.2), sample(1:ncol(wine_missing), size = ncol(wine_missing)*0.2)] &lt;- NA\n\n\n\n\nimport numpy as np\nimport random\nnp.random.seed(123)\n# Create a copy of the wine dataset and randomly remove 20% of the data\nwine_missing = wine.copy()\nix = [(row, col) for row in range(wine_missing.shape[0]) for col in range(wine_missing.shape[1])]\nfor row, col in random.sample(ix, int(round(.2*len(ix)))):\n    wine_missing.iat[row, col] = np.nan",
    "crumbs": [
      "Labs",
      "Autoencoders"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#train-with-complete-data",
    "href": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#train-with-complete-data",
    "title": "Autoencoders",
    "section": "Train with complete data",
    "text": "Train with complete data\nNow, we can use an autoencoder to infer these missing values. To do so, we can train the autoencoder with the complete data and then use it to infer the missing values.\n\nRPython\n\n\n\n# Re-define the autoencoder model\ninput_layer &lt;- layer_input(shape = input_dim)\nencoder_layer &lt;-\n  layer_dense(units = encoding_dim, activation = 'relu')(input_layer)\ndecoder_layer &lt;-\n  layer_dense(units = input_dim, activation = 'linear')(encoder_layer)\nautoencoder &lt;- keras_model(input_layer, decoder_layer)\nsummary(autoencoder)\nautoencoder %&gt;% compile(optimizer = \"adam\", loss = \"mse\")\n\n# Train the autoencoder with the complete data\nhist &lt;- \n  autoencoder %&gt;% fit(\n  x = as.matrix(wine[, -12]),\n  y = as.matrix(wine[, -12]),\n  epochs = 300,\n  batch_size = 32,\n  shuffle = TRUE,\n  verbose = 0, #set it as `1` if you want to see the training messages\n  validation_split = 0.2,\n  callbacks = callback_early_stopping(\n    monitor = \"val_loss\",\n    patience = 5,\n    restore_best_weights = TRUE\n  ),\n)\n\n## uncomment if you want to see the training plot\n# plot(hist)\n\n# Replace missing values with the inferred values\nwine_missing_original &lt;- wine_missing\n\n# replace the missing values with something before being able to make a prediction on it (model cannot re-construct NA directly)\nwine_missing[is.na(wine_missing)] &lt;- 0  # Mask the missing values as 0\n\n# Use the autoencoder to infer the missing values\npredicted_wine &lt;- autoencoder %&gt;% predict(as.matrix(wine_missing[, -12]))\n\n\n\n\n# to introduce early stopping\nfrom keras.callbacks import EarlyStopping\n\n# Re-define the autoencoder model\ninput_layer = Input(shape=(input_dim,))\nencoder_layer = Dense(encoding_dim, activation='relu')(input_layer)\ndecoder_layer = Dense(input_dim, activation='linear')(encoder_layer)\nautoencoder = Model(input_layer, decoder_layer)\nautoencoder.compile(optimizer='adam', loss='mse')\n\n# Train the autoencoder with the complete data\nhist = autoencoder.fit(wine.iloc[:, :-1], \n  wine.iloc[:, :-1], \n  epochs=300, \n  batch_size=32, \n  shuffle=True, \n  verbose=0, #set it as `1` if you want to see the training messages\n  validation_split=0.2, \n  callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)])\n\n# # uncomment if you want to see the training plot\n# plt.clf()\n# plt.plot(hist.history['loss'])\n# plt.plot(hist.history['val_loss'])\n# plt.title('Model loss')\n# plt.ylabel('Loss')\n# plt.xlabel('Epoch')\n# plt.legend(['Train', 'Val'], loc='upper right')\n# plt.show()\n\n# Replace missing values with the inferred values\nwine_missing_original = wine_missing.copy()\n\n# replace the missing values with something before being able to make a prediction on it (model cannot re-construct NA directly)\nwine_missing.fillna(0, inplace=True)  # Mask the missing values as 0\n\n# Use the autoencoder to infer the missing values\npredicted_wine = autoencoder.predict(wine_missing.iloc[:, :-1].values)",
    "crumbs": [
      "Labs",
      "Autoencoders"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#predict-missing-values",
    "href": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#predict-missing-values",
    "title": "Autoencoders",
    "section": "Predict missing values",
    "text": "Predict missing values\nFinally, we replace the missing values in the wine dataset with the inferred values from the autoencoder.\n\nRPython\n\n\n\nwine_missing[is.na(wine_missing_original)] &lt;- predicted_wine[is.na(wine_missing_original)]\n\nlibrary(dplyr)\n# see the old vs new values by filtering for the rows that contained an NA\nmissing_rows &lt;- wine_missing_original %&gt;% \n  mutate(row_index = row_number()) %&gt;%\n  dplyr::filter_at(vars(dplyr::everything()),any_vars(is.na(.)))\n\n# you can see the new values that were re-constructed\nwine_missing_original[missing_rows$row_index,]\nwine_missing[missing_rows$row_index,]\n\n\n\n\n# Identify the missing values in the original dataframe\nmissing_mask = wine_missing_original.iloc[:, :-1].isna()\n\n# Replace the missing values with the predicted ones\nfor i in range(wine_missing.shape[1] - 1):  # iterate over all columns except the last one\n    wine_missing.loc[missing_mask.iloc[:, i], wine_missing.columns[i]] = predicted_wine[missing_mask.iloc[:, i], i]\n\n# see the old vs new values by filtering for the rows that contained an NA\nmissing_rows = wine_missing_original[wine_missing_original.iloc[:, :-1].isna().any(axis=1)]\n\n# you can see the new values that were re-constructed\nprint(wine_missing_original.loc[missing_rows.index, :])\nprint(wine_missing.loc[missing_rows.index, :])\n\n\n\n\nNote how the dataset wine_missing doesn’t contain any missing values. The missing values have been inferred by the autoencoder. This method can be a powerful tool to deal with missing data in machine learning projects.\n\n\n\n\n\n\nImportant\n\n\n\nThere are two essential things to note about using autoencoders for imputation:\n\nAlternative to our approach, you can train with incomplete data and mask the “NA” in the training data with 0s (as shown for inference). In theory, you can put anything for the mask value, for example, the mean or median value for the missing inputs, and then try to recover those (although if you scale your data, 0 may work better in practice). Once the model is trained, you can recover the value for NA while masking the NAs again with 0s (as already implemented by us). In that case, you’re pushing your model to predict 0 for the missing instances, which can sometimes be inappropriate.\nPlease note for auto-encoders to work well, you’ll need a lot of observations. Additionally, you should always compare the performance of this technique against simply using mean and median values for the missing data. Another library in R commonly used for dealing with missing data is called missForest, which uses a random forest for imputation. If you need techniques to deal with missing data, feel free to check it out and make sure you understand how it works (it falls beyond the scope of this course).",
    "crumbs": [
      "Labs",
      "Autoencoders"
    ]
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#objectives",
    "href": "labs/slides/lab_1/slide.html#objectives",
    "title": "MLBA Lab 1 - Setup",
    "section": "Objectives",
    "text": "Objectives\n\nLearn about vesion-control, Git & GitHub\nUse R and Rstudio\nLearn about virtual environments in R to ensure reproducibility.\nLearn about using python 🐍 in (and with) R. This is useful for some ML lab sessions, and cutting-edge ML is often first implemented in python."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#git-github-for-collaboration",
    "href": "labs/slides/lab_1/slide.html#git-github-for-collaboration",
    "title": "MLBA Lab 1 - Setup",
    "section": "Git & GitHub for Collaboration",
    "text": "Git & GitHub for Collaboration\n\n\nWhat is Git? Git is a version-control system that helps track changes, collaborate on projects, and revert to previous versions of a file.\nWhat is GitHub? GitHub provides a cloud-based hosting service that lets you manage Git repositories.\nGit vs. GitHub: Git is a version-control technology to manage source code history, while GitHub is only one of the hosting service for Git repositories."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#creating-a-repository-on-github",
    "href": "labs/slides/lab_1/slide.html#creating-a-repository-on-github",
    "title": "MLBA Lab 1 - Setup",
    "section": "Creating a Repository on GitHub",
    "text": "Creating a Repository on GitHub\n\n\n\nNavigate to GitHub and create a new repository.\nChoose a name and description for your repository.\nSelect whether the repository is public or private.\nClick “Create repository.”\n\n\n\n\n\n\n\nInstall GitHub Desktop app to help you with using GitHub. Additionally, you can see our FAQ for obtaining professional accounts."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#github-workflow",
    "href": "labs/slides/lab_1/slide.html#github-workflow",
    "title": "MLBA Lab 1 - Setup",
    "section": "GitHub Workflow",
    "text": "GitHub Workflow\n\nCloning a Repository with GitHub Desktop\n\nOpen GitHub Desktop and clone the repository to your local machine.\nThis creates a local copy of the repository for work and synchronization.\n\nCommitting Changes\n\nMake changes to your files in the project directory.\nUse GitHub Desktop to commit these changes, adding a meaningful commit message.\n\nPush and Pull Changes\n\nPush your committed changes to GitHub to share with collaborators.\nPull changes made by others to keep your local repository up to date."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#using-rstudio-projects",
    "href": "labs/slides/lab_1/slide.html#using-rstudio-projects",
    "title": "MLBA Lab 1 - Setup",
    "section": "Using RStudio Projects",
    "text": "Using RStudio Projects\n\nRStudio projects simplify the management of R soure code.\nUse the {here} package for easy file path management within projects."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#virtual-environments-in-r-renv",
    "href": "labs/slides/lab_1/slide.html#virtual-environments-in-r-renv",
    "title": "MLBA Lab 1 - Setup",
    "section": "Virtual environments in R (renv)",
    "text": "Virtual environments in R (renv)\nThe What & The Why\n\nrenv is a package management tool that helps you manage the packages used in an R project.\nEnsures that your project is reproducible.\nProvides a consistent environment by isolating the packages used in your project.\nSimplifies installation and setup.\nHelps you avoid compatibility issues.\nMakes it easy to share your work with others."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#virtual-environments-in-r-renv-1",
    "href": "labs/slides/lab_1/slide.html#virtual-environments-in-r-renv-1",
    "title": "MLBA Lab 1 - Setup",
    "section": "Virtual environments in R (renv)",
    "text": "Virtual environments in R (renv)\nThe How\n\nCreate a new renv project with renv::init().\nrenv::restore() to install packages from the renv.lock file.\nUse renv::snapshot() to occasionally update your packages.\nUse renv::status() to see if the list in renv.lock needs updating."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#python-in-r-reticulate",
    "href": "labs/slides/lab_1/slide.html#python-in-r-reticulate",
    "title": "MLBA Lab 1 - Setup",
    "section": "Python 🐍 in R (reticulate)",
    "text": "Python 🐍 in R (reticulate)\nSmall Motivation\n\nPython is arguably more demanded in machine learning than R.\nWidely-used language in the industry.\nPowerful libraries for data manipulation, analysis, and modeling.\nRelatively easy to pick up even for beginners to programming.\nCombining the strengths of both R and Python can enhance your skills."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#python-in-r-reticulate-1",
    "href": "labs/slides/lab_1/slide.html#python-in-r-reticulate-1",
    "title": "MLBA Lab 1 - Setup",
    "section": "Python 🐍 in R (reticulate)",
    "text": "Python 🐍 in R (reticulate)\nConfiguration\n\nInstall reticulate package in R.\nUse reticulate::use_python() or reticulate::use_condaenv() to specify the location of your python environment.\n\n\nlibrary(reticulate)\nuse_condaenv('MLBA')\npy_config()\n\n\nUse reticulate::import() to import python modules in R.\n\n\npd &lt;- import(\"pandas\")\n\n\nUse reticulate::py_run_string() to execute python code in R.\n\n\npy_run_string(\"x = 3; y = 4; print('The sum of x and y is:', x + y)\")"
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#running-python-code-in-r",
    "href": "labs/slides/lab_1/slide.html#running-python-code-in-r",
    "title": "MLBA Lab 1 - Setup",
    "section": "Running Python code in R",
    "text": "Running Python code in R\n\nTo run Python code in R, use {python} at the beginning of the code chunk.\n\n```{python}\nmy_dict = {'a' : 3, 'b' : 5, 'c' : 6} \n```\n\nTo access R objects in Python, use r.OBJECT_NAME.\n\n```{r}\nmy_list &lt;- list(a = 1, b = 2, c = 3)\n```\n```{python}\nprint(r.my_list['b'])\n```\n\nTo access Python objects in R, use py$OBJECT_NAME.\n\n```{r}\nprint(py$my_dict$b)\n```"
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#object-casting-in-python-r",
    "href": "labs/slides/lab_1/slide.html#object-casting-in-python-r",
    "title": "MLBA Lab 1 - Setup",
    "section": "Object casting in Python & R",
    "text": "Object casting in Python & R\n\nUse reticulate::r_to_py() and reticulate::py_to_r() to explicitly change between objects."
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#example-plotting-with-r-python",
    "href": "labs/slides/lab_1/slide.html#example-plotting-with-r-python",
    "title": "MLBA Lab 1 - Setup",
    "section": "Example: Plotting with R & Python",
    "text": "Example: Plotting with R & Python\n\n\n\nLoad some data\n\n## load mtcars dataset\ndata(mtcars)\n\n\n \n\n\n\n\n\nPlotting with base R\n\n# Using base R plot\nplot(mtcars$mpg, mtcars$disp)\n\n\n\n\n\n\nPlotting by using python within R\n\n# Using `matplotlib`\nplt &lt;- reticulate::import(\"matplotlib.pyplot\")\nplt$scatter(mtcars$mpg, mtcars$disp)\nplt$xlabel('mpg', fontsize = 12)\nplt$ylabel('disp', fontsize = 12)"
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#example-regression-in-r-python",
    "href": "labs/slides/lab_1/slide.html#example-regression-in-r-python",
    "title": "MLBA Lab 1 - Setup",
    "section": "Example: Regression in R & Python",
    "text": "Example: Regression in R & Python\n\nLoading dataRPythonPython within R\n\n\n\n\n\nLoading the data R\n\n# load the data in R\ndata(iris)\n# exclude the `species` column (we focus on regression here)\niris &lt;- select(iris, -'Species')\nhead(iris)\n\n\n\n\n\n\n\nModelling in pure R\n\n# example of running a model on iris data\nr_lm &lt;- lm(\"Sepal.Length ~. \", data = iris)\nsummary(r_lm)\n\n\n\n\n\n\n\nModelling in pure Python\n\n# load the required libraries\nimport statsmodels.api as sm\nimport pandas as pd\n# Fit linear regression model to iris coming from R\nX = r.iris[['Sepal.Width','Petal.Length','Petal.Width']]\ny = r.iris['Sepal.Length']\nX = sm.add_constant(X)\npy_lm_fit = sm.OLS(y, X).fit()\n#print regression results\nprint(py_lm_fit.summary())\n\n\n\n\n\n\n\nModelling in R with Python libraries\n\n# load the library\nsm &lt;- import(\"statsmodels.api\")\n# model the data\npy_lm &lt;- sm$OLS(dplyr::pull(iris, 'Sepal.Length'), dplyr::select(iris, -'Sepal.Length'))\n# fit the data\npy_lm_fit &lt;- py_lm$fit()\n# print the summary\nprint(py_lm_fit$summary())"
  },
  {
    "objectID": "labs/slides/lab_1/slide.html#conclusion",
    "href": "labs/slides/lab_1/slide.html#conclusion",
    "title": "MLBA Lab 1 - Setup",
    "section": "Conclusion",
    "text": "Conclusion\n\nrenv helps with managing packages in R, ensuring reproducibility, and making your work easier to share.\nreticulate allows you to use python in R and combine the strengths of both languages.\nLearning these tools will help you become more effective in machine learning.\nLet’s get started with the lab exercises!"
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html",
    "title": "Ensemble Methods",
    "section": "",
    "text": "In this part of the lab, we will look at how the randomForest library (alternative to ranger) can be applied for classification and regression tasks. At the very end, please feel free to apply these techniques to one of your favorite datasets seen in class (classification or regression).\n\n\n\n\n\n\nHyperparameters of RF\n\n\n\nR (using the randomForest library):\n\nntree: The number of trees in the forest (equivalent to n_estimators in python).\nmtry: The number of features to consider when looking for the best split. (similar to max_features in python)\nmax.depth: The maximum depth of each tree.\nnodesize: The minimum number of samples required to split an internal node (equivalent to min_samples_split in python).\n\nPython (using the sklearn library):\n\nn_estimators: The number of trees in the forest.\nmax_features: The number of features to consider when looking for the best split.\nmax_depth: The maximum depth of each tree.\nmin_samples_split: The minimum number of samples required to split an internal node.\nmin_samples_leaf: The minimum number of samples required to be at a leaf node.\n\n\n\n\n\n\n\n\n\nFew tips on RF hyperparameters\n\n\n\nA few (among many) tips for finding the ideal hyperparameters for RF:\n\nntree (R) / n_estimators (python): Use a large number of trees in the forest to improve model performance, but be aware of the increased computation time. The default value is usually a good starting point.\nmtry (R) / max_features (python): Experiment with different values, usually starting with the default (square root of the number of features for classification or one-third of the number of features for regression). Increasing this value may improve model performance but can also increase computation time.\nmax.depth: Control the depth of each tree to manage overfitting. Deeper trees capture more complex patterns but can lead to overfitting. Experiment with different values, keeping in mind that a shallower tree can be more interpretable and less prone to overfitting.\nnodesize (R) / min_samples_split (python): Increasing this value can help reduce overfitting, but setting it too high might lead to underfitting. Experiment with different values to find the optimal balance.\n\n\n\n\n\n\n\nLoad the library randomForest in R. Then, load the wine data set. This dataset is about white wine quality (in fact Portuguese vinho verde). The data contains 11 numerical features and 1 factor variable:\n\nfixed.acidity\nvolatile.acidity\ncitric.acid\nresidual.sugar\nchlorides\nfree.sulfur.dioxide\ntotal.sulfur.dioxide\ndensity\npH\nsulphates\nalcohol\nquality: Good/Bad\n\nAll the numerical features have units. The data source can be found here. For simplicity, only an extraction of 200 wines are used in this exercise. Note that in the original data set, the quality is a score (0 to 10) that was turned as factor here for the exercise (Bad: 0 to 5, Good: 6 to 10). Also, note that in the data source, the objective is to predict the quality from the other features (supervised learning).\nAs mentioned, the outcome variable used for this dataset is the wine quality. We should first coerce the classes as factors. Then, we make the training/test set random split with a 75/25 scheme.\n\nRPython\n\n\n\nlibrary(randomForest)\nwine &lt;- read.csv(here::here(\"data/Wine.csv\"))\nwine$quality &lt;- as.factor(wine$quality)\n\n# define a function to get the splitting index (training and testing) of a given dataset\nget_split_index &lt;- function(dataset, train_proportion = 0.75) {\n  set.seed(123)\n  index &lt;-\n    sample(\n      x = 1:2,\n      size = nrow(dataset),\n      replace = TRUE,\n      prob = c(train_proportion, 1 - train_proportion)\n    )\n  return(index)\n}\n\nwine_index &lt;- get_split_index(wine)\nwine_tr &lt;- wine[wine_index == 1, ]\nwine_te &lt;- wine[wine_index == 2, ]\n\n\n\n\n# Load the course python environment as usual with a r code chunks.\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\nSimilar to the previous labs, in python, we can use the usual sklearn library to do all our modelling. Please note that we will load the data again in python to make the demo easier. Additionally, we’ll load all the necessary libraries for this lab in this code chunk.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# we first move up one directory to achieve relative paths\nwine = pd.read_csv('../data/Wine.csv')\n\nwine['quality'] = wine['quality'].astype('category')\n\n# Split wine dataset into train and test\ntrain_wine, test_wine = train_test_split(wine, test_size=0.25, random_state=123)\n\n\n\n\nNote: Here, we have written a function to make the split as we will also need to apply it also for another dataset in the regression part.\n\n\n\nFit a random forest on the train set. The target is the taste variable that we want to predict. Specify for the number of trees ntree=1000 (by default, the function selects \\(500\\) trees). Remember to exclude quality in the predictors of the formula. Also, use the option importance=TRUE, we will need it afterward. Then test the model by computing the accuracy on the test set. You may use confusionMatrix from caret.\n\nRPython\n\n\n\nwine_rf &lt;- randomForest(quality~., data=wine_tr, ntree=1000, importance=TRUE)\nwine.pred_rf &lt;- predict(wine_rf, newdata=wine_te)\n\nlibrary(caret)\nconfusionMatrix(data=wine.pred_rf, reference = wine_te$quality)\n\n\n\n\n# Fit a Random Forest classifier on the train set\nwine_rf = RandomForestClassifier(n_estimators=1000, random_state=123)\nwine_rf.fit(train_wine.drop('quality', axis=1), train_wine['quality'])\n\n# Test the model and compute the accuracy\nwine_pred_rf = wine_rf.predict(test_wine.drop('quality', axis=1))\nprint(confusion_matrix(test_wine['quality'], wine_pred_rf))\nprint(\"Accuracy:\", accuracy_score(test_wine['quality'], wine_pred_rf))\n\nThis model is worse than the R version mostly because of the different defaults.\n\n\n\n\n\n\nExtract the model-specific variable importance using the functions varImpPlot (plots) and importance (values) on the model. Observe well that the mean decrease in accuracy of each variable is also computed for each specific class. In particular, what makes density special for predicting Good compare to another variable (like for example citric.acid)?\n\nRPython\n\n\n\nvarImpPlot(wine_rf)\nimportance(wine_rf)\n\n\n\n\n# Variable importance\nwine_importances = pd.Series(wine_rf.feature_importances_, index=train_wine.drop('quality', axis=1).columns)\nwine_importances.sort_values(ascending=False).plot(kind='bar')\n\n\n\n\ndensity is important for predicting the Good since their predictions is much less accurate if we do not use it. citric.acid is both overall less important than density but especially for prediction of Good.\n\n\n\n\nIn this part, we will be using the real_estate_data.csv once again. After reading the data, apply a random forest to predict price using all the other variables except No, Month and Year. Compute the RMSE and inspect the prediction quality with a graph. Note that the importance is not specific to any class here.\n\nRPython\n\n\n\nlibrary(dplyr)\nlibrary(magrittr)\nreal_estate_data &lt;- read.csv(here::here(\"labs/data/real_estate_data.csv\"))\n\n# select the columns of interest\nreal_estate_data &lt;- \n  real_estate_data %&gt;% \n  select(-c(No, Month, Year))\n\n# once again, divide the data into training and testing sets using the function created earlier\nrestate_index &lt;- get_split_index(real_estate_data)\nrestate_tr &lt;- real_estate_data[restate_index == 1, ]\nrestate_te &lt;- real_estate_data[restate_index == 2, ]\n\n# apply the RF model as a regression\nrestate_rf &lt;- randomForest(Price~., data=restate_tr, ntree=1000, importance=TRUE)\nrestate.pred_rf&lt;-predict(restate_rf, newdata=restate_te)\n\n# compute rmse and plot the results as well the VarImp\n(rmse &lt;- sqrt(mean((restate_te$Price - restate.pred_rf)^2)))\nplot(restate_te$Price ~ restate.pred_rf)\nabline(0,1)\nvarImpPlot(restate_rf)\nimportance(restate_rf)\n\n\n\n\n# Load real estate dataset\nreal_estate_data = pd.read_csv(\"../data/real_estate_data.csv\")\n\nreal_estate_data = real_estate_data.drop(['No', 'Month', 'Year'], axis=1)\n\n# Split real estate dataset into train and test\ntrain_restate, test_restate = train_test_split(real_estate_data, test_size=0.25, random_state=123)\n\n# Fit a Random Forest regressor on the train set\nrestate_rf = RandomForestRegressor(n_estimators=1000, random_state=123)\nrestate_rf.fit(train_restate.drop('Price', axis=1), train_restate['Price'])\n\n# Test the model and compute the RMSE\nrestate_pred_rf = restate_rf.predict(test_restate.drop('Price', axis=1))\nrmse = np.sqrt(mean_squared_error(test_restate['Price'], restate_pred_rf))\nprint(\"RMSE:\", rmse)\n\n# Plot the prediction quality\nplt.scatter(test_restate['Price'], restate_pred_rf)\nplt.xlabel('Actual Price')\nplt.ylabel('Predicted Price')\nplt.plot([min(test_restate['Price']), max(test_restate['Price'])], [min(test_restate['Price']), max(test_restate['Price'])], color='red')\nplt.show()\n\n# Variable importance\nrestate_importances = pd.Series(restate_rf.feature_importances_, index=train_restate.drop('Price', axis=1).columns)\nrestate_importances.sort_values(ascending=False).plot(kind='bar')\n\n\n\n\nCompare this model with the one you came up with in Ex_ML_LinLogReg . Which one would you go for?",
    "crumbs": [
      "Labs",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html#classification",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html#classification",
    "title": "Ensemble Methods",
    "section": "",
    "text": "Load the library randomForest in R. Then, load the wine data set. This dataset is about white wine quality (in fact Portuguese vinho verde). The data contains 11 numerical features and 1 factor variable:\n\nfixed.acidity\nvolatile.acidity\ncitric.acid\nresidual.sugar\nchlorides\nfree.sulfur.dioxide\ntotal.sulfur.dioxide\ndensity\npH\nsulphates\nalcohol\nquality: Good/Bad\n\nAll the numerical features have units. The data source can be found here. For simplicity, only an extraction of 200 wines are used in this exercise. Note that in the original data set, the quality is a score (0 to 10) that was turned as factor here for the exercise (Bad: 0 to 5, Good: 6 to 10). Also, note that in the data source, the objective is to predict the quality from the other features (supervised learning).\nAs mentioned, the outcome variable used for this dataset is the wine quality. We should first coerce the classes as factors. Then, we make the training/test set random split with a 75/25 scheme.\n\nRPython\n\n\n\nlibrary(randomForest)\nwine &lt;- read.csv(here::here(\"data/Wine.csv\"))\nwine$quality &lt;- as.factor(wine$quality)\n\n# define a function to get the splitting index (training and testing) of a given dataset\nget_split_index &lt;- function(dataset, train_proportion = 0.75) {\n  set.seed(123)\n  index &lt;-\n    sample(\n      x = 1:2,\n      size = nrow(dataset),\n      replace = TRUE,\n      prob = c(train_proportion, 1 - train_proportion)\n    )\n  return(index)\n}\n\nwine_index &lt;- get_split_index(wine)\nwine_tr &lt;- wine[wine_index == 1, ]\nwine_te &lt;- wine[wine_index == 2, ]\n\n\n\n\n# Load the course python environment as usual with a r code chunks.\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\nSimilar to the previous labs, in python, we can use the usual sklearn library to do all our modelling. Please note that we will load the data again in python to make the demo easier. Additionally, we’ll load all the necessary libraries for this lab in this code chunk.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# we first move up one directory to achieve relative paths\nwine = pd.read_csv('../data/Wine.csv')\n\nwine['quality'] = wine['quality'].astype('category')\n\n# Split wine dataset into train and test\ntrain_wine, test_wine = train_test_split(wine, test_size=0.25, random_state=123)\n\n\n\n\nNote: Here, we have written a function to make the split as we will also need to apply it also for another dataset in the regression part.\n\n\n\nFit a random forest on the train set. The target is the taste variable that we want to predict. Specify for the number of trees ntree=1000 (by default, the function selects \\(500\\) trees). Remember to exclude quality in the predictors of the formula. Also, use the option importance=TRUE, we will need it afterward. Then test the model by computing the accuracy on the test set. You may use confusionMatrix from caret.\n\nRPython\n\n\n\nwine_rf &lt;- randomForest(quality~., data=wine_tr, ntree=1000, importance=TRUE)\nwine.pred_rf &lt;- predict(wine_rf, newdata=wine_te)\n\nlibrary(caret)\nconfusionMatrix(data=wine.pred_rf, reference = wine_te$quality)\n\n\n\n\n# Fit a Random Forest classifier on the train set\nwine_rf = RandomForestClassifier(n_estimators=1000, random_state=123)\nwine_rf.fit(train_wine.drop('quality', axis=1), train_wine['quality'])\n\n# Test the model and compute the accuracy\nwine_pred_rf = wine_rf.predict(test_wine.drop('quality', axis=1))\nprint(confusion_matrix(test_wine['quality'], wine_pred_rf))\nprint(\"Accuracy:\", accuracy_score(test_wine['quality'], wine_pred_rf))\n\nThis model is worse than the R version mostly because of the different defaults.\n\n\n\n\n\n\nExtract the model-specific variable importance using the functions varImpPlot (plots) and importance (values) on the model. Observe well that the mean decrease in accuracy of each variable is also computed for each specific class. In particular, what makes density special for predicting Good compare to another variable (like for example citric.acid)?\n\nRPython\n\n\n\nvarImpPlot(wine_rf)\nimportance(wine_rf)\n\n\n\n\n# Variable importance\nwine_importances = pd.Series(wine_rf.feature_importances_, index=train_wine.drop('quality', axis=1).columns)\nwine_importances.sort_values(ascending=False).plot(kind='bar')\n\n\n\n\ndensity is important for predicting the Good since their predictions is much less accurate if we do not use it. citric.acid is both overall less important than density but especially for prediction of Good.",
    "crumbs": [
      "Labs",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html#regression",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html#regression",
    "title": "Ensemble Methods",
    "section": "",
    "text": "In this part, we will be using the real_estate_data.csv once again. After reading the data, apply a random forest to predict price using all the other variables except No, Month and Year. Compute the RMSE and inspect the prediction quality with a graph. Note that the importance is not specific to any class here.\n\nRPython\n\n\n\nlibrary(dplyr)\nlibrary(magrittr)\nreal_estate_data &lt;- read.csv(here::here(\"labs/data/real_estate_data.csv\"))\n\n# select the columns of interest\nreal_estate_data &lt;- \n  real_estate_data %&gt;% \n  select(-c(No, Month, Year))\n\n# once again, divide the data into training and testing sets using the function created earlier\nrestate_index &lt;- get_split_index(real_estate_data)\nrestate_tr &lt;- real_estate_data[restate_index == 1, ]\nrestate_te &lt;- real_estate_data[restate_index == 2, ]\n\n# apply the RF model as a regression\nrestate_rf &lt;- randomForest(Price~., data=restate_tr, ntree=1000, importance=TRUE)\nrestate.pred_rf&lt;-predict(restate_rf, newdata=restate_te)\n\n# compute rmse and plot the results as well the VarImp\n(rmse &lt;- sqrt(mean((restate_te$Price - restate.pred_rf)^2)))\nplot(restate_te$Price ~ restate.pred_rf)\nabline(0,1)\nvarImpPlot(restate_rf)\nimportance(restate_rf)\n\n\n\n\n# Load real estate dataset\nreal_estate_data = pd.read_csv(\"../data/real_estate_data.csv\")\n\nreal_estate_data = real_estate_data.drop(['No', 'Month', 'Year'], axis=1)\n\n# Split real estate dataset into train and test\ntrain_restate, test_restate = train_test_split(real_estate_data, test_size=0.25, random_state=123)\n\n# Fit a Random Forest regressor on the train set\nrestate_rf = RandomForestRegressor(n_estimators=1000, random_state=123)\nrestate_rf.fit(train_restate.drop('Price', axis=1), train_restate['Price'])\n\n# Test the model and compute the RMSE\nrestate_pred_rf = restate_rf.predict(test_restate.drop('Price', axis=1))\nrmse = np.sqrt(mean_squared_error(test_restate['Price'], restate_pred_rf))\nprint(\"RMSE:\", rmse)\n\n# Plot the prediction quality\nplt.scatter(test_restate['Price'], restate_pred_rf)\nplt.xlabel('Actual Price')\nplt.ylabel('Predicted Price')\nplt.plot([min(test_restate['Price']), max(test_restate['Price'])], [min(test_restate['Price']), max(test_restate['Price'])], color='red')\nplt.show()\n\n# Variable importance\nrestate_importances = pd.Series(restate_rf.feature_importances_, index=train_restate.drop('Price', axis=1).columns)\nrestate_importances.sort_values(ascending=False).plot(kind='bar')\n\n\n\n\nCompare this model with the one you came up with in Ex_ML_LinLogReg . Which one would you go for?",
    "crumbs": [
      "Labs",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html#classification-1",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html#classification-1",
    "title": "Ensemble Methods",
    "section": "Classification",
    "text": "Classification\n\nTraining and testing the model\nWe now fit a GBM model on the wine training set and apply it to the same target variable quality. We can train the model and add\n\nRPython\n\n\n\nlibrary(gbm)\nset.seed(123)\nwine_gbm &lt;- gbm(quality~., data=wine_tr, distribution=\"multinomial\", n.trees=1000, interaction.depth=4, shrinkage=0.01)\nwine.pred_gbm &lt;- predict(wine_gbm, newdata=wine_te, n.trees=1000, type=\"response\")\nwine.pred_gbm_class &lt;- apply(wine.pred_gbm, 1, which.max)\nlevels(wine_te$quality) &lt;- 1:length(levels(wine_te$quality))\nconfusionMatrix(factor(wine.pred_gbm_class), wine_te$quality)\n\n\n\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Fit a Gradient Boosting classifier on the train set\nwine_gbm = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.01, max_depth=4, random_state=123)\nwine_gbm.fit(train_wine.drop('quality', axis=1), train_wine['quality'])\n\n# Test the model and compute the accuracy\nwine_pred_gbm = wine_gbm.predict(test_wine.drop('quality', axis=1))\nprint(confusion_matrix(test_wine['quality'], wine_pred_gbm))\nprint(\"Accuracy:\", accuracy_score(test_wine['quality'], wine_pred_gbm))",
    "crumbs": [
      "Labs",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html#regression-1",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html#regression-1",
    "title": "Ensemble Methods",
    "section": "Regression",
    "text": "Regression\nIn this part, we will continue using the real_estate_data.csv. Fit a GBM model on the real estate training set to predict price using all the other variables except No, Month, and Year. Then compute the metrics and plot the predictions.\n\nRPython\n\n\n\nset.seed(123)\nrestate_gbm &lt;- gbm(Price~., data=restate_tr, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.01)\nrestate.pred_gbm&lt;-predict(restate_gbm, newdata=restate_te, n.trees=1000)\n\n# compute rmse and plot the results\n(rmse_gbm &lt;- sqrt(mean((restate_te$Price - restate.pred_gbm)^2)))\nplot(restate_te$Price ~ restate.pred_gbm)\nabline(0,1)\n\n\n\n\n# run the code below if you have not cleared the plot yet\nplt.clf()\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n# Fit a Gradient Boosting regressor on the train set\nrestate_gbm = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01, max_depth=4, random_state=123)\nrestate_gbm.fit(train_restate.drop('Price', axis=1), train_restate['Price'])\n# Test the model and compute the RMSE\n\nrestate_pred_gbm = restate_gbm.predict(test_restate.drop('Price', axis=1))\nrmse_gbm = np.sqrt(mean_squared_error(test_restate['Price'], restate_pred_gbm))\nprint(\"RMSE:\", rmse_gbm)\n# Plot the prediction quality\n\nplt.scatter(test_restate['Price'], restate_pred_gbm)\nplt.xlabel('Actual Price')\nplt.ylabel('Predicted Price')\nplt.plot([min(test_restate['Price']), max(test_restate['Price'])], [min(test_restate['Price']), max(test_restate['Price'])], color='red')\nplt.show()\n\n\n\n\nCompare the GBM model with the Random Forest model you came up with earlier. Which one would you go for?",
    "crumbs": [
      "Labs",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html#what-is-xgboost",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html#what-is-xgboost",
    "title": "Ensemble Methods",
    "section": "What is XGBoost?",
    "text": "What is XGBoost?\nXGBoost (Extreme Gradient Boosting) is an optimized implementation of the gradient boosting algorithm. It is designed for high performance and efficient memory usage. XGBoost improves upon the base Gradient Boosting Machine (GBM) by incorporating regularization to prevent overfitting and implementing parallel processing techniques for faster training. The algorithm also offers built-in cross-validation and early stopping to save time and resources during model training.",
    "crumbs": [
      "Labs",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html#modelling-with-xgboost",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html#modelling-with-xgboost",
    "title": "Ensemble Methods",
    "section": "Modelling with XGBoost",
    "text": "Modelling with XGBoost\nWe’ll use the xgboost library in both R and python. You can see some of the hyperparameters below:\n\n\n\n\n\n\nHyperparameters of XGBoost\n\n\n\n\neta: Controls the learning rate, which determines the step size at each iteration while updating the model weights. Smaller values make the model more robust to overfitting but require more iterations to converge. Typical values range from 0.01 to 0.3.\nmax_depth: Controls the maximum depth of each tree. Deeper trees can model more complex relationships but are more prone to overfitting. Experiment with different values, keeping in mind that a shallower tree can be more interpretable and less prone to overfitting.\nmin_child_weight: Controls the minimum sum of instance weights needed in a child node. Increasing this value helps to prevent overfitting by making the model more conservative.\n\n\n\nYou can read more about the package in its documentation.\n\nRPython\n\n\n\n# Install and load the package\n# install.packages(\"xgboost\")\nlibrary(xgboost)\n\n# Prepare data for XGBoost\ndtrain &lt;- xgb.DMatrix(data = as.matrix(restate_tr[, -ncol(restate_tr)]), label = restate_tr$Price)\ndtest &lt;- xgb.DMatrix(data = as.matrix(restate_te[, -ncol(restate_te)]), label = restate_te$Price)\n\n# Set hyperparameters\nparams &lt;- list(\n  objective = \"reg:squarederror\",\n  eta = 0.1,\n  max_depth = 5,\n  min_child_weight = 1,\n  subsample = 1,\n  colsample_bytree = 1\n)\n\n# Train the model\nxgb_model &lt;- xgb.train(params, dtrain, nrounds = 1000)\n\n# Test the model and compute the RMSE\nrestate_pred_xgb &lt;- predict(xgb_model, dtest)\nrmse_xgb &lt;- sqrt(mean((restate_te$Price - restate_pred_xgb)^2))\nprint(paste(\"RMSE:\", rmse_xgb))\n\n\n\nWe used the python installation of xgboost from our lab setup.\n\n# Install and load the package\nimport xgboost as xgb\n\n# Prepare data for XGBoost\ndtrain = xgb.DMatrix(train_restate.drop(\"Price\", axis=1), label=train_restate[\"Price\"])\ndtest = xgb.DMatrix(test_restate.drop(\"Price\", axis=1), label=test_restate[\"Price\"])\n\n# Set hyperparameters\nparams = {\n    \"objective\": \"reg:squarederror\",\n    \"eta\": 0.1,\n    \"max_depth\": 3,\n    \"min_child_weight\": 1,\n    \"subsample\": 1,\n    \"colsample_bytree\": 1,\n}\n\n# Train the model\nxgb_model = xgb.train(params, dtrain, num_boost_round=1000)\n\n# Test the model and compute the RMSE\nrestate_pred_xgb = xgb_model.predict(dtest)\nrmse_xgb = np.sqrt(mean_squared_error(test_restate[\"Price\"], restate_pred_xgb))\nprint(\"RMSE:\", rmse_xgb)\n\nAlthough initially our GBM suffered compared to the RF, we can see that XGBoost can help improve the result (the case for the python implementation). However, random forest still outperforms all the other models.\n\n\n\nFeel free to apply XGBoost to the dataset of your choice.",
    "crumbs": [
      "Labs",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html",
    "title": "Models: Neural Networks",
    "section": "",
    "text": "In this tutorial, we will demonstrate how to use keras and tensorflow in R for two common tasks in deep learning (DL): regression for structured data and image classification. We will use two datasets for this purpose: the Boston Housing dataset for regression and the CIFAR-10 dataset for image classification.",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#boston-housing-dataset",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#boston-housing-dataset",
    "title": "Models: Neural Networks",
    "section": "Boston Housing Dataset",
    "text": "Boston Housing Dataset\nThe Boston Housing dataset is a well-known dataset used for regression tasks. It contains 506 instances and 13 features, including the median value of owner-occupied homes in $1000s. We will use this dataset to demonstrate how to perform regression on the sales price.\n\nlibrary(keras)\n\nhouses &lt;- dataset_boston_housing()\n\ntrain_x &lt;- houses$train$x\ntrain_y &lt;- houses$train$y\n\ntest_x &lt;- houses$test$x\ntest_y &lt;- houses$test$y",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#data-preparation",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#data-preparation",
    "title": "Models: Neural Networks",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe need to normalize the data. This is especially relevant for neural networks to stabilize the computation of the gradients and consequently improve the training process.\n\nmeans &lt;- apply(train_x, 2, mean)\nsds &lt;- apply(train_x, 2, sd)\n\ntrain_x &lt;- scale(train_x, means, sds)\ntest_x &lt;- scale(test_x, means, sds)",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-definition",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-definition",
    "title": "Models: Neural Networks",
    "section": "Model Definition",
    "text": "Model Definition\nWe will use a simple neural network with two hidden layers to perform regression.\n\n\n\n\n\n\nLearning about some of the hyperparameters\n\n\n\nYou have many options for different hyperparameters; however, one lab session barely scratches the surface of DL and its hyperparameters. There are a couple of points that we need to specify here:\n\nYou can use any activation function in the middle layers, from a simple linear regression (linear) to more common ones such as hyperbolic tagnet or tanh (often suitable for tabular data) to more sophisticated ones such as rectified linear unit or relu (better suited to high dimensional data). What is imperative is that in the last dense layer, the number of units and the activation function determine the kind of task (e.g., classification, regression, etc.) you’re trying to accomplish. If you’re doing a regression, the last dense layer has to have 1 unit and a linear activation function. If you’re doing a binary classification (logistic regression), you still use 1 dense unit but must apply the sigmoid activation function. If you’re doing multi-class classification, then the number of dense units must equal the number of classes, and the activation function is softmax (which is a generalization of sigmoid for multiple classes). Google also provides a nice visual that explains the difference between the classification models. If you remove the &gt;0.5 rule (i.e., sigmoid), you essentially get a linear regression for that layer.\n\n\n\n\n\n\n\n\n\n\n\nIt is imperative that, depending on the task, you use the correct type of loss function. For instance, you can use mean squared error (mse) for regression and categorical cross-entropy for multi-class classification.\n\nAs mentioned, during the ML course, we cannot cover the details of DL. Suppose you want to understand these hyperparameters better and learn about many new ones. In that case, you can check out the video recordings and the slides for the Deep learning course previously taught at HEC (last given in 2021).\n\n\n\nRPython\n\n\n\n# we set a seed (this sometimes disables GPU performance)\ntensorflow::set_random_seed(123)\n\n# define the model structure\nmodel_reg &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 64, activation = \"relu\", input_shape = c(ncol(train_x))) %&gt;%\n  layer_dense(units = 64, activation = \"relu\") %&gt;%\n  layer_dense(units = 1, activation = \"linear\")\n\n# compile the model\nmodel_reg %&gt;% compile(\n  optimizer = \"adam\",\n  loss = \"mean_squared_error\",\n  metrics = c(\"mean_absolute_error\")\n)\n\n\n\nWe can run the same thing in Python with the datasets created in R, e.g. r.train_x.\n\n# import necessary libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Set random seed\ntf.random.set_seed(123)\n\n# define the model structure\nmodel_reg = keras.Sequential([\n    layers.Dense(units=64, activation='relu', input_shape=(r.train_x.shape[1],)),\n    layers.Dense(units=64, activation='relu'),\n    layers.Dense(units=1, activation='linear')\n])\n\n# compile the model\nmodel_reg.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-training",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-training",
    "title": "Models: Neural Networks",
    "section": "Model Training",
    "text": "Model Training\nWe can then train the model:\n\nRPython\n\n\n\nhistory &lt;- model_reg %&gt;% fit(\n  train_x, train_y,\n  epochs = 10, #300 to get the best results\n  batch_size = 32,\n  validation_split = 0.2,\n  callbacks = callback_early_stopping(patience = 30,restore_best_weights = T),\n  verbose = 1 # to control the printing of the output\n)\n\nYou don’t need to assign model %&gt;% fit() to history. We only do that to plot the results later (e.g., with plot(history).\n\n\n\n# import necessary libraries\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# fit the model\nmodel_reg.fit(\n    r.train_x, r.train_y,\n    epochs=10, #300\n    batch_size=32,\n    validation_split=0.2,\n    callbacks=[EarlyStopping(patience=30,restore_best_weights=True)],\n    verbose=1\n)",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-evaluation",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-evaluation",
    "title": "Models: Neural Networks",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nFinally, we can evaluate the model on the testing set:\n\nRPython\n\n\n\nnn_results &lt;- model_reg %&gt;% evaluate(test_x, test_y)\nnn_results\n# alternatively, you can use the `predict` attribute from `model` as shown below\n# nn_results &lt;- model$predict(test_x)\n# caret::MAE(obs = test_y, pred = as.vector(nn_results))\n\n\n\n\nnn_results = model_reg.evaluate(r.test_x, r.test_y)\nprint(nn_results)\n\n\n\n\nTo put these results (from R) in context, we can compare it with a simple linear regression:\n\ndf_tr &lt;- data.frame(train_y, train_x)\nlm_mod &lt;- lm(train_y ~ ., data = df_tr)\nlm_preds &lt;- as.vector(predict(lm_mod, newdata=data.frame(test_x))) # `predict()` for lm doesn't accept matrices\n\n# calculate mean absolute error with caret `MAE` installed in the `K-NN excercises`\ncaret::MAE(obs = test_y, pred = lm_preds)\n\nWe see that the neural network does significantly better than a regression model. We can also compare it with the trees seen in the CART series.\n\nlibrary(rpart)\nset.seed(123)\ntree_model &lt;- rpart(train_y ~ ., data=df_tr)\ntree_preds &lt;- predict(tree_model,newdata = data.frame(test_x))\n\n# calculate the performance on tree\ncaret::MAE(obs = test_y, pred = tree_preds)\n\nThe neural network also outperforms CART (if you run NN for 300 epochs). This is due to multiple reasons, including a higher complexity of the neural network (more parameters), using a validation set, and so on. You will learn about ensemble methods (bagging and boosting) in the upcoming lectures. Ensemble methods are the true champions for structured data and usually outperform neural networks for structured/low-dimensional data.",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#cifar-10-dataset",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#cifar-10-dataset",
    "title": "Models: Neural Networks",
    "section": "CIFAR-10 Dataset",
    "text": "CIFAR-10 Dataset\nThe CIFAR-10 dataset is a well-known dataset used for image classification tasks. It contains 50,000 training images and 10,000 testing images of size 32x32 pixels, each belonging to one of ten classes. We will use this dataset to demonstrate how to perform image classification.\nIn this case, we had our inputs already prepared, but if it was not the case, you can always do:\n\n# Load the CIFAR-10 dataset (it'll take a while for it to download)\ncifar10 &lt;- dataset_cifar10()\n\n# Split the data into training and testing sets\ntrain_images &lt;- cifar10$train$x\ntrain_labels &lt;- cifar10$train$y\ntest_images &lt;- cifar10$test$x\ntest_labels &lt;- cifar10$test$y\n\nIf you do an str(train_images), you can see that the output is a four-dimensional array. In this array, the first element is the number of images in the training set, the second and third elements represent the (pixel values) for each image’s width and height, respectively, and the last element is the color channel of the image. If you had a black-and-white image (called greyscale), the number of channels would be just one. Typically, an image in color is represented with a combination of three colors known as RGB (red, green, and blue). Therefore, here we have three elements associated with the pixel number for each color, and their combination provides us with the colors you see in the image.",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#data-preparation-1",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#data-preparation-1",
    "title": "Models: Neural Networks",
    "section": "Data Preparation",
    "text": "Data Preparation\nBefore building our image classification model, we need to prepare the data by normalizing the values to obtain pixel values between 0-1. Similar to standardization of tabular data, this helps to avoid exploding/vanishing gradients and can improve the convergence of the model.\n\n# Normalize the pixel values\ntrain_images &lt;- train_images / 255\ntest_images &lt;- test_images / 255\n\nWe can also plot some of the example images.\n\n# Plot the first 9 images\nlabel_names &lt;- c(\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n\npar(mfrow = c(3, 3))\nfor (i in 1:9) {\n  img &lt;- train_images[i,,,]\n  # label &lt;- train_labels[i]\n  label &lt;- label_names[train_labels[i] + 1]  # Add 1 to convert from 0-based to 1-based indexing\n  img &lt;- array_reshape(img, c(32, 32, 3))\n  plot(as.raster(img))\n  title(paste(\"Label:\", label))\n}\n\nWe can now one-hot encode our labels to be better adapted to the loss function we will be using (otherwise you can use “sparse” variation of the loss functions).\n\ntrain_labels &lt;- to_categorical(train_labels)\ntest_labels &lt;- to_categorical(test_labels)",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-definition-1",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-definition-1",
    "title": "Models: Neural Networks",
    "section": "Model definition",
    "text": "Model definition\nWe can build our image classification model using keras by defining the layers of the neural network. For this task we’ll be using Convolutional neural networks (CNNs). CNNs are a type of deep learning model that are commonly used for image and video processing. They consist of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The convolutional layers use filters to extract features from the input image, and the pooling layers downsample the output of the convolutional layers. The fully connected layers combine the extracted features to produce the final output of the network. CNNs have been shown to be effective in a wide range of applications, including image classification, object detection, and semantic segmentation.\n\n# Set a seed again for reproducibility\ntensorflow::set_random_seed(123)\n\n# Define the model\nmodel_cnn &lt;-\n  keras_model_sequential()%&gt;%\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3), input_shape = c(32, 32, 3),  activation = \"relu\") %&gt;%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;%\n  layer_conv_2d(filters = 64, kernel_size = c(5, 5), activation = \"relu\") %&gt;%\n  layer_max_pooling_2d(pool_size = c(3, 3)) %&gt;%\n  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = \"relu\") %&gt;%\n  layer_flatten() %&gt;%\n  layer_dense(units = 64, activation = \"relu\") %&gt;%\n  layer_dense(units = 10, activation = \"softmax\")\n\n# Compile the model\nmodel_cnn %&gt;% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = optimizer_adam(),\n  metrics = c(\"accuracy\")\n)\n\nDepending on youer computer, this may take some time to run.\n\nhistory &lt;- model_cnn %&gt;% fit(\n  x = train_images,\n  y = train_labels,\n  epochs = 10,\n  batch_size = 64,\n  # validation_split = 0.2\n  validation_data= list(test_images, test_labels)\n)\n\n\nmodel_cnn %&gt;% evaluate(\n  x = test_images,\n  y = test_labels\n)\n\nYou may notice that we did not obtain extremely high accuracy, but this is okay for multiple reasons:\n\nWe only trained the model for a few epochs and stopped the training very early. You’re welcome to let it run for more epochs.\nWe’re dealing with at least ten classes; this is a rather complicated problem.\nIn DL, you can define many different architectures and hyperparameters, and since we did not play around with these values, this could also explain the lower performance.",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html",
    "title": "Models: Linear and logistic regressions",
    "section": "",
    "text": "The dataset we’ll be using for the first part of the exercise is real estate transaction prices in Taiwan, which can be accessed from this link this link. This dataset was modified for this exercise. The modified file real_estate_data.csv is in the exercise folder under /data/.\nThe aim is to predict the house prices from available features: No, Month, Year, TransDate, HouseAge, Dist, NumStores, Lat, Long, Price. No is the transaction number and will not be used.\n\n\nFirst, an EDA of the data is needed. After exploring the structure, the Price is shown with the year and month.\n\nreal_estate_data &lt;- read.csv(here::here(\"labs/data/real_estate_data.csv\"))\n\n## adapt the path to the data\n# if you encountered any error with the encoding of the data (`Error in gregexpr...`), just re-run the code again\nstr(real_estate_data)\nlibrary(summarytools)\ndfSummary(real_estate_data)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nreal_estate_data %&gt;% ggplot(aes(x=Month, y=Price, fill=as.factor(Year))) + \n  geom_boxplot()+ facet_wrap(~as.factor(Year))\n\nThe results show how important it is to make an EDA! It appears that the data does not contain transactions for all the months of 2012 and 2013, but just some months by the end of 2012 and the first half of 2013. This shows that it is pointless to use month and year here. This is why we prefer TransDate, a value indicating the transaction time on a linear scale (e.g., 2013.250 is March 2013).\nNow we focus on the link between Price and the other features.\n\nlibrary(GGally)\nreal_estate_data %&gt;% \n  select(Price, HouseAge, Dist, Lat, Long, TransDate) %&gt;% \n  ggpairs()\n\nNo clear link appears. The linear regression will help to discover if a combination of the features can predict the price.\n\n\n\nFirst, we split the data into training/test set (75/25).\n\nset.seed(234)\nindex &lt;- sample(x=c(1,2), size=nrow(real_estate_data), replace=TRUE, prob=c(0.75,0.25)) # 1==training set, 2==test set\ndat_tr_restate &lt;- real_estate_data[index==1,]\ndat_te_restate &lt;- real_estate_data[index==2,]\n\nThen, we fit the linear regression to the training set.\n\nRPython\n\n\n\nmod_lm &lt;- lm(Price~TransDate+\n               HouseAge+\n               Dist+\n               NumStores+\n               Lat+\n               Long, data=dat_tr_restate)\nsummary(mod_lm)\n\n\n\n\n# In R, we load the conda environment as usual\nlibrary(reticulate)\nreticulate::use_condaenv(\"MLBA\", required = TRUE)\ngc(full = TRUE)\n\nIn python, we then use the statsmodels library to fit a linear regression model to the training data and perform feature elimination. We use the .fit() method to fit the model with the formula for the variable names. Note that python’s summary() function is unique to the statsmodels libraries and produces similar information to its R counterpart.\n\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"\n\nimport mkl\n# %env OMP_NUM_THREADS=1\n# set the number of threads. Here we set it to 1 to avoid parallelization when rendering quarto, but you can set it to higher values.\nmkl.set_num_threads(1)\n\n\n# Import necessary library\nimport statsmodels.formula.api as smf\n\n# Fit a linear regression model to the training data & print the summary\nmod_lm_py = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long', data=r.dat_tr_restate).fit()\nprint(mod_lm_py.summary())\n\nIt’s not a suprise that the results are same as the ones obtained in R.\n\n\n\n\n\n\nThe stepwise variable selection can be performed using the function step. By default, it is a backward selection; see ?step for details (parameter direction is backward when scope is empty).\n\nRPython\n\n\n\nstep(mod_lm) # see the result\nmod_lm_sel &lt;- step(mod_lm) # store the final model into mod_lm_sel\nsummary(mod_lm_sel)\n\n\n\nAs python does not have an exact equivalent of stats::step() function, which performs both forward and backward selection based on AIC, we have to implement it manually. We start with the full model and iteratively remove the feature with the highest p-value and add the feature with the lowest AIC until we can no longer improve the AIC. The final model is stored in mod_lm_sel. For an extensive explanation of what this while loop is doing and how the backward+forward is computed, check the code below:\n\n\nExplaining feature elimination in python (while loop)\n\nWe start by setting mod_lm_sel_py to the full model mod_lm_py. Then, we enter a while loop that continues until we break out of it. In each iteration of the loop, we store the current model in prev_model for later comparison. We start by dropping the feature with the highest p-value from the current model using idxmax(), which returns the label of the maximum value in the pvalues attribute of the mod_lm_sel_py object. We exclude the intercept term from the list of labels by specifying labels=['Intercept']. We then create a new model using smf.ols() with the feature removed and fit it to the training data using fit(). We store this new model in mod_lm_sel_py.\nNext, we check whether the AIC of the new model is larger than the previous model’s. If it is, we break out of the while loop and use the previous model (prev_model) as the final model. If not, we continue to the next step of the loop. Here, we look for the feature with the lowest AIC among the remaining features using idxmin() on the pvalues attribute, again excluding the intercept term. We create a new model by adding this feature to the current model using smf.ols(), fit it to the training data using fit(), and store it in mod_lm_sel_new.\nWe then check whether the AIC of the new model is larger than that of the current model. If it is, we break out of the while loop and use the current model (mod_lm_sel_py) as the final model. If not, we update mod_lm_sel_py with the new model and continue to the next iteration of the loop. This way, we iteratively remove the feature with the highest p-value and add the feature with the lowest AIC until we can no longer improve the AIC. The final model is stored in mod_lm_sel_py.\n\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# perform both forward and backward selection using AIC\nmod_lm_sel_py = mod_lm_py\n\nwhile True:\n    prev_model = mod_lm_sel_py\n    # drop the feature with the highest p-value\n    feature_to_drop = mod_lm_sel_py.pvalues.drop(labels=['Intercept']).idxmax()\n    mod_lm_sel_py = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long - ' + feature_to_drop, data=r.dat_tr_restate).fit()\n    # check if AIC has increased, if yes, break the loop and use the previous model\n    if mod_lm_sel_py.aic &gt; prev_model.aic:\n        mod_lm_sel_py = prev_model\n        break\n    \n    # add the feature with the lowest AIC\n    feature_to_add = mod_lm_sel_py.pvalues.drop(labels=['Intercept']).idxmin()\n    mod_lm_sel_py_new = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long + ' + feature_to_add, data=r.dat_tr_restate).fit()\n    # check if AIC has increased, if yes, break the loop and use the previous model\n    if mod_lm_sel_py_new.aic &gt; mod_lm_sel_py.aic:\n        break\n    mod_lm_sel_py = mod_lm_sel_py_new\n    \nprint(mod_lm_sel_py.summary())\n\n\n\n\nAfter identifying the most important features, you can fit a new model using only those features and evaluate its performance using the test set.\nThe final model does not contain Long. In terms of interpretations, for example:\n\nThe price increased on average by 3.7 per year (TransDate)\nIt diminishes in average by (-2)2.4 per year (HouseAge)\netc.\n\n\n\n\nWe now predict the prices in the test set. We can make a scatter plot of the predictions versus the observed prices to inspect that. We already know by looking at the \\(R^2\\) in the summary that the prediction quality is not good.\n\nRPython\n\n\n\nmod_lm_pred &lt;- predict(mod_lm_sel, newdata=dat_te_restate)\nplot(dat_te_restate$Price ~ mod_lm_pred, xlab=\"Prediction\", ylab=\"Observed prices\")\nabline(0,1) # line showing the obs -- pred agreement\n\n\n\n\nmod_lm_sel_pred = mod_lm_sel_py.predict(r.dat_te_restate)\nfig, ax = plt.subplots()\nax.scatter(x=mod_lm_sel_pred, y=r.dat_te_restate['Price'])\nax.set_xlabel('Prediction')\nax.set_ylabel('Observed prices')\nax.plot(ax.get_xlim(), ax.get_ylim(), ls=\"--\", c=\".3\")\nplt.show()\n\n\n\n\nIt appears that the lowest and the highest prices are underestimated. At the center (around 30), the prices are slightly overestimated.\nAs an exercise, write down the prediction equation of the selected model. Use this equation to explain how instances 1 and 2 (test set) are predicted and calculate the predictions manually. Verify your results using the predict function from the previous R code.\n\n\nAnswer\n\n\nmod_lm_pred[c(1,2)]\n\n\\[\ny = -0.000133 + 3.66\\times TransDate -0.243\\times HouseAge \\\\-0.00464\\times Dist + 1.027\\times NumStores + 237.8\\times Lat\n\\]",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#eda",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#eda",
    "title": "Models: Linear and logistic regressions",
    "section": "",
    "text": "First, an EDA of the data is needed. After exploring the structure, the Price is shown with the year and month.\n\nreal_estate_data &lt;- read.csv(here::here(\"labs/data/real_estate_data.csv\"))\n\n## adapt the path to the data\n# if you encountered any error with the encoding of the data (`Error in gregexpr...`), just re-run the code again\nstr(real_estate_data)\nlibrary(summarytools)\ndfSummary(real_estate_data)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nreal_estate_data %&gt;% ggplot(aes(x=Month, y=Price, fill=as.factor(Year))) + \n  geom_boxplot()+ facet_wrap(~as.factor(Year))\n\nThe results show how important it is to make an EDA! It appears that the data does not contain transactions for all the months of 2012 and 2013, but just some months by the end of 2012 and the first half of 2013. This shows that it is pointless to use month and year here. This is why we prefer TransDate, a value indicating the transaction time on a linear scale (e.g., 2013.250 is March 2013).\nNow we focus on the link between Price and the other features.\n\nlibrary(GGally)\nreal_estate_data %&gt;% \n  select(Price, HouseAge, Dist, Lat, Long, TransDate) %&gt;% \n  ggpairs()\n\nNo clear link appears. The linear regression will help to discover if a combination of the features can predict the price.",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#modelling",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#modelling",
    "title": "Models: Linear and logistic regressions",
    "section": "",
    "text": "First, we split the data into training/test set (75/25).\n\nset.seed(234)\nindex &lt;- sample(x=c(1,2), size=nrow(real_estate_data), replace=TRUE, prob=c(0.75,0.25)) # 1==training set, 2==test set\ndat_tr_restate &lt;- real_estate_data[index==1,]\ndat_te_restate &lt;- real_estate_data[index==2,]\n\nThen, we fit the linear regression to the training set.\n\nRPython\n\n\n\nmod_lm &lt;- lm(Price~TransDate+\n               HouseAge+\n               Dist+\n               NumStores+\n               Lat+\n               Long, data=dat_tr_restate)\nsummary(mod_lm)\n\n\n\n\n# In R, we load the conda environment as usual\nlibrary(reticulate)\nreticulate::use_condaenv(\"MLBA\", required = TRUE)\ngc(full = TRUE)\n\nIn python, we then use the statsmodels library to fit a linear regression model to the training data and perform feature elimination. We use the .fit() method to fit the model with the formula for the variable names. Note that python’s summary() function is unique to the statsmodels libraries and produces similar information to its R counterpart.\n\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"\n\nimport mkl\n# %env OMP_NUM_THREADS=1\n# set the number of threads. Here we set it to 1 to avoid parallelization when rendering quarto, but you can set it to higher values.\nmkl.set_num_threads(1)\n\n\n# Import necessary library\nimport statsmodels.formula.api as smf\n\n# Fit a linear regression model to the training data & print the summary\nmod_lm_py = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long', data=r.dat_tr_restate).fit()\nprint(mod_lm_py.summary())\n\nIt’s not a suprise that the results are same as the ones obtained in R.",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#variable-selection-interpretation",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#variable-selection-interpretation",
    "title": "Models: Linear and logistic regressions",
    "section": "",
    "text": "The stepwise variable selection can be performed using the function step. By default, it is a backward selection; see ?step for details (parameter direction is backward when scope is empty).\n\nRPython\n\n\n\nstep(mod_lm) # see the result\nmod_lm_sel &lt;- step(mod_lm) # store the final model into mod_lm_sel\nsummary(mod_lm_sel)\n\n\n\nAs python does not have an exact equivalent of stats::step() function, which performs both forward and backward selection based on AIC, we have to implement it manually. We start with the full model and iteratively remove the feature with the highest p-value and add the feature with the lowest AIC until we can no longer improve the AIC. The final model is stored in mod_lm_sel. For an extensive explanation of what this while loop is doing and how the backward+forward is computed, check the code below:\n\n\nExplaining feature elimination in python (while loop)\n\nWe start by setting mod_lm_sel_py to the full model mod_lm_py. Then, we enter a while loop that continues until we break out of it. In each iteration of the loop, we store the current model in prev_model for later comparison. We start by dropping the feature with the highest p-value from the current model using idxmax(), which returns the label of the maximum value in the pvalues attribute of the mod_lm_sel_py object. We exclude the intercept term from the list of labels by specifying labels=['Intercept']. We then create a new model using smf.ols() with the feature removed and fit it to the training data using fit(). We store this new model in mod_lm_sel_py.\nNext, we check whether the AIC of the new model is larger than the previous model’s. If it is, we break out of the while loop and use the previous model (prev_model) as the final model. If not, we continue to the next step of the loop. Here, we look for the feature with the lowest AIC among the remaining features using idxmin() on the pvalues attribute, again excluding the intercept term. We create a new model by adding this feature to the current model using smf.ols(), fit it to the training data using fit(), and store it in mod_lm_sel_new.\nWe then check whether the AIC of the new model is larger than that of the current model. If it is, we break out of the while loop and use the current model (mod_lm_sel_py) as the final model. If not, we update mod_lm_sel_py with the new model and continue to the next iteration of the loop. This way, we iteratively remove the feature with the highest p-value and add the feature with the lowest AIC until we can no longer improve the AIC. The final model is stored in mod_lm_sel_py.\n\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# perform both forward and backward selection using AIC\nmod_lm_sel_py = mod_lm_py\n\nwhile True:\n    prev_model = mod_lm_sel_py\n    # drop the feature with the highest p-value\n    feature_to_drop = mod_lm_sel_py.pvalues.drop(labels=['Intercept']).idxmax()\n    mod_lm_sel_py = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long - ' + feature_to_drop, data=r.dat_tr_restate).fit()\n    # check if AIC has increased, if yes, break the loop and use the previous model\n    if mod_lm_sel_py.aic &gt; prev_model.aic:\n        mod_lm_sel_py = prev_model\n        break\n    \n    # add the feature with the lowest AIC\n    feature_to_add = mod_lm_sel_py.pvalues.drop(labels=['Intercept']).idxmin()\n    mod_lm_sel_py_new = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long + ' + feature_to_add, data=r.dat_tr_restate).fit()\n    # check if AIC has increased, if yes, break the loop and use the previous model\n    if mod_lm_sel_py_new.aic &gt; mod_lm_sel_py.aic:\n        break\n    mod_lm_sel_py = mod_lm_sel_py_new\n    \nprint(mod_lm_sel_py.summary())\n\n\n\n\nAfter identifying the most important features, you can fit a new model using only those features and evaluate its performance using the test set.\nThe final model does not contain Long. In terms of interpretations, for example:\n\nThe price increased on average by 3.7 per year (TransDate)\nIt diminishes in average by (-2)2.4 per year (HouseAge)\netc.",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#inference",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#inference",
    "title": "Models: Linear and logistic regressions",
    "section": "",
    "text": "We now predict the prices in the test set. We can make a scatter plot of the predictions versus the observed prices to inspect that. We already know by looking at the \\(R^2\\) in the summary that the prediction quality is not good.\n\nRPython\n\n\n\nmod_lm_pred &lt;- predict(mod_lm_sel, newdata=dat_te_restate)\nplot(dat_te_restate$Price ~ mod_lm_pred, xlab=\"Prediction\", ylab=\"Observed prices\")\nabline(0,1) # line showing the obs -- pred agreement\n\n\n\n\nmod_lm_sel_pred = mod_lm_sel_py.predict(r.dat_te_restate)\nfig, ax = plt.subplots()\nax.scatter(x=mod_lm_sel_pred, y=r.dat_te_restate['Price'])\nax.set_xlabel('Prediction')\nax.set_ylabel('Observed prices')\nax.plot(ax.get_xlim(), ax.get_ylim(), ls=\"--\", c=\".3\")\nplt.show()\n\n\n\n\nIt appears that the lowest and the highest prices are underestimated. At the center (around 30), the prices are slightly overestimated.\nAs an exercise, write down the prediction equation of the selected model. Use this equation to explain how instances 1 and 2 (test set) are predicted and calculate the predictions manually. Verify your results using the predict function from the previous R code.\n\n\nAnswer\n\n\nmod_lm_pred[c(1,2)]\n\n\\[\ny = -0.000133 + 3.66\\times TransDate -0.243\\times HouseAge \\\\-0.00464\\times Dist + 1.027\\times NumStores + 237.8\\times Lat\n\\]",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#modelling-1",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#modelling-1",
    "title": "Models: Linear and logistic regressions",
    "section": "Modelling",
    "text": "Modelling\nWe can split our data and fit the logistic regression. The function for this is glm. This function encompasses a larger class of models (namely, the generalized linear models) which includes the logistic regression, accessible with family=“binomial”.\n\nset.seed(234)\nindex &lt;- sample(x=c(1,2), size=nrow(DocVis), replace=TRUE, prob=c(0.75,0.25)) # 1==training set, 2==test set\ndat_tr_visit &lt;- DocVis[index==1,]\ndat_te_visit &lt;- DocVis[index==2,]\n\n\nRPython\n\n\n\nvis_logr &lt;- glm(visits~., data=dat_tr_visit, family=\"binomial\")\nsummary(vis_logr)\n\n\n\n\n# a hack around this technique to not type all the variable names\nvis_formula = 'visits ~ ' + ' + '.join(r.dat_tr_visit.columns.difference(['visits']))\n\n# create a logistic regression model\nvis_logr_py = sm.formula.logit(formula= vis_formula, data=r.dat_tr_visit).fit()\n\nprint(vis_logr_py.summary())\n\n\n\n\n\n\n\nUsing . for formulas in R vs Python\n\n\n\nIn R, the dot . is used as shorthand to indicate that we want to include all other variables in the formula as predictors except for the outcome variable. So, if our outcome variable is y and we want to include all other variables in our data frame as predictors, we can write y ~ . in the formula.\nIn Python, however, the dot . is not used in the same way in formulas. Instead, to include all other variables as predictors except for y, we would write y ~ x1 + x2 + ... where x1, x2, etc. represent the names of the predictor variables. Also, statsmodels has a similar syntax to R base regressions. In most other typical ML libraries in Python, you must provide the column values instead of using the column names.\n\n\nNote that the family=\"binomial\" argument in R is not needed in Python since sm.formula.logit() assumes the logistic regression model is fitted using a binomial distribution by default.",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#variable-selection-interpretation-1",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#variable-selection-interpretation-1",
    "title": "Models: Linear and logistic regressions",
    "section": "Variable selection & interpretation",
    "text": "Variable selection & interpretation\nNow, we can apply the variable selection:\n\nRPython\n\n\n\nvis_logr_sel &lt;- step(vis_logr)\nsummary(vis_logr_sel)\n\n\n\nAs already seen in the linear regression part, in python, we don’t have the same implementation of the step function, hence why we designed the while loop earlier. It is good practice to create a single function with this step while loop to handle all cases (linear, logistic etc); however, we only implement it here for logistic regression. Therefore, to tackle this, we will create a function that does step-wise elimination for us. We define a function called forward_selected that performs forward selection on a given dataset to select the best predictors for a response variable based on AIC. The function takes two arguments: data, a pandas DataFrame containing the predictors and response variable, and response, a string specifying the name of the response variable.\n\n\nFor more explanation of the code, click on me\n\nThe function first initializes two sets: remaining and selected. remaining contains the names of all columns in the data DataFrame except for the response variable, while selected is initially empty. The function then initializes current_aic and best_new_aic to infinity. The main loop of the function continues as long as remaining is not empty and current_aic is equal to best_new_aic. At each iteration, the function iterates over all columns in remaining and computes the AIC for a logistic regression model that includes the response variable and the currently selected predictors, as well as the current candidate predictor. The function then adds the candidate predictor and its AIC to a list of (aic, candidate) tuples, and sorts the list by increasing AIC. The function then selects the candidate with the lowest AIC and adds it to the selected set, removes it from the remaining set, and updates current_aic to the new lowest AIC. The function continues this process until no candidate can improve the AIC. Finally, the function fits a logistic regression model using the selected predictors and returns the resulting model.\n\n\n# code taken from the link below and adjusted for logistic regression with AIC criteria\n# https://planspace.org/20150423-forward_selection_with_statsmodels/\n\ndef forward_selected(data, response):\n    \"\"\"Linear model designed by forward selection.\n\n    Parameters:\n    -----------\n    data : pandas DataFrame with all possible predictors and response\n\n    response: string, name of response column in data\n\n    Returns:\n    --------\n    model: an \"optimal\" fitted statsmodels linear model\n           with an intercept\n           selected by forward selection\n           evaluated by AIC\n    \"\"\"\n    remaining = set(data.columns)\n    remaining.remove(response)\n    selected = []\n    current_aic, best_new_aic = float(\"inf\"), float(\"inf\")\n    while remaining and current_aic == best_new_aic:\n        aics_with_candidates = []\n        for candidate in remaining:\n            formula = \"{} ~ {} + 1\".format(response,\n                                           ' + '.join(selected + [candidate]))\n            model = smf.logit(formula, data).fit(disp=0)\n            aic = model.aic\n            aics_with_candidates.append((aic, candidate))\n        aics_with_candidates.sort()\n        best_new_aic, best_candidate = aics_with_candidates.pop(0)\n        if current_aic &gt; best_new_aic:\n            remaining.remove(best_candidate)\n            selected.append(best_candidate)\n            current_aic = best_new_aic\n    formula = \"{} ~ {} + 1\".format(response,\n                                   ' + '.join(selected))\n    model = smf.logit(formula, data).fit(disp=0)\n    return model\n\n\nmod_logit_sel_py = forward_selected(r.dat_tr_visit, 'visits')\n\nprint(mod_logit_sel_py.summary())\n\nWe can see that the results of mod_logit_sel_py model are slightly different from the R version, but nevertheless, we have reduced the features and the interpretations (see below) with both R and python versions remain the same.\n\n\n\nWe can see that the probability of a visit is\n\nsmaller for males\nincreasing with age\nlarger with illness\netc.",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#inference-1",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#inference-1",
    "title": "Models: Linear and logistic regressions",
    "section": "Inference",
    "text": "Inference\n\nRPython\n\n\nThe predict function with type=“response” will predict the probability of the positive class (“1”). If it is set to “link” it produces the linear predictor (i.e., the \\(z\\)). To make the prediction, we thus have to identify if the predicted probability is larger or lower than 0.5.\n\nprob_te_visit &lt;- predict(vis_logr_sel, newdata = dat_te_visit, type=\"response\")\npred_te_visit &lt;- ifelse(prob_te_visit &gt;= 0.5, 1, 0)\ntable(Pred=pred_te_visit, Obs=dat_te_visit$visits)\n\n\n\nThe explanation is similar to that of R, with a slight different that here we use pandas.crosstab to make our confusion matrix.\n\nimport pandas as pd\n\nprob_te_visit = mod_logit_sel_py.predict(r.dat_te_visit)\npred_te_visit = [1 if p &gt;= 0.5 else 0 for p in prob_te_visit]\nconf_mat = pd.crosstab(pred_te_visit, r.dat_te_visit['visits'], rownames=['Pred'], colnames=['Obs'])\nprint(conf_mat)\n\nThe results are extremely close to the R version.\n\n\n\nThe predictions are not really good. It is in fact a difficult data set. Indeed, the number of 0 is so large compare to the 1, that predicting a 0 always provides a good model overall. That issue will be addressed further later on in the course.\nFor now, this can be further inspected by looking at the predicted probabilities per observed label.\n\nRPython\n\n\n\nboxplot(prob_te_visit~dat_te_visit$visits)\n\n\n\n\nfig, ax = plt.subplots()\nax.boxplot([prob_te_visit[r.dat_te_visit['visits']==0], prob_te_visit[r.dat_te_visit['visits']==1]])\nax.set_xticklabels(['No Visit', 'Visit'])\nax.set_ylabel('Predicted Probability')\nax.set_title('Predicted Probabilities by Visit Status')\nplt.show()\n\n\n\n\nWe see that if the lowest predicted probabilities are usually assigned to 0-observations, most of the probabilities remain below 0.5 (even for the 1-observations). A good model would have two well separated boxplots, well away from 0.5.\nNow, as an exercise, write down the prediction equation of the selected model, like you did for linear regression. Use this equation to explain how instance 1 and 2 (test set) are predicted, and calculate the predictions manually. Verify your results using the function predict used before.\n\n\nAnswer\n\n\nprob_te_visit[c(1,2)]\n\n\\[\nz(x) = -2.31795-0.31838\\times gender\\_male+0.39762\\times age+\\\\0.28431\\times illness+0.16340\\times reduced+0.05589\\times health+\\\\0.27249\\times private\\_eyes -0.65344\\times freepoor\\_yes+\\\\0.38038\\times freerepat\\_yes  \n\\] Then \\[\nP(Y=1 | X=x) = \\frac{e^{z(x)}}{1+e^{z(x)}}\n\\]",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#linear-regression-nursing-home-data",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#linear-regression-nursing-home-data",
    "title": "Models: Linear and logistic regressions",
    "section": "Linear regression: nursing home data",
    "text": "Linear regression: nursing home data\nNow it is your turn. Make an linear regression (also feel free to try lasso and ridge regressions) on the nursing data described below (found also in /data/nursing_data.csv). Afterwards, use linear regression to build a predictor of the cost using the other features. Replicate the analysis. Split the data, build a model, make the variable selection, make the predictions and analyze the results. Make also an analysis of the coefficients in terms of the associations between the costs and the features.\n\n\nData Description\n\nThe data set is about patients in a nursing home, where elderly people are helped with daily living needs, also known as Activities of Daily Living (ADL, i.e. communication, eating, walking, showering, going to a toilet, etc.).\nSince the stay in such facilities is very expensive, it is important to classify the new-coming patient, and estimate the duration of the stay and the corresponding costs.\nIn practice, there are different types of patients who require different types of help and, consequently, different duration of the stay. For example, there could be a person with severe mobility issues, who requires the help with most of the needs every day; or a person with mental deviations, who don’t need help with daily routine, but requires extra communication hours.\nHere, we will focus of total amount of help (measured in minutes of help provided to a person per week) provided and measure the costs of stay of a person.\nThe data set on which the analysis is based has the following columns:\n\ngender: a categorical variable with levels “M” for male and “F” for female\nage: integer variable\nmobil: categorical variable that represents the physical mobility with levels\n\n1 = Full mobility\n2 = Reduced mobility\n3 = Restricted mobility in the house\n4 = Null mobility\n\norient: categorical variable that represents the orientation (interactions with the environment) with levels\n\n1 = Full orientation\n2 = Moderate disturbance of orientation\n3 = Disorientation\n\nindepend: categorical variable that represents the independence of ADL with levels\n\n1 = Independent of help\n2 = Dependent less than 24 hours per day\n3 = Dependent at unpredictable time intervals for most of the needs\n\nminut_mob: numerical variable that represents the total number of minutes of help with movement per week\nneed_comm: categorical variable with levels “Yes” for a person who needs extra communication sessions with an employee, and “No” otherwise\nminut_comm: numerical variable that represents the total number of minutes of communication per week\ntot_minut: numerical variable that represents the total number of minutes spent on a patient per week, \\(tot\\_minut = minut\\_mob + minut\\_comm\\)\ncost: numerical variable that represents the total costs of having a patient in the nursing house per month.\n\n\nNote: since tot_minut=minut_mob+minut_comm, you may not find any meaningful result using the 3 features. This is perfectly normal. Just use 2 features only among these 3 (arbitrary choice).",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#logistic-regression-the-credit-quality",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#logistic-regression-the-credit-quality",
    "title": "Models: Linear and logistic regressions",
    "section": "Logistic regression: the credit quality",
    "text": "Logistic regression: the credit quality\nThe German Credit Quality Dataset consists of a set of attributes as good or bad credit risks. In order to find find a detailed description of the features, please refer to the original link to the dataset. The german.csv file can also be found in /data/german.csv whichis the mdified version of the original dataset to simplify the analysis, especially the data loading in R.\nThe aim here is to predict the credit quality from the other features. The outcome Quality is 0 for “bad” and 1 for “good”. Make an analysis of the data and develop the learner. You can follow these notable steps:\n\nMake a simple EDA of the features\nSplit the data and train the model.\nMake variable selection and check out the result.\nInterpret the coefficients.\nInspect the quality of the model by making the predictions (confusion table and boxplot of the predicted probabilities).\n\nNote that the data are unbalanced again and that you may not find a very good predictor. This issue is quite difficult and will be addressed later.",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/07_InterpretableML/Ex_ML_VarImp.html",
    "href": "labs/07_InterpretableML/Ex_ML_VarImp.html",
    "title": "Variable Importance",
    "section": "",
    "text": "This exercise shows an example of model-agnostic variable importance for a regression problem. The dataset that we will be working with is Carseats from the ISLR library which has already been used during some of the exercises such as Ex_ML_Tree and Ex_ML_SVM. It is highly recommended that you try to implement some parts of the exercise yourself before checking the answers.\n\nLoad the data from ISLR package, then assign 90% of the data for training and the remainder for testing. Please keep in mind that we make a training split running the feature importance (instead of using the entire dataset) as we “may” want to re-train the model with only a fewer features rather than biasing our decision by also including the testing data.\nCreate three models including a linear regression, a regression tree and a support-vector machine.\n\n\n\nAnswer\n\n\n\nR\n\n\n\nlibrary(rpart)\nlibrary(e1071)\nlibrary(dplyr)\nlibrary(ISLR)\n\n# divide the data into training and testing sets\nset.seed(2022)\ncarseats_index &lt;- sample(x=1:nrow(Carseats), size=0.9*nrow(Carseats), replace=FALSE)\ncarseats_tr &lt;- Carseats[carseats_index,]\ncarseats_te &lt;- Carseats[-carseats_index,]\n\n# define a linear regression\ncarseats_lm &lt;- lm(Sales~., data=carseats_tr)\n\n# define a regression tree (you can also use `adabag::autoprune()`here\ncarseats_rt &lt;- rpart(Sales~., data=carseats_tr)\n\n# define a support-vector machine\ncarseats_svm &lt;- svm(Sales ~ ., data=carseats_tr)\n\n\n\n\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import fetch_openml\n\n# Use the training and test sets created in R (no easy way to get the `Carseats` data in python)\n# Convert the categorical columns to one-hot encoded ones\ncarseats_train, carseats_test = pd.get_dummies(r.carseats_tr.copy()), pd.get_dummies(r.carseats_tr.copy())\n\n# Define a linear regression\ncarseats_lr = LinearRegression()\ncarseats_lr.fit(carseats_train.drop(columns=['Sales']), carseats_train['Sales'])\n\n# Define a decision tree regressor\ncarseats_dtr = DecisionTreeRegressor()\ncarseats_dtr.fit(carseats_train.drop(columns=['Sales']), carseats_train['Sales'])\n\n# Define a support-vector machine\ncarseats_svr = SVR()\ncarseats_svr.fit(carseats_train.drop(columns=['Sales']), carseats_train['Sales'])",
    "crumbs": [
      "Labs",
      "Interpretable ML"
    ]
  },
  {
    "objectID": "labs/07_InterpretableML/Ex_ML_VarImp.html#creating-an-explain-object",
    "href": "labs/07_InterpretableML/Ex_ML_VarImp.html#creating-an-explain-object",
    "title": "Variable Importance",
    "section": "Creating an explain object",
    "text": "Creating an explain object\nDALEX has an explain object which allows you to do various kind of explanatory analysis. Try reading about the required inputs for it by referring to its documentations (?DALEX::explain()) and also referring to the book mentioned at the beginning of this exercise (“Hands-on Machine Learning with R”). Create one explain object per model for the training data and set the inputs that you need which are model, data (data frame of features) and y (vector of observed outcomes). Also, you can give a caption to your model through the label argument.\n\nRPython\n\n\n\nlibrary(dplyr)\nlibrary(DALEX)\n\nx_train &lt;- select(carseats_tr, -Sales)\ny_train &lt;- pull(carseats_tr, Sales)\n\nexplainer_lm &lt;- DALEX::explain(model = carseats_lm, \n                                 data = x_train, \n                                 y = y_train,\n                                 label = \"Linear Regression\")\n\nexplainer_rt &lt;- DALEX::explain(model = carseats_rt,\n                               data = x_train,\n                               y = y_train,\n                               label = \"Regression Tree\")\n\nexplainer_svm &lt;- DALEX::explain(model = carseats_svm,\n                                data = x_train,\n                                y = y_train,\n                                label = \"Support Vector Machine\")\n\n\n\nTo calculate the feature importances using Python, we’ll be using the permutation_importance function from the sklearn library.\n\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the features (relevant to SVM)\nscaler = StandardScaler()\ncarseats_train_scaled = carseats_train.copy()\ncarseats_test_scaled = carseats_test.copy()\n\n# Calculate feature importances for each model\nimportance_lr = permutation_importance(carseats_lr, carseats_train.drop(columns=['Sales']), carseats_train['Sales'], n_repeats=10, random_state=2022)\nimportance_dtr = permutation_importance(carseats_dtr, carseats_train.drop(columns=['Sales']), carseats_train['Sales'], n_repeats=10, random_state=2022)\nimportance_svr = permutation_importance(carseats_svr, carseats_train.drop(columns=['Sales']), carseats_train['Sales'], n_repeats=10, random_state=2022)\n# Train the SVM model on scaled data\ncarseats_svr_scaled = SVR(kernel='linear')\ncarseats_svr_scaled.fit(carseats_train_scaled.drop(columns=['Sales']), carseats_train_scaled['Sales'])\nimportance_svr_scaled = permutation_importance(carseats_svr_scaled, carseats_train_scaled.drop(columns=['Sales']), carseats_train_scaled['Sales'], n_repeats=10, random_state=2022)\n\n# Print the feature importances\nimportance_df = pd.DataFrame(data={\n    'Feature': carseats_train.drop(columns=['Sales']).columns,\n    'Linear Regression': importance_lr.importances_mean,\n    'Decision Tree': importance_dtr.importances_mean,\n    'Support Vector Machine (unscaled)': importance_svr.importances_mean,\n    'Support Vector Machine (scaled)': importance_svr_scaled.importances_mean\n})\n\nprint(importance_df)\n\nYou can observe the value of scaling for SVM. The results seems to agree that Price is the most important feature.",
    "crumbs": [
      "Labs",
      "Interpretable ML"
    ]
  },
  {
    "objectID": "labs/07_InterpretableML/Ex_ML_VarImp.html#plotting-the-feature-importance",
    "href": "labs/07_InterpretableML/Ex_ML_VarImp.html#plotting-the-feature-importance",
    "title": "Variable Importance",
    "section": "Plotting the feature importance",
    "text": "Plotting the feature importance\nNow that you have created the DALEX::explain objects, we will use another function called model_parts which takes care of the feature permutation. Try reading about the function DALEX::model_parts() . The main arguments that you need to provide to the explain function are:\n\nAn explainer object (what you created above).\nB which is the number of permutations (i.e. how many times you want to randomly shuffle each column).\nThe type of scores you would like it to return (raw score vs differences vs ratios) which in this case we set to ratio and you can read more the differences in the documentation.\nN argument which you can set to N=NULL which essentially asks how many samples you would like to use for calculating the variable importance, where setting it to NULL means that we use the entire training set.\nThere is also a loss_function which by default is RMSE for regression (our case) and 1-AUC for classification, by there are a few more which you can find out about by referring to the documentations (e.g. through ?DALEX::loss_root_mean_square).\n\nAfter assigning model_parts to a variable, try plotting each model to see the most important variables. What do you see? Are there important features that are in common?\n\ncalculate_importance &lt;- function(your_model_explainer, n_permutations = 10) {\n  imp &lt;- model_parts(explainer = your_model_explainer,\n                     B = n_permutations,\n                     type = \"ratio\",\n                     N = NULL)\n  return(imp)\n}\n\nimportance_lm  &lt;- calculate_importance(explainer_lm)\nimportance_rt  &lt;- calculate_importance(explainer_rt)\nimportance_svm &lt;- calculate_importance(explainer_svm)\n\nlibrary(ggplot2)\nplot(importance_lm, importance_rt, importance_svm) +\n  ggtitle(\"Mean variable-importance ratio over 10 permutations\", \"\")",
    "crumbs": [
      "Labs",
      "Interpretable ML"
    ]
  },
  {
    "objectID": "labs/07_InterpretableML/Ex_ML_VarImp.html#svm",
    "href": "labs/07_InterpretableML/Ex_ML_VarImp.html#svm",
    "title": "Variable Importance",
    "section": "SVM",
    "text": "SVM\n\nRPython\n\n\nlime package does not support the SVM model from the e1071 package out of the box.You can see the list of supported models via ?model_type. There are solutions to this:\n\nRe-train your model with the caret library which we then work directly with this library (also may be good practice to build your models with caret).\n\n\n# Load the lime library\nlibrary(lime)\n\n# Create a caret model using a support vector machine\nsvm_caret_model &lt;- caret::train(Sales ~ ., data = carseats_tr, method = \"svmLinear2\", trControl = trainControl(method = \"none\"))\n\n# Predict on a test instance\ntest_instance &lt;- carseats_te[1:4, -1]\n\n# Create a lime explainer object for the SVM model\nlime_svm_explainer &lt;- lime::lime(carseats_tr[, -1], \n                                 svm_caret_model)\n\n# Explain a prediction using lime\nlime_svm_explanation &lt;- explain(test_instance, lime_svm_explainer, n_features = 10)\nplot_features(lime_svm_explanation)\n\n\nCreate custom predict_model and model_type methods for the SVM model.\n\n\n# Custom predict_model function for SVM\npredict_model.svm &lt;- function(x, newdata, type, ...) {\n  if (type == \"raw\") {\n    res &lt;- predict(x, newdata = newdata, ...)\n    return(data.frame(Response = res, stringsAsFactors = FALSE))\n  } else if (type == \"prob\") {\n    res &lt;- predict(x, newdata = newdata, ...)\n    prob &lt;- kernlab::kernel(x, newdata, j = -1)\n    return(as.data.frame(prob, check.names = FALSE))\n  }\n}\n\n# Custom model_type function for SVM\nmodel_type.svm &lt;- function(x, ...) {\n  if (x$type == \"C-classification\") {\n    return(\"classification\")\n  } else {\n    return(\"regression\")\n  }\n}\n\n# Create a LIME explainer for the SVM model\nlime_svm_explainer &lt;- lime(x_train, carseats_svm)\n\n# Choose a specific instance from the test set to explain\ntest_instance &lt;- carseats_te[1:4,-1]\n\n# Generate explanations for the chosen instance\nlime_svm_explanation &lt;- explain(test_instance, lime_svm_explainer, n_features = 10)\n\n# Visualize the explanation\nplot_features(lime_svm_explanation)\n\nYou can see the prediction plot for 4 test observations. We can see several bar charts. On the y-axis, you see the features (and their intervals), while the x-axis shows the relative strength of each feature at a given value or interval. The positive value (blue color) shows that the feature support or increases the value of the prediction, while the negative value (red color) has a negative effect or decreases the prediction value. Please note that the interpretation for each observation can be different (this explanation has been taken from this blog, which you can visit for further details).\nWe give the interpretation of the first test observation as an example. The first subplot shows that a price of less than 100 results in purchasing a higher quantity than expected. Additionally, people between the ages of 40 and 55 were most likely to buy the seat, which are people who are not too young nor too old. However, in a typical scenario, we would generally expect younger people to buy car seats, but that’s probably because of the high ages in our dataset (1st. quantile of age is around 40). If the price by the competitor (CompPrice) is also low, it’ll impact the sales units badly. Once again, please note that this is specific to the first observation (i.e., the first subplot).\nThe next element is Explanation Fit. These values indicate how well LIME explains the model, similar to an R-Squared in linear regression. Here we see the explanation Fit only has values around 0.50-0.7 (50%-70%), which can be interpreted that LIME can only explain a little about our model (in some cases, like the 3rd sub-plot, this value is extremely low). You may choose not to trust the LIME output since it only has a low Explanation Fit.\n\n\nWe’ll be using provide a small demonstration on how this can be achieved in python. Please note the same logic for the interpretation (and explanation) of the R version applies here, therefore, the code is shorter and there’s no further comment provided for its output.\nWe use lime package in python (already installed in the lab setup). Then we can run our model in the same way as R:\n\nfrom sklearn import svm\nfrom sklearn.pipeline import make_pipeline\nfrom lime import lime_tabular\nimport matplotlib.pyplot as plt\n\n# Assuming carseats_tr and carseats_te are already defined as pandas DataFrames\nX_train = carseats_train.drop(columns='Sales')\ny_train = carseats_train['Sales']\nX_test = carseats_test.drop(columns='Sales')\n\n# Create a support vector machine model\nsvm_caret_model = svm.LinearSVR(random_state=2022)\n\n# Train the model\nsvm_caret_model.fit(X_train, y_train)\n\n# Predict on a test instance\ntest_instance = X_test.iloc[0:4]\n\n# Create a lime explainer object for the SVM model\nlime_svm_explainer = lime_tabular.LimeTabularExplainer(X_train.values,\n                                                       feature_names=X_train.columns,\n                                                       class_names=['Sales'],\n                                                       mode='regression')\n\n# Explain a prediction using lime\nlime_svm_explanation = lime_svm_explainer.explain_instance(test_instance.values[0], svm_caret_model.predict, num_features=10)\n\n# Plot the features\n# plt.clf()\nlime_svm_explanation.as_pyplot_figure()\nplt.show()",
    "crumbs": [
      "Labs",
      "Interpretable ML"
    ]
  },
  {
    "objectID": "labs/07_InterpretableML/Ex_ML_VarImp.html#bonus-xgboost",
    "href": "labs/07_InterpretableML/Ex_ML_VarImp.html#bonus-xgboost",
    "title": "Variable Importance",
    "section": "Bonus: XGBoost",
    "text": "Bonus: XGBoost\nTo give you an example for a classification problem, we can also train an XGBoost using the xgboost library:\n\nlibrary(xgboost)\n\n# Load and prepare the data\ncarseats_df &lt;- Carseats\ncarseats_df$High &lt;- ifelse(carseats_df$Sales &lt;= 8, \"yes\", \"no\")\ncarseats_df$High &lt;- as.factor(carseats_df$High)\ncarseats_df$ShelveLoc &lt;- as.factor(carseats_df$ShelveLoc)\ncarseats_df$Urban &lt;- as.factor(carseats_df$Urban)\ncarseats_df$US &lt;- as.factor(carseats_df$US)\n \n# Prepare the data for xgboost (as shown in the boosting excercises)\nxgb_data &lt;- model.matrix(High ~ ., data = carseats_df)[,-1]\nxgb_label &lt;- as.numeric(carseats_df$High) - 1\nxgb_dmatrix &lt;- xgb.DMatrix(data = xgb_data, label = xgb_label)\n\n# Train a gradient boosting model\nset.seed(42)\ncarseats_xgb &lt;- xgboost(data = xgb_dmatrix, nrounds = 100, objective = \"binary:logistic\", eval_metric = \"logloss\")\n\nIdentify instances with predicted probabilities close to 1, 0, and 0.5:\n\n# LIME explanations for a gradient boosting model\nxgb_preds &lt;- predict(carseats_xgb, xgb_dmatrix)\n\nwhich.max(xgb_preds)\nwhich.min(xgb_preds)\nwhich.min(abs(xgb_preds - 0.5))\n\nGenerate LIME explanations for the selected instances:\n\n# before making the prediction, we need to also one-hot encode the categorical variables\nto_explain &lt;- data.frame(model.matrix(~.,data = carseats_df[c(120, 4, 60), -ncol(carseats_df)])[,-1])\n\n# we can finally run LIME on our results\ncarseats_lime_xgb &lt;- lime(data.frame(xgb_data), carseats_xgb, bin_continuous = TRUE, quantile_bins = FALSE)\ncarseats_expl_xgb &lt;- lime::explain(to_explain, carseats_lime_xgb, n_labels = 1, n_features = 10)\n\nVisualize the LIME explanations\n\nplot_features(carseats_expl_xgb, ncol = 2)\n\nWhat can you observe from these subplots?",
    "crumbs": [
      "Labs",
      "Interpretable ML"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html",
    "title": "Data splitting",
    "section": "",
    "text": "In this series, we practice the data splitting strategies seen in class. The data are the doctor visits, already used in previous applications: cross-validation, bootstrap, and balancing.\nWe’ll be using the DocVis dataset for this lab session.\n\nDocVis &lt;- read.csv(here::here(\"labs/data/DocVis.csv\"))\nDocVis$visits &lt;- as.factor(DocVis$visits) ## make sure that visits is a factor\n\nWe need to set aside a test set. This will be used after to check that there was no overfitting during the training of the model and to ensure that the score we have obtained generalizes outside the training data.\n\nlibrary(caret)\nlibrary(dplyr)\nset.seed(346)\nindex_tr &lt;- createDataPartition(y = DocVis$visits, p= 0.8, list = FALSE)\ndf_tr &lt;- DocVis[index_tr,]\ndf_te &lt;- DocVis[-index_tr,]\n\nNote that the splitting techniques used are applied on the training set only.",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#first-fold",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#first-fold",
    "title": "Data splitting",
    "section": "First fold",
    "text": "First fold\n\nRPython\n\n\nFirst, we create the folds by using the createFolds function of caret.\n\nindex_CV &lt;- createFolds(y = df_tr$visits, k=10)\nindex_CV[[1]]\n\nAs seen before, the index_CV object is a list of row indices. The first element of the list index_CV[[1]] corresponds to the first fold. It is the vector of row indices of the validation set for the first fold (i.e., the validation is made of the rows of the training set that are in this vector). All the indices that are not in index_CV[[1]] will be in the training set (for this fold).\n\ndf_cv_tr &lt;- df_tr[-index_CV[[1]],]\ndf_cv_val &lt;- df_tr[index_CV[[1]],]\n\nFor this fold, df_cv_tr is the training set (it contains 9/10 of the original training set df_tr) and df_cv_val is the validation set (it contains 1/10 of the original training set df_tr). These two sets are disjoints.\nNow, we simply fit the model with the training set and compute its accuracy on the validation set. For this exercise, we use a logistic regression with AIC-based variable selection.\n\nDoc_cv &lt;- glm(visits~., data=df_cv_tr, family=\"binomial\") %&gt;% step()\nDoc_cv_prob &lt;- predict(Doc_cv, newdata=df_cv_val, type=\"response\")\nDoc_cv_pred &lt;- ifelse(Doc_cv_prob&gt;0.5,\"Yes\",\"No\")\nconfusionMatrix(data=as.factor(Doc_cv_pred), reference = df_cv_val$visits)$overall[1]\n\n\n\n\n# Load the course python environment as usual with a r code chunks.\nlibrary(reticulate)\nuse_condaenv(\"MLBA\", required = TRUE)\n\nWe will one-hot encode the categorical variables using pd.get_dummies() and then divide the data into X_train, y_train, X_test and y_test.\n\nimport pandas as pd\n\n# One-hot encoding the categorical columns\nX_train = pd.get_dummies(r.df_tr.drop('visits', axis=1))\ny_train = r.df_tr['visits']\nX_test = pd.get_dummies(r.df_te.drop('visits', axis=1))\ny_test = r.df_te['visits']\n\nThen, we create the folds by using the KFold function of scikit-learn.\n\nfrom sklearn.model_selection import KFold\n# We setup the 10-k fold\nkf = KFold(n_splits=10, random_state=346, shuffle=True)\nfold_indices = list(kf.split(X_train, y_train))\nfirst_fold_train, first_fold_val = fold_indices[0]\n\nAs seen before, the fold_indices object is a list of tuple pairs. The first element of the list fold_indices[0] corresponds to the first fold. It is the tuple of row indices of the training set and the validation set for the first fold. All the indices that are not in first_fold_val will be in the training set (for this fold).\n\nX_cv_tr = X_train.iloc[first_fold_train, :]\ny_cv_tr = y_train.iloc[first_fold_train]\n\nX_cv_val = X_train.iloc[first_fold_val, :]\ny_cv_val = y_train.iloc[first_fold_val]\n\nThis part, will be slightly different from the R approach. Here, we fit the model with the training set and compute its accuracy on the validation set. For the python approach, we use a logistic regression with recursive feature elimination to select the best number of features.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.feature_selection import RFE\nimport numpy as np\n\nmodel = LogisticRegression(solver='liblinear')\nrfe = RFE(model)\nrfe.fit(X_cv_tr, y_cv_tr)\npred_probs = rfe.predict_proba(X_cv_val)[:, 1]\nDoc_cv_pred = np.where(pred_probs &gt; 0.5, \"Yes\", \"No\")\nacc = accuracy_score(y_cv_val, Doc_cv_pred)\nacc\n\n# alternatively, you could use `cross_val_score`\n# from sklearn.model_selection import cross_val_score\n# cv_scores = cross_val_score(rfe, X_cv_tr, y_cv_tr, cv=kf, scoring=\"accuracy\")\n# cv_scores",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#loop-on-the-10-folds",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#loop-on-the-10-folds",
    "title": "Data splitting",
    "section": "Loop on the 10 folds",
    "text": "Loop on the 10 folds\nNow we repeat the previous steps for all the folds.\n\nRPython\n\n\nIn order to track the 10 accuracy measures obtained, we store them into a vector (acc_vec). Note also that the option trace=0 was set in the function step to avoid all the print outs of the AIC selections.\n\nacc_vec &lt;- numeric(10)\nfor (i in 1:10){\n  df_cv_tr &lt;- df_tr[-index_CV[[i]],]\n  df_cv_val &lt;- df_tr[index_CV[[i]],]\n  \n  Doc_cv &lt;- glm(visits~., data=df_cv_tr, family=\"binomial\") %&gt;% step(trace=0)\n  Doc_cv_prob &lt;- predict(Doc_cv, newdata=df_cv_val, type=\"response\")\n  Doc_cv_pred &lt;- ifelse(Doc_cv_prob&gt;0.5,\"Yes\",\"No\")\n  acc_vec[i] &lt;- confusionMatrix(data=as.factor(Doc_cv_pred), reference = df_cv_val$visits)$overall[1]\n}\nacc_vec\n\nBy definition of the CV, all the 10 validations sets in this loop are disjoints. Thus, these 10 accuracy measures are in a way representative of what can be expected on the test set, except if we are very unlucky when we created the test set.\nNow we can estimate the expected accuracy (i.e., the mean) and its variation (below we use the standard deviation).\n\nmean(acc_vec)\nsd(acc_vec)\n\nThe small SD shows that the results are reliable and that we have good chance that the model, trained on the whole training set, will have this accuracy on the test set.\n\n\nFor python, in order to track the 10 accuracy measures obtained, we store them into a list (acc_list).\n\nacc_list = []\nfor train_idx, val_idx in kf.split(X_train, y_train):\n    X_cv_tr, y_cv_tr = X_train.iloc[train_idx, :], y_train.iloc[train_idx]\n    X_cv_val, y_cv_val = X_train.iloc[val_idx, :], y_train.iloc[val_idx]\n    \n    rfe.fit(X_cv_tr, y_cv_tr)\n    pred_probs = rfe.predict_proba(X_cv_val)[:, 1]\n    Doc_cv_pred = np.where(pred_probs &gt; 0.5, \"Yes\", \"No\")\n    acc = accuracy_score(y_cv_val, Doc_cv_pred)\n    acc_list.append(acc)\nacc_list\n\nOnce again, we can estimate the expected accuracy and its variation.\n\nmean_acc = np.mean(acc_list)\nstd_acc = np.std(acc_list)\nmean_acc, std_acc\n\nNow, we fit the final model using the whole training set and evaluate its performance on the test set.\n\nrfe.fit(X_train, y_train)\ny_test_pred = rfe.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\ntest_cm = confusion_matrix(y_test, y_test_pred)\ntest_accuracy, test_cm",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#automated-approach",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#automated-approach",
    "title": "Data splitting",
    "section": "Automated approach",
    "text": "Automated approach\n\nR - caretPython - sklearn\n\n\nThe 10-CV can be easily obtained from caret. First, set up the splitting data method using the trainControl function.\n\ntrctrl &lt;- trainControl(method = \"cv\", number=10)\n\nThen pass this method to the train function (from caret). In addition, we use the model (below unhappily called “method” also) glmStepAIC which, combined with the binomial family, applies a logistic regression and a AIC-based variable selection (backward; exactly like the step function used above). Of course, we also provide the model formula.\n\nset.seed(346)\nDoc_cv &lt;- train(visits ~., data = df_tr, method = \"glmStepAIC\", family=\"binomial\",\n                    trControl=trctrl, trace=0)\nDoc_cv\n\nNote that the function “only” provides the expected accuracy and the expected kappa. It does not provides their standard deviations.\nThe final model (i.e., the model trained on the whole training set df_tr) is stored in Doc_cv$finalModel. It can be used to compute the accuracy on the test set.\n\nDoc_pred &lt;- predict(Doc_cv, newdata = df_te)\nconfusionMatrix(data=Doc_pred, reference = df_te$visits)\n\n\n\nIn python, a similar approach demonstrated in caret would be using GridSearchCV from scikit-learn. We use the same 10-CV kf object created earlier. Then pass this method to the GridSearchCV function with LogisticRegression model with RFE for feature selection with the grid of parameters we would like to search for (in this case the number of features). Then, we output the (hyper-)parameters with the best performance.\n\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = LogisticRegression(solver='liblinear')\nrfe = RFE(model)\nparam_grid = {'n_features_to_select': list(range(1, X_train.shape[1] + 1))}\ngrid_search = GridSearchCV(rfe, param_grid, scoring='accuracy', cv=kf)\ngrid_search.fit(X_train, y_train)\nprint(grid_search.best_score_, grid_search.best_params_)\n\nThe final model (i.e., the model trained on the whole training set X_train) is stored in grid_search.best_estimator_. It can be used to compute the accuracy on the test set.\n\ny_test_pred = grid_search.best_estimator_.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\ntest_cm = confusion_matrix(y_test, y_test_pred)\nprint(test_accuracy, test_cm)\n\nOur results did improve compared to the last python model.",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#first-sample",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#first-sample",
    "title": "Data splitting",
    "section": "First sample",
    "text": "First sample\n\nRPython\n\n\nWe need to first create the replicates using the caret function createResample.\n\nset.seed(897)\nindex_boot &lt;- createResample(y=df_tr$visits, times=100)\nindex_boot[[1]]\n\nAgain, it creates a list of indices. The first element of the list, index_boot[[1]], contains the row indices that will be in the training set. Note that, it is of length 4,153. In other words, the training set during this first replicate is of the same dimension as the whole training set df_tr. Note also that, in index_boot[[1]], there are indices that are replicated. This is the bootstrap sampling process. Some rows will be replicated in the training set. This also means that some rows of df_tr will not be in index_boot[[1]]. These rows are said to be out-of-bag and form the validation set. See below the dimensions of the data frames.\n\ndf_boot_tr &lt;- df_tr[index_boot[[1]],]\ndim(df_boot_tr)\ndf_boot_val &lt;- df_tr[-index_boot[[1]],]\ndim(df_boot_val)\n\nWe now fit the data to this first sample training set.\n\nDoc_boot &lt;- glm(visits~., data=df_boot_tr, family=\"binomial\") %&gt;% step()\n\nThe accuracy is then computed with the 632-rule: first, the apparent accuracy is computed (accuracy on the sample training set), then the out-of-bag accuracy (the accuracy on the validation set), then the final accuracy estimate is the 0.368/0.632-combination of the two.\n\nDoc_boot_prob_val &lt;- predict(Doc_boot, newdata=df_boot_val, type=\"response\")\nDoc_boot_pred_val &lt;- ifelse(Doc_boot_prob_val&gt;0.5,\"Yes\",\"No\")\noob_acc &lt;- confusionMatrix(data=as.factor(Doc_boot_pred_val), reference = df_boot_val$visits)$overall[1]\n\nDoc_boot_prob_tr &lt;- predict(Doc_boot, newdata=df_boot_tr, type=\"response\")\nDoc_boot_pred_tr &lt;- ifelse(Doc_boot_prob_tr&gt;0.5,\"Yes\",\"No\")\napp_acc &lt;- confusionMatrix(data=as.factor(Doc_boot_pred_tr), reference = df_boot_tr$visits)$overall[1]\n\noob_acc ## out-of-bag accuracy\napp_acc ## apparent accuracy\n0.368*app_acc + 0.632*oob_acc ## accuracy estimate\n\n\n\nFirst, we create the replicates using the resample function from sklearn.utils. Please note that unlike caret::createResample(), in sklearn (to the best of our knowledge), there’s no method that returns a list of the samples, so with resample, we get one set of resampled data points. This doesn’t matter for now, but in the following sub-section, we will write a function to do the same thing in python.\n\nfrom sklearn.utils import resample\n\nnp.random.seed(897)\ndf_boot_tr = resample(r.df_tr, n_samples=len(r.df_tr), random_state=897)\n\nThe resample function returns a new data frames with the same number of samples as the original r.df_tr, but some rows will be replicated. This also means that some rows of r.df_tr will not be in the bootstrapped data frames These rows are said to be out-of-bag and form the validation set. See below the dimensions of the data frames.\n\ndf_boot_tr.shape\noob_mask = ~r.df_tr.index.isin(df_boot_tr.index.values)\ndf_boot_val = r.df_tr[oob_mask]\ndf_boot_val.shape\n\nThere’s a difference between the shape of df_boot_val because of the difference in random generators between R & python. If you change the number for the random generator in python (i.e., np.random.seed(897)) or in R (i.e., set.seed(897)), you’ll see that the results will be slightly different.",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#loop-on-the-100-sample",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#loop-on-the-100-sample",
    "title": "Data splitting",
    "section": "Loop on the 100 sample",
    "text": "Loop on the 100 sample\n\nRPython\n\n\nThe previous code is looped. The accuracy measures are stored in vectors. The code can be quite long to run.\n\noob_acc_vec &lt;- numeric(100)\napp_acc_vec &lt;- numeric(100)\nacc_vec &lt;- numeric(100)\nfor (i in 1:100){\n  df_boot_tr &lt;- df_tr[index_boot[[i]],]\n  df_boot_val &lt;- df_tr[-index_boot[[i]],]\n  \n  Doc_boot &lt;- glm(visits~., data=df_boot_tr, family=\"binomial\") %&gt;% step(trace=0)\n  Doc_boot_prob_val &lt;- predict(Doc_boot, newdata=df_boot_val, type=\"response\")\n  Doc_boot_pred_val &lt;- ifelse(Doc_boot_prob_val&gt;0.5,\"Yes\",\"No\")\n  oob_acc_vec[i] &lt;- confusionMatrix(data=as.factor(Doc_boot_pred_val), reference = df_boot_val$visits)$overall[1]\n  \n  Doc_boot_prob_tr &lt;- predict(Doc_boot, newdata=df_boot_tr, type=\"response\")\n  Doc_boot_pred_tr &lt;- ifelse(Doc_boot_prob_tr&gt;0.5,\"Yes\",\"No\")\n  app_acc_vec[i] &lt;- confusionMatrix(data=as.factor(Doc_boot_pred_tr), reference = df_boot_tr$visits)$overall[1]\n  \n  acc_vec[i] &lt;- 0.368*app_acc_vec[i] + 0.632*oob_acc_vec[i]\n}\n\nacc_vec\n\nLike for the CV, we can estimate the expected accuracy and its dispersion.\n\nmean(acc_vec)\nsd(acc_vec)\n\n\n\nIn this part of the code, we perform the bootstrap procedure with 100 samples. To that, we will implement our own function to create the index n-times for a given dataset. This ensures that we get a similar output to caret::createResample(). In this case, we apply this function to our main training dataframe 100 times.\n\ndef create_resample(data, times=100, random_seed=None):\n    # If you're not familiar with the documentation below, they are called\n    # `docstrings` and whenever you ask help for a function or see it's documentation,\n    # they are generated from that\n    \"\"\"\n    Generate bootstrap sample indices for data.\n    \n    Args:\n    - data (array-like): The data to bootstrap.\n    - times (int): The number of bootstrap samples to generate.\n    - random_seed (int): The random seed to use for reproducibility.\n    \n    Returns:\n    - samples (list of arrays): A list of times bootstrap sample indices.\n    \"\"\"\n    np.random.seed(random_seed)\n    n_samples = len(data)\n    samples = []\n    for _ in range(times):\n        indices = np.random.choice(n_samples, n_samples, replace=True)\n        samples.append(indices)\n    return samples\n\n# apply the new created function\nindex_boot = create_resample(r.df_tr, times=100, random_seed = 123)\n# we can see if we successfully replicated the sampling process 100 times\nprint(len(index_boot))\n# check if we have the correct number of rows (e.g. for the first element)\nprint(len(index_boot[0]))\n# alternatively to see this information, you can uncomment & run the code below\n# np.asarray(index_boot).shape\n\n\n\n\n\n\n\nNote\n\n\n\nOne thing to note is that we could have used the sklearn.utils.resample introduced earlier to directly get a list of dataframes with the randomly chosen indices. The issue here would be rather a computational one, as we have to extract the rows many times from the dataset and then hold all this data in memory, which is redundant. So although this may not be a problem for 100 replications, it can quickly start to become an issue if you want to replicated many more times (e.g., 100,000 times). The best approach is to get the indices, and then subset the rows only when needed.\n\nimport numpy as np\nfrom sklearn.utils import resample\n\ndef create_n_resamples(data, times, random_seed=None):\n    \"\"\"\n    Generate n_bootstraps bootstrap samples of data.\n    \n    Args:\n    - data (array-like): The data to bootstrap.\n    - n_bootstraps (int): The number of bootstrap samples to generate.\n    - random_seed (int): The random seed to use for reproducibility.\n    \n    Returns:\n    - bootstrap_samples (list of lists): A list of n_bootstraps bootstrap samples.\n    \"\"\"\n    np.random.seed(random_seed)\n    bootstrap_samples = []\n    for i in range(times):\n        sample = resample(data)\n        bootstrap_samples.append(sample)\n    return bootstrap_samples\n\ndfs_boot = create_n_resamples(r.df_tr, times=100, random_seed = 123) \n\n\n\nFor each sample, we calculate the out-of-bag accuracy, the apparent accuracy, and the final accuracy estimate using the 0.368/0.632 rule. This is done using a loop that iterates 100 times, once for each bootstrap sample. The steps that the code follows are similar to R, and are outlined below:\n\nWe set up three arrays to store the out-of-bag accuracy, the apparent accuracy, and the final accuracy estimate for each of the 100 bootstrap samples.\nIn the loop, we perform the following steps for each bootstrap sample:\n\nUse the generate a random list of indices with replacement, which forms the bootstrap training set.\nCreate a mask to extract the out-of-bag (validation) set from the original training set.\nTrain the logistic regression model with RFE on the bootstrap training set.\nCompute the out-of-bag accuracy by predicting on the validation set and comparing the predicted labels to the true labels.\nCompute the apparent accuracy by predicting on the bootstrap training set and comparing the predicted labels to the true labels.\nCalculate the final accuracy estimate for the current bootstrap sample using the 0.368/0.632 rule.\n\nOnce the loop is complete, the acc_vec array will contain the final accuracy estimates for all 100 bootstrap samples. We can then calculate the mean and standard deviation of these accuracy estimates to get an overall understanding of the model’s performance.\n\n\n# we one-hote encode the categorical variables\n## notice that we didn't use the argument `drop_first` before, since this is like\n## making dummy variable m - 1 where m is the number of variables you have\nr.df_tr_encoded = pd.get_dummies(r.df_tr, drop_first=True)\n\noob_acc_vec = np.zeros(100)\napp_acc_vec = np.zeros(100)\nacc_vec = np.zeros(100)\n\nfor i in range(100):\n    df_boot_tr = r.df_tr_encoded.iloc[index_boot[i]]\n    y_boot_tr = df_boot_tr[\"visits_Yes\"].astype(int)\n    X_boot_tr = df_boot_tr.drop(\"visits_Yes\", axis=1)\n\n    oob_mask = ~r.df_tr_encoded.index.isin(df_boot_tr.index.values)\n    df_boot_val = r.df_tr_encoded[oob_mask]\n    y_boot_val = df_boot_val[\"visits_Yes\"].astype(int)\n    X_boot_val = df_boot_val.drop(\"visits_Yes\", axis=1)\n\n    model = LogisticRegression(solver='liblinear')\n    rfe = RFE(model)\n    rfe.fit(X_boot_tr, y_boot_tr)\n\n    pred_probs_val = rfe.predict_proba(X_boot_val)[:, 1]\n    Doc_boot_pred_val = (pred_probs_val &gt; 0.5).astype(int)\n    oob_acc = accuracy_score(y_boot_val, Doc_boot_pred_val)\n    oob_acc_vec[i] = oob_acc\n\n    pred_probs_tr = rfe.predict_proba(X_boot_tr)[:, 1]\n    Doc_boot_pred_tr = (pred_probs_tr &gt; 0.5).astype(int)\n    app_acc = accuracy_score(y_boot_tr, Doc_boot_pred_tr)\n    app_acc_vec[i] = app_acc\n\n    acc_vec[i] = 0.368 * app_acc + 0.632 * oob_acc\n\nprint(acc_vec)",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#automated-approach-1",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#automated-approach-1",
    "title": "Data splitting",
    "section": "Automated approach",
    "text": "Automated approach\n\nR - caretPython - sklearn & mlxtend\n\n\nWe only need to change the method in the trainControl function. The corresponding method is “boot632”.\n\nset.seed(346)\ntrctrl &lt;- trainControl(method = \"boot632\", number=100)\nDoc_boot &lt;- train(visits ~., data = df_tr, method = \"glmStepAIC\", family=\"binomial\",\n                   trControl=trctrl, trace = 0)\nDoc_boot\n\n\n\nAs sklearn does not offer bootstrap with the 0.632 rule, we use bootstrap_point632_score function from the mlxtend library to perform bootstrapping with the 0.632 rule for our Logistic Regression model. We will use mlxtend with R for bootstrapping with the 0.632 rule.\nPlease note for this part, we don’t make any step-wise feature selection here as in the case of caret (i.e., glmStepAIC), but similar feature selections such as sklearn.feature_selection.RFE can be implemented since, as mentioned in the Ex_ML_LinLogReg exercises, there are no exact implementations of step-wise AIC regression with the libraries of interest in python.\n\nfrom mlxtend.evaluate import bootstrap_point632_score\n\nnp.random.seed(346)\n\n# Fit the logistic regression model\nmodel = LogisticRegression(solver='liblinear')\n\n# Compute bootstrap point 632 scores\nscores = bootstrap_point632_score(estimator=model, X=X_train, y=y_train, n_splits=100, random_seed=123)\n\n# Print the mean accuracy and standard deviation\nprint(\"Mean accuracy:\", np.mean(scores))\nprint(\"Standard deviation:\", np.std(scores))\n\nThe results are now very close to our model in caret.",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#sub-sampling",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#sub-sampling",
    "title": "Data splitting",
    "section": "Sub-sampling",
    "text": "Sub-sampling\nBalancing using sub-sampling consists of taking all the cases in the smallest class (i.e., Yes) and extract at random the same amount of cases in the largest category (i.e., No).\n\nRPython\n\n\n\nn_yes &lt;- min(table(df_tr$visits)) ## 840\n\ndf_tr_no &lt;- filter(df_tr, visits==\"No\") ## the \"No\" cases\ndf_tr_yes &lt;- filter(df_tr, visits==\"Yes\") ## The \"Yes\" cases\n\nindex_no &lt;- sample(size=n_yes, x=1:nrow(df_tr_no), replace=FALSE) ## sub-sample 840 instances from the \"No\"\n\ndf_tr_subs &lt;- data.frame(rbind(df_tr_yes, df_tr_no[index_no,])) ## Bind all the \"Yes\" and the sub-sampled \"No\"\ntable(df_tr_subs$visits) ## The cases are balanced\n\nNow let us see the result on the accuracy measures.\n\nDoc_lr_subs &lt;- glm(visits~., data=df_tr_subs, family=\"binomial\") %&gt;% step(trace=0)\nDoc_lr_subs_prob &lt;- predict(Doc_lr_subs, newdata=df_te, type=\"response\")\nDoc_lr_subs_pred &lt;- ifelse(Doc_lr_subs_prob&gt;0.5,\"Yes\",\"No\")\nconfusionMatrix(data=as.factor(Doc_lr_subs_pred), reference = df_te$visits)\n\n\n\n\nn_yes = min(r.df_tr['visits'].value_counts()) ## 840\n\ndf_tr_no = r.df_tr[r.df_tr['visits'] == \"No\"] ## the \"No\" cases\ndf_tr_yes = r.df_tr[r.df_tr['visits'] == \"Yes\"] ## The \"Yes\" cases\n\nindex_no = np.random.choice(df_tr_no.index, size=n_yes, replace=False)\n\ndf_tr_subs = pd.concat([df_tr_yes, df_tr_no.loc[index_no]])\ndf_tr_subs['visits'].value_counts() ## The cases like R are balanced\n\nNow to the calculating the scores again:\n\nX_train_subs = pd.get_dummies(df_tr_subs.drop(columns=['visits']))\ny_train_subs = df_tr_subs['visits']\n\nlr_subs = LogisticRegression(solver='liblinear')\nlr_subs.fit(X_train_subs, y_train_subs)\nlr_subs_pred = lr_subs.predict(X_test)\nlr_subs_cf = confusion_matrix(y_test, lr_subs_pred)\n\ntn_subs, fp_subs, fn_subs, tp_subs = lr_subs_cf.ravel()\n\nspecificity_subs = tn_subs / (tn_subs + fp_subs)\nsensitivity_subs = recall_score(y_test, lr_subs_pred, pos_label='Yes')\nbalanced_acc_subs = balanced_accuracy_score(y_test, lr_subs_pred)\naccuracy_subs = accuracy_score(y_test, lr_subs_pred)\n\nprint(lr_subs_cf)\nprint(f\"Accuracy: {accuracy_subs:.3f}\")\nprint(f\"Balanced Accuracy: {balanced_acc_subs:.3f}\")\nprint(f\"Specificity: {specificity_subs:.3f}\")\nprint(f\"Sensitivity: {sensitivity_subs:.3f}\")\n\nSame conclusion as R (albeit with slightly different values).\n\n\n\nAs expected, the accuracy has decreased but the balanced accuracy has increased. Depending on the aim of the prediction, this model may be much better to use than the one trained on the unbalanced data.",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#resampling",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#resampling",
    "title": "Data splitting",
    "section": "Resampling",
    "text": "Resampling\nBalancing by resampling follows the same aim. The difference with sub-sampling is that the resampling increases the number of cases in the smallest class by resampling at random from them. The codes below are explicit:\n\nRPython\n\n\n\nn_no &lt;- max(table(df_tr$visits)) ## 3313\n\ndf_tr_no &lt;- filter(df_tr, visits==\"No\")\ndf_tr_yes &lt;- filter(df_tr, visits==\"Yes\")\n\nindex_yes &lt;- sample(size=n_no, x=1:nrow(df_tr_yes), replace=TRUE)\ndf_tr_res &lt;- data.frame(rbind(df_tr_no, df_tr_yes[index_yes,]))\ntable(df_tr_res$visits)\n\nNow, we have a balanced data set where each class has the same amount as the largest class (i.e., “No”) in the original training set. The effect on the model fit is very similar to the subsampling:\n\nDoc_lr_res &lt;- glm(visits~., data=df_tr_res, family=\"binomial\") %&gt;% step(trace=0)\nDoc_lr_res_prob &lt;- predict(Doc_lr_res, newdata=df_te, type=\"response\")\nDoc_lr_res_pred &lt;- ifelse(Doc_lr_res_prob&gt;0.5,\"Yes\",\"No\")\nconfusionMatrix(data=as.factor(Doc_lr_res_pred), reference = df_te$visits)\n\n\n\n\nn_no = max(r.df_tr['visits'].value_counts()) ## 3313\n\ndf_tr_no = r.df_tr[r.df_tr['visits'] == \"No\"]\ndf_tr_yes = r.df_tr[r.df_tr['visits'] == \"Yes\"]\n\nindex_yes = np.random.choice(df_tr_yes.index, size=n_no, replace=True)\ndf_tr_res = pd.concat([df_tr_no, df_tr_yes.loc[index_yes]])\ndf_tr_res['visits'].value_counts()\n\nNow we can model again with the resampled data\n\nX_train_res = pd.get_dummies(df_tr_res.drop(columns=['visits']))\ny_train_res = df_tr_res['visits']\n\nlr_res = LogisticRegression(solver='liblinear')\nlr_res.fit(X_train_res, y_train_res)\nlr_res_pred = lr_res.predict(X_test)\n\nlr_res_cf = confusion_matrix(y_test, lr_res_pred)\n\ntn_res, fp_res, fn_res, tp_res = lr_res_cf.ravel()\n\nspecificity_res = tn_res / (tn_res + fp_res)\nsensitivity_res = recall_score(y_test, lr_res_pred, pos_label='Yes')\nbalanced_acc_res = balanced_accuracy_score(y_test, lr_res_pred)\naccuracy_res = accuracy_score(y_test, lr_res_pred)\n\nprint(lr_res_cf)\nprint(f\"Accuracy: {accuracy_res:.3f}\")\nprint(f\"Balanced Accuracy: {balanced_acc_res:.3f}\")\nprint(f\"Specificity: {specificity_res:.3f}\")\nprint(f\"Sensitivity: {sensitivity_res:.3f}\")\n\n\n\n\nWhether one should prefer sub-sampling or resampling depends on the amount and the richness of the data.",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "lectures/08_UnsupervisedLearning/081_Clustering/ML_Clustering.html",
    "href": "lectures/08_UnsupervisedLearning/081_Clustering/ML_Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "In-class R script 📄",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  },
  {
    "objectID": "lectures/08_UnsupervisedLearning/081_Clustering/ML_Clustering.html#footnotes",
    "href": "lectures/08_UnsupervisedLearning/081_Clustering/ML_Clustering.html#footnotes",
    "title": "Clustering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOther criteria are possible. Several algorithms exists (e.g., Ward 1963, Murtagh and Legendre 2014).↩︎\nThere are 30 index available in the function NbClust in R.↩︎",
    "crumbs": [
      "Lectures",
      "Clustering"
    ]
  },
  {
    "objectID": "lectures/08_UnsupervisedLearning/080_Introduction/ML_UnsupIntro.html",
    "href": "lectures/08_UnsupervisedLearning/080_Introduction/ML_UnsupIntro.html",
    "title": "Introduction to Unsupervised Learning",
    "section": "",
    "text": "Supervised vs unsupervised learning\nSupervised learning aims to predict an outcome \\(y\\) from features \\(x\\). The quality of the model can be inspected by comparing the predictions to the true outcomes. Unsupervised learning aims to analyze the link between the features. There is no “supervisor”. Unsupervised methods can be separated in two main ones:\n\nClustering: group instances according to their similarities across the features.\nDimension reduction: link the features according to their similarities across the instances, find commonalities, and combine the features in fewer dimensions.",
    "crumbs": [
      "Lectures",
      "Intro to Unsuperised Learning"
    ]
  },
  {
    "objectID": "lectures/03_Models/032_Trees/ML_Trees.html",
    "href": "lectures/03_Models/032_Trees/ML_Trees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "In-class R script 📄",
    "crumbs": [
      "Lectures",
      "Decision Trees"
    ]
  },
  {
    "objectID": "lectures/03_Models/032_Trees/ML_Trees.html#footnotes",
    "href": "lectures/03_Models/032_Trees/ML_Trees.html#footnotes",
    "title": "Decision Trees",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor logistic regression, it is the cross-entropy.↩︎\nSeveral trees may be equivalent.↩︎",
    "crumbs": [
      "Lectures",
      "Decision Trees"
    ]
  },
  {
    "objectID": "lectures/03_Models/031_LinearLogisticRegression/ML_LinLogReg.html",
    "href": "lectures/03_Models/031_LinearLogisticRegression/ML_LinLogReg.html",
    "title": "Linear & Logistic Regression",
    "section": "",
    "text": "In-class R script 📄",
    "crumbs": [
      "Lectures",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/03_Models/031_LinearLogisticRegression/ML_LinLogReg.html#footnotes",
    "href": "lectures/03_Models/031_LinearLogisticRegression/ML_LinLogReg.html#footnotes",
    "title": "Linear & Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs opposed to prediction interval for the mean.↩︎\nA little thought on that will brings us back to the entropy.↩︎",
    "crumbs": [
      "Lectures",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "lectures/03_Models/034_SupportVectorMachine/ML_SVM.html",
    "href": "lectures/03_Models/034_SupportVectorMachine/ML_SVM.html",
    "title": "Support Vector Machines",
    "section": "",
    "text": "In-class R script 📄",
    "crumbs": [
      "Lectures",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "lectures/03_Models/034_SupportVectorMachine/ML_SVM.html#footnotes",
    "href": "lectures/03_Models/034_SupportVectorMachine/ML_SVM.html#footnotes",
    "title": "Support Vector Machines",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAgain see your favorite Optimization course.↩︎",
    "crumbs": [
      "Lectures",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "lectures/04_Metrics/ML_Metrics.html",
    "href": "lectures/04_Metrics/ML_Metrics.html",
    "title": "Metrics",
    "section": "",
    "text": "In-class R script 📄",
    "crumbs": [
      "Lectures",
      "Metrics"
    ]
  },
  {
    "objectID": "lectures/04_Metrics/ML_Metrics.html#footnotes",
    "href": "lectures/04_Metrics/ML_Metrics.html#footnotes",
    "title": "Metrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOften used in psychology↩︎\nThe calculations are the same as for the chi-square test for independence.↩︎\nThe well known German credit data↩︎\nNote that the Cohen’s kappa takes this into account.↩︎",
    "crumbs": [
      "Lectures",
      "Metrics"
    ]
  },
  {
    "objectID": "lectures/05_DataSplitting/ML_DataSplitting.html",
    "href": "lectures/05_DataSplitting/ML_DataSplitting.html",
    "title": "Data Splitting",
    "section": "",
    "text": "In-class R script 📄",
    "crumbs": [
      "Lectures",
      "Data Splitting"
    ]
  },
  {
    "objectID": "lectures/05_DataSplitting/ML_DataSplitting.html#footnotes",
    "href": "lectures/05_DataSplitting/ML_DataSplitting.html#footnotes",
    "title": "Data Splitting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMild though.↩︎\nThere is no specific name to distinguish between the first “big” training set, and the second one, that is smaller.↩︎\nTechnically, this is the method used in 1-SE rule for trees.↩︎",
    "crumbs": [
      "Lectures",
      "Data Splitting"
    ]
  }
]