%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Styles, packages and new commands
\input{../../Main/ML_Main.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Edit the title page
\title{Machine Learning}
\subtitle{Module 3.4 - Models: Support Vector Machine}
\author[MOB]{Marc-Olivier Boldi}
\institute[HEC MSc Mgt BA]{Master in Management, Business Analytics, HEC UNIL}
\date[Spring 2024]{Spring 2024}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \titlepage
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Table of Contents}
	\tableofcontents
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Concept}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Concept}
Consider a binary classification with two features (axes) 
\begin{center}
\includegraphics[width=5cm]{../../Graphs/SVM_Borders.png} 
\end{center}
The 3 dashed lines represents 3 different classifiers, equally good in terms of predictions. However, line b may be preferable...
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Concept}
Indeed, line b has more capacity to generalize. Suppose we observe a new diamond, with line a, the model would make an incorrect prediction, even if this diamond is not especially unexpected. The symmetric situation with circles would exclude line c also $=>$ line b is the most robust choice.
\begin{center}
\includegraphics[width=5cm]{../../Graphs/SVM_Borders_2.png} 
\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{The margins}
Line b is better because it has a large margin $M$. SVM are built on that principle: to look for a good classifier (good prediction) but also robust to new observations (large margin).
\begin{center}
\includegraphics[width=5cm]{../../Graphs/SVM_Borders_3.png} 
\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Geometry}
The borders can be expressed by a vector $w=(w_1,\ldots,w_p)$ and a constant $b$. The central line is all the $x=(x_1,\ldots,x_p)$ such that
$$
w_1x_1 + w_2x_2 + \cdots + w_p x_p + b = 0
$$
i.e.
$$
w^T x + b = 0.
$$
The borders are such that $w^T x + b = -1$ and $w^T x + b = 1$. In the next slide example,
\begin{itemize}
\item the number of features is $p=2$,
\item $w=(1,-1)$, 
\item $b=0$.
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Geometry}
\begin{center}
\includegraphics[width=7cm]{../../Graphs/SVM_Borders_4.pdf}
\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Geometry}
For a good separation, we generally look for a central line 
$$
w^TX + b = 0,
$$ 
at the middle of the two closest points ($A$ and $B$ on the next slide) and such that 
\begin{itemize}
\item $w^T X_A + b = -1$
\item $w^T X_B + b = 1$
\end{itemize}
In addition, we want that all the red diamonds are above the upper border, and all the blue circles are below the lower border. This translates into
\begin{itemize}
\item $w^T X_i + b \leq -1$, for all $i$ such that $Y_i$ is "red diamond".
\item $w^T X_i + b \geq 1$, for all $i$ such that $Y_i$ is "blue circle".
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Geometry}
\begin{center}
\includegraphics[width=7cm]{../../Graphs/SVM_Borders_5.pdf}
\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Geometry}
It can be shown that the distance between the two borders is the margin $M$, and can be computed as 
$$
M=2/\sqrt{w^T w}.
$$
Thus, one should look for 
\begin{itemize}
\item all the points are separated (previous inequalities) 
\item the margin $M$ is maximum or, equivalently, $w^Tw$ is minimum.
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Geometry}
To further simplify the notation, note that, in this perfect configuration, 
\begin{itemize}
\item Blue circles (below) are such that $w^T x_i + b \geq 1$
\item Red diamonds (above) are such that $w^T x_i + b \leq -1$
\end{itemize}
Thus, we assign the outcome values $y_i=-1$ to red diamonds and $y_i=1$ to blue circles, we have all these inequality can be written as
$$
y_i (w^T x_i + b) \geq 1, \quad \mbox{for all } i.
$$
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Optimization problem}
Overall, to solve the SVM, one looks for $w$ for which $w^Tw$ is minimum and 
$$
y_i (w^T x_i + b) \geq 1, \quad \mbox{for all } i.
$$
This can be written as the optimization problem
\begin{eqnarray*}
\min_{w, b} && \frac{1}{2}w^Tw  \\
\mbox{s.t. } && y_i(w^Tx_i + b) \geq 1, \quad i=1,\ldots ,n
\end{eqnarray*}
This problem can be solved using quadratic program under linear constraints (see your favorite Optimization course).\\
\vspace{0.2cm}
The solution of this problem will provide a perfect separation between the 2 categories with the largest margin.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Support vectors}
Points that are well inside their margins are not playing a big role. In fact, the margins (and hence the central line) are completely defined by the points {\bf on} the margins, i.e. those $i$ such that
$$
y_i(w^T x_i + b) = 1.
$$ 
These points are called {\bf support vectors}.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Non linearly separable case}
However, this approach is valid only if the categories are {\bf linearly perfectly separable}: there exists a line that perfectly separates the classes.\\
\vspace{0.3cm}
Most cases are non-separable: no line can reach the perfect separation.
\begin{center}
\includegraphics[width=6cm]{../../Graphs/SVM_SoftMargins.pdf}
\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Soft margins}
Under the perfect separation case, $y_i(w^Tx_i + b) \geq 1$, for all $i$.\\
\vspace{0.3cm}
In non-separable case, some $i$ are not well classified, i.e., there is a tolerance $z_i\geq 0$ such that
$$
y_i(w^Tx_i + b) \geq 1 - z_i.
$$
\begin{itemize}
\item If $z_i=0$ then the instance $i$ is correctly classified,
\item If $z_i>0$ then the instance $i$ is misclassified. 
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Soft margins}
If all the $z_i$'s are large, then all the points are misclassified. To reach a good classification, we impose a small sum of the $z_i$'s 
$$
\sum_{i=1}^n z_i
$$ 
The optimization problem is modified accordingly to
\begin{eqnarray}
\label{SVM_Primal}
\nonumber \min_{w, b, z} && \frac{1}{2}w^Tw + C\sum_{i=1}^n z_i  \\
\mbox{s.t. } && y_i(w^Tx_i + b) \geq 1-z_i, \quad i=1,\ldots ,n\\
\nonumber && z_i\geq 0, \quad i=1,\ldots ,n
\end{eqnarray}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Soft margins}
Parameter $C\geq 0$ is called the {\bf cost} and is fixed by the user.\\
\vspace{0.3cm} 
It is a way to control the tolerance to bad classification:
\begin{itemize}
\item If $C=0$, there is no penalty on the $z_i$'s, thus they will be large, and so all the points can be misclassified.
\item If $C$ is large, then most $z_i$'s will be small, thus only few misclassifications are allowed. 
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Soft margin and support vector}
With soft margins, the {\bf support vectors} are those $i$ such that
$$
y_i (x_i^Tw + b) = 1-z_i.
$$
\begin{itemize}
\item If $z_i=0$, the support vectors lies exactly on the margin and thus are correctly classified.\\
\item If $z_i > 0$, the support vector is away from the margin, on the wrong side, and is thus incorrectly classified.
\end{itemize} 
Remember that, like in the perfectly separable case, any instance $i$ is correctly classified if $z_i=0$, that is, 
$$
y_i (x_i^Tw + b) \geq 1.
$$
Therefore, a support vector can be misclassified.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{The dual problem}
An equivalent way of writing the optimization problem (\ref{SVM_Primal}) is in its dual form\footnote{Again see your favorite Optimization course.}
\begin{eqnarray}
\label{SVM_Dual}
\nonumber \max_{a} &&   \sum_{i=1}^n a_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n a_i a_j y_iy_j x_i^Tx_j\\
\mbox{s.t. } && \sum_{i=1}^n a_i y_i = 0\\
\nonumber && 0 \leq a_i \leq C, \quad i=1,\ldots ,n
\end{eqnarray}
Support vectors are $i$ such that $a_i>0$. 
\begin{itemize}
\item If $a_i < C$ then they are on their margin (equiv. $z_i=0$)
\item If $a_i=C$ then they are inside the margin (equiv. $z_i>0$).
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{The prediction formula}
If the dual is more difficult to interpret, it provides a nice prediction formula. The prediction of the class of a new instance $x$ is based on a decision score (below $\theta$ contains all the parameters of the SVM)
$$
d(x;\theta) = \sum_{i=1}^n a_i y_i x_i^Tx.
$$
The prediction then is  
$$
f(x;\theta) = \left\{
\begin{array}{rl}
1, & \mbox{if } d(x;\theta) > 0,\\
-1, & \mbox{if } d(x;\theta) < 0.\\
\end{array}
\right.
$$
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Kernel-based prediction}
The decision has an interesting form:
$$
d(x;\theta) = \sum_{i=1}^n a_i y_i x_i^Tx.
$$
It is a weighted sum of the $y_i$'s in the training set. The weights are
\begin{itemize}
\item The support vectors, $0 < a_i \leq C$, enter into the sum with more or less weights. The other cases (non-support vectors), have $a_i=0$ do not enter into the sum.
\item The vectors such that $x_i^Tx$ is large will participate more to the sum. This $x_i^Tx$ is a measure of {\bf proximity} (the larger, the more $x_i$ and $x$ are similar). 
\end{itemize}
In summary, the prediction of $y$ will be similar to the important support vectors $y_i$ for which the features $x_i$ are close to the new features $x$.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Kernel-based prediction}
The proximity $x_i^Tx$ is called a {\bf kernel}. In general,
$$
d(x;\theta) = \sum_{i=1}^n a_i y_i k(x_i,x).
$$
The dual (\ref{SVM_Dual}) is written
\begin{eqnarray*}
\nonumber \max_{a} &&   \sum_{i=1}^n a_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n a_i a_j y_iy_j k(x_i,x_j)\\
\mbox{s.t. } && \sum_{i=1}^n a_i y_i = 0\\
\nonumber && 0 \leq a_i \leq C, \quad i=1,\ldots ,n
\end{eqnarray*}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Other kernels}
Usual kernels are 
\begin{itemize}
\item Linear
$$
K(x,x') = x^Tx'
$$
\item Polynomial of degree $q$
$$
K(x,x') = \left(k_0 + \gamma x^Tx'\right)^q
$$
\item Radial basis
$$
K(x,x') = \exp\left(-\gamma (x-x')^T(x-x')\right).
$$
\item Sigmoid
$$
K(x,x') = \tanh\left(-\gamma x^Tx' + k_0\right).
$$
\end{itemize}
For each, the parameters $k_0$ and $\gamma$ are set by the user.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Underlying features}
Using a kernel other than $x^Tx'$ can be shown to be equivalent to create new features $h(x)$. New features can help in separating the cases: e.g., below a non-separable case in $(x,y)$ becomes separable by the new feature $(x^2+y^2)$:\\
\vspace{0.3cm}

\begin{columns}
\begin{column}{0.66\linewidth}
\includegraphics[width=7cm]{../../Graphs/svm_quad.pdf} 
\end{column}
\begin{column}{0.34\linewidth}
\hspace{-1cm}
\includegraphics[width=3.4cm]{../../Graphs/svm_quad2.pdf} 
\vspace{0.2cm}
\end{column}
\end{columns}
However, with the kernel, the exact form of these new features cannot be known.
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Interpretability}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Interpretability}
Support Vector Machine models are {\bf not interpretable}. \\
\vspace{0.3cm}
Unlike regressions and trees, they do not provide any way to know the association between the features and the outcome.\\
\vspace{0.3cm}
Interpretability must therefore rely on generic methods (see later in the course).
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Selection of variables}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Model complexity}
Like for regressions and trees, to apply the Occam's razor, we need to be able to control the model complexity. In the case of SVM, this is done by controlling the {\bf cost} $C$. Reminder,
\begin{itemize}
\item If $C=0$, then all the points can be misclassified. Therefore, {\bf the model is simple}.
\item If $C$ is large, then only few misclassifications are allowed. Therefore, {\bf the model is complex}.
\end{itemize}
The choice of $C$ is made by the user and can be selected as a hyperparameter using data splitting strategies (see later in the course).
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{General cases}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{SVM for multiclass}
So far, we have seen only SVM for a {\bf binary} classification. For the multiclass case ($K>2$ classes), the principle remains the same except that several classifiers are built according to one of the following strategies:
\begin{itemize}
\item One versus the rest: for a new $x$, $d_k(x;\theta)$ is the decision function for class $k$ versus all the others.  The final prediction is the class that has the highest decision value,
$$
f(x;\theta) = \arg\max_{k=1,\ldots,K} d_k(x;\theta).
$$
\item One versus one: for all the $\binom{k}{2}$ unique pair of classes $(k,k')$, build a svm on the outcomes in these two classes. Then, the prediction is obtained by voting: the predicted class $f(x;\theta)$ is the one that has the highest number of decisions in its favor.
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{SVM for regression}
This topic is outside the range of this course, but SVM can be adapted to regression with a similar property of robustness.\\
\vspace{0.3cm}
Similarly to the classification case, the final prediction is of the kernel form
$$
f(x;\theta) = \sum_{i=1}^n a_i y_i k(x_i,x).
$$ 
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{A kernel predictor}
The classification of an instance with features $x$ can be recovered as :
$$
w^Th(x) + b = \sum_{i=1}^n a_i y_i h(x)^Th(x_i) = \sum_{i=1}^n a_i y_i K(x_i,x). 
$$
Therefore, 
\begin{itemize}
\item we do not need at all the new features $h(x)$ (automatic feature engineering).
\item the prediction is a weighted sum of the $y_j$'s, for which $a_i>0$ (the support vectors), with a weight $K(x,x_i)$ measuring how $x$ and $x_i$ are similar. It is a {\bf kernel method} (similar to K-NN, but more efficient).
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Non-linearity}
What does happen if the class is not separable by a line? Then maybe a combination of the features makes it separable:
\vspace{-0.5cm}
\begin{center}
\includegraphics[width=7cm]{../../Graphs/svm_quad.pdf} 
\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Non-linear case and extension}
In this example, adding a feature $r = \sqrt{x^2+y^2}$ allowed to perfectly separate the red and black points. \\
\vspace{0.3cm}
Therefore, by adding/replacing with new features, the problem becomes linearly separable and could be solved exactly. \\
\vspace{0.3cm}
The problem now is to find the correct set on features that can be built from the original ones $x$. For the example, it is
$$
h(x,y) = \left(x, y, \sqrt{x^2+y^2}\right).
$$
However, finding appropriate $h$ can be very laborious. Fortunately, there is a trick...
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Example}
\scriptsize
4 radial kernel: $\gamma=0.1, 1, 10, 100$ 
\normalsize
\begin{center}
\includegraphics[width=4cm]{../../Graphs/SVM_radial1.pdf} 
\includegraphics[width=4cm]{../../Graphs/SVM_radial2.pdf} \\
\includegraphics[width=4cm]{../../Graphs/SVM_radial3.pdf} 
\includegraphics[width=4cm]{../../Graphs/SVM_radial4.pdf} 
\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example}
\scriptsize
Radial kernel: $\gamma=1$ and cost $C=1000, 10, 0.1, 0.0001$ 
\normalsize
\begin{center}
\includegraphics[width=4cm]{../../Graphs/SVM_cost1.pdf} 
\includegraphics[width=4cm]{../../Graphs/SVM_cost2.pdf} \\
\includegraphics[width=4cm]{../../Graphs/SVM_cost3.pdf} 
\includegraphics[width=4cm]{../../Graphs/SVM_cost4.pdf} 
\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Final words}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Final words}
\begin{itemize}
\item SVM can be extended to multi-class.
\item SVM is only for classification (no regression task).
\item SVM is singular by putting the robustness at the center of the method.
\item The mathematics are needed to understand the concept but not to apply the method.
\item All the specifications (cost, kernel, etc.) boils down to selecting hyperparameters (it can be time consuming).
\item SVM is known to be very good predictors. It used to be the best and, in some applications, are still very useful. 
\item However, SVM is difficult to interpret.
\end{itemize}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}

\begin{frame}
\frametitle{Support vectors}
{\bf Example}: classification of iris flowers\footnote{Some points have been removed to have a separable case for illustration.} from the Petal and Sepal features (sum of the length and width in the original iris data set). 
\begin{columns}
\begin{column}{0.4\textwidth}
\vspace{-2cm}

Squares show the support vectors.
\end{column}
\begin{column}{0.6\textwidth}
\includegraphics[width=6cm]{../../Graphs/SVM_iris.pdf} 
\end{column}
\end{columns}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
