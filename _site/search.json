[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This course introduces students to the discipline of statistics as a science of understanding and analyzing data. Themes include data collection, exploratory analysis, inference, and modeling. Focus on principles underlying quantitative research in social sciences, humanities, and public policy. Research projects teach the process of scientific discovery and synthesis and critical evaluation of research and statistical arguments. Readings give perspective on why in 1950, S. Wilks said, “Statistical thinking will one day be as necessary a qualification for efficient citizenship as the ability to read and write.”\nIn this course, students learn how to effectively make use of data in the face of uncertainty: how to collect data, how to analyze data, and how to use data to make inferences and conclusions about real world phenomena. Critiquing data-based claims and evaluating data-based decisions is at the core of this course. Throughout the course students acquire a conceptual understanding and mastery of statistical and quantitative reasoning tools in order to be able to make such critiques and evaluations.\nIn addition, students are presented with novel data sets and application examples on a daily basis, and they use these data to model outcomes and make inferences about unknown population characteristics. Students learn that the first step of any analysis is identifying the assumptions and conditions necessary to apply the statistical technique(s) required to answer the research question at hand. Students not only learn the mechanics of the quantitative analysis, but also how to interpret conclusions based on quantitative evidence in context of the data and the research questions as well as identifying limitations due to data collection and study design.\nFor the lab component of this course students prepare weekly lab reports presenting statistical analysis of real data. In addition, students complete two independent data analysis projects where they answer significant research questions via the analysis of real data using statistical inference and modeling tools."
  },
  {
    "objectID": "syllabus.html#course-description",
    "href": "syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "This course introduces students to the discipline of statistics as a science of understanding and analyzing data. Themes include data collection, exploratory analysis, inference, and modeling. Focus on principles underlying quantitative research in social sciences, humanities, and public policy. Research projects teach the process of scientific discovery and synthesis and critical evaluation of research and statistical arguments. Readings give perspective on why in 1950, S. Wilks said, “Statistical thinking will one day be as necessary a qualification for efficient citizenship as the ability to read and write.”\nIn this course, students learn how to effectively make use of data in the face of uncertainty: how to collect data, how to analyze data, and how to use data to make inferences and conclusions about real world phenomena. Critiquing data-based claims and evaluating data-based decisions is at the core of this course. Throughout the course students acquire a conceptual understanding and mastery of statistical and quantitative reasoning tools in order to be able to make such critiques and evaluations.\nIn addition, students are presented with novel data sets and application examples on a daily basis, and they use these data to model outcomes and make inferences about unknown population characteristics. Students learn that the first step of any analysis is identifying the assumptions and conditions necessary to apply the statistical technique(s) required to answer the research question at hand. Students not only learn the mechanics of the quantitative analysis, but also how to interpret conclusions based on quantitative evidence in context of the data and the research questions as well as identifying limitations due to data collection and study design.\nFor the lab component of this course students prepare weekly lab reports presenting statistical analysis of real data. In addition, students complete two independent data analysis projects where they answer significant research questions via the analysis of real data using statistical inference and modeling tools."
  },
  {
    "objectID": "syllabus.html#learning-objectives",
    "href": "syllabus.html#learning-objectives",
    "title": "Syllabus",
    "section": "Learning objectives",
    "text": "Learning objectives\nThe course learning objectives are as follows:\n\nRecognize the importance of data collection, identify limitations in data collection methods, and determine how they affect the scope of inference.\nUse statistical software to summarize data numerically and visually, and to perform data analysis.\nHave a conceptual understanding of the unified nature of statistical inference.\nApply estimation and testing methods to analyze single variables or the relationship between two variables in order to understand natural phenomena and make data-based decisions.\nModel numerical response variables using a single or multiple explanatory variables.\nInterpret results correctly, effectively, and in context without relying on statistical jargon.\nCritique data-based claims and evaluate data-based decisions.\nComplete research projects demonstrating mastery of statistical data analysis from exploratory analysis to inference to modeling."
  },
  {
    "objectID": "syllabus.html#prerequisites",
    "href": "syllabus.html#prerequisites",
    "title": "Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis course has no pre-requisites."
  },
  {
    "objectID": "syllabus.html#workload",
    "href": "syllabus.html#workload",
    "title": "Syllabus",
    "section": "Workload",
    "text": "Workload\nYou are expected to put in ~6 hours of work / week outside of class. Some of you will do well with less time than this, and some of you will need more."
  },
  {
    "objectID": "syllabus.html#tips-for-success",
    "href": "syllabus.html#tips-for-success",
    "title": "Syllabus",
    "section": "Tips for success",
    "text": "Tips for success\n\nComplete the reading before a new unit begins, and then review again after the unit is over.\nBe an active participant during lectures and labs.\nAsk questions - during class or office hours, or by email. Ask me, your TAs, and your classmates.\nDo the problem sets - start early and make sure you attempt and understand all questions.\nStart your project early and and allow adequate time to complete it.\nGive yourself plenty of time time to prepare a good cheat sheet for exams. This requires going through the material and taking the time to review the concepts that you’re not comfortable with.\nDo not procrastinate - don’t let a unit go by with unanswered questions as it will just make the following unit’s material even more difficult to follow."
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nReadings for the course will come from the following textbooks. They are freely available online and you do not need to purchase a physical copy of either book to succeed in this class.\n\n[ims]: Mine Çetinkaya-Rundel and Jo Hardin. Introduction to Modern Statistics. (in progress) 2nd edition. OpenIntro, 2023.\n[r4ds]: Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. R for Data Science. 2nd edition. O’Reilly, 2022."
  },
  {
    "objectID": "syllabus.html#course-community",
    "href": "syllabus.html#course-community",
    "title": "Syllabus",
    "section": "Course community",
    "text": "Course community\n\nDuke Community Standard\nAll students must adhere to the Duke Community Standard (DCS): Duke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, and accountability. Citizens of this community commit to reflect upon these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nTo uphold the Duke Community Standard, students agree:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\n\n\nInclusive community\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength, and benefit. It is my intent to present materials and activities that are respectful of diversity and in alignment with Duke’s Commitment to Diversity and Inclusion. Your suggestions are encouraged and appreciated. Please let me know ways to improve the effectiveness of the course for you personally, or for other students or student groups.\nFurthermore, I would like to create a learning environment for my students that supports a diversity of thoughts, perspectives and experiences, and honors your identities. To help accomplish this:\n\nIf you have a name that differs from those that appear in your official Duke records, please let me know! You’ll be able to note this in the Getting to know you survey.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your academic dean is an excellent resource.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please let me or a member of the teaching team know.\n\n\n\nPronouns\nPronouns are meaningful tools to communicate identities and experiences, and using pronouns supports a campus environment where all community members can thrive.\nPlease update your gender pronouns in Duke Hub. You can learn more at the Center for Sexual and Gender Diversity’s website.\n\n\nAccessibility\nIf there is any portion of the course that is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments. Students should be in touch with the Student Disability Access Office to request or update accommodations under these circumstances.\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website: sta101-f23.github.io.\nI will regularly send course announcements via email and Canvas, make sure to check one or the other of these regularly. If an announcement is sent Monday through Thursday, I will assume that you have read the announcement by the next day. If an announcement is sent on a Friday or over the weekend, I will assume that you have read it by Monday.\n\n\nWhere to get help\n\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. Please use them!\nOutside of class and office hours, any general questions about course content or assignments should be posted on the course Slack. There is a chance another student has already asked a similar question, so please check the other posts on Slack before adding a new question. If you know the answer to a question posted on Slack, I encourage you to respond!\n\nCheck out the Support page for more resources.\nI want to make sure that you learn everything you were hoping to learn from this class. If this requires flexibility, please don’t hesitate to ask.\n\nYou never owe me personal information about your health (mental or physical) but you’re always welcome to talk to me. If I can’t help, I likely know someone who can.\nI want you to learn lots of things from this class, but I primarily want you to stay healthy, balanced, and grounded during this crisis."
  },
  {
    "objectID": "syllabus.html#lectures-and-lab",
    "href": "syllabus.html#lectures-and-lab",
    "title": "Syllabus",
    "section": "Lectures and lab",
    "text": "Lectures and lab\nThe goal of both the lectures and the labs is for them to be as interactive as possible. My role as instructor is to introduce you new tools and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities throughout each lecture and lab. Attendance will not be taken during class but you are expected to attend all lecture and lab sessions and meaningfully contribute to in-class exercises and discussion.\nYou are expected to bring a laptop to each class so that you can take part in the in-class exercises. Please make sure your laptop is fully charged before you come to class as the number of outlets in the classroom will not be sufficient to accommodate everyone. See Duke LIFE loaner laptop program if you need a loaner laptop."
  },
  {
    "objectID": "syllabus.html#assessments-and-grading",
    "href": "syllabus.html#assessments-and-grading",
    "title": "Syllabus",
    "section": "Assessments and grading",
    "text": "Assessments and grading\nThe final course grade will be calculated as follows:\n\n\n\nCategory\nPercentage\n\n\n\n\nAttendance and participation\n5%\n\n\nInteractive tutorials\n5%\n\n\nLabs\n25%\n\n\nExam 1\n20%\n\n\nExam 2\n20%\n\n\nProject 1\n10%\n\n\nProject 2\n15%\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n&gt;= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60\n\n\n\nThese are upper bounds for grade cutoffs, depending on the class performance the cutoffs may be lowered but they won’t be increased.\nAll work is expected to be submitted by the deadline and there are no make ups for any missed assessments. See Section 10.2 for policies on late work.\n\nAttendance and participation\nYou are expected to be present at class meeting and actively participate in the discussion. Your attendance and participation during class, as well as your activity on the course Slack will make up a non-insignificant portion of your grade in this class. While I might sometimes call on you during the class discussion, it is your responsibility to be an active participant without being called on.\nIf you attend at least 80% of the classes, you’ll get all available points for this component.\n\n\nInteractive tutorials\nYou will be assigned a number of interactive tutorials each week from the textbook. You will be asked to submit these on a weekly basis and graded on a check/no check basis.\nMake sure to add your name and your Net ID before generating the hash. Submit these in Canvas.\nIf you’ve completed at least 80% of the tutorials, you’ll get all available points for this component.\n\n\nLabs\nIn labs, you will apply the concepts discussed in lecture to various data analysis scenarios. Labs will focus on both computation and conceptualization. Lab assignments will be completed using Quarto and submitted as PDF for grading in Gradescope. While you may collaborate with others on lab assignments, your final solution should be your own.\nLowest lab score will be dropped.\n\n\nExams\nThere will be two exams. Each exam will be comprised of two components:\n\nIn class: 75 minute in-class exam. This exam is closed book, however you are allowed to use one sheet of notes (“cheat sheet”) to the midterm and the final. This sheet must be no larger than 8 1/2 x 11, and must be prepared by you. You may use both sides of the sheet. (70% of the grade)\nTake home: Following the in class exam, you’ll have 48 hours to complete the take home portion of your exam. The take home portion will follow from the in class exam and focus on the analysis of a dataset introduced in the take home exam. (30% of the grade)\n\nThrough these exams you have the opportunity to demonstrate what you’ve learned in the course thus far. Each exam will include small analyses and computational tasks related to the content in application exercises and labs. More details about the content and structure of the exams will be discussed during the semester.\nSee Section 12 for dates and times of the exams. Exam dates cannot be changed and no make-up exams will be given. If you can’t take the exams on these dates, you should drop this class.\n\n\nProjects\nThere will be a mid-semester prediction project and a final project. The prediction project will introduce you to conducting independent analyses and writing a formal report using a pre-specified data set. The final project allows you to explore a question and data set of your own. More details about the projects will be provided during the semester. Projects will be completed in teams.\nYou will be assigned to a different team for each of your two projects. You are encouraged to sit with your teammates in lecture and you will also work with them in the lab sessions. All team members are expected to contribute equally to the completion of each project and you will be asked to evaluate your team members after each assignment is due. Failure to adequately contribute to an assignment will result in a penalty to your mark relative to the team’s overall mark.\nSee Section 12 for dates and times of project deadlines. Project deadlines cannot be changed. If you can’t be in class for the final project presentation, you should drop this class."
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic honesty\nTL;DR: Don’t cheat!\nPlease abide by the following as you work on assignments in this course:\n\nCollaboration: Only work that is clearly assigned as team work should be completed collaboratively.\n\nThe labs must also be completed individually and you are welcomed to discuss the assignment with classmates at a high level (e.g., discuss what’s the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share answers to lab questions (including any code) with anyone other than myself and the teaching assistants.\nFor the projects, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project across teams.\nOn individual assignments you may not directly share code with another student in this class, and on team assignments you may not directly share code with another team in this class.\n\nOnline resources: I am well aware that a huge volume of code is available on the web to solve any number of problems. Unless I explicitly tell you not to use something, the course’s policy is that you may make use of any online resources (e.g., StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nUse of generative artificial intelligence (AI): You should treat generative AI, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course:1 (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n✅ AI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\n❌ AI tools for narrative: Unless instructed otherwise, you may not use generative AI to write narrative on assignments. In general, you may use generative AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content.\n\n\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all Duke policies, including academic integrity (e.g., completing one’s own work, following proper citation of sources, adhering to guidance around group work projects, and more). Ignoring these requirements is a violation of the Duke Community Standard. Any questions and/or concerns regarding academic integrity can be directed to the Office of Student Conduct and Community Standards at conduct@duke.edu.\nAny violations in academic honesty standards as outlined in the Duke Community Standard and those specific to this course will\n\nautomatically result in a 0 for the assignment,\ncan further impact your overall course grade, and\nwill be reported to the Office of Student Conduct for further action.\n\n\n\nLate work & extensions\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. We understand that things come up periodically that could make it difficult to submit an assignment by the deadline.\nPolicy on late work depends on the particular course component:\n\nLabs:\n\nLate, but within 24 hours of deadline: -20% of available points.\nAny later: No credit, and we will not provide written feedback.\nNote that lowest lab score will be dropped, even if that score is a 0.\n\nExams:\n\nIn class portions of the exams can obviously not be turned in late.\nLate exams are not accepted.\n\nProjects: The following three components contribute to your project score.\n\nPresentation: Late presentations are not accepted and there are no make ups for missed presentations.\nWrite up: GitHub repositories will be closed to contributions at the deadline. If you need to submit your work late, Slack/email me to reopen your repository.\n\nLate, but within 24 hours of deadline: -20% of available points.\nAny later: No credit, and we will not provide written feedback.\n\nPeer evaluation: Late peer evaluations are not accepted. If you do not turn in your peer evaluation, you get 0 points for your own peer score as well, regardless of how your teammates have evaluated you.\n\n\n\n\nWaiver for extenuating circumstances\nIf there are circumstances that prevent you from completing a lab or homework assignment by the stated due date, you may email the Head TA (Shuo Wang, shuo.wang717@duke.edu) before the deadline to waive the late penalty. In your email, you only need to request the waiver; you do not need to provide explanation. This waiver may only be used for once in the semester, so only use it for a truly extenuating circumstance.\nIf there are circumstances that are having a longer-term impact on your academic performance, please let your academic dean know, as they can be a resource. Please let me know if you need help contacting your academic dean.\n\n\nRegrade requests\nEvery effort will be made to mark your work accurately. We are on your side, and want you to receive every point you have worked to earn. However, sometimes grading mistakes happen. If you believe that an error has been made, return the paper to the instructor within four days, stating your claim in writing.\nThe following claims will be considered for re-grading:\n\npoints are not totaled correctly;\nthe grader did not see a correct answer that is on your paper;\nyour answer is the same as the correct answer, but in a different form (e.g., you wrote a correct answer as 1/3 and the grader was looking for 0.333);\nyour answer to a free response question is essentially correct but stated slightly differently than the grader’s expectation.\n\nThe following claims will not be considered for re-grading:\n\narguments about the number of points lost;\narguments about question wording.\n\nConsidering re-grades consumes time and resources that TAs and the instructor would rather spend helping you understand material. Please bring only claims of type 1-4 to our attention.\nNote that during the regrade process your score could go up or go down or not change.\n\n\n\n\n\n\nWarning\n\n\n\nNo grades will be changed after the project presentations.\n\n\n\n\nAttendance policy\nResponsibility for class attendance rests with individual students. Since regular and punctual class attendance is expected, students must accept the consequences of failure to attend. More details on Trinity attendance policies are available here.\nHowever, there may be many reasons why you cannot be in class on a given day, particularly with possible extra personal and academic stress and health concerns this semester. All course lectures will be recorded and available to enrolled students after class. If you miss a lecture, make sure to watch the recording and review the material before the next class session. Lab time is dedicated to working on your assignments and collaborating with your teammates on your project. If you miss a lab session, make sure to communicate with your team about how you can make up your contribution. Given the technologies we use in the course, this is straightforward to do asynchronously. If you know you’re going to miss a lab session and you’re feeling well enough to do so, notify your teammates ahead of time. Overall these policies are put in place to ensure communication between team members, respect for each others’ time, and also to give you a safety net in the case of illness or other reasons that keep you away from attending class.\nNote that attendance and participation is part of your grade as well.\n\n\nAttendance policy related to COVID symptoms, exposure, or infection\nStudent health, safety, and well-being are the university’s top priorities. To help ensure your well-being and the well-being of those around you, please do not come to class if you have tested positive for COVID-19 or have possible symptoms and have not yet been tested. If any of these situations apply to you, you must follow university guidance related to the ongoing COVID-19 pandemic and current health and safety protocols. If you are experiencing any COVID-19 symptoms, contact student health (dshcheckin@duke.edu, 919- 681-9355). Learn more about current university policy related to COVID-19 at https://coronavirus.duke.edu. To keep the university community as safe and healthy as possible, you will be expected to follow these guidelines. Please reach out to me and your academic dean as soon as possible if you need to quarantine or isolate so that we can discuss arrangements for your continued participation in class.\n\n\nInclement weather policy\nIn the event of inclement weather or other connectivity-related events that prohibit class attendance, I will notify you how we will make up missed course content and work. This might entail holding the class on Zoom synchronously, we may rely on Duke’s designated make-up days, or you may be asked to watch a recording of the class.\n\n\nPolicy on video recording course content\nAll lectures will be recorded and available on Panopto, so students should not need to create their own recordings of lectures. If you feel that you need record the lectures yourself, you must get permission from me ahead of time and these recordings should be used for personal study only, no for distribution. The full policy on recording of lectures falls under the Duke University Policy on Intellectual Property Rights, available at https://policies.provost.duke.edu/docs/faculty-handbook-appendix-m-intellectual-property. Unauthorized distribution is a cause for disciplinary action by the Judicial Board."
  },
  {
    "objectID": "syllabus.html#accommodations",
    "href": "syllabus.html#accommodations",
    "title": "Syllabus",
    "section": "Accommodations",
    "text": "Accommodations\n\nAcademic accommodations\nIf you are a student with a disability and need accommodations for this class, it is your responsibility to register with the Student Disability Access Office (SDAO) and provide them with documentation of your disability. SDAO will work with you to determine what accommodations are appropriate for your situation. Please note that accommodations are not retroactive and disability accommodations cannot be provided until a Faculty Accommodation Letter has been given to me. Please contact SDAO for more information: sdao@duke.edu or access.duke.edu.\n\n\nReligious accommodations\nStudents are permitted by university policy to be absent from class to observe a religious holiday. Accordingly, Trinity College of Arts & Sciences and the Pratt School of Engineering have established procedures to be followed by students for notifying their instructors of an absence necessitated by the observance of a religious holiday. Please submit requests for religious accommodations at the beginning of the semester so that we can work to make suitable arrangements well ahead of time. You can find the policy and relevant notification form here: https://trinity.duke.edu/undergraduate/academic-policies/religious-holidays.\nNote: If you’ve read this far in the syllabus, email me a picture of your pet if you have one or your favorite meme!"
  },
  {
    "objectID": "syllabus.html#sec-important-dates",
    "href": "syllabus.html#sec-important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nMonday, August 28: Classes begin\nMonday, September 4: Labor Day - No lecture\nFriday, September 8: Drop/add ends\nWednesday, October 4: Exam 1 - In class\nFriday, October 6: Exam 1 - Take home due\nFriday, October 13: Project 1 due + Mid-semester grades reported\nMonday, October 16: Fall Break - No lecture\nFriday, November 10: Last day to withdraw with W\nWednesday, November 15: Exam 2 - In class\nFriday, November 17: Exam 2 - Take home due\nWednesday, November 22: Thanksgiving Break - No lecture\nFriday, November 24: Thanksgiving Break - No lab\nFriday, December 8: Classes end\nSaturday, December 9 - Tuesday, December 12: Reading period\nThursday, December 14, 2-5pm: Project 2 presentations\n\nFor more important dates, see the full Duke Academic Calendar."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.↩︎"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Extra credit",
    "section": "",
    "text": "Create a GitHub account at https://github.com, make sure to use your real name in your profile so that I can match your name to you. You can use whatever username you like, but here is some helpful username advice.1\nGo to https://github.com/OpenIntroStat/ims/issues/new/choose and select either “Suggestion” or “Typo”.\n\nIf Suggestion, describe in detail your suggestion, giving as much information and justification as possible.\nIf Typo, fill in the blanks to provide as much information as possible for the typo to be fixed.\n\nWhen done, hit Submit.\nLater in the course you will be asked to fill out a form where you let us know if you’ve opened any feedback issues in the GitHub repo. I’ll confirm your answers by referring back to the issues you opened, and you’ll have the opportunity to earn 2 extra points (out of 100) on each of your exams for opening “useful” issues. “Useful” can be based on the number of issues (quantity) or the depth of your feedback (quality).\nBefore opening an issue, you must the existing ones at https://github.com/OpenIntroStat/ims/issues to make sure you’re not reporting something someone else has already reported.\nIssues can refer to the text of the book or the interactive tutorials.",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#extra-credit",
    "href": "resources.html#extra-credit",
    "title": "Extra credit",
    "section": "",
    "text": "Create a GitHub account at https://github.com, make sure to use your real name in your profile so that I can match your name to you. You can use whatever username you like, but here is some helpful username advice.1\nGo to https://github.com/OpenIntroStat/ims/issues/new/choose and select either “Suggestion” or “Typo”.\n\nIf Suggestion, describe in detail your suggestion, giving as much information and justification as possible.\nIf Typo, fill in the blanks to provide as much information as possible for the typo to be fixed.\n\nWhen done, hit Submit.\nLater in the course you will be asked to fill out a form where you let us know if you’ve opened any feedback issues in the GitHub repo. I’ll confirm your answers by referring back to the issues you opened, and you’ll have the opportunity to earn 2 extra points (out of 100) on each of your exams for opening “useful” issues. “Useful” can be based on the number of issues (quantity) or the depth of your feedback (quality).\nBefore opening an issue, you must the existing ones at https://github.com/OpenIntroStat/ims/issues to make sure you’re not reporting something someone else has already reported.\nIssues can refer to the text of the book or the interactive tutorials.",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#footnotes",
    "href": "resources.html#footnotes",
    "title": "Extra credit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGitHub is a code hosting platform for collaboration and version control. You will not need to learn about GitHub for this class (it’s something you will learn in a higher level statistics or data science course) however it is also where our open-source textbook is hosted so we’re asking you to make an account on GitHub to be able to provide feedback on the book.↩︎",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Useful links",
    "section": "",
    "text": "Posit Cloud\n🔗 Posit Cloud\n\n\nLecture recordings\n🔗 Panopto\n\n\nSlack\n🔗 Slack\n\n\nGradebook\n🔗 on Canvas\n\n\nAnnouncements\n🔗 on Canvas\n\n\nTutorial submission\n🔗 on Canvas\n\n\nTextbooks\n🔗 Introduction to Modern Statistics, 2nd Edition\n🔗 R for Data Science, 2nd Edition\n\n\nPackage documentation\n🔗 tidyverse: tidyverse.org\n🔗 tidymodels: tidymodels.org",
    "crumbs": [
      "Course information",
      "Useful links"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html",
    "href": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "In this series of exercises, we illustrate PCA on the wine data already used for clustering. We first load the data.\n\n\nR\nPython\n\n\n\n\nwine &lt;- read.csv(here::here(\"labs/data/Wine.csv\"))\nrow.names(wine) &lt;- paste(\"W\", c(1:nrow(wine)), sep=\"\") \nwine[,-12] &lt;- scale(wine[,-12])\n\n\n\n\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nwine = pd.read_csv('../../data/Wine.csv')\nwine.index = [\"W\" + str(i) for i in range(1, len(wine)+1)]\nscaler = StandardScaler()\nwine.iloc[:, :-1] = scaler.fit_transform(wine.iloc[:, :-1])\n\n\n\n\nNote that here the scaling of the variables is optional. The PCA can be applied on the correlation matrix which is equivalent to use scaled features. We could alternatively use unscaled data. The results would of course be different and dependent on the scales themselves. That choice depends on the practical application.",
    "crumbs": [
      "Labs",
      "PCA"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#circle-of-correlations-and-interpretation",
    "href": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#circle-of-correlations-and-interpretation",
    "title": "Principal Component Analysis",
    "section": "Circle of correlations and interpretation",
    "text": "Circle of correlations and interpretation\nTo produces a circle of correlations:\n\n\nR\nPython\n\n\n\n\nfviz_pca_var(wine_pca)\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfeatures = wine.columns[:-1]\nloading_matrix = pd.DataFrame(pca.components_.T, columns=[f\"PC{i+1}\" for i in range(11)], index=features)\nloading_matrix = loading_matrix.iloc[:, :2]\n\n# Normalize the PCA loadings\nloading_matrix_normalized = loading_matrix / np.sqrt(np.sum(loading_matrix**2, axis=0))\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.scatter(loading_matrix_normalized[\"PC1\"], loading_matrix_normalized[\"PC2\"], color=\"r\", marker=\"o\", s=100)\n\nfor i, feature in enumerate(loading_matrix_normalized.index):\n    ax.arrow(0, 0, loading_matrix_normalized.loc[feature, \"PC1\"], loading_matrix_normalized.loc[feature, \"PC2\"], color=\"r\", alpha=0.8)\n    ax.text(loading_matrix_normalized.loc[feature, \"PC1\"], loading_matrix_normalized.loc[feature, \"PC2\"], feature, color=\"black\", fontsize=12)\n\nax.axhline(0, color=\"black\", linewidth=1)\nax.axvline(0, color=\"black\", linewidth=1)\nax.set_xlim(-1.1, 1.1)\nax.set_ylim(-1.1, 1.1)\ncircle = plt.Circle((0, 0), 1, color='gray', fill=False, linestyle=\"--\", linewidth=1)\nax.add_artist(circle)\n\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.set_title(\"Circle of Correlations\")\nax.grid()\nplt.show()\n\n\n\n\n\n\n\nNote\n\n\n\nPlease do note for this plot:\n\nValues on the x-axis have been normalized to be between -1 and 1. For the rest of the python plots, we’ll not apply this.\nTo get the same results as R, the coordinates of PC2 should be flipped. The results appear mirrored because PCA is sensitive to the orientation of the data. The PCA algorithm calculates eigenvectors as the principal components, and eigenvectors can have either positive or negative signs. This means that the orientation of the principal components can vary depending on the implementation of PCA in different libraries. You can get the same results as R with the code below:\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Normalize the PCA loadings\nloading_matrix_normalized = loading_matrix / np.sqrt(np.sum(loading_matrix**2, axis=0))\n\n# Add this if you want the same results\nloading_matrix_normalized[\"PC2\"] = -loading_matrix_normalized[\"PC2\"]\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.scatter(loading_matrix_normalized[\"PC1\"], loading_matrix_normalized[\"PC2\"], color=\"r\", marker=\"o\", s=100)\n\nfor i, feature in enumerate(loading_matrix_normalized.index):\n    ax.arrow(0, 0, loading_matrix_normalized.loc[feature, \"PC1\"], loading_matrix_normalized.loc[feature, \"PC2\"], color=\"r\", alpha=0.8)\n    ax.text(loading_matrix_normalized.loc[feature, \"PC1\"], loading_matrix_normalized.loc[feature, \"PC2\"], feature, color=\"black\", fontsize=12)\n\nax.axhline(0, color=\"black\", linewidth=1)\nax.axvline(0, color=\"black\", linewidth=1)\nax.set_xlim(-1.1, 1.1)\nax.set_ylim(-1.1, 1.1)\ncircle = plt.Circle((0, 0), 1, color='gray', fill=False, linestyle=\"--\", linewidth=1)\nax.add_artist(circle)\n\nax.set_xlabel(\"PC1\")\nax.set_ylabel(\"PC2\")\nax.set_title(\"Circle of Correlations\")\nax.grid()\nplt.show()\n\n\n\n\n\n\nThis is for the two first principal components. We see\n\nPC1 explains 31.4\\% of the variance of the data, PC2 explains 15.3\\%. In total, 46.7\\% of the variance of the data is explained by these two components.\nPC1 is positively correlated with density, negatively with alcohol. Which confirms that these two features are negatively correlated. It is also positively correlated with residual.sugar.\nPC2 is positively correlated with pH, negatively with fixed.acidity and, a little less, with citric.acid.\nFeatures with shorts arrows are not explained here: volatile.acidity, sulphates, etc.\n\nTo even better interpret the dimensions, we can extract the contributions of each features in the dimension. Below, for PC1.\n\n\nR\nPython\n\n\n\n\nfviz_contrib(wine_pca, choice = \"var\", axes = 1)\n\n\n\n\nexplained_variance = pca.explained_variance_ratio_\nsquare_loading_matrix = loading_matrix**2\ncontributions = square_loading_matrix * 100 / explained_variance[:2]\n\ncontributions = contributions[\"PC1\"].sort_values(ascending=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=contributions.values, y=contributions.index, palette=\"viridis\")\nplt.xlabel(\"Contribution (%)\")\nplt.ylabel(\"Features\")\nplt.title(\"Feature Contributions for PC1\")\nplt.show()\n\n\n\n\nWe recover our conclusions from the circle (for PC1).",
    "crumbs": [
      "Labs",
      "PCA"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#map-of-the-individual-and-biplot",
    "href": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#map-of-the-individual-and-biplot",
    "title": "Principal Component Analysis",
    "section": "Map of the individual and biplot",
    "text": "Map of the individual and biplot\nWe can represent the wines in the (PC1,PC2) map. To better interpret the map, we add on it the correlation circle: a biplot.\n\n\nR\nPython\n\n\n\n\n## fviz_pca_ind(wine_pca) ## only the individuals\nfviz_pca_biplot(wine_pca) ## biplot\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the scores for the first two principal components\nscores = wine_pca.iloc[:, :2]\n\n# Define the loadings for the first two principal components\nloadings = pca.components_.T[:, :2]\n\n# Scale the loadings by the square root of the variance\nloadings_scaled = loadings * np.sqrt(pca.explained_variance_[:2])\n\n# Calculate the scaling factor for the arrows\narrow_max = 0.9 * np.max(np.max(np.abs(scores)))\nscale_factor = arrow_max / np.max(np.abs(loadings_scaled))\n\n# Create a scatter plot of the scores\nplt.figure(figsize=(10, 8))\nplt.scatter(scores.iloc[:, 0], scores.iloc[:, 1], s=50, alpha=0.8)\n\n# Add arrows for each variable's loadings\nfor i, variable in enumerate(wine.columns[:-1]):\n    plt.arrow(0, 0, loadings_scaled[i, 0]*scale_factor, loadings_scaled[i, 1]*scale_factor, color='r', alpha=0.8)\n    plt.text(loadings_scaled[i, 0]*scale_factor, loadings_scaled[i, 1]*scale_factor, variable, color='black', fontsize=12)\n\n# Add axis labels and title\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Biplot of Wine Dataset')\n\n# Add grid lines\nplt.grid()\n\n# Show the plot\nplt.show()\n\n\n\n\nIt’s a bit difficult to see all the patterns, but for instance\n\nWine 168 has a large fixed.acidity and a low pH.\nWine 195 has a large alcohol and a low density.\netc.\n\nWhen we say “large” or “low”, it is not in absolute value but relative to the data set, i.e., “larger than the average”; the average being at the center of the graph (PC1=0, PC2=0).",
    "crumbs": [
      "Labs",
      "PCA"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#how-many-dimensions",
    "href": "labs/08_UnsupervisedLearning/082_DimensionReduction/Ex_ML_PCA.html#how-many-dimensions",
    "title": "Principal Component Analysis",
    "section": "How many dimensions",
    "text": "How many dimensions\nFor graphical representation one a single graph, we need to keep only two PCs. But if we use it to reduce the dimension of our data set, or if we want to represent the data on several graphs, then we need to know how many components are needed to reach a certain level of variance. This can be achieved by looking at the eigenvalues (screeplot).\n\n\nR\nPython\n\n\n\n\nfviz_eig(wine_pca, addlabels = TRUE, ncp=11)\n\n\n\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 12), explained_variance * 100, 'o-')\nplt.xticks(range(1, 12), [f\"PC{i}\" for i in range(1, 12)])\nplt.xlabel(\"Principal Components\")\nplt.ylabel(\"Explained Variance (%)\")\nplt.title(\"Scree Plot\")\nplt.grid()\nplt.show()\n\n\n\n\nIf we want to achieve 75\\% of representation of the data (i.e., of the variance of the data), we need 5 dimensions. This means that the three biplots below represent &gt;75\\% of the data (in fact 84.6\\%).\n\nlibrary(gridExtra)\np1 &lt;- fviz_pca_biplot(wine_pca, axes = 1:2) \np2 &lt;- fviz_pca_biplot(wine_pca, axes = 3:4) \np3 &lt;- fviz_pca_biplot(wine_pca, axes = 5:6) \ngrid.arrange(p1, p2, p3, nrow = 2, ncol=2)",
    "crumbs": [
      "Labs",
      "PCA"
    ]
  },
  {
    "objectID": "labs/07_InterpretableML/Ex_ML_VarImp.html",
    "href": "labs/07_InterpretableML/Ex_ML_VarImp.html",
    "title": "Variable Importance",
    "section": "",
    "text": "This exercise shows an example of model-agnostic variable importance for a regression problem. The dataset that we will be working with is Carseats from the ISLR library which has already been used during some of the exercises such as Ex_ML_Tree and Ex_ML_SVM. It is highly recommended that you try to implement some parts of the exercise yourself before checking the answers.\n\nLoad the data from ISLR package, then assign 90% of the data for training and the remainder for testing. Please keep in mind that we make a training split running the feature importance (instead of using the entire dataset) as we “may” want to re-train the model with only a fewer features rather than biasing our decision by also including the testing data.\nCreate three models including a linear regression, a regression tree and a support-vector machine.\n\nAnswer\n\n\n\nR\n\n\n\nlibrary(rpart)\nlibrary(e1071)\nlibrary(dplyr)\nlibrary(ISLR)\n\n# divide the data into training and testing sets\nset.seed(2022)\ncarseats_index &lt;- sample(x=1:nrow(Carseats), size=0.9*nrow(Carseats), replace=FALSE)\ncarseats_tr &lt;- Carseats[carseats_index,]\ncarseats_te &lt;- Carseats[-carseats_index,]\n\n# define a linear regression\ncarseats_lm &lt;- lm(Sales~., data=carseats_tr)\n\n# define a regression tree (you can also use `adabag::autoprune()`here\ncarseats_rt &lt;- rpart(Sales~., data=carseats_tr)\n\n# define a support-vector machine\ncarseats_svm &lt;- svm(Sales ~ ., data=carseats_tr)\n\n\n\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import fetch_openml\n\n# Use the training and test sets created in R (no easy way to get the `Carseats` data in python)\n# Convert the categorical columns to one-hot encoded ones\ncarseats_train, carseats_test = pd.get_dummies(r.carseats_tr.copy()), pd.get_dummies(r.carseats_tr.copy())\n\n# Define a linear regression\ncarseats_lr = LinearRegression()\ncarseats_lr.fit(carseats_train.drop(columns=['Sales']), carseats_train['Sales'])\n\n\n\n\nLinearRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n# Define a decision tree regressor\ncarseats_dtr = DecisionTreeRegressor()\ncarseats_dtr.fit(carseats_train.drop(columns=['Sales']), carseats_train['Sales'])\n\n\n\n\nDecisionTreeRegressor()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor() \n\n\n# Define a support-vector machine\ncarseats_svr = SVR()\ncarseats_svr.fit(carseats_train.drop(columns=['Sales']), carseats_train['Sales'])\n\n\n\n\nSVR()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  SVR?Documentation for SVRiFittedSVR()",
    "crumbs": [
      "Labs",
      "Interpretable ML"
    ]
  },
  {
    "objectID": "labs/07_InterpretableML/Ex_ML_VarImp.html#creating-an-explain-object",
    "href": "labs/07_InterpretableML/Ex_ML_VarImp.html#creating-an-explain-object",
    "title": "Variable Importance",
    "section": "Creating an explain object",
    "text": "Creating an explain object\nDALEX has an explain object which allows you to do various kind of explanatory analysis. Try reading about the required inputs for it by referring to its documentations (?DALEX::explain()) and also referring to the book mentioned at the beginning of this exercise (“Hands-on Machine Learning with R”). Create one explain object per model for the training data and set the inputs that you need which are model, data (data frame of features) and y (vector of observed outcomes). Also, you can give a caption to your model through the label argument.\n\n\nR\nPython\n\n\n\n\nlibrary(dplyr)\nlibrary(DALEX)\n\nx_train &lt;- select(carseats_tr, -Sales)\ny_train &lt;- pull(carseats_tr, Sales)\n\nexplainer_lm &lt;- DALEX::explain(model = carseats_lm, \n                                 data = x_train, \n                                 y = y_train,\n                                 label = \"Linear Regression\")\n\nexplainer_rt &lt;- DALEX::explain(model = carseats_rt,\n                               data = x_train,\n                               y = y_train,\n                               label = \"Regression Tree\")\n\nexplainer_svm &lt;- DALEX::explain(model = carseats_svm,\n                                data = x_train,\n                                y = y_train,\n                                label = \"Support Vector Machine\")\n\n\n\nTo calculate the feature importances using Python, we’ll be using the permutation_importance function from the sklearn library.\n\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the features (relevant to SVM)\nscaler = StandardScaler()\ncarseats_train_scaled = carseats_train.copy()\ncarseats_test_scaled = carseats_test.copy()\n\n# Calculate feature importances for each model\nimportance_lr = permutation_importance(carseats_lr, carseats_train.drop(columns=['Sales']), carseats_train['Sales'], n_repeats=10, random_state=2022)\nimportance_dtr = permutation_importance(carseats_dtr, carseats_train.drop(columns=['Sales']), carseats_train['Sales'], n_repeats=10, random_state=2022)\nimportance_svr = permutation_importance(carseats_svr, carseats_train.drop(columns=['Sales']), carseats_train['Sales'], n_repeats=10, random_state=2022)\n# Train the SVM model on scaled data\ncarseats_svr_scaled = SVR(kernel='linear')\ncarseats_svr_scaled.fit(carseats_train_scaled.drop(columns=['Sales']), carseats_train_scaled['Sales'])\n\n\n\n\nSVR(kernel='linear')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  SVR?Documentation for SVRiFittedSVR(kernel='linear') \n\n\nimportance_svr_scaled = permutation_importance(carseats_svr_scaled, carseats_train_scaled.drop(columns=['Sales']), carseats_train_scaled['Sales'], n_repeats=10, random_state=2022)\n\n# Print the feature importances\nimportance_df = pd.DataFrame(data={\n    'Feature': carseats_train.drop(columns=['Sales']).columns,\n    'Linear Regression': importance_lr.importances_mean,\n    'Decision Tree': importance_dtr.importances_mean,\n    'Support Vector Machine (unscaled)': importance_svr.importances_mean,\n    'Support Vector Machine (scaled)': importance_svr_scaled.importances_mean\n})\n\nprint(importance_df)\n\nYou can observe the value of scaling for SVM. The results seems to agree that Price is the most important feature.",
    "crumbs": [
      "Labs",
      "Interpretable ML"
    ]
  },
  {
    "objectID": "labs/07_InterpretableML/Ex_ML_VarImp.html#plotting-the-feature-importance",
    "href": "labs/07_InterpretableML/Ex_ML_VarImp.html#plotting-the-feature-importance",
    "title": "Variable Importance",
    "section": "Plotting the feature importance",
    "text": "Plotting the feature importance\nNow that you have created the DALEX::explain objects, we will use another function called model_parts which takes care of the feature permutation. Try reading about the function DALEX::model_parts() . The main arguments that you need to provide to the explain function are:\n\nAn explainer object (what you created above).\nB which is the number of permutations (i.e. how many times you want to randomly shuffle each column).\nThe type of scores you would like it to return (raw score vs differences vs ratios) which in this case we set to ratio and you can read more the differences in the documentation.\nN argument which you can set to N=NULL which essentially asks how many samples you would like to use for calculating the variable importance, where setting it to NULL means that we use the entire training set.\nThere is also a loss_function which by default is RMSE for regression (our case) and 1-AUC for classification, by there are a few more which you can find out about by referring to the documentations (e.g. through ?DALEX::loss_root_mean_square).\n\nAfter assigning model_parts to a variable, try plotting each model to see the most important variables. What do you see? Are there important features that are in common?\n\ncalculate_importance &lt;- function(your_model_explainer, n_permutations = 10) {\n  imp &lt;- model_parts(explainer = your_model_explainer,\n                     B = n_permutations,\n                     type = \"ratio\",\n                     N = NULL)\n  return(imp)\n}\n\nimportance_lm  &lt;- calculate_importance(explainer_lm)\nimportance_rt  &lt;- calculate_importance(explainer_rt)\nimportance_svm &lt;- calculate_importance(explainer_svm)\n\nlibrary(ggplot2)\nplot(importance_lm, importance_rt, importance_svm) +\n  ggtitle(\"Mean variable-importance ratio over 10 permutations\", \"\")",
    "crumbs": [
      "Labs",
      "Interpretable ML"
    ]
  },
  {
    "objectID": "labs/07_InterpretableML/Ex_ML_VarImp.html#svm",
    "href": "labs/07_InterpretableML/Ex_ML_VarImp.html#svm",
    "title": "Variable Importance",
    "section": "SVM",
    "text": "SVM\n\n\nR\nPython\n\n\n\nlime package does not support the SVM model from the e1071 package out of the box.You can see the list of supported models via ?model_type. There are solutions to this:\n\nRe-train your model with the caret library which we then work directly with this library (also may be good practice to build your models with caret).\n\n\n# Load the lime library\nlibrary(lime)\n\n# Create a caret model using a support vector machine\nsvm_caret_model &lt;- caret::train(Sales ~ ., data = carseats_tr, method = \"svmLinear2\", trControl = trainControl(method = \"none\"))\n\n# Predict on a test instance\ntest_instance &lt;- carseats_te[1:4, -1]\n\n# Create a lime explainer object for the SVM model\nlime_svm_explainer &lt;- lime::lime(carseats_tr[, -1], \n                                 svm_caret_model)\n\n# Explain a prediction using lime\nlime_svm_explanation &lt;- explain(test_instance, lime_svm_explainer, n_features = 10)\nplot_features(lime_svm_explanation)\n\n\nCreate custom predict_model and model_type methods for the SVM model.\n\n\n# Custom predict_model function for SVM\npredict_model.svm &lt;- function(x, newdata, type, ...) {\n  if (type == \"raw\") {\n    res &lt;- predict(x, newdata = newdata, ...)\n    return(data.frame(Response = res, stringsAsFactors = FALSE))\n  } else if (type == \"prob\") {\n    res &lt;- predict(x, newdata = newdata, ...)\n    prob &lt;- kernlab::kernel(x, newdata, j = -1)\n    return(as.data.frame(prob, check.names = FALSE))\n  }\n}\n\n# Custom model_type function for SVM\nmodel_type.svm &lt;- function(x, ...) {\n  if (x$type == \"C-classification\") {\n    return(\"classification\")\n  } else {\n    return(\"regression\")\n  }\n}\n\n# Create a LIME explainer for the SVM model\nlime_svm_explainer &lt;- lime(x_train, carseats_svm)\n\n# Choose a specific instance from the test set to explain\ntest_instance &lt;- carseats_te[1:4,-1]\n\n# Generate explanations for the chosen instance\nlime_svm_explanation &lt;- explain(test_instance, lime_svm_explainer, n_features = 10)\n\n# Visualize the explanation\nplot_features(lime_svm_explanation)\n\nYou can see the prediction plot for 4 test observations. We can see several bar charts. On the y-axis, you see the features (and their intervals), while the x-axis shows the relative strength of each feature at a given value or interval. The positive value (blue color) shows that the feature support or increases the value of the prediction, while the negative value (red color) has a negative effect or decreases the prediction value. Please note that the interpretation for each observation can be different (this explanation has been taken from this blog, which you can visit for further details).\nWe give the interpretation of the first test observation as an example. The first subplot shows that a price of less than 100 results in purchasing a higher quantity than expected. Additionally, people between the ages of 40 and 55 were most likely to buy the seat, which are people who are not too young nor too old. However, in a typical scenario, we would generally expect younger people to buy car seats, but that’s probably because of the high ages in our dataset (1st. quantile of age is around 40). If the price by the competitor (CompPrice) is also low, it’ll impact the sales units badly. Once again, please note that this is specific to the first observation (i.e., the first subplot).\nThe next element is Explanation Fit. These values indicate how well LIME explains the model, similar to an R-Squared in linear regression. Here we see the explanation Fit only has values around 0.50-0.7 (50%-70%), which can be interpreted that LIME can only explain a little about our model (in some cases, like the 3rd sub-plot, this value is extremely low). You may choose not to trust the LIME output since it only has a low Explanation Fit.\n\n\nWe’ll be using provide a small demonstration on how this can be achieved in python. Please note the same logic for the interpretation (and explanation) of the R version applies here, therefore, the code is shorter and there’s no further comment provided for its output.\nWe first need to install lime in python:\n\nreticulate::py_install(c(\"lime\"), envname = \"MLBA\", pip=TRUE)\n\nThen we can run our model in the same way as R\n\nfrom sklearn import svm\nfrom sklearn.pipeline import make_pipeline\nfrom lime import lime_tabular\nimport matplotlib.pyplot as plt\n\n# Assuming carseats_tr and carseats_te are already defined as pandas DataFrames\nX_train = carseats_train.drop(columns='Sales')\ny_train = carseats_train['Sales']\nX_test = carseats_test.drop(columns='Sales')\n\n# Create a support vector machine model\nsvm_caret_model = svm.LinearSVR(random_state=2022)\n\n# Train the model\nsvm_caret_model.fit(X_train, y_train)\n\n\n\n\nLinearSVR(random_state=2022)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  LinearSVR?Documentation for LinearSVRiFittedLinearSVR(random_state=2022) \n\n\n# Predict on a test instance\ntest_instance = X_test.iloc[0:4]\n\n# Create a lime explainer object for the SVM model\nlime_svm_explainer = lime_tabular.LimeTabularExplainer(X_train.values,\n                                                       feature_names=X_train.columns,\n                                                       class_names=['Sales'],\n                                                       mode='regression')\n\n# Explain a prediction using lime\nlime_svm_explanation = lime_svm_explainer.explain_instance(test_instance.values[0], svm_caret_model.predict, num_features=10)\n\n# Plot the features\n# plt.clf()\nlime_svm_explanation.as_pyplot_figure()\nplt.show()",
    "crumbs": [
      "Labs",
      "Interpretable ML"
    ]
  },
  {
    "objectID": "labs/07_InterpretableML/Ex_ML_VarImp.html#bonus-xgboost",
    "href": "labs/07_InterpretableML/Ex_ML_VarImp.html#bonus-xgboost",
    "title": "Variable Importance",
    "section": "Bonus: XGBoost",
    "text": "Bonus: XGBoost\nTo give you an example for a classification problem, we can also train an XGBoost using the xgboost library:\n\nlibrary(xgboost)\n\n# Load and prepare the data\ncarseats_df &lt;- Carseats\ncarseats_df$High &lt;- ifelse(carseats_df$Sales &lt;= 8, \"yes\", \"no\")\ncarseats_df$High &lt;- as.factor(carseats_df$High)\ncarseats_df$ShelveLoc &lt;- as.factor(carseats_df$ShelveLoc)\ncarseats_df$Urban &lt;- as.factor(carseats_df$Urban)\ncarseats_df$US &lt;- as.factor(carseats_df$US)\n \n# Prepare the data for xgboost (as shown in the boosting excercises)\nxgb_data &lt;- model.matrix(High ~ ., data = carseats_df)[,-1]\nxgb_label &lt;- as.numeric(carseats_df$High) - 1\nxgb_dmatrix &lt;- xgb.DMatrix(data = xgb_data, label = xgb_label)\n\n# Train a gradient boosting model\nset.seed(42)\ncarseats_xgb &lt;- xgboost(data = xgb_dmatrix, nrounds = 100, objective = \"binary:logistic\", eval_metric = \"logloss\")\n\nIdentify instances with predicted probabilities close to 1, 0, and 0.5:\n\n# LIME explanations for a gradient boosting model\nxgb_preds &lt;- predict(carseats_xgb, xgb_dmatrix)\n\nwhich.max(xgb_preds)\nwhich.min(xgb_preds)\nwhich.min(abs(xgb_preds - 0.5))\n\nGenerate LIME explanations for the selected instances:\n\n# before making the prediction, we need to also one-hot encode the categorical variables\nto_explain &lt;- data.frame(model.matrix(~.,data = carseats_df[c(120, 4, 60), -ncol(carseats_df)])[,-1])\n\n# we can finally run LIME on our results\ncarseats_lime_xgb &lt;- lime(data.frame(xgb_data), carseats_xgb, bin_continuous = TRUE, quantile_bins = FALSE)\ncarseats_expl_xgb &lt;- lime::explain(to_explain, carseats_lime_xgb, n_labels = 1, n_features = 10)\n\nVisualize the LIME explanations\n\nplot_features(carseats_expl_xgb, ncol = 2)\n\nWhat can you observe from these subplots?",
    "crumbs": [
      "Labs",
      "Interpretable ML"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html",
    "title": "Data splitting",
    "section": "",
    "text": "In this series, we practice the data splitting strategies seen in class. The data are the doctor visits, already used in previous applications: cross-validation, bootstrap, and balancing.\nWe’ll be using the DocVis dataset for this lab session.\n\nDocVis &lt;- read.csv(here::here(\"labs/data/DocVis.csv\"))\nDocVis$visits &lt;- as.factor(DocVis$visits) ## make sure that visits is a factor\n\nWe need to set aside a test set. This will be used after to check that there was no overfitting during the training of the model and to ensure that the score we have obtained generalizes outside the training data.\n\nlibrary(caret)\nlibrary(dplyr)\nset.seed(346)\nindex_tr &lt;- createDataPartition(y = DocVis$visits, p= 0.8, list = FALSE)\ndf_tr &lt;- DocVis[index_tr,]\ndf_te &lt;- DocVis[-index_tr,]\n\nNote that the splitting techniques used are applied on the training set only.",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#first-fold",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#first-fold",
    "title": "Data splitting",
    "section": "First fold",
    "text": "First fold\n\n\nR\nPython\n\n\n\nFirst, we create the folds by using the createFolds function of caret.\n\nindex_CV &lt;- createFolds(y = df_tr$visits, k=10)\nindex_CV[[1]]\n\nAs seen before, the index_CV object is a list of row indices. The first element of the list index_CV[[1]] corresponds to the first fold. It is the vector of row indices of the validation set for the first fold (i.e., the validation is made of the rows of the training set that are in this vector). All the indices that are not in index_CV[[1]] will be in the training set (for this fold).\n\ndf_cv_tr &lt;- df_tr[-index_CV[[1]],]\ndf_cv_val &lt;- df_tr[index_CV[[1]],]\n\nFor this fold, df_cv_tr is the training set (it contains 9/10 of the original training set df_tr) and df_cv_val is the validation set (it contains 1/10 of the original training set df_tr). These two sets are disjoints.\nNow, we simply fit the model with the training set and compute its accuracy on the validation set. For this exercise, we use a logistic regression with AIC-based variable selection.\n\nDoc_cv &lt;- glm(visits~., data=df_cv_tr, family=\"binomial\") %&gt;% step()\nDoc_cv_prob &lt;- predict(Doc_cv, newdata=df_cv_val, type=\"response\")\nDoc_cv_pred &lt;- ifelse(Doc_cv_prob&gt;0.5,\"Yes\",\"No\")\nconfusionMatrix(data=as.factor(Doc_cv_pred), reference = df_cv_val$visits)$overall[1]\n\n\n\n\n# Load the course python environment as usual with a r code chunks.\nlibrary(reticulate)\nuse_condaenv(\"MLBA\", required = TRUE)\n\nWe will one-hot encode the categorical variables using pd.get_dummies() and then divide the data into X_train, y_train, X_test and y_test.\n\nimport pandas as pd\n\n# One-hot encoding the categorical columns\nX_train = pd.get_dummies(r.df_tr.drop('visits', axis=1))\ny_train = r.df_tr['visits']\nX_test = pd.get_dummies(r.df_te.drop('visits', axis=1))\ny_test = r.df_te['visits']\n\nThen, we create the folds by using the KFold function of scikit-learn.\n\nfrom sklearn.model_selection import KFold\n# We setup the 10-k fold\nkf = KFold(n_splits=10, random_state=346, shuffle=True)\nfold_indices = list(kf.split(X_train, y_train))\nfirst_fold_train, first_fold_val = fold_indices[0]\n\nAs seen before, the fold_indices object is a list of tuple pairs. The first element of the list fold_indices[0] corresponds to the first fold. It is the tuple of row indices of the training set and the validation set for the first fold. All the indices that are not in first_fold_val will be in the training set (for this fold).\n\nX_cv_tr = X_train.iloc[first_fold_train, :]\ny_cv_tr = y_train.iloc[first_fold_train]\n\nX_cv_val = X_train.iloc[first_fold_val, :]\ny_cv_val = y_train.iloc[first_fold_val]\n\nThis part, will be slightly different from the R approach. Here, we fit the model with the training set and compute its accuracy on the validation set. For the python approach, we use a logistic regression with recursive feature elimination to select the best number of features.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.feature_selection import RFE\nimport numpy as np\n\nmodel = LogisticRegression(solver='liblinear')\nrfe = RFE(model)\nrfe.fit(X_cv_tr, y_cv_tr)\n\n\n\n\nRFE(estimator=LogisticRegression(solver='liblinear'))\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n  RFE?Documentation for RFEiFittedRFE(estimator=LogisticRegression(solver='liblinear')) \n\n\nestimator: LogisticRegressionLogisticRegression(solver='liblinear') \n\n LogisticRegression?Documentation for LogisticRegressionLogisticRegression(solver='liblinear') \n\n\n\n\npred_probs = rfe.predict_proba(X_cv_val)[:, 1]\nDoc_cv_pred = np.where(pred_probs &gt; 0.5, \"Yes\", \"No\")\nacc = accuracy_score(y_cv_val, Doc_cv_pred)\nacc\n\n# alternatively, you could use `cross_val_score`\n# from sklearn.model_selection import cross_val_score\n# cv_scores = cross_val_score(rfe, X_cv_tr, y_cv_tr, cv=kf, scoring=\"accuracy\")\n# cv_scores",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#loop-on-the-10-folds",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#loop-on-the-10-folds",
    "title": "Data splitting",
    "section": "Loop on the 10 folds",
    "text": "Loop on the 10 folds\nNow we repeat the previous steps for all the folds.\n\n\nR\nPython\n\n\n\nIn order to track the 10 accuracy measures obtained, we store them into a vector (acc_vec). Note also that the option trace=0 was set in the function step to avoid all the print outs of the AIC selections.\n\nacc_vec &lt;- numeric(10)\nfor (i in 1:10){\n  df_cv_tr &lt;- df_tr[-index_CV[[i]],]\n  df_cv_val &lt;- df_tr[index_CV[[i]],]\n  \n  Doc_cv &lt;- glm(visits~., data=df_cv_tr, family=\"binomial\") %&gt;% step(trace=0)\n  Doc_cv_prob &lt;- predict(Doc_cv, newdata=df_cv_val, type=\"response\")\n  Doc_cv_pred &lt;- ifelse(Doc_cv_prob&gt;0.5,\"Yes\",\"No\")\n  acc_vec[i] &lt;- confusionMatrix(data=as.factor(Doc_cv_pred), reference = df_cv_val$visits)$overall[1]\n}\nacc_vec\n\nBy definition of the CV, all the 10 validations sets in this loop are disjoints. Thus, these 10 accuracy measures are in a way representative of what can be expected on the test set, except if we are very unlucky when we created the test set.\nNow we can estimate the expected accuracy (i.e., the mean) and its variation (below we use the standard deviation).\n\nmean(acc_vec)\nsd(acc_vec)\n\nThe small SD shows that the results are reliable and that we have good chance that the model, trained on the whole training set, will have this accuracy on the test set.\n\n\nFor python, in order to track the 10 accuracy measures obtained, we store them into a list (acc_list).\n\nacc_list = []\nfor train_idx, val_idx in kf.split(X_train, y_train):\n    X_cv_tr, y_cv_tr = X_train.iloc[train_idx, :], y_train.iloc[train_idx]\n    X_cv_val, y_cv_val = X_train.iloc[val_idx, :], y_train.iloc[val_idx]\n    \n    rfe.fit(X_cv_tr, y_cv_tr)\n    pred_probs = rfe.predict_proba(X_cv_val)[:, 1]\n    Doc_cv_pred = np.where(pred_probs &gt; 0.5, \"Yes\", \"No\")\n    acc = accuracy_score(y_cv_val, Doc_cv_pred)\n    acc_list.append(acc)\n\n\n\n\nRFE(estimator=LogisticRegression(solver='liblinear'))\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n  RFE?Documentation for RFEiFittedRFE(estimator=LogisticRegression(solver='liblinear')) \n\n\nestimator: LogisticRegressionLogisticRegression(solver='liblinear') \n\n LogisticRegression?Documentation for LogisticRegressionLogisticRegression(solver='liblinear') \n\n\n\n\nacc_list\n\nOnce again, we can estimate the expected accuracy and its variation.\n\nmean_acc = np.mean(acc_list)\nstd_acc = np.std(acc_list)\nmean_acc, std_acc\n\nNow, we fit the final model using the whole training set and evaluate its performance on the test set.\n\nrfe.fit(X_train, y_train)\n\n\n\n\nRFE(estimator=LogisticRegression(solver='liblinear'))\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n  RFE?Documentation for RFEiFittedRFE(estimator=LogisticRegression(solver='liblinear')) \n\n\nestimator: LogisticRegressionLogisticRegression(solver='liblinear') \n\n LogisticRegression?Documentation for LogisticRegressionLogisticRegression(solver='liblinear') \n\n\n\n\ny_test_pred = rfe.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\ntest_cm = confusion_matrix(y_test, y_test_pred)\ntest_accuracy, test_cm",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#automated-approach",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#automated-approach",
    "title": "Data splitting",
    "section": "Automated approach",
    "text": "Automated approach\n\n\nR - caret\nPython - sklearn\n\n\n\nThe 10-CV can be easily obtained from caret. First, set up the splitting data method using the trainControl function.\n\ntrctrl &lt;- trainControl(method = \"cv\", number=10)\n\nThen pass this method to the train function (from caret). In addition, we use the model (below unhappily called “method” also) glmStepAIC which, combined with the binomial family, applies a logistic regression and a AIC-based variable selection (backward; exactly like the step function used above). Of course, we also provide the model formula.\n\nset.seed(346)\nDoc_cv &lt;- train(visits ~., data = df_tr, method = \"glmStepAIC\", family=\"binomial\",\n                    trControl=trctrl, trace=0)\nDoc_cv\n\nNote that the function “only” provides the expected accuracy and the expected kappa. It does not provides their standard deviations.\nThe final model (i.e., the model trained on the whole training set df_tr) is stored in Doc_cv$finalModel. It can be used to compute the accuracy on the test set.\n\nDoc_pred &lt;- predict(Doc_cv, newdata = df_te)\nconfusionMatrix(data=Doc_pred, reference = df_te$visits)\n\n\n\nIn python, a similar approach demonstrated in caret would be using GridSearchCV from scikit-learn. We use the same 10-CV kf object created earlier. Then pass this method to the GridSearchCV function with LogisticRegression model with RFE for feature selection with the grid of parameters we would like to search for (in this case the number of features). Then, we output the (hyper-)parameters with the best performance.\n\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = LogisticRegression(solver='liblinear')\nrfe = RFE(model)\nparam_grid = {'n_features_to_select': list(range(1, X_train.shape[1] + 1))}\ngrid_search = GridSearchCV(rfe, param_grid, scoring='accuracy', cv=kf)\ngrid_search.fit(X_train, y_train)\n\n\n\n\nGridSearchCV(cv=KFold(n_splits=10, random_state=346, shuffle=True),\n             estimator=RFE(estimator=LogisticRegression(solver='liblinear')),\n             param_grid={'n_features_to_select': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n                                                  11, 12, 13, 14, 15, 16, 17]},\n             scoring='accuracy')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=KFold(n_splits=10, random_state=346, shuffle=True),\n             estimator=RFE(estimator=LogisticRegression(solver='liblinear')),\n             param_grid={'n_features_to_select': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n                                                  11, 12, 13, 14, 15, 16, 17]},\n             scoring='accuracy') \n\n\nestimator: RFERFE(estimator=LogisticRegression(solver='liblinear')) \n\n\nestimator: LogisticRegressionLogisticRegression(solver='liblinear') \n\n LogisticRegression?Documentation for LogisticRegressionLogisticRegression(solver='liblinear') \n\n\n\n\n\nprint(grid_search.best_score_, grid_search.best_params_)\n\nThe final model (i.e., the model trained on the whole training set X_train) is stored in grid_search.best_estimator_. It can be used to compute the accuracy on the test set.\n\ny_test_pred = grid_search.best_estimator_.predict(X_test)\ntest_accuracy = accuracy_score(y_test, y_test_pred)\ntest_cm = confusion_matrix(y_test, y_test_pred)\nprint(test_accuracy, test_cm)\n\nOur results did improve compared to the last python model.",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#first-sample",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#first-sample",
    "title": "Data splitting",
    "section": "First sample",
    "text": "First sample\n\n\nR\nPython\n\n\n\nWe need to first create the replicates using the caret function createResample.\n\nset.seed(897)\nindex_boot &lt;- createResample(y=df_tr$visits, times=100)\nindex_boot[[1]]\n\nAgain, it creates a list of indices. The first element of the list, index_boot[[1]], contains the row indices that will be in the training set. Note that, it is of length 4,153. In other words, the training set during this first replicate is of the same dimension as the whole training set df_tr. Note also that, in index_boot[[1]], there are indices that are replicated. This is the bootstrap sampling process. Some rows will be replicated in the training set. This also means that some rows of df_tr will not be in index_boot[[1]]. These rows are said to be out-of-bag and form the validation set. See below the dimensions of the data frames.\n\ndf_boot_tr &lt;- df_tr[index_boot[[1]],]\ndim(df_boot_tr)\ndf_boot_val &lt;- df_tr[-index_boot[[1]],]\ndim(df_boot_val)\n\nWe now fit the data to this first sample training set.\n\nDoc_boot &lt;- glm(visits~., data=df_boot_tr, family=\"binomial\") %&gt;% step()\n\nThe accuracy is then computed with the 632-rule: first, the apparent accuracy is computed (accuracy on the sample training set), then the out-of-bag accuracy (the accuracy on the validation set), then the final accuracy estimate is the 0.368/0.632-combination of the two.\n\nDoc_boot_prob_val &lt;- predict(Doc_boot, newdata=df_boot_val, type=\"response\")\nDoc_boot_pred_val &lt;- ifelse(Doc_boot_prob_val&gt;0.5,\"Yes\",\"No\")\noob_acc &lt;- confusionMatrix(data=as.factor(Doc_boot_pred_val), reference = df_boot_val$visits)$overall[1]\n\nDoc_boot_prob_tr &lt;- predict(Doc_boot, newdata=df_boot_tr, type=\"response\")\nDoc_boot_pred_tr &lt;- ifelse(Doc_boot_prob_tr&gt;0.5,\"Yes\",\"No\")\napp_acc &lt;- confusionMatrix(data=as.factor(Doc_boot_pred_tr), reference = df_boot_tr$visits)$overall[1]\n\noob_acc ## out-of-bag accuracy\napp_acc ## apparent accuracy\n0.368*app_acc + 0.632*oob_acc ## accuracy estimate\n\n\n\nFirst, we create the replicates using the resample function from sklearn.utils. Please note that unlike caret::createResample(), in sklearn (to the best of our knowledge), there’s no method that returns a list of the samples, so with resample, we get one set of resampled data points. This doesn’t matter for now, but in the following sub-section, we will write a function to do the same thing in python.\n\nfrom sklearn.utils import resample\n\nnp.random.seed(897)\ndf_boot_tr = resample(r.df_tr, n_samples=len(r.df_tr), random_state=897)\n\nThe resample function returns a new data frames with the same number of samples as the original r.df_tr, but some rows will be replicated. This also means that some rows of r.df_tr will not be in the bootstrapped data frames These rows are said to be out-of-bag and form the validation set. See below the dimensions of the data frames.\n\ndf_boot_tr.shape\noob_mask = ~r.df_tr.index.isin(df_boot_tr.index.values)\ndf_boot_val = r.df_tr[oob_mask]\ndf_boot_val.shape\n\nThere’s a difference between the shape of df_boot_val because of the difference in random generators between R & python. If you change the number for the random generator in python (i.e., np.random.seed(897)) or in R (i.e., set.seed(897)), you’ll see that the results will be slightly different.",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#loop-on-the-100-sample",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#loop-on-the-100-sample",
    "title": "Data splitting",
    "section": "Loop on the 100 sample",
    "text": "Loop on the 100 sample\n\n\nR\nPython\n\n\n\nThe previous code is looped. The accuracy measures are stored in vectors. The code can be quite long to run.\n\noob_acc_vec &lt;- numeric(100)\napp_acc_vec &lt;- numeric(100)\nacc_vec &lt;- numeric(100)\nfor (i in 1:100){\n  df_boot_tr &lt;- df_tr[index_boot[[i]],]\n  df_boot_val &lt;- df_tr[-index_boot[[i]],]\n  \n  Doc_boot &lt;- glm(visits~., data=df_boot_tr, family=\"binomial\") %&gt;% step(trace=0)\n  Doc_boot_prob_val &lt;- predict(Doc_boot, newdata=df_boot_val, type=\"response\")\n  Doc_boot_pred_val &lt;- ifelse(Doc_boot_prob_val&gt;0.5,\"Yes\",\"No\")\n  oob_acc_vec[i] &lt;- confusionMatrix(data=as.factor(Doc_boot_pred_val), reference = df_boot_val$visits)$overall[1]\n  \n  Doc_boot_prob_tr &lt;- predict(Doc_boot, newdata=df_boot_tr, type=\"response\")\n  Doc_boot_pred_tr &lt;- ifelse(Doc_boot_prob_tr&gt;0.5,\"Yes\",\"No\")\n  app_acc_vec[i] &lt;- confusionMatrix(data=as.factor(Doc_boot_pred_tr), reference = df_boot_tr$visits)$overall[1]\n  \n  acc_vec[i] &lt;- 0.368*app_acc_vec[i] + 0.632*oob_acc_vec[i]\n}\n\nacc_vec\n\nLike for the CV, we can estimate the expected accuracy and its dispersion.\n\nmean(acc_vec)\nsd(acc_vec)\n\n\n\nIn this part of the code, we perform the bootstrap procedure with 100 samples. To that, we will implement our own function to create the index n-times for a given dataset. This ensures that we get a similar output to caret::createResample(). In this case, we apply this function to our main training dataframe 100 times.\n\ndef create_resample(data, times=100, random_seed=None):\n    # If you're not familiar with the documentation below, they are called\n    # `docstrings` and whenever you ask help for a function or see it's documentation,\n    # they are generated from that\n    \"\"\"\n    Generate bootstrap sample indices for data.\n    \n    Args:\n    - data (array-like): The data to bootstrap.\n    - times (int): The number of bootstrap samples to generate.\n    - random_seed (int): The random seed to use for reproducibility.\n    \n    Returns:\n    - samples (list of arrays): A list of times bootstrap sample indices.\n    \"\"\"\n    np.random.seed(random_seed)\n    n_samples = len(data)\n    samples = []\n    for _ in range(times):\n        indices = np.random.choice(n_samples, n_samples, replace=True)\n        samples.append(indices)\n    return samples\n\n# apply the new created function\nindex_boot = create_resample(r.df_tr, times=100, random_seed = 123)\n# we can see if we successfully replicated the sampling process 100 times\nprint(len(index_boot))\n# check if we have the correct number of rows (e.g. for the first element)\nprint(len(index_boot[0]))\n# alternatively to see this information, you can uncomment & run the code below\n# np.asarray(index_boot).shape\n\n\n\n\n\n\n\nNote\n\n\n\nOne thing to note is that we could have used the sklearn.utils.resample introduced earlier to directly get a list of dataframes with the randomly chosen indices. The issue here would be rather a computational one, as we have to extract the rows many times from the dataset and then hold all this data in memory, which is redundant. So although this may not be a problem for 100 replications, it can quickly start to become an issue if you want to replicated many more times (e.g., 100,000 times). The best approach is to get the indices, and then subset the rows only when needed.\n\nimport numpy as np\nfrom sklearn.utils import resample\n\ndef create_n_resamples(data, times, random_seed=None):\n    \"\"\"\n    Generate n_bootstraps bootstrap samples of data.\n    \n    Args:\n    - data (array-like): The data to bootstrap.\n    - n_bootstraps (int): The number of bootstrap samples to generate.\n    - random_seed (int): The random seed to use for reproducibility.\n    \n    Returns:\n    - bootstrap_samples (list of lists): A list of n_bootstraps bootstrap samples.\n    \"\"\"\n    np.random.seed(random_seed)\n    bootstrap_samples = []\n    for i in range(times):\n        sample = resample(data)\n        bootstrap_samples.append(sample)\n    return bootstrap_samples\n\ndfs_boot = create_n_resamples(r.df_tr, times=100, random_seed = 123) \n\n\n\nFor each sample, we calculate the out-of-bag accuracy, the apparent accuracy, and the final accuracy estimate using the 0.368/0.632 rule. This is done using a loop that iterates 100 times, once for each bootstrap sample. The steps that the code follows are similar to R, and are outlined below:\n\nWe set up three arrays to store the out-of-bag accuracy, the apparent accuracy, and the final accuracy estimate for each of the 100 bootstrap samples.\nIn the loop, we perform the following steps for each bootstrap sample:\n\nUse the generate a random list of indices with replacement, which forms the bootstrap training set.\nCreate a mask to extract the out-of-bag (validation) set from the original training set.\nTrain the logistic regression model with RFE on the bootstrap training set.\nCompute the out-of-bag accuracy by predicting on the validation set and comparing the predicted labels to the true labels.\nCompute the apparent accuracy by predicting on the bootstrap training set and comparing the predicted labels to the true labels.\nCalculate the final accuracy estimate for the current bootstrap sample using the 0.368/0.632 rule.\n\n\nOnce the loop is complete, the acc_vec array will contain the final accuracy estimates for all 100 bootstrap samples. We can then calculate the mean and standard deviation of these accuracy estimates to get an overall understanding of the model’s performance.\n\n\n# we one-hote encode the categorical variables\n## notice that we didn't use the argument `drop_first` before, since this is like\n## making dummy variable m - 1 where m is the number of variables you have\nr.df_tr_encoded = pd.get_dummies(r.df_tr, drop_first=True)\n\noob_acc_vec = np.zeros(100)\napp_acc_vec = np.zeros(100)\nacc_vec = np.zeros(100)\n\nfor i in range(100):\n    df_boot_tr = r.df_tr_encoded.iloc[index_boot[i]]\n    y_boot_tr = df_boot_tr[\"visits_Yes\"].astype(int)\n    X_boot_tr = df_boot_tr.drop(\"visits_Yes\", axis=1)\n\n    oob_mask = ~r.df_tr_encoded.index.isin(df_boot_tr.index.values)\n    df_boot_val = r.df_tr_encoded[oob_mask]\n    y_boot_val = df_boot_val[\"visits_Yes\"].astype(int)\n    X_boot_val = df_boot_val.drop(\"visits_Yes\", axis=1)\n\n    model = LogisticRegression(solver='liblinear')\n    rfe = RFE(model)\n    rfe.fit(X_boot_tr, y_boot_tr)\n\n    pred_probs_val = rfe.predict_proba(X_boot_val)[:, 1]\n    Doc_boot_pred_val = (pred_probs_val &gt; 0.5).astype(int)\n    oob_acc = accuracy_score(y_boot_val, Doc_boot_pred_val)\n    oob_acc_vec[i] = oob_acc\n\n    pred_probs_tr = rfe.predict_proba(X_boot_tr)[:, 1]\n    Doc_boot_pred_tr = (pred_probs_tr &gt; 0.5).astype(int)\n    app_acc = accuracy_score(y_boot_tr, Doc_boot_pred_tr)\n    app_acc_vec[i] = app_acc\n\n    acc_vec[i] = 0.368 * app_acc + 0.632 * oob_acc\n\n\n\n\nRFE(estimator=LogisticRegression(solver='liblinear'))\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n  RFE?Documentation for RFEiFittedRFE(estimator=LogisticRegression(solver='liblinear')) \n\n\nestimator: LogisticRegressionLogisticRegression(solver='liblinear') \n\n LogisticRegression?Documentation for LogisticRegressionLogisticRegression(solver='liblinear') \n\n\n\n\nprint(acc_vec)",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#automated-approach-1",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#automated-approach-1",
    "title": "Data splitting",
    "section": "Automated approach",
    "text": "Automated approach\n\n\nR - caret\nPython - sklearn & mlxtend\n\n\n\nWe only need to change the method in the trainControl function. The corresponding method is “boot632”.\n\nset.seed(346)\ntrctrl &lt;- trainControl(method = \"boot632\", number=100)\nDoc_boot &lt;- train(visits ~., data = df_tr, method = \"glmStepAIC\", family=\"binomial\",\n                   trControl=trctrl, trace = 0)\nDoc_boot\n\n\n\nAs sklearn does not offer bootstrap with the 0.632 rule, we use bootstrap_point632_score function from the mlxtend library to perform bootstrapping with the 0.632 rule for our Logistic Regression model. We will use mlxtend with R for bootstrapping with the 0.632 rule.\nPlease note for this part, we don’t make any step-wise feature selection here as in the case of caret (i.e., glmStepAIC), but similar feature selections such as sklearn.feature_selection.RFE can be implemented since, as mentioned in the Ex_ML_LinLogReg exercises, there are no exact implementations of step-wise AIC regression with the libraries of interest in python.\n\nfrom mlxtend.evaluate import bootstrap_point632_score\n\nnp.random.seed(346)\n\n# Fit the logistic regression model\nmodel = LogisticRegression(solver='liblinear')\n\n# Compute bootstrap point 632 scores\nscores = bootstrap_point632_score(estimator=model, X=X_train, y=y_train, n_splits=100, random_seed=123)\n\n# Print the mean accuracy and standard deviation\nprint(\"Mean accuracy:\", np.mean(scores))\nprint(\"Standard deviation:\", np.std(scores))\n\nThe results are now very close to our model in caret.",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#sub-sampling",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#sub-sampling",
    "title": "Data splitting",
    "section": "Sub-sampling",
    "text": "Sub-sampling\nBalancing using sub-sampling consists of taking all the cases in the smallest class (i.e., Yes) and extract at random the same amount of cases in the largest category (i.e., No).\n\n\nR\nPython\n\n\n\n\nn_yes &lt;- min(table(df_tr$visits)) ## 840\n\ndf_tr_no &lt;- filter(df_tr, visits==\"No\") ## the \"No\" cases\ndf_tr_yes &lt;- filter(df_tr, visits==\"Yes\") ## The \"Yes\" cases\n\nindex_no &lt;- sample(size=n_yes, x=1:nrow(df_tr_no), replace=FALSE) ## sub-sample 840 instances from the \"No\"\n\ndf_tr_subs &lt;- data.frame(rbind(df_tr_yes, df_tr_no[index_no,])) ## Bind all the \"Yes\" and the sub-sampled \"No\"\ntable(df_tr_subs$visits) ## The cases are balanced\n\nNow let us see the result on the accuracy measures.\n\nDoc_lr_subs &lt;- glm(visits~., data=df_tr_subs, family=\"binomial\") %&gt;% step(trace=0)\nDoc_lr_subs_prob &lt;- predict(Doc_lr_subs, newdata=df_te, type=\"response\")\nDoc_lr_subs_pred &lt;- ifelse(Doc_lr_subs_prob&gt;0.5,\"Yes\",\"No\")\nconfusionMatrix(data=as.factor(Doc_lr_subs_pred), reference = df_te$visits)\n\n\n\n\nn_yes = min(r.df_tr['visits'].value_counts()) ## 840\n\ndf_tr_no = r.df_tr[r.df_tr['visits'] == \"No\"] ## the \"No\" cases\ndf_tr_yes = r.df_tr[r.df_tr['visits'] == \"Yes\"] ## The \"Yes\" cases\n\nindex_no = np.random.choice(df_tr_no.index, size=n_yes, replace=False)\n\ndf_tr_subs = pd.concat([df_tr_yes, df_tr_no.loc[index_no]])\ndf_tr_subs['visits'].value_counts() ## The cases like R are balanced\n\nNow to the calculating the scores again:\n\nX_train_subs = pd.get_dummies(df_tr_subs.drop(columns=['visits']))\ny_train_subs = df_tr_subs['visits']\n\nlr_subs = LogisticRegression(solver='liblinear')\nlr_subs.fit(X_train_subs, y_train_subs)\n\n\n\n\nLogisticRegression(solver='liblinear')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(solver='liblinear') \n\n\nlr_subs_pred = lr_subs.predict(X_test)\nlr_subs_cf = confusion_matrix(y_test, lr_subs_pred)\n\ntn_subs, fp_subs, fn_subs, tp_subs = lr_subs_cf.ravel()\n\nspecificity_subs = tn_subs / (tn_subs + fp_subs)\nsensitivity_subs = recall_score(y_test, lr_subs_pred, pos_label='Yes')\nbalanced_acc_subs = balanced_accuracy_score(y_test, lr_subs_pred)\naccuracy_subs = accuracy_score(y_test, lr_subs_pred)\n\nprint(lr_subs_cf)\nprint(f\"Accuracy: {accuracy_subs:.3f}\")\nprint(f\"Balanced Accuracy: {balanced_acc_subs:.3f}\")\nprint(f\"Specificity: {specificity_subs:.3f}\")\nprint(f\"Sensitivity: {sensitivity_subs:.3f}\")\n\nSame conclusion as R (albeit with slightly different values).\n\n\n\nAs expected, the accuracy has decreased but the balanced accuracy has increased. Depending on the aim of the prediction, this model may be much better to use than the one trained on the unbalanced data.",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#resampling",
    "href": "labs/05_DataSplitting/Ex_ML_Data_Splitting.html#resampling",
    "title": "Data splitting",
    "section": "Resampling",
    "text": "Resampling\nBalancing by resampling follows the same aim. The difference with sub-sampling is that the resampling increases the number of cases in the smallest class by resampling at random from them. The codes below are explicit:\n\n\nR\nPython\n\n\n\n\nn_no &lt;- max(table(df_tr$visits)) ## 3313\n\ndf_tr_no &lt;- filter(df_tr, visits==\"No\")\ndf_tr_yes &lt;- filter(df_tr, visits==\"Yes\")\n\nindex_yes &lt;- sample(size=n_no, x=1:nrow(df_tr_yes), replace=TRUE)\ndf_tr_res &lt;- data.frame(rbind(df_tr_no, df_tr_yes[index_yes,]))\ntable(df_tr_res$visits)\n\nNow, we have a balanced data set where each class has the same amount as the largest class (i.e., “No”) in the original training set. The effect on the model fit is very similar to the subsampling:\n\nDoc_lr_res &lt;- glm(visits~., data=df_tr_res, family=\"binomial\") %&gt;% step(trace=0)\nDoc_lr_res_prob &lt;- predict(Doc_lr_res, newdata=df_te, type=\"response\")\nDoc_lr_res_pred &lt;- ifelse(Doc_lr_res_prob&gt;0.5,\"Yes\",\"No\")\nconfusionMatrix(data=as.factor(Doc_lr_res_pred), reference = df_te$visits)\n\n\n\n\nn_no = max(r.df_tr['visits'].value_counts()) ## 3313\n\ndf_tr_no = r.df_tr[r.df_tr['visits'] == \"No\"]\ndf_tr_yes = r.df_tr[r.df_tr['visits'] == \"Yes\"]\n\nindex_yes = np.random.choice(df_tr_yes.index, size=n_no, replace=True)\ndf_tr_res = pd.concat([df_tr_no, df_tr_yes.loc[index_yes]])\ndf_tr_res['visits'].value_counts()\n\nNow we can model again with the resampled data\n\nX_train_res = pd.get_dummies(df_tr_res.drop(columns=['visits']))\ny_train_res = df_tr_res['visits']\n\nlr_res = LogisticRegression(solver='liblinear')\nlr_res.fit(X_train_res, y_train_res)\n\n\n\n\nLogisticRegression(solver='liblinear')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(solver='liblinear') \n\n\nlr_res_pred = lr_res.predict(X_test)\n\nlr_res_cf = confusion_matrix(y_test, lr_res_pred)\n\ntn_res, fp_res, fn_res, tp_res = lr_res_cf.ravel()\n\nspecificity_res = tn_res / (tn_res + fp_res)\nsensitivity_res = recall_score(y_test, lr_res_pred, pos_label='Yes')\nbalanced_acc_res = balanced_accuracy_score(y_test, lr_res_pred)\naccuracy_res = accuracy_score(y_test, lr_res_pred)\n\nprint(lr_res_cf)\nprint(f\"Accuracy: {accuracy_res:.3f}\")\nprint(f\"Balanced Accuracy: {balanced_acc_res:.3f}\")\nprint(f\"Specificity: {specificity_res:.3f}\")\nprint(f\"Sensitivity: {sensitivity_res:.3f}\")\n\n\n\n\nWhether one should prefer sub-sampling or resampling depends on the amount and the richness of the data.",
    "crumbs": [
      "Labs",
      "Data Splitting"
    ]
  },
  {
    "objectID": "labs/03_Models/035_KNN/Ex_ML_KNN.html",
    "href": "labs/03_Models/035_KNN/Ex_ML_KNN.html",
    "title": "Models: K-Nearest Neighbors",
    "section": "",
    "text": "K-NN: a gentle introduction\nIn this first part, we apply the K-NN to the iris data set with examples in both R & python. In both cases, we first load the iris built-in data set found dplyr.\n\nlibrary(dplyr)\niris\nstr(iris)\n\n\n\nR\nPython\n\n\n\nIn R, we can model iris using the knn3 function from the caret package (named knn3 because knn is already taken by another package). This function is limited to the Euclidean distance with numerical features.\nWe first make the prediction using a 2-NN (with Euclidean distance).\n\nlibrary(caret)\nmod_knn &lt;- knn3(data=iris, Species~., k=2) ## build the K-NN model\npredict(mod_knn, newdata = iris, type=\"class\")\n\nNow, we want to know if the predictions are good. To do this, we must ensure that the model is not overfitting the data. For this, we must split the data into training and test sets (these concepts will be seen in detail in a future course). To do so, we randomly take 75% of the iris data that will be the training set, and the remaining 25% are the test set.\n\nset.seed(123) ## for replication purpose\n\nindex_tr &lt;- sample(1:nrow(iris), replace=FALSE, size=0.75*nrow(iris))\nindex_tr ## the index of the rows of iris that will be in the training set\n\niris_tr &lt;- iris[index_tr,] ## the training set\niris_te &lt;- iris[-index_tr,] ## the test set\n\nNow we can use the 2-NN to predict the test set using the training set. Note that the model is fitted on the training set, and the predictions are computed on the test set.\n\nmod_knn &lt;- knn3(data=iris_tr, Species~., k=2)\niris_te_pred &lt;- predict(mod_knn, newdata = iris_te, type=\"class\")\n\nTo compare the predictions above and the true species (the one in the test set), we can build a table. It is called a confusion matrix (again, this will be explained in detail later on).\n\ntable(Pred=iris_te_pred, Observed=iris_te[,5])\n\nThe prediction is almost perfect. It is so good that it is pointless to try to improve the prediction by changing K at that point. However, just to illustrate, below we use K=3.\n\nmod_knn &lt;- knn3(data=iris_tr, Species~., k=3)\niris_te_pred &lt;- predict(mod_knn, newdata = iris_te, type=\"class\")\n\ntable(Pred=iris_te_pred, Observed=iris_te[,5])\n\nNote that, in th formula “Species~.”, the dot means “all the other variables”. It is equivalent (but shorter) to “Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width”\n\n\nBefore running the python, we must ensure we’re using the MLBA conda (virtual) environment created during setup.Rmd. As we already installed some of the packages, it should suffice for this part of the exercise:\n\n# make sure we're using the right environment\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\npy_config()\n\nFirst, let’s load the iris dataset and print its structure. You can access the r data object by using r.iris or only the pure python data as shown below (you don’t need the chunk below):\nimport pandas as pd\nfrom sklearn.datasets import load_iris\n\niris = load_iris()\niris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\niris_df['species'] = iris.target_names[iris.target]\nprint(iris_df)\nprint(iris_df.info())\nNow, in python, we will also apply K-NN to the iris data set using the KNeighborsClassifier function from the sklearn package in python, where we set K=2. This function can be applied to both numerical and categorical features. We then make predictions with this python model.\n\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# call iris data from r\ny = r.iris[[\"Species\"]]  # select column \"Species\"\nX = r.iris.drop(columns=[\"Species\"])  # drop column \"Species\"\n\n# if you imported the data directly in python, you can instead run the commands below\n# y = iris.target\n# X = iris.data\n\nk = 2\nknn_model = KNeighborsClassifier(n_neighbors=k)\nknn_model.fit(X, y)\n\n\n\n\nKNeighborsClassifier(n_neighbors=2)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=2) \n\n\npred = knn_model.predict(X)\nprint(pred)\n\nWe will split the data again into training and test sets. We will randomly select 75% of the data for the training set and the remaining 25% for the test set. To achieve this, we will use numpy, most often used for efficient numerical computing. This split of training and test sets will only be done once in the exercises as we already created the same objects in R. Nevertheless, this is one way of dividing data into training and test sets in python:\n\nimport numpy as np\n\nnp.random.seed(123)\nindex_tr = np.random.choice(range(len(r.iris)), size=int(0.75*len(r.iris)), replace=False)\nindex_te = np.setdiff1d(range(len(r.iris)), index_tr)\n\niris_tr = r.iris.iloc[index_tr, :]\niris_te = r.iris.iloc[index_te, :]\n\nNow, we can fit the K-NN model to the training set and make predictions on the test set.\n\nknn_model = KNeighborsClassifier(n_neighbors=k)\nknn_model.fit(iris_tr.iloc[:, :-1], iris_tr.iloc[:, -1])\n\n\n\n\nKNeighborsClassifier(n_neighbors=2)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=2) \n\n\npred = knn_model.predict(iris_te.iloc[:, :-1])\nprint(pred)\n\nTo evaluate the performance of our model, we can construct a confusion matrix.\n\nfrom sklearn.metrics import confusion_matrix\n\n# for the confusion matrix in python, we need to specify the column names\nlabels = r.iris['Species'].unique()\n\n# create the confusion matrix with the labels as column names\nconf_mat = confusion_matrix(iris_te.iloc[:, -1], pred, labels=labels)\nconf_mat_df = pd.DataFrame(conf_mat, columns=labels, index=labels)\n\n# print the confusion matrix\nprint(conf_mat_df)\n\nThe prediction is almost perfect (as was the case in R), so it is not necessary to try to improve the prediction by changing k at this point. However, just for illustration, we can repeat the process with K=3.\n\nk = 3\n\nknn_model = KNeighborsClassifier(n_neighbors=k)\nknn_model.fit(iris_tr.iloc[:, :-1], iris_tr.iloc[:, -1])\n\n\n\n\nKNeighborsClassifier(n_neighbors=3)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier(n_neighbors=3) \n\n\npred = knn_model.predict(iris_te.iloc[:, :-1])\nconf_mat = confusion_matrix(iris_te.iloc[:, -1], pred, labels=labels)\nprint(pd.DataFrame(conf_mat, index=labels, columns=labels))\n\n\n\n\n\n\n\nWhy results are different in Python vs R\n\n\n\nPlease note that the training and test sets we used in python are not the same ones used in R due to a different generator for the seed (i.e., set.seed(123) is not generating the same numbers as np.random.seed(123)). Therefore, different observations were taken for the test set in the two languages. If you want to see the same performance, you can also run in both languages with the same dataset. For instance, running the code below would give you the same results:\n```{r}\nmod_knn_py &lt;- knn3(data=py$iris_tr, Species~., k=2)\niris_te_pred_py &lt;- predict(mod_knn_py, newdata = py$iris_te, type=\"class\")\ntable(Pred=iris_te_pred_py, Observed=py$iris_te[,5])\n```\n\n\n\n\n\nK-NN: regression\nIn the case of a regression task, the prediction is obtained by averaging the K nearest neighbors. To illustrate this, we will use the imports.85 data set. The aim is to predict the price using only the numerical features (categorical features are illustrated later).\nBelow, we identify the numerical columns (don’t forget to load the data first!). The new data frame is named tmp (temporary…). Then, we remove all the rows containing an NA.\n\nimports_85 &lt;- read.csv(here::here(\"labs/data/imports-85.data\"), \n                       header=FALSE, na.strings=\"?\")\n\nnames = c(\"symboling\",\"normalized.losses\",\"make\",\"fuel.type\",\n         \"aspiration\",\"num.of.doors\",\"body.style\",\"drive.wheels\",\n         \"engine.location\",\"wheel.base\",\"length\",\"width\",\n         \"height\",\"curb.weight\",\"engine.type\",\"num.of.cylinders\",\n         \"engine.size\",\"fuel.system\",\"bore\",\"stroke\",\n         \"compression.ratio\",\"horsepower\",\"peak.rpm\",\"city.mpg\",\n         \"highway.mpg\",\"price\")\n\nnames(imports_85) = names\n\ntmp &lt;- imports_85 %&gt;% select(where(is.numeric))\ntmp &lt;- filter(tmp, complete.cases(tmp))\ntmp %&gt;% head()\n\nNow, we make a 75%-25% split for our training and testing.\n\nset.seed(123)\nindex_tr &lt;- sample(1:nrow(tmp), size=0.75*nrow(tmp), replace = FALSE)\ntmp_tr &lt;- tmp[index_tr,]\ntmp_te &lt;- tmp[-index_tr,]\n\n\n\nR\nPython\n\n\n\nWe now fit the knnreg function to fit the model in R.\n\nmod_knn &lt;- knnreg(data=tmp_tr, price~., k=3)\ntmp_pred &lt;- predict(mod_knn, newdata=tmp_te)\n\nWe now graphically compare the real prices in the test set with the predicted ones (the diagonal line shows the equality between the prediction and the real price).\n\ntmp_te %&gt;% mutate(pred=tmp_pred) %&gt;%\nggplot(aes(x=price, y=pred)) + \n  geom_point() + geom_abline(slope=1, intercept = 0)\n\nIt looks like there is still room for improvement. Check for yourself if this can be improved by changing K.\n\n\nAs you already learnt how to import the data in R, and divided into training and test sets, we will use that directly (which also allows for better comparisons between modelling in R vs python).\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsRegressor\n\nX_tr = r.tmp_tr.drop(columns=[\"price\"])\ny_tr = r.tmp_tr[\"price\"]\nmod_knn_py = KNeighborsRegressor(n_neighbors=3)\nmod_knn_py.fit(X_tr, y_tr)\n\n\n\n\nKNeighborsRegressor(n_neighbors=3)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  KNeighborsRegressor?Documentation for KNeighborsRegressoriFittedKNeighborsRegressor(n_neighbors=3) \n\n\n\nX_te = r.tmp_te.drop(columns=[\"price\"])\ny_te = r.tmp_te[\"price\"]\ntmp_pred = mod_knn_py.predict(X_te)\n\nWe do the same graphical comparison in python, however, we will use matplotlib to demonstrate the results.\n\nimport matplotlib.pyplot as plt\n\n# we have to make a copy otherwise `r.temp_te` is not directly modified (i.e. we can't add a column)\ntmp_test_plt = r.tmp_te.copy()\ntmp_test_plt[\"pred\"]= tmp_pred\nfig, ax = plt.subplots()\nax.plot(tmp_test_plt[\"price\"], tmp_test_plt[\"pred\"], '.', color='black')\nax.plot(tmp_test_plt[\"price\"], tmp_test_plt[\"price\"], '-', color='blue')\nax.set_xlabel(\"Price\")\nax.set_ylabel(\"Predicted price\")\nplt.show()\n\nWe obtained the same results as R.\n\n\n\nK-NN: mixture of feature types\nIn this part, we illustrate how to incorporate categorical variables with K-NN. As a reminder, to use categorical variables, the easiest way is probably to cast them to dummy variables.\nThe code below identifies the categorical columns (i.e., non numerical), then create the dummies using and add them to the data frame. In addition, the numerical variables are standardized.\nThere are several difficulties in the code below.\n\nThe price (the outcome) is first set aside because we do not want to standardize it.\nTwo brand names (variable make) contain “-”, namely “alfa-romeo” and “mercedes-benz”. After the creation of the dummy variables, these are used as titles. However, knnreg does not support “-” in the variable names. They thus have to be removed first (in the code below they are turned to “_”).\n\n\n## tmp is imports.85 without incomplete cases and without price\nimp_comp &lt;- imports_85 %&gt;% filter(complete.cases(imports_85))\ny &lt;- imp_comp$price\nimp_comp &lt;- imp_comp %&gt;% select(!price)\n\n# we will use the standardize function shown by Marc-Olivier during the course (was called `my_fun`)\n# normalize num variable: (x - min(x))/(max(x) - min(x))\nmy_normalize &lt;- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n# additionally, you can use the `scale()` function which centers and scales variables\n\n## the numerical variables in tmp are standardized\nimp_num &lt;- imp_comp %&gt;% select(where(is.numeric)) %&gt;% my_normalize() %&gt;% as.data.frame()\n\n## the dummy variables of the numerical features of tmp are created\nlibrary(fastDummies)\nimp_dumm &lt;- imp_comp %&gt;% select(!where(is.numeric)) %&gt;% \n  dummy_cols(remove_first_dummy = FALSE, remove_selected_columns = TRUE)\n\n## These dummy variables are added to tmp\nimp_dat &lt;- data.frame(imp_num, imp_dumm)\n\n## The names of the two problematic brands are changed\nnames(imp_dat)[c(23)] &lt;- c(\"make_mercedes_benz\")\n\n## The raw price is added to tmp\nimp_dat$price &lt;- y\n\nAt this stage, tmp contains all the variables that we want. We once again divide our data into training and test sets.\n\nset.seed(123)\nindex_tr &lt;- sample(1:nrow(imp_dat), size=0.75*nrow(imp_dat), replace = FALSE)\nimp_tr &lt;- imp_dat[index_tr,]\nimp_te &lt;- imp_dat[-index_tr,]\n\n\n\nR\nPython\n\n\n\nWe can now run our 3-NN directly, like in the previous exercise.\n\nmod_knn &lt;- knnreg(data=imp_tr, price~., k=3)\nimp_pred &lt;- predict(mod_knn, newdata=imp_te)\nimp_te %&gt;% mutate(pred=imp_pred) %&gt;%\n  ggplot(aes(x=price, y=pred)) + \n  geom_point() + geom_abline(slope=1, intercept = 0)\n\nHere, like always, it is difficult to compare the scales of the scatterplot, and thus to tell if the quality of that 3-NN is better or worst than the previous one. The issue of model scoring will be studied later in the course. For now, it is just about being able to mix categorical and numeric variables in a K-NN.\n\n\nThe same approach in Python with the same.\n\nX_tr = r.imp_tr.drop(columns=[\"price\"])\ny_tr = r.imp_tr[\"price\"]\nmod_knn_py = KNeighborsRegressor(n_neighbors=3)\nmod_knn_py.fit(X_tr, y_tr)\n\n\n\n\nKNeighborsRegressor(n_neighbors=3)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  KNeighborsRegressor?Documentation for KNeighborsRegressoriFittedKNeighborsRegressor(n_neighbors=3) \n\n\nX_te = r.imp_te.drop(columns=[\"price\"])\ny_te = r.imp_te[\"price\"]\ntmp_pred = mod_knn_py.predict(X_te)\n\ntmp_test_plt = r.imp_te.copy()\ntmp_test_plt[\"pred\"]= tmp_pred\nfig, ax = plt.subplots()\nax.plot(tmp_test_plt[\"price\"], tmp_test_plt[\"pred\"], '.', color='black')\nax.plot(tmp_test_plt[\"price\"], tmp_test_plt[\"price\"], '-', color='blue')\nax.set_xlabel(\"Price\")\nax.set_ylabel(\"Predicted price\")\nplt.show()\n\n\n\n\nAnalysis of nursing home data\nNow it is your turn. Develop a K-NN model to predict the cost using the other variables. Inspect the quality of the prediction using a training set and a test set.",
    "crumbs": [
      "Labs",
      "K-Nearest Neighbors"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html",
    "title": "Models: Neural Networks",
    "section": "",
    "text": "In this tutorial, we will demonstrate how to use keras and tensorflow in R for two common tasks in deep learning (DL): regression for structured data and image classification. We will use two datasets for this purpose: the Boston Housing dataset for regression and the CIFAR-10 dataset for image classification.",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#boston-housing-dataset",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#boston-housing-dataset",
    "title": "Models: Neural Networks",
    "section": "Boston Housing Dataset",
    "text": "Boston Housing Dataset\nThe Boston Housing dataset is a well-known dataset used for regression tasks. It contains 506 instances and 13 features, including the median value of owner-occupied homes in $1000s. We will use this dataset to demonstrate how to perform regression on the sales price.\n\nlibrary(keras)\n\nhouses &lt;- dataset_boston_housing()\n\ntrain_x &lt;- houses$train$x\ntrain_y &lt;- houses$train$y\n\ntest_x &lt;- houses$test$x\ntest_y &lt;- houses$test$y",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#data-preparation",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#data-preparation",
    "title": "Models: Neural Networks",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe need to normalize the data. This is especially relevant for neural networks to stabilize the computation of the gradients and consequently improve the training process.\n\nmeans &lt;- apply(train_x, 2, mean)\nsds &lt;- apply(train_x, 2, sd)\n\ntrain_x &lt;- scale(train_x, means, sds)\ntest_x &lt;- scale(test_x, means, sds)",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-definition",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-definition",
    "title": "Models: Neural Networks",
    "section": "Model Definition",
    "text": "Model Definition\nWe will use a simple neural network with two hidden layers to perform regression.\n\n\n\n\n\n\nLearning about some of the hyperparameters\n\n\n\nYou have many options for different hyperparameters; however, one lab session barely scratches the surface of DL and its hyperparameters. There are a couple of points that we need to specify here:\n\nYou can use any activation function in the middle layers, from a simple linear regression (leaner) to more common ones such as hyperbolic tagnet or tanh (often suitable for tabular data) to more sophisticated ones such as rectified linear unit or relu (better suited to high dimensional data). What is imperative is that in the last dense layer, the number of units and the activation function determine the kind of task (e.g., classification, regression, etc.) you’re trying to accomplish. If you’re doing a regression, the last dense layer has to have 1 unit and a linear activation function. If you’re doing a binary classification (logistic regression), you still use 1 dense unit but must apply the sigmoid activation function. If you’re doing multi-class classification, then the number of dense units must equal the number of classes, and the activation function is softmax (which is a generalization of sigmoid for multiple classes). Google also provides a nice visual that explains the difference between the classification models. If you remove the &gt;0.5 rule (i.e., sigmoid), you essentially get a linear regression for that layer.\n\n\n\n\n\n\n\n\n\n\nIt is imperative that, depending on the task, you use the correct type of loss function. For instance, you can use mean squared error (mse) for regression and categorical cross-entropy for multi-class classification.\n\nAs mentioned, during the ML course, we cannot cover the details of DL. Suppose you want to understand these hyperparameters better and learn about many new ones. In that case, you can check out the video recordings and the slides for the Deep learning course previously taught at HEC (last given in 2021).\n\n\n\n\nR\nPython\n\n\n\n\n# we set a seed (this sometimes disables GPU performance)\ntensorflow::set_random_seed(123)\n\n# define the model structure\nmodel_reg &lt;- keras_model_sequential() %&gt;%\n  layer_dense(units = 64, activation = \"relu\", input_shape = c(ncol(train_x))) %&gt;%\n  layer_dense(units = 64, activation = \"relu\") %&gt;%\n  layer_dense(units = 1, activation = \"linear\")\n\n# compile the model\nmodel_reg %&gt;% compile(\n  optimizer = \"adam\",\n  loss = \"mean_squared_error\",\n  metrics = c(\"mean_absolute_error\")\n)\n\n\n\nWe can run the same thing in Python with the datasets created in R, e.g. r.train_x.\n\n# import necessary libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Set random seed\ntf.random.set_seed(123)\n\n# define the model structure\nmodel_reg = keras.Sequential([\n    layers.Dense(units=64, activation='relu', input_shape=(r.train_x.shape[1],)),\n    layers.Dense(units=64, activation='relu'),\n    layers.Dense(units=1, activation='linear')\n])\n\n# compile the model\nmodel_reg.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-training",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-training",
    "title": "Models: Neural Networks",
    "section": "Model Training",
    "text": "Model Training\nWe can then train the model:\n\n\nR\nPython\n\n\n\n\nhistory &lt;- model_reg %&gt;% fit(\n  train_x, train_y,\n  epochs = 10, #300 to get the best results\n  batch_size = 32,\n  validation_split = 0.2,\n  callbacks = callback_early_stopping(patience = 30,restore_best_weights = T),\n  verbose = 1 # to control the printing of the output\n)\n\nYou don’t need to assign model %&gt;% fit() to history. We only do that to plot the results later (e.g., with plot(history).\n\n\n\n# import necessary libraries\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# fit the model\nmodel_reg.fit(\n    r.train_x, r.train_y,\n    epochs=10, #300\n    batch_size=32,\n    validation_split=0.2,\n    callbacks=[EarlyStopping(patience=30,restore_best_weights=True)],\n    verbose=1\n)",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-evaluation",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-evaluation",
    "title": "Models: Neural Networks",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nFinally, we can evaluate the model on the testing set:\n\n\nR\nPython\n\n\n\n\nnn_results &lt;- model_reg %&gt;% evaluate(test_x, test_y)\nnn_results\n# alternatively, you can use the `predict` attribute from `model` as shown below\n# nn_results &lt;- model$predict(test_x)\n# caret::MAE(obs = test_y, pred = as.vector(nn_results))\n\n\n\n\nnn_results = model_reg.evaluate(r.test_x, r.test_y)\nprint(nn_results)\n\n\n\n\nTo put these results (from R) in context, we can compare it with a simple linear regression:\n\ndf_tr &lt;- data.frame(train_y, train_x)\nlm_mod &lt;- lm(train_y ~ ., data = df_tr)\nlm_preds &lt;- as.vector(predict(lm_mod, newdata=data.frame(test_x))) # `predict()` for lm doesn't accept matrices\n\n# calculate mean absolute error with caret `MAE` installed in the `K-NN excercises`\ncaret::MAE(obs = test_y, pred = lm_preds)\n\nWe see that the neural network does significantly better than a regression model. We can also compare it with the trees seen in the CART series.\n\nlibrary(rpart)\nset.seed(123)\ntree_model &lt;- rpart(train_y ~ ., data=df_tr)\ntree_preds &lt;- predict(tree_model,newdata = data.frame(test_x))\n\n# calculate the performance on tree\ncaret::MAE(obs = test_y, pred = tree_preds)\n\nThe neural network also outperforms CART (if you run NN for 300 epochs). This is due to multiple reasons, including a higher complexity of the neural network (more parameters), using a validation set, and so on. You will learn about ensemble methods (beginning and boosting) in the upcoming lectures. Ensemble methods are the true champions for structured data (at least as of 2023) and usually outperform neural networks for structured/low-dimensional data.",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#cifar-10-dataset",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#cifar-10-dataset",
    "title": "Models: Neural Networks",
    "section": "CIFAR-10 Dataset",
    "text": "CIFAR-10 Dataset\nThe CIFAR-10 dataset is a well-known dataset used for image classification tasks. It contains 50,000 training images and 10,000 testing images of size 32x32 pixels, each belonging to one of ten classes. We will use this dataset to demonstrate how to perform image classification.\nIn this case, we had our inputs already prepared, but if it was not the case, you can always do:\n\n# Load the CIFAR-10 dataset (it'll take a while for it to download)\ncifar10 &lt;- dataset_cifar10()\n\n# Split the data into training and testing sets\ntrain_images &lt;- cifar10$train$x\ntrain_labels &lt;- cifar10$train$y\ntest_images &lt;- cifar10$test$x\ntest_labels &lt;- cifar10$test$y\n\nIf you do an str(train_images), you can see that the output is a four-dimensional array. In this array, the first element is the number of images in the training set, the second and third elements represent the (pixel values) for each image’s width and height, respectively, and the last element is the color channel of the image. If you had a black-and-white image (called greyscale), the number of channels would be just one. Typically, an image in color is represented with a combination of three colors known as RGB (red, green, and blue). Therefore, here we have three elements associated with the pixel number for each color, and their combination provides us with the colors you see in the image.",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#data-preparation-1",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#data-preparation-1",
    "title": "Models: Neural Networks",
    "section": "Data Preparation",
    "text": "Data Preparation\nBefore building our image classification model, we need to prepare the data by normalizing the values to obtain pixel values between 0-1. Similar to standardization of tabular data, this helps to avoid exploding/vanishing gradients and can improve the convergence of the model.\n\n# Normalize the pixel values\ntrain_images &lt;- train_images / 255\ntest_images &lt;- test_images / 255\n\nWe can also plot some of the example images.\n\n# Plot the first 9 images\nlabel_names &lt;- c(\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")\n\npar(mfrow = c(3, 3))\nfor (i in 1:9) {\n  img &lt;- train_images[i,,,]\n  # label &lt;- train_labels[i]\n  label &lt;- label_names[train_labels[i] + 1]  # Add 1 to convert from 0-based to 1-based indexing\n  img &lt;- array_reshape(img, c(32, 32, 3))\n  plot(as.raster(img))\n  title(paste(\"Label:\", label))\n}\n\nWe can now one-hot encode our labels to be better adapted to the loss function we will be using (otherwise you can use “sparse” variation of the loss functions).\n\ntrain_labels &lt;- to_categorical(train_labels)\ntest_labels &lt;- to_categorical(test_labels)",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-definition-1",
    "href": "labs/03_Models/033_NeuralNetworks/EX_ML_NN.html#model-definition-1",
    "title": "Models: Neural Networks",
    "section": "Model definition",
    "text": "Model definition\nWe can build our image classification model using keras by defining the layers of the neural network. For this task we’ll be using Convolutional neural networks (CNNs). CNNs are a type of deep learning model that are commonly used for image and video processing. They consist of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The convolutional layers use filters to extract features from the input image, and the pooling layers downsample the output of the convolutional layers. The fully connected layers combine the extracted features to produce the final output of the network. CNNs have been shown to be effective in a wide range of applications, including image classification, object detection, and semantic segmentation.\n\n# Set a seed again for reproducibility\ntensorflow::set_random_seed(123)\n\n# Define the model\nmodel_cnn &lt;-\n  keras_model_sequential()%&gt;%\n  layer_conv_2d(filters = 32, kernel_size = c(3, 3), input_shape = c(32, 32, 3),  activation = \"relu\") %&gt;%\n  layer_max_pooling_2d(pool_size = c(2, 2)) %&gt;%\n  layer_conv_2d(filters = 64, kernel_size = c(5, 5), activation = \"relu\") %&gt;%\n  layer_max_pooling_2d(pool_size = c(3, 3)) %&gt;%\n  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = \"relu\") %&gt;%\n  layer_flatten() %&gt;%\n  layer_dense(units = 64, activation = \"relu\") %&gt;%\n  layer_dense(units = 10, activation = \"softmax\")\n\n# Compile the model\nmodel_cnn %&gt;% compile(\n  loss = \"categorical_crossentropy\",\n  optimizer = optimizer_adam(),\n  metrics = c(\"accuracy\")\n)\n\nDepending on youer computer, this may take some time to run.\n\nhistory &lt;- model_cnn %&gt;% fit(\n  x = train_images,\n  y = train_labels,\n  epochs = 10,\n  batch_size = 64,\n  # validation_split = 0.2\n  validation_data= list(test_images, test_labels)\n)\n\n\nmodel_cnn %&gt;% evaluate(\n  x = test_images,\n  y = test_labels\n)\n\nYou may notice that we did not obtain extremely high accuracy, but this is okay for multiple reasons:\n\nWe only trained the model for a few epochs and stopped the training very early. You’re welcome to let it run for more epochs.\nWe’re dealing with at least ten classes; this is a rather complicated problem.\nIn DL, you can define many different architectures and hyperparameters, and since we did not play around with these values, this could also explain the lower performance.",
    "crumbs": [
      "Labs",
      "Neural Networks"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html",
    "title": "Models: Linear and logistic regressions",
    "section": "",
    "text": "The dataset we’ll be using for the first part of the exercise is real estate transaction prices in Taiwan, which can be accessed from this link this link. This dataset was modified for this exercise. The modified file real_estate_data.csv is in the exercise folder under /data/.\nThe aim is to predict the house prices from available features: No, Month, Year, TransDate, HouseAge, Dist, NumStores, Lat, Long, Price. No is the transaction number and will not be used.\n\nFirst, an EDA of the data is needed. After exploring the structure, the Price is shown with the year and month.\n\nreal_estate_data &lt;- read.csv(here::here(\"labs/data/real_estate_data.csv\"))\n\n## adapt the path to the data\n# if you encountered any error with the encoding of the data (`Error in gregexpr...`), just re-run the code again\nstr(real_estate_data)\nlibrary(summarytools)\ndfSummary(real_estate_data)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nreal_estate_data %&gt;% ggplot(aes(x=Month, y=Price, fill=as.factor(Year))) + \n  geom_boxplot()+ facet_wrap(~as.factor(Year))\n\nThe results show how important it is to make an EDA! It appears that the data does not contain transactions for all the months of 2012 and 2013, but just some months by the end of 2012 and the first half of 2013. This shows that it is pointless to use month and year here. This is why we prefer TransDate, a value indicating the transaction time on a linear scale (e.g., 2013.250 is March 2013).\nNow we focus on the link between Price and the other features.\n\nlibrary(GGally)\nreal_estate_data %&gt;% \n  select(Price, HouseAge, Dist, Lat, Long, TransDate) %&gt;% \n  ggpairs()\n\nNo clear link appears. The linear regression will help to discover if a combination of the features can predict the price.\n\nFirst, we split the data into training/test set (75/25).\n\nset.seed(234)\nindex &lt;- sample(x=c(1,2), size=nrow(real_estate_data), replace=TRUE, prob=c(0.75,0.25)) # 1==training set, 2==test set\ndat_tr_restate &lt;- real_estate_data[index==1,]\ndat_te_restate &lt;- real_estate_data[index==2,]\n\nThen, we fit the linear regression to the training set.\n\n\nR\nPython\n\n\n\n\nmod_lm &lt;- lm(Price~TransDate+\n               HouseAge+\n               Dist+\n               NumStores+\n               Lat+\n               Long, data=dat_tr_restate)\nsummary(mod_lm)\n\n\n\n\n# In R, we load the conda environment as usual\nlibrary(reticulate)\nreticulate::use_condaenv(\"MLBA\", required = TRUE)\ngc(full = TRUE)\n\nIn python, we then use the statsmodels library to fit a linear regression model to the training data and perform feature elimination. We use the .fit() method to fit the model with the formula for the variable names. Note that python’s summary() function is unique to the statsmodels libraries and produces similar information to its R counterpart.\n\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"\n\nimport mkl\n# %env OMP_NUM_THREADS=1\n# set the number of threads. Here we set it to 1 to avoid parallelization when rendering quarto, but you can set it to higher values.\nmkl.set_num_threads(1)\n\n\n# Import necessary library\nimport statsmodels.formula.api as smf\n\n# Fit a linear regression model to the training data & print the summary\nmod_lm_py = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long', data=r.dat_tr_restate).fit()\nprint(mod_lm_py.summary())\n\nIt’s not a suprise that the results are same as the ones obtained in R.\n\n\n\n\nThe stepwise variable selection can be performed using the function step. By default, it is a backward selection; see ?step for details (parameter direction is backward when scope is empty).\n\n\nR\nPython\n\n\n\n\nstep(mod_lm) # see the result\nmod_lm_sel &lt;- step(mod_lm) # store the final model into mod_lm_sel\nsummary(mod_lm_sel)\n\n\n\nAs python does not have an exact equivalent of stats::step() function, which performs both forward and backward selection based on AIC, we have to implement it manually. We start with the full model and iteratively remove the feature with the highest p-value and add the feature with the lowest AIC until we can no longer improve the AIC. The final model is stored in mod_lm_sel. For an extensive explanation of what this while loop is doing and how the backward+forward is computed, check the code below:\n\nExplaining feature elimination in python (while loop)\nWe start by setting mod_lm_sel_py to the full model mod_lm_py. Then, we enter a while loop that continues until we break out of it. In each iteration of the loop, we store the current model in prev_model for later comparison. We start by dropping the feature with the highest p-value from the current model using idxmax(), which returns the label of the maximum value in the pvalues attribute of the mod_lm_sel_py object. We exclude the intercept term from the list of labels by specifying labels=['Intercept']. We then create a new model using smf.ols() with the feature removed and fit it to the training data using fit(). We store this new model in mod_lm_sel_py.\nNext, we check whether the AIC of the new model is larger than the previous model’s. If it is, we break out of the while loop and use the previous model (prev_model) as the final model. If not, we continue to the next step of the loop. Here, we look for the feature with the lowest AIC among the remaining features using idxmin() on the pvalues attribute, again excluding the intercept term. We create a new model by adding this feature to the current model using smf.ols(), fit it to the training data using fit(), and store it in mod_lm_sel_new.\nWe then check whether the AIC of the new model is larger than that of the current model. If it is, we break out of the while loop and use the current model (mod_lm_sel_py) as the final model. If not, we update mod_lm_sel_py with the new model and continue to the next iteration of the loop. This way, we iteratively remove the feature with the highest p-value and add the feature with the lowest AIC until we can no longer improve the AIC. The final model is stored in mod_lm_sel_py.\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# perform both forward and backward selection using AIC\nmod_lm_sel_py = mod_lm_py\n\nwhile True:\n    prev_model = mod_lm_sel_py\n    # drop the feature with the highest p-value\n    feature_to_drop = mod_lm_sel_py.pvalues.drop(labels=['Intercept']).idxmax()\n    mod_lm_sel_py = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long - ' + feature_to_drop, data=r.dat_tr_restate).fit()\n    # check if AIC has increased, if yes, break the loop and use the previous model\n    if mod_lm_sel_py.aic &gt; prev_model.aic:\n        mod_lm_sel_py = prev_model\n        break\n    \n    # add the feature with the lowest AIC\n    feature_to_add = mod_lm_sel_py.pvalues.drop(labels=['Intercept']).idxmin()\n    mod_lm_sel_py_new = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long + ' + feature_to_add, data=r.dat_tr_restate).fit()\n    # check if AIC has increased, if yes, break the loop and use the previous model\n    if mod_lm_sel_py_new.aic &gt; mod_lm_sel_py.aic:\n        break\n    mod_lm_sel_py = mod_lm_sel_py_new\n    \nprint(mod_lm_sel_py.summary())\n\n\n\n\nAfter identifying the most important features, you can fit a new model using only those features and evaluate its performance using the test set.\nThe final model does not contain Long. In terms of interpretations, for example:\n\nThe price increased on average by 3.7 per year (TransDate)\nIt diminishes in average by (-2)2.4 per year (HouseAge)\netc.\n\nWe now predict the prices in the test set. We can make a scatter plot of the predictions versus the observed prices to inspect that. We already know by looking at the R^2 in the summary that the prediction quality is not good.\n\n\nR\nPython\n\n\n\n\nmod_lm_pred &lt;- predict(mod_lm_sel, newdata=dat_te_restate)\nplot(dat_te_restate$Price ~ mod_lm_pred, xlab=\"Prediction\", ylab=\"Observed prices\")\nabline(0,1) # line showing the obs -- pred agreement\n\n\n\n\nmod_lm_sel_pred = mod_lm_sel_py.predict(r.dat_te_restate)\nfig, ax = plt.subplots()\nax.scatter(x=mod_lm_sel_pred, y=r.dat_te_restate['Price'])\nax.set_xlabel('Prediction')\nax.set_ylabel('Observed prices')\nax.plot(ax.get_xlim(), ax.get_ylim(), ls=\"--\", c=\".3\")\nplt.show()\n\n\n\n\nIt appears that the lowest and the highest prices are underestimated. At the center (around 30), the prices are slightly overestimated.\nAs an exercise, write down the prediction equation of the selected model. Use this equation to explain how instances 1 and 2 (test set) are predicted and calculate the predictions manually. Verify your results using the predict function from the previous R code.\n\nAnswer\n\nmod_lm_pred[c(1,2)]\n\n\ny = -0.000133 + 3.66\\times TransDate -0.243\\times HouseAge \\\\-0.00464\\times Dist + 1.027\\times NumStores + 237.8\\times Lat",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#eda",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#eda",
    "title": "Models: Linear and logistic regressions",
    "section": "",
    "text": "First, an EDA of the data is needed. After exploring the structure, the Price is shown with the year and month.\n\nreal_estate_data &lt;- read.csv(here::here(\"labs/data/real_estate_data.csv\"))\n\n## adapt the path to the data\n# if you encountered any error with the encoding of the data (`Error in gregexpr...`), just re-run the code again\nstr(real_estate_data)\nlibrary(summarytools)\ndfSummary(real_estate_data)\n\nlibrary(dplyr)\nlibrary(ggplot2)\nreal_estate_data %&gt;% ggplot(aes(x=Month, y=Price, fill=as.factor(Year))) + \n  geom_boxplot()+ facet_wrap(~as.factor(Year))\n\nThe results show how important it is to make an EDA! It appears that the data does not contain transactions for all the months of 2012 and 2013, but just some months by the end of 2012 and the first half of 2013. This shows that it is pointless to use month and year here. This is why we prefer TransDate, a value indicating the transaction time on a linear scale (e.g., 2013.250 is March 2013).\nNow we focus on the link between Price and the other features.\n\nlibrary(GGally)\nreal_estate_data %&gt;% \n  select(Price, HouseAge, Dist, Lat, Long, TransDate) %&gt;% \n  ggpairs()\n\nNo clear link appears. The linear regression will help to discover if a combination of the features can predict the price.",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#modelling",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#modelling",
    "title": "Models: Linear and logistic regressions",
    "section": "",
    "text": "First, we split the data into training/test set (75/25).\n\nset.seed(234)\nindex &lt;- sample(x=c(1,2), size=nrow(real_estate_data), replace=TRUE, prob=c(0.75,0.25)) # 1==training set, 2==test set\ndat_tr_restate &lt;- real_estate_data[index==1,]\ndat_te_restate &lt;- real_estate_data[index==2,]\n\nThen, we fit the linear regression to the training set.\n\n\nR\nPython\n\n\n\n\nmod_lm &lt;- lm(Price~TransDate+\n               HouseAge+\n               Dist+\n               NumStores+\n               Lat+\n               Long, data=dat_tr_restate)\nsummary(mod_lm)\n\n\n\n\n# In R, we load the conda environment as usual\nlibrary(reticulate)\nreticulate::use_condaenv(\"MLBA\", required = TRUE)\ngc(full = TRUE)\n\nIn python, we then use the statsmodels library to fit a linear regression model to the training data and perform feature elimination. We use the .fit() method to fit the model with the formula for the variable names. Note that python’s summary() function is unique to the statsmodels libraries and produces similar information to its R counterpart.\n\nimport os\nos.environ[\"OMP_NUM_THREADS\"] = \"1\"\nos.environ[\"MKL_NUM_THREADS\"] = \"1\"\n\nimport mkl\n# %env OMP_NUM_THREADS=1\n# set the number of threads. Here we set it to 1 to avoid parallelization when rendering quarto, but you can set it to higher values.\nmkl.set_num_threads(1)\n\n\n# Import necessary library\nimport statsmodels.formula.api as smf\n\n# Fit a linear regression model to the training data & print the summary\nmod_lm_py = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long', data=r.dat_tr_restate).fit()\nprint(mod_lm_py.summary())\n\nIt’s not a suprise that the results are same as the ones obtained in R.",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#variable-selection-interpretation",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#variable-selection-interpretation",
    "title": "Models: Linear and logistic regressions",
    "section": "",
    "text": "The stepwise variable selection can be performed using the function step. By default, it is a backward selection; see ?step for details (parameter direction is backward when scope is empty).\n\n\nR\nPython\n\n\n\n\nstep(mod_lm) # see the result\nmod_lm_sel &lt;- step(mod_lm) # store the final model into mod_lm_sel\nsummary(mod_lm_sel)\n\n\n\nAs python does not have an exact equivalent of stats::step() function, which performs both forward and backward selection based on AIC, we have to implement it manually. We start with the full model and iteratively remove the feature with the highest p-value and add the feature with the lowest AIC until we can no longer improve the AIC. The final model is stored in mod_lm_sel. For an extensive explanation of what this while loop is doing and how the backward+forward is computed, check the code below:\n\nExplaining feature elimination in python (while loop)\nWe start by setting mod_lm_sel_py to the full model mod_lm_py. Then, we enter a while loop that continues until we break out of it. In each iteration of the loop, we store the current model in prev_model for later comparison. We start by dropping the feature with the highest p-value from the current model using idxmax(), which returns the label of the maximum value in the pvalues attribute of the mod_lm_sel_py object. We exclude the intercept term from the list of labels by specifying labels=['Intercept']. We then create a new model using smf.ols() with the feature removed and fit it to the training data using fit(). We store this new model in mod_lm_sel_py.\nNext, we check whether the AIC of the new model is larger than the previous model’s. If it is, we break out of the while loop and use the previous model (prev_model) as the final model. If not, we continue to the next step of the loop. Here, we look for the feature with the lowest AIC among the remaining features using idxmin() on the pvalues attribute, again excluding the intercept term. We create a new model by adding this feature to the current model using smf.ols(), fit it to the training data using fit(), and store it in mod_lm_sel_new.\nWe then check whether the AIC of the new model is larger than that of the current model. If it is, we break out of the while loop and use the current model (mod_lm_sel_py) as the final model. If not, we update mod_lm_sel_py with the new model and continue to the next iteration of the loop. This way, we iteratively remove the feature with the highest p-value and add the feature with the lowest AIC until we can no longer improve the AIC. The final model is stored in mod_lm_sel_py.\n\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n# perform both forward and backward selection using AIC\nmod_lm_sel_py = mod_lm_py\n\nwhile True:\n    prev_model = mod_lm_sel_py\n    # drop the feature with the highest p-value\n    feature_to_drop = mod_lm_sel_py.pvalues.drop(labels=['Intercept']).idxmax()\n    mod_lm_sel_py = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long - ' + feature_to_drop, data=r.dat_tr_restate).fit()\n    # check if AIC has increased, if yes, break the loop and use the previous model\n    if mod_lm_sel_py.aic &gt; prev_model.aic:\n        mod_lm_sel_py = prev_model\n        break\n    \n    # add the feature with the lowest AIC\n    feature_to_add = mod_lm_sel_py.pvalues.drop(labels=['Intercept']).idxmin()\n    mod_lm_sel_py_new = smf.ols(formula='Price ~ TransDate + HouseAge + Dist + NumStores + Lat + Long + ' + feature_to_add, data=r.dat_tr_restate).fit()\n    # check if AIC has increased, if yes, break the loop and use the previous model\n    if mod_lm_sel_py_new.aic &gt; mod_lm_sel_py.aic:\n        break\n    mod_lm_sel_py = mod_lm_sel_py_new\n    \nprint(mod_lm_sel_py.summary())\n\n\n\n\nAfter identifying the most important features, you can fit a new model using only those features and evaluate its performance using the test set.\nThe final model does not contain Long. In terms of interpretations, for example:\n\nThe price increased on average by 3.7 per year (TransDate)\nIt diminishes in average by (-2)2.4 per year (HouseAge)\netc.",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#inference",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#inference",
    "title": "Models: Linear and logistic regressions",
    "section": "",
    "text": "We now predict the prices in the test set. We can make a scatter plot of the predictions versus the observed prices to inspect that. We already know by looking at the R^2 in the summary that the prediction quality is not good.\n\n\nR\nPython\n\n\n\n\nmod_lm_pred &lt;- predict(mod_lm_sel, newdata=dat_te_restate)\nplot(dat_te_restate$Price ~ mod_lm_pred, xlab=\"Prediction\", ylab=\"Observed prices\")\nabline(0,1) # line showing the obs -- pred agreement\n\n\n\n\nmod_lm_sel_pred = mod_lm_sel_py.predict(r.dat_te_restate)\nfig, ax = plt.subplots()\nax.scatter(x=mod_lm_sel_pred, y=r.dat_te_restate['Price'])\nax.set_xlabel('Prediction')\nax.set_ylabel('Observed prices')\nax.plot(ax.get_xlim(), ax.get_ylim(), ls=\"--\", c=\".3\")\nplt.show()\n\n\n\n\nIt appears that the lowest and the highest prices are underestimated. At the center (around 30), the prices are slightly overestimated.\nAs an exercise, write down the prediction equation of the selected model. Use this equation to explain how instances 1 and 2 (test set) are predicted and calculate the predictions manually. Verify your results using the predict function from the previous R code.\n\nAnswer\n\nmod_lm_pred[c(1,2)]\n\n\ny = -0.000133 + 3.66\\times TransDate -0.243\\times HouseAge \\\\-0.00464\\times Dist + 1.027\\times NumStores + 237.8\\times Lat",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#modelling-1",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#modelling-1",
    "title": "Models: Linear and logistic regressions",
    "section": "Modelling",
    "text": "Modelling\nWe can split our data and fit the logistic regression. The function for this is glm. This function encompasses a larger class of models (namely, the generalized linear models) which includes the logistic regression, accessible with family=“binomial”.\n\nset.seed(234)\nindex &lt;- sample(x=c(1,2), size=nrow(DocVis), replace=TRUE, prob=c(0.75,0.25)) # 1==training set, 2==test set\ndat_tr_visit &lt;- DocVis[index==1,]\ndat_te_visit &lt;- DocVis[index==2,]\n\n\n\nR\nPython\n\n\n\n\nvis_logr &lt;- glm(visits~., data=dat_tr_visit, family=\"binomial\")\nsummary(vis_logr)\n\n\n\n\n# a hack around this technique to not type all the variable names\nvis_formula = 'visits ~ ' + ' + '.join(r.dat_tr_visit.columns.difference(['visits']))\n\n# create a logistic regression model\nvis_logr_py = sm.formula.logit(formula= vis_formula, data=r.dat_tr_visit).fit()\nprint(vis_logr_py.summary())\n\n\n\n\n\n\n\nUsing . for formulas in R vs Python\n\n\n\nIn R, the dot . is used as shorthand to indicate that we want to include all other variables in the formula as predictors except for the outcome variable. So, if our outcome variable is y and we want to include all other variables in our data frame as predictors, we can write y ~ . in the formula.\nIn Python, however, the dot . is not used in the same way in formulas. Instead, to include all other variables as predictors except for y, we would write y ~ x1 + x2 + ... where x1, x2, etc. represent the names of the predictor variables. Also, statsmodels has a similar syntax to R base regressions. In most other typical ML libraries in Python, you must provide the column values instead of using the column names.\n\n\nNote that the family=\"binomial\" argument in R is not needed in Python since sm.formula.logit() assumes the logistic regression model is fitted using a binomial distribution by default.",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#variable-selection-interpretation-1",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#variable-selection-interpretation-1",
    "title": "Models: Linear and logistic regressions",
    "section": "Variable selection & interpretation",
    "text": "Variable selection & interpretation\nNow, we can apply the variable selection:\n\n\nR\nPython\n\n\n\n\nvis_logr_sel &lt;- step(vis_logr)\nsummary(vis_logr_sel)\n\n\n\nAs already seen in the linear regression part, in python, we don’t have the same implementation of the step function, hence why we designed the while loop earlier. It is good practice to create a single function with this step while loop to handle all cases (linear, logistic etc); however, we only implement it here for logistic regression. Therefore, to tackle this, we will create a function that does step-wise elimination for us. We define a function called forward_selected that performs forward selection on a given dataset to select the best predictors for a response variable based on AIC. The function takes two arguments: data, a pandas DataFrame containing the predictors and response variable, and response, a string specifying the name of the response variable.\n\nFor more explanation of the code, click on me\nThe function first initializes two sets: remaining and selected. remaining contains the names of all columns in the data DataFrame except for the response variable, while selected is initially empty. The function then initializes current_aic and best_new_aic to infinity. The main loop of the function continues as long as remaining is not empty and current_aic is equal to best_new_aic. At each iteration, the function iterates over all columns in remaining and computes the AIC for a logistic regression model that includes the response variable and the currently selected predictors, as well as the current candidate predictor. The function then adds the candidate predictor and its AIC to a list of (aic, candidate) tuples, and sorts the list by increasing AIC. The function then selects the candidate with the lowest AIC and adds it to the selected set, removes it from the remaining set, and updates current_aic to the new lowest AIC. The function continues this process until no candidate can improve the AIC. Finally, the function fits a logistic regression model using the selected predictors and returns the resulting model.\n\n# code taken from the link below and adjusted for logistic regression with AIC criteria\n# https://planspace.org/20150423-forward_selection_with_statsmodels/\n\ndef forward_selected(data, response):\n    \"\"\"Linear model designed by forward selection.\n\n    Parameters:\n    -----------\n    data : pandas DataFrame with all possible predictors and response\n\n    response: string, name of response column in data\n\n    Returns:\n    --------\n    model: an \"optimal\" fitted statsmodels linear model\n           with an intercept\n           selected by forward selection\n           evaluated by AIC\n    \"\"\"\n    remaining = set(data.columns)\n    remaining.remove(response)\n    selected = []\n    current_aic, best_new_aic = float(\"inf\"), float(\"inf\")\n    while remaining and current_aic == best_new_aic:\n        aics_with_candidates = []\n        for candidate in remaining:\n            formula = \"{} ~ {} + 1\".format(response,\n                                           ' + '.join(selected + [candidate]))\n            model = smf.logit(formula, data).fit(disp=0)\n            aic = model.aic\n            aics_with_candidates.append((aic, candidate))\n        aics_with_candidates.sort()\n        best_new_aic, best_candidate = aics_with_candidates.pop(0)\n        if current_aic &gt; best_new_aic:\n            remaining.remove(best_candidate)\n            selected.append(best_candidate)\n            current_aic = best_new_aic\n    formula = \"{} ~ {} + 1\".format(response,\n                                   ' + '.join(selected))\n    model = smf.logit(formula, data).fit(disp=0)\n    return model\n\n\nmod_logit_sel_py = forward_selected(r.dat_tr_visit, 'visits')\n\nprint(mod_logit_sel_py.summary())\n\nWe can see that the results of mod_logit_sel_py model are slightly different from the R version, but nevertheless, we have reduced the features and the interpretations (see below) with both R and python versions remain the same.\n\n\n\nWe can see that the probability of a visit is\n\nsmaller for males\nincreasing with age\nlarger with illness\netc.",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#inference-1",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#inference-1",
    "title": "Models: Linear and logistic regressions",
    "section": "Inference",
    "text": "Inference\n\n\nR\nPython\n\n\n\nThe predict function with type=“response” will predict the probability of the positive class (“1”). If it is set to “link” it produces the linear predictor (i.e., the z). To make the prediction, we thus have to identify if the predicted probability is larger or lower than 0.5.\n\nprob_te_visit &lt;- predict(vis_logr_sel, newdata = dat_te_visit, type=\"response\")\npred_te_visit &lt;- ifelse(prob_te_visit &gt;= 0.5, 1, 0)\ntable(Pred=pred_te_visit, Obs=dat_te_visit$visits)\n\n\n\nThe explanation is similar to that of R, with a slight different that here we use pandas.crosstab to make our confusion matrix.\n\nimport pandas as pd\n\nprob_te_visit = mod_logit_sel_py.predict(r.dat_te_visit)\npred_te_visit = [1 if p &gt;= 0.5 else 0 for p in prob_te_visit]\nconf_mat = pd.crosstab(pred_te_visit, r.dat_te_visit['visits'], rownames=['Pred'], colnames=['Obs'])\nprint(conf_mat)\n\nThe results are extremely close to the R version.\n\n\n\nThe predictions are not really good. It is in fact a difficult data set. Indeed, the number of 0 is so large compare to the 1, that predicting a 0 always provides a good model overall. That issue will be addressed further later on in the course.\nFor now, this can be further inspected by looking at the predicted probabilities per observed label.\n\n\nR\nPython\n\n\n\n\nboxplot(prob_te_visit~dat_te_visit$visits)\n\n\n\n\nfig, ax = plt.subplots()\nax.boxplot([prob_te_visit[r.dat_te_visit['visits']==0], prob_te_visit[r.dat_te_visit['visits']==1]])\nax.set_xticklabels(['No Visit', 'Visit'])\nax.set_ylabel('Predicted Probability')\nax.set_title('Predicted Probabilities by Visit Status')\nplt.show()\n\n\n\n\nWe see that if the lowest predicted probabilities are usually assigned to 0-observations, most of the probabilities remain below 0.5 (even for the 1-observations). A good model would have two well separated boxplots, well away from 0.5.\nNow, as an exercise, write down the prediction equation of the selected model, like you did for linear regression. Use this equation to explain how instance 1 and 2 (test set) are predicted, and calculate the predictions manually. Verify your results using the function predict used before.\n\nAnswer\n\nprob_te_visit[c(1,2)]\n\n\nz(x) = -2.31795-0.31838\\times gender\\_male+0.39762\\times age+\\\\0.28431\\times illness+0.16340\\times reduced+0.05589\\times health+\\\\0.27249\\times private\\_eyes -0.65344\\times freepoor\\_yes+\\\\0.38038\\times freerepat\\_yes  \n Then \nP(Y=1 | X=x) = \\frac{e^{z(x)}}{1+e^{z(x)}}",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#linear-regression-nursing-home-data",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#linear-regression-nursing-home-data",
    "title": "Models: Linear and logistic regressions",
    "section": "Linear regression: nursing home data",
    "text": "Linear regression: nursing home data\nNow it is your turn. Make an linear regression (also feel free to try lasso and ridge regressions) on the nursing data described below (found also in /data/nursing_data.csv). Afterwards, use linear regression to build a predictor of the cost using the other features. Replicate the analysis. Split the data, build a model, make the variable selection, make the predictions and analyze the results. Make also an analysis of the coefficients in terms of the associations between the costs and the features.\n\nData Description\nThe data set is about patients in a nursing home, where elderly people are helped with daily living needs, also known as Activities of Daily Living (ADL, i.e. communication, eating, walking, showering, going to a toilet, etc.).\nSince the stay in such facilities is very expensive, it is important to classify the new-coming patient, and estimate the duration of the stay and the corresponding costs.\nIn practice, there are different types of patients who require different types of help and, consequently, different duration of the stay. For example, there could be a person with severe mobility issues, who requires the help with most of the needs every day; or a person with mental deviations, who don’t need help with daily routine, but requires extra communication hours.\nHere, we will focus of total amount of help (measured in minutes of help provided to a person per week) provided and measure the costs of stay of a person.\nThe data set on which the analysis is based has the following columns:\n\n\ngender: a categorical variable with levels “M” for male and “F” for female\n\nage: integer variable\n\nmobil: categorical variable that represents the physical mobility with levels\n\n1 = Full mobility\n2 = Reduced mobility\n3 = Restricted mobility in the house\n4 = Null mobility\n\n\n\norient: categorical variable that represents the orientation (interactions with the environment) with levels\n\n1 = Full orientation\n2 = Moderate disturbance of orientation\n3 = Disorientation\n\n\n\nindepend: categorical variable that represents the independence of ADL with levels\n\n1 = Independent of help\n2 = Dependent less than 24 hours per day\n3 = Dependent at unpredictable time intervals for most of the needs\n\n\n\nminut_mob: numerical variable that represents the total number of minutes of help with movement per week\n\nneed_comm: categorical variable with levels “Yes” for a person who needs extra communication sessions with an employee, and “No” otherwise\n\nminut_comm: numerical variable that represents the total number of minutes of communication per week\n\ntot_minut: numerical variable that represents the total number of minutes spent on a patient per week, tot\\_minut = minut\\_mob + minut\\_comm\n\n\ncost: numerical variable that represents the total costs of having a patient in the nursing house per month.\nNote: since tot_minut=minut_mob+minut_comm, you may not find any meaningful result using the 3 features. This is perfectly normal. Just use 2 features only among these 3 (arbitrary choice).",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#logistic-regression-the-credit-quality",
    "href": "labs/03_Models/031_LinearLogisticRegression/Ex_ML_LinLogReg.html#logistic-regression-the-credit-quality",
    "title": "Models: Linear and logistic regressions",
    "section": "Logistic regression: the credit quality",
    "text": "Logistic regression: the credit quality\nThe German Credit Quality Dataset consists of a set of attributes as good or bad credit risks. In order to find find a detailed description of the features, please refer to the original link to the dataset. The german.csv file can also be found in /data/german.csv whichis the mdified version of the original dataset to simplify the analysis, especially the data loading in R.\nThe aim here is to predict the credit quality from the other features. The outcome Quality is 0 for “bad” and 1 for “good”. Make an analysis of the data and develop the learner. You can follow these notable steps:\n\nMake a simple EDA of the features\nSplit the data and train the model.\nMake variable selection and check out the result.\nInterpret the coefficients.\nInspect the quality of the model by making the predictions (confusion table and boxplot of the predicted probabilities).\n\nNote that the data are unbalanced again and that you may not find a very good predictor. This issue is quite difficult and will be addressed later.",
    "crumbs": [
      "Labs",
      "Linear & Logistic Regression"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning in Business Analytics 2023-2024",
    "section": "",
    "text": "🧑🏻‍🏫 Teaching staff: Dr. Marc-Olivier Boldi (Lecturer) & Ilia Azizi (TA).\n🕛 Time: Mondays 8:30-12:00, starting on Monday the 19th of February 2024.\n🏫 Room: Anthropole 3174 (no live broadcasting or recording).\n\n📖 Content\nThis course presents several machine learning techniques in business and management contexts. The list of topics is meant to cover mainly supervised methods of classification and prediction although unsupervised methods are also seen. Below is a tentative lists of topics. It will be adapted according to the pace of the class:\n\nSupervised learning models: regressions, trees, support vector machine, neural networks.\nData splitting: training/test sets, cross-validation, bootstrap.\nMetrics: MSE, Accuracy, …\nEnsemble methods: bagging, random forests, boosting.\nUnsupervised learning: clustering, PCA, FAMD, Auto-Encoder\nInterpretable machine learning: variale importance, partial dependence plots, LIME.\n\nExercises and theory are equally important for the success of the class. For the excercises we use primarly R, with equivalent code in Python.\n\n\n📝 Evaluation\n\nPartial written exam: organized during the semester.\nApplied project: individual or in group (depending on the number of participants to the course).\n\nOne report (incl. supplementary material, codes, etc.)\nOne final presentation will be organized during the semester or during the exam session (depending on the number of participants).\n\n\nFinal grade = (0.3 x exam) + (0.3 x presentation) + (0.4 x report)\n\n\n\n\n\n\nProject member contributions\n\n\n\nThe project grade is a group grade. However, if the contribution of the members of the groups to the project is unbalanced, an individual adaptation of the grade will be made, e.g., absence of one of the members of the group during the presentation (in catch-up, a subsequent and adapted presentation of the absent member may be required).\n\n\nFurther directives and guidelines are provided here.",
    "crumbs": [
      "Course information",
      "Overview"
    ]
  },
  {
    "objectID": "labs/00_lab/setup.html",
    "href": "labs/00_lab/setup.html",
    "title": "ML-Lab: General Instructions",
    "section": "",
    "text": "Objectives\n\n\n\nThis setup tutorial is optional for those who are interested in the following:\n\nLearning about (and using) virtual environments in R to ensure reproducibility (benefits explained below).\nLearning about using python 🐍 in (and with) R. This is useful for some ML lab sessions, and cutting-edge ML is often first implemented in python.\n\nIf you are not interested in either objective, skip this Rmd and jump into the modelling exercises starting with KNN.",
    "crumbs": [
      "Labs",
      "Structure & Setup"
    ]
  },
  {
    "objectID": "labs/00_lab/setup.html#the-what-the-why",
    "href": "labs/00_lab/setup.html#the-what-the-why",
    "title": "ML-Lab: General Instructions",
    "section": "The What & The Why",
    "text": "The What & The Why\nAs you start working on machine learning in R, you’ll be using various R packages that provide different functionalities. As you might know, managing multiple packages and their dependencies can be a daunting task, especially when you’re new to programming.\nThis is where renv comes in handy. renv is a package management tool that helps you manage the packages used in an R project, making it easier to handle dependencies and ensuring that your project is reproducible. Here are some specific reasons why renv is particularly useful for machine learning lab sessions:\n\nConsistent environment: When you work on machine learning lab sessions, you’ll often be working with complex models that use multiple R packages. It’s crucial to ensure that all the packages used in your project are compatible with each other. With renv, you can create a consistent environment by isolating the packages used in your project and making sure they work well together.\nEasy installation and setup: renv makes it easy to set up a new R project with the required packages. Once you’ve created an renv project, you can easily install all the required packages with a single command. This saves you time and ensures that you have all the necessary packages for your machine learning lab sessions.\nReproducibility: Reproducibility is critical in machine learning lab sessions. With renv, you can easily share your code and the exact packages used in your project with your peers or instructors, making it easy for them to reproduce your results.\n\nIn summary, renv is an essential tool for managing packages in R and ensuring that your machine learning lab sessions are efficient and reproducible. It helps you avoid compatibility issues, simplifies installation and setup, and makes it easy to share your work with others. In python, you have similar tools such as virtualenv, venv and conda. We hope you find renv useful as you begin your journey into machine learning with R!\n\n# Check if renv is installed\nif (!require(\"renv\")) {\n  # Install renv if it is not already installed\n  install.packages(\"renv\")\n}",
    "crumbs": [
      "Labs",
      "Structure & Setup"
    ]
  },
  {
    "objectID": "labs/00_lab/setup.html#the-how",
    "href": "labs/00_lab/setup.html#the-how",
    "title": "ML-Lab: General Instructions",
    "section": "The How",
    "text": "The How\nTo create a new renv project, first set your working directory to the location where you want to create the project, and then run the command renv::init() which has to be executed only once. We recommend creating a main folder for all your ML-related exercises and running the command at the top directory. Running the initialization will create a bunch of auxiliary files, such as renv.lock to keep track of the packages that you’re using and all their versions. If you move to another computer or something goes wrong with your packages, you can always re-install the packages listed in your renv.lock to the previous versions using renv::restore(). To occasionally update your packages, you can always use renv::snapshot(); however, every time that you start a new R session, if renv detects any changes, it will automatically ask you to run renv::status() to see if the list in renv.lock needs updating.\n\n# Check if renv is already initialized\nif (!file.exists(\"renv.lock\")) {\n  # Initialize renv project\n  renv::init()\n\n  # Restore packages\n  renv::restore()\n}",
    "crumbs": [
      "Labs",
      "Structure & Setup"
    ]
  },
  {
    "objectID": "labs/00_lab/setup.html#small-motivation",
    "href": "labs/00_lab/setup.html#small-motivation",
    "title": "ML-Lab: General Instructions",
    "section": "Small Motivation",
    "text": "Small Motivation\nMost careers after HEC require data literacy and data-driven insights to solve business problems. To that end, R is a potent tool; however, in the realm of machine learning, Python is arguably more demanded and, therefore, can be a good tool in your toolbox. Additionally, python is a widely-used language in the industry and offers powerful libraries for data manipulation, analysis, and modeling. Furthermore, you don’t need to be a technical person to learn python as there are many resources available online, and it’s a language that is relatively easy to pick up, even for absolute beginners to programming. Combining the strengths of both R and Python can enhance your workflow and improve your ability to work with data. To understand how R and Python compare, you are highly encouraged to watch this video by IBM on R vs Python.",
    "crumbs": [
      "Labs",
      "Structure & Setup"
    ]
  },
  {
    "objectID": "labs/00_lab/setup.html#configuration",
    "href": "labs/00_lab/setup.html#configuration",
    "title": "ML-Lab: General Instructions",
    "section": "Configuration",
    "text": "Configuration\nYou need to follow a few steps to ensure that everything runs smoothly. There are several solutions for running the python part of the exercises. Our preferred method is to run python in R using the reticulate package as recently there has been a smooth integration between the two languages, especially if your IDE of choice (integrated development environment) is Rstudio. This library provides a comprehensive set of tools for interoperability (i.e., exchanging languages) between python and R. You can either do this in your own central python installation or use a virtual environment where we’ll install the desired python packages. In python, virtual environments are similar to those of R (i.e. renv), allowing you to create isolated python installations, making it easier to manage different projects, and avoiding compatibility issues between dependencies. This setup usually works well, but if there are any individual issues, do not hesitate to contact me (Ilia). With that said, let’s get started with python!\nFirst, load all the corresponding libraries to install and run python:\n\nlibrary(reticulate)\nlibrary(tidyverse)\n\nTo install python in R, we will use miniconda, a smaller version of Anaconda package manager. If you don’t have Miniconda or Anaconda, first run reticulate::install_miniconda(), which will automatically create a virtual environment for you.\n\n# reticulate::install_miniconda() # if you got an error, you could also try `install_python()` or installing conda seperately on your OS\n\nNow, we will create our virtual environment with the command reticulate::conda_create(). We will then make sure our rstudio is using this correct conda environment by enforcing it via reticulate::use_condaenv().\n\n# assign the right virtual environment for the exercises\nenv_name &lt;- \"MLBA\"\n\n# if the virtual enviroment does not already exist, only then create then\nif (!env_name %in% reticulate::conda_list()$name) {\n  reticulate::conda_create(env_name, pip = TRUE) # we use pip for installations\n}\n# make sure we're using the right environment\nreticulate::use_condaenv(env_name)\n\n# if you preferred, you can also use your own version of python with `use_python()`\n# you can see all the versions of your path here and which one has been assigned\nreticulate::py_config()\n\n# Check if python is setup properly now\nreticulate::py_available()\n\nIf you have made it here so far and see the name MLBA in your python path, you have successfully installed/configured python and setup the virtual environment(s)🥳.\nBonus: Using Rstudio to select Python path\nIn case you wanted to use the Rstudio interface, you can always go to Tools &gt; Project Options (or Global Options if you’re not using renv) and then select the version inside the Python field as shown below.",
    "crumbs": [
      "Labs",
      "Structure & Setup"
    ]
  },
  {
    "objectID": "labs/00_lab/setup.html#python-very-brief-overview",
    "href": "labs/00_lab/setup.html#python-very-brief-overview",
    "title": "ML-Lab: General Instructions",
    "section": "Python (very brief) overview",
    "text": "Python (very brief) overview\nWe do not teach the principles of python programming during this course. With that said, if you already have a background in R from your previous courses (e.g., DSFBA, QMM), we provide a few links to help you get started. If you’re new to python, feel free to continue reading; otherwise, skip to the section Calling Python in R.\nIf you would like to see a crash course in python on different data structures and types, check out this video on Python for Data Science [Crash Course] (you can skip the installation part). You may notice that much of the syntax is similar to R.\nPython data-oriented libraries\nPython provides a range of libraries for data manipulation, analysis, and modeling, including Pandas (similar to tibble+dplyr), which offers easy-to-use data structures for working with tabular data, and NumPy (dplyr+data.table), which provides powerful tools for array manipulation, linear algebra, and Fourier analysis. For data visualization, python offers Matplotlib (similar to plot() in base R). For machine learning, Scikit-learn (similar to the caret package in R which we will be introduced later) provides a wide range of tools for classification, regression, clustering, and dimensionality reduction, while Tensorflow & Keras (both used for deep learning and neural networks) are also popular libraries in this space which are available in both R and Python. These libraries are just a few examples of the many tools available in python that can help you work with data and build machine learning models. If you’re interested to learn about these libraries and how to manipulate Pandas dataframes, you can check out this other video on Data Analysis with Python - Full Course for Beginners (Numpy, Pandas, Matplotlib, Seaborn) (it’s slightly long).\nAssigning variables in Python vs. R\nFor those of you new to python, there’s an important difference between R and Python when assignment variables. In Python, when you assign a variable to another variable, you are creating a reference to the same object in memory, so any changes made to one variable will be reflected in the other variable as well. This is demonostrated in the example below:\n\na = [1, 2, 3]\nb = a\nprint(a, b)\nb[0] = 4 # we only change `b`\nprint(a, b) # the variable `a` also changed unlike how R treats the variable\n\n\n\n\n\n\n\nIndexing in R vs. Python\n\n\n\nAnother difference between R & Python is that the index (first value of any object) starts from 0 (python) rather than 1 (R). So if you have a list, the first element is python is starting at element 0.\n\n\nTo create a separate copy of the object, you need to use the .copy() method (python way of saying a function).\n\na = [1, 2, 3]\nb = a.copy() # create a separate copy of the list\nprint(a, b)\nb[0] = 4 # modify only `b`\nprint(a, b) # variable `a` does not change\n\nIn R, on the other hand, assignment creates a copy by default, so you don’t need to use a .copy() method. Any changes you make to one variable will not affect the other variable, because they are separate copies of the same object.",
    "crumbs": [
      "Labs",
      "Structure & Setup"
    ]
  },
  {
    "objectID": "labs/00_lab/setup.html#use-python-in-r",
    "href": "labs/00_lab/setup.html#use-python-in-r",
    "title": "ML-Lab: General Instructions",
    "section": "Calling Python in R",
    "text": "Calling Python in R\nInstalling Python libraries\nYou can install any python package in R using the reticulate::py_install() command. This is similar to calling install.packages() in R. You may have heard of CRAN in R, a central repository for all R packages. In Python, the equivalent of CRAN is PyPI (Python Package Index). If you want the latest version of the packages, it is recommended to install packages using pip. To do so, you must set the argument pip=TRUE inside reticulate::py_install(), as demonstrated below.\nWe will install all the packages we need for this particular setup. This should be go smoothly with the following command:\n\n# Install python package into virtual environment\nreticulate::py_install(c(\"jupyter\", \"pandas\", \"matplotlib\",\"statsmodels\",\"scikit-learn\", \"seaborn\", \"mlxtend\", \"lime\", \"mkl-service\"), envname = \"MLBA\", pip=TRUE)\n\nLet’s checked if the package is installed successfully.\n\n# import package that is used for dealing with data.frames in Python (equivalent of tibble+dplyr)\npd &lt;- reticulate::import(\"pandas\")\n# import the package for plotting in python\nplt &lt;- reticulate::import(\"matplotlib.pyplot\")\n# import the library which we will use for linear regression\nsm &lt;- reticulate::import(\"statsmodels.api\")\n\nNo error messages? Then installation was successful!\nCalling Python and R objects interchangeably\nAll the data types in R vs. Python are explained below (image from reticulate’s home page). If you need to explicitly change between objects in R and Python (usually R handles that automatically) to get the objects from the image above, you can use reticulate::r_to_py() and reticulate::py_to_r() (e.g., R dataframes to pandas dataframes).\n\nTo run objects from python in R, you have to use $ to access their elements. For instance, when you want to load a python library, you can use reticulate::import() function and then assign it to a variable with the name of your choice as we did in the previous part. If that library contains a function (in python called module or sub-module), it always follows the format LIBRARY$FUNCTION(). We can see an example of that below:\n\n# Using R\n## load mtcars dataset\ndata(mtcars)\n\n## plot it using base R plot function (or ggplot)\nplot(mtcars$mpg, mtcars$disp)\n\n# Using Python\n# plot it using matplotlib in python (or another python library for plots)\nplt$scatter(mtcars$mpg,mtcars$disp)\nplt$xlabel('mpg', fontsize = 12)\nplt$ylabel('disp', fontsize = 12)\n\n# save the figure and then include in the Rmd\nplt$savefig(\"pyplot.png\")\nknitr::include_graphics(\"pyplot.png\")\n# alternatively, when not knitting, you can uncomment and run the two following lines\n# instead of save the figure\n# plt$show() # you always have to call this for the plot to be made\n# plt$clf() #this means clear figure\n\nNow, this is using R inside Python, but if you wanted to do it the other way around, that’s also possible by using a dot . instead of $. If you’re running python in markdown, you can replace {r...} at the beginning of the code chunk with {python...}, and it’ll run python code. Additionally, if you would like to run a script instead (interactively or the whole script), you can do it by going to file &gt; New File &gt; Python Script in your Rstudio, and then running any part of your python script starts the interactive python session using reticulate::repl_python(). Alternatively, you can call the repl_python() to start the interactive session.\n```{python}\n# Your python code goes here\n```\nWe will now create a code chunk that purely runs Python and accesses the objects object with r.OBJECT_NAME:\n\n# we access mtcars dataset from R\nprint(r.mtcars.head())\n\nfrom sklearn import datasets\nimport pandas as pd\n\n# we open iris data from `sklearn` python package\niris = datasets.load_iris()\niris_data = pd.DataFrame(data=iris['data'], columns=iris['feature_names'])\n\n# Get the head of the DataFrame\nprint(iris_data.head())\n\nWe can do the same thing in R by calling py$OBJECT_NAME by using reticulate::py$OBJECT_NAME:\n\n# plotting the iris data from python\nplot(py$iris_data)",
    "crumbs": [
      "Labs",
      "Structure & Setup"
    ]
  },
  {
    "objectID": "labs/00_lab/setup.html#modelling-in-r-python",
    "href": "labs/00_lab/setup.html#modelling-in-r-python",
    "title": "ML-Lab: General Instructions",
    "section": "Modelling in R & Python",
    "text": "Modelling in R & Python\nLet’s take a simple use case of making a regression in R and Python so you can see how the two languages compare:\n\n# remove the spaces and `(cm)` from the column names\nnames(py$iris_data) &lt;- gsub(' ', '_', names(py$iris_data))\nnames(py$iris_data) &lt;- gsub('_\\\\(cm\\\\)', '', names(py$iris_data))\n\n# example of running a model on iris data\nr_lm &lt;- lm(\"sepal_length ~. \", data = py$iris_data)\nsummary(r_lm)\n\nWithin R, you can still use python to run the same linear regression with the statsmodels python library.\n\n# example of runnning lm model in python -&gt; firstly, process the data\n# specify your dependent variable and independent variables\ny_iris = select(py$iris_data, \"sepal_length\")\nx_iris = select(py$iris_data, -\"sepal_length\")\n\n# for python approach, we need to add a constant to predictor variables\nx_iris = sm$add_constant(x_iris)\n\n# create a linear regression model and fit it to the data\npy_lm = sm$OLS(y_iris, x_iris)$fit()\n\n# get the model summary\nprint(py_lm$summary())\n\nAs you can see, the results are the same (those of statsmodels have been rounded). This could have also been done purely in python, as demonstrated below. Please note that here we have brought the iris data from python to R, then manipulated it in R (with the select() operation), and now we have two options to use these objects in pure python (just one is needed):\n\nWe can simply call the objects from R by r.x_iris and r.y_axis.\nWe can do the same select() operation in python with it’s own syntax on the original python iris data:\n\n\npy_y_iris = iris_data[\"sepal_length\"]\npy_x_iris = iris_data.drop(\"sepal_length\", axis=1)\n\nNeedless to say that in this case, the most efficient approach is the first one as demonstrated below:\n\n# load the sm library with it's alias\nimport statsmodels.api as sm\n\n# add the constant again\nx_iris = sm.add_constant(r.x_iris)\n\n# create a linear regression model and fit it to the data\npy_lm = sm.OLS(r.y_iris, r.x_iris).fit()\n\n# get the model summary\nprint(py_lm.summary())\n\nThe outcome is the same as calling python within R.",
    "crumbs": [
      "Labs",
      "Structure & Setup"
    ]
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Tip\n\n\n\nIf you do not find the answer to your question here, you can use the search bar on the top left to search the entire site. If you are still unable to find the answer, you can ask your question during the exercise hours. If you found the solution and think it would be useful to other students, you can report an issue (see on the right) with the questions and the solution, and we will add it to the FAQ.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "faq.html#faq",
    "href": "faq.html#faq",
    "title": "FAQ",
    "section": "FAQ",
    "text": "FAQ\n\nOrganizational\n\nWhich language will we be using?\nYou can use either R or Python. We will be providing code examples and suppport in all three languages.\n\n\nWhat software do I need to install?\nYou will need to use git, and a text editor or an Integrated Development Environment (IDE).\n\n\n\nHow do I submit an assignment?\nWe will be using Github-classroom to distribute and collect assignments. You will need to create a Github account if you don’t already have one.\n\n\n\nTechnical issues\n\nI am lost with Github, what should I do?\nGitHub has possibly the best tutorials and documentation of any software out there. You can find the Hello world there. Most of the time googling your issue will lead you to the right place.\n\n\nI have bugs in my code/ something doesn’t work/ I don’t know how to do something, what should I do?\nFirst google your issue. 90 times out of 100 this will solve your issues. Another 9% can be resolved by reading the documentation of the software you are using. For the very last percent you can ask your question during the exercises hours.\n\n\nI cannot find the repository linked to the assignment, what should I do?\nAt the top of each assignment there should be a link you can manually click to accept the assignment. If this happens after the first assignment, please let us know.",
    "crumbs": [
      "Course information",
      "FAQ"
    ]
  },
  {
    "objectID": "labs/03_Models/032_Trees/Ex_ML_Tree.html",
    "href": "labs/03_Models/032_Trees/Ex_ML_Tree.html",
    "title": "Models: CART",
    "section": "",
    "text": "In this exercise, the classification tree method is used to analyze the data set Carseats from the package ISLR. The exercise took some inspiration from this video.\n\nFirst, install the package ISLR in order to access the data set Carseats. Use ?Carseats to read its description. To apply a classification of the sales, we first create a categorical outcome SaleHigh which equals “Yes” if Sales &gt; 7.5 and “No” otherwise. Then we create a data frame MyCarseats containing SaleHigh and all the features of Carseats except Sales. Finally, split MyCarseats into a training and a test set (2/3 vs 1/3). Below we call them df_tr and df_te.\n\nlibrary(ISLR)\nlibrary(dplyr)\nMyCarseats &lt;- Carseats %&gt;% mutate(SaleHigh=ifelse(Sales &gt; 7.5, \"Yes\", \"No\"))\nMyCarseats &lt;- MyCarseats %&gt;% select(-Sales)\n\nset.seed(123) # for reproducibility \nindex_tr &lt;- sample(x=1:nrow(MyCarseats), size=2/3*nrow(MyCarseats), replace=FALSE)\ndf_tr &lt;- MyCarseats[index_tr,]\ndf_te &lt;- MyCarseats[-index_tr,]\n\nNote: this is not the point of this exercise but remember that in a real situation the first step of the data analysis would be an EDA.\n\n\n\nR\nPython\n\n\n\nThe rpart function in the package rpart can be used to fit a classification tree with the same type of formulas as naive_bayes. It can then be plotted using the function rpart.plot of the package rpart.plot.\n\nlibrary(rpart)\nlibrary(rpart.plot)\nset.seed(1234)\ncarseats_tree &lt;- rpart(SaleHigh ~ ., data=df_tr)\nrpart.plot(carseats_tree)\n\n\n\n\n# Load MLBA environment\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\nWe use the df_train and df_te created in R to carry out our CART training. For this, we will use the sklearn library. First, we copy our dataset from the R variable to a python variable. Then we use any encoder (e.g. OneHotEncoder, LabelEncoder, OrdinalEncoder) from sklearn.preprocessing to convert categorical data into a numeric form, which many sklearn machine learning algorithms require. In this case, we use the OneHoteEncoder which is similar to making dummy variables and turn each level of the category into a column. Also, we standardize the data in the case of python for faster computations using StandardScaler, which also helps to bring numerical stability and improve the results. After this, we divide the training and test sets into predictors (e.g., X_train) and the outcome (e.g., y_train) and initialize our classification tree and fit the model to the data.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n# We copy the training and testing datasets from R to Python.\ncarset_tr_py = r.df_tr.copy()\ncarset_te_py= r.df_te.copy()\n\n# We encode categorical variables as numeric, which is necessary for CART.\nle = OneHotEncoder()\ncat_vars = [\"ShelveLoc\", \"Urban\", \"US\", \"SaleHigh\"]\nfor var in cat_vars:\n    # Fit the LabelEncoder to the training set and transform the training and testing sets\n    # We need to use the same encoder for both sets to ensure consistency\n    carset_tr_py_encoded = le.fit_transform(carset_tr_py[[var]].values.reshape(-1, 1))\n    carset_te_py_encoded = le.transform(carset_te_py[[var]].values.reshape(-1, 1))\n    carset_tr_py[var] = carset_tr_py_encoded.toarray()\n    carset_te_py[var] = carset_te_py_encoded.toarray()\n\n# Split the data into training and testing sets\nX_train, y_train = carset_tr_py.drop(columns=[\"SaleHigh\"]), carset_tr_py[\"SaleHigh\"]\nX_test, y_test = carset_te_py.drop(columns=[\"SaleHigh\"]), carset_te_py[\"SaleHigh\"]\n\n# Standardize only the continuous variables\ncont_vars = [\"CompPrice\", \"Income\", \"Advertising\", \"Population\", \"Price\", \"Age\", \"Education\"]\nscaler = StandardScaler()\nX_train[cont_vars] = scaler.fit_transform(X_train[cont_vars])\nX_test[cont_vars] = scaler.transform(X_test[cont_vars])\n\n# To speed up the operation you can also transform the inputs\n# X_train = X_train.to_numpy()\n# y_train = y_train.to_numpy()\n\nWe can now train and plot our decision tree using DecisionTreeClassifier and plot_tree functions.\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nimport matplotlib.pyplot as plt\n\n# we clear any previous figures\nplt.clf()\n\nnp.random.seed(1234)\ncarseats_tree = DecisionTreeClassifier().fit(X_train, y_train)\nplt.figure(figsize=(20,10))\nplot_tree(carseats_tree,filled=True, feature_names=X_train.columns, label = 'root',fontsize=5)\nplt.savefig('tree_high_dpi', dpi=300)\n# for a better quality, save the image and load it again\n#plt.show()\n\nAs the image dimensions are not always great for matplotlib plots in Rstudio, we saved the image and now load it again below using an R chunk.\n\nknitr::include_graphics(\"tree_high_dpi.png\")\n\n\n\n\n\n\n\nR\nPython\n\n\n\nThe analysis of the tree complexity can be obtained using function plotcp.\n\nplotcp(carseats_tree)\n\nFrom the graph, we can identify that, according to the 1-SE rule, the tree with 8 nodes is equivalent to the tree with 12 nodes. This 8-nodes tree should be preferred.\nTo prune the tree (i.e., extract the tree with 8 nodes), we can use the function prune with argument cp. The cp of the tree can be read on the bottom x-axis of the plotcp. The argument in prune should be set to any value between the cp of 8-nodes tree (0.031) and the 11-node tree (0.019). Here 0.025 is OK.\n\ncarseats_tree_prune &lt;- prune(carseats_tree, cp=0.025)\nrpart.plot(carseats_tree_prune)\n\nImportant note: The CP evaluation relies on a cross-validation procedure, which uses random number generation. This is why we have set the seed to some value (1234). You may not find the same result with another seed or if you do not set it. In any case, just be coherent with your results and prune the tree accordingly.\nLet the computer do the work for you: Pruning using the 1-SE rule can be automatically obtained with the function autoprune in package adabag. Note that, because of the randomness involved in the CP evaluation, you may not find exactly the same result as the one obtained by hand. Use set.seed to make your result reproducible.\n\nlibrary(adabag)\nset.seed(123455)\nrpart.plot(autoprune(SaleHigh ~ ., data=df_tr))\n\n\n\nThe rpart.plot package in R provides a convenient plotcp() function to plot the complexity parameter table for a decision tree fit with rpart(). Unfortunately, scikit-learn doesn’t provide an equivalent function out of the box. However, you can still calculate the complexity parameter values for your decision tree and plot them using matplotlib.\n\n# Get the paths of the leaf nodes for the Car Seats decision tree using cost complexity pruning\npath = carseats_tree.cost_complexity_pruning_path(X_train, y_train)\n# Extract the effective alphas and total impurities of the leaf nodes from the path object\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n# Create a plot to visualize the relationship between effective alphas and total impurities\nfig, ax = plt.subplots()\nax.plot(ccp_alphas, impurities, marker='o', linestyle=\"-\")\nax.set_xlabel(\"Effective alpha\")\nax.set_ylabel(\"Total impurity of leaves\")\nax.set_title(\"Total Impurity vs Effective alpha for Car Seats dataset\")\nax.invert_xaxis()\nplt.show()\n\nThis plot only shows the training set results, which doesn’t tell us about over-fitting. A better approach is to compute accuracy (a different metric than rpart) on a second test set, called the validation set used solely for finding the ideal hyperparameters in machine learning. Once we choose the best hyperparameters, we re-train the model one final time with those parameters and compare everything on the test set (but we should no longer change our models based on the test set). We will learn more about this validation set during the upcoming lectures. Here we’ll use the two functions cross_val_score and KFold from the sklearn.module_selection sub-module. We will use ten folds to find the ideal alpha (equivalent to cp from rpart::rpart()). This is not using the 1-SE rule but proposes a good alternative.\n\nfrom sklearn.model_selection import cross_val_score, KFold\n\ndef plotcp(X_train, y_train, random_state=123):\n    # Create a decision tree classifier\n    clf = DecisionTreeClassifier(random_state=random_state)\n\n    # Calculate the cross-validation scores for different values of alpha\n    path = clf.cost_complexity_pruning_path(X_train, y_train)\n    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n    # Perform cross-validation for each alpha\n    kfold = KFold(n_splits=10, shuffle=True, random_state=random_state)\n    scores = []\n    for ccp_alpha in ccp_alphas:\n        clf = DecisionTreeClassifier(random_state=random_state, ccp_alpha=ccp_alpha)\n        score = cross_val_score(clf, X_train, y_train, cv=kfold, scoring='accuracy')\n        scores.append(np.mean(score))\n\n    # Plot the cross-validation scores vs alpha\n    fig, ax = plt.subplots()\n    ax.plot(ccp_alphas, scores, marker='o', linestyle=\"-\")\n    ax.set_xlabel(\"ccp_alpha\")\n    ax.set_ylabel(\"Cross-validation score (accuracy)\")\n    ax.set_title(\"Pruning Complexity Parameter (ccp) vs Cross-validation Score\")\n    ax.invert_xaxis()\n    plt.show()\n\nplotcp(X_train, y_train)\n\nWe can see that the best validation scores are obtained around a ccp_alpha of 0.008. Similar to cp from rpart, in scikit-learn, the parameter controls the complexity of a classification tree by setting a penalty on the number of leaf nodes. A higher value of results in a simpler tree with fewer splits and more nodes being pruned. More specifically, alpha is the regularization parameter used for controlling the cost complexity of the tree. The cost complexity is the sum of the misclassification cost and the complexity cost of the tree. The complexity cost is proportional to the number of terminal nodes (leaves) in the tree. A higher value of alpha thus means that the model is less likely to overfit the training data and more likely to generalize better to new, unseen data. However, setting alpha too high can result in underfitting and poor model performance on both the training and test data. The optimal alpha value depends on the specific dataset and the problem being solved and can be determined through cross-validation or other model selection techniques, as demonstrated above.\nOnce you find the ideal alpha, you can specify it with the ccp_alpha argument in DecisionTreeClassifier(). Here we will take ccp_alpha as 0.008 for simplicity.\n\n# Create a decision tree classifier with a ccp_alpha of 0.025\ncarseats_tree_prune = DecisionTreeClassifier(random_state=123, ccp_alpha=0.008)\n\n# Fit the model to the data\ncarseats_tree_prune.fit(X_train, y_train)\n\n\n\n\nDecisionTreeClassifier(ccp_alpha=0.008, random_state=123)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(ccp_alpha=0.008, random_state=123) \n\n\n\n# You can again plot the figure with\nplt.clf()\nplt.figure(figsize=(12,10))\nplot_tree(carseats_tree_prune,filled=True, feature_names=X_train.columns, label = 'root',fontsize=5)\nplt.savefig('tree_pruned_high_dpi', dpi=200)\nplt.close()\n\n\nknitr::include_graphics(\"tree_pruned_high_dpi.png\")\n\nThis model is a lot simpler compared to the first tree made using python (where ccp_alpha was 0 by default).\nFor automatically pruning the tree, unlike the adabag::autoprune() function in R’s adabag package, scikit-learn does not have a built-in function for automatic pruning of decision trees. Instead, you can use cross-validation to determine the optimal tree depth and use that to prune the tree.\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Set up the decision tree classifier\ntree = DecisionTreeClassifier()\n\n# Set up the grid search parameters\nparam_grid = {'max_depth': range(1, 11)}\n\n# Run grid search cross-validation to find optimal tree depth\ngrid_search = GridSearchCV(tree, param_grid=param_grid, cv=10)\ngrid_search.fit(X_train, y_train)\n\n\n\n\nGridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n             param_grid={'max_depth': range(1, 11)})\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n             param_grid={'max_depth': range(1, 11)}) \n\n\nestimator: DecisionTreeClassifierDecisionTreeClassifier() \n\n DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier() \n\n\n\n\n# Use the best estimator to fit and prune the tree\npruned_tree = grid_search.best_estimator_\n\nplt.clf()\nfig, ax = plt.subplots(figsize=(15, 10))\nplot_tree(pruned_tree, ax=ax, feature_names=X_train.columns)\nplt.show()\n# you can choose to re-train the model once again with this new parameter\n\n\n\n\n\n\n\nPython approach vs adabag::autoprune()\n\n\n\nThe technique above does not explicitly use the 1-SE rule for pruning the decision tree. Instead, it uses cross-validation to find the optimal tree depth based on the mean test score across all folds. According to the documentation of adabag::autoprune(), “The cross validation estimation of the error (xerror) has a random component. To avoid this randomness the 1-SE rule (or 1-SD rule) selects the simplest model with a xerror equal or less than the minimum xerror plus the standard deviation of the minimum xerror.” If you’re interested in the 1-SE, see the adapted python code for it below.\nImplementing GridSearchCV with 1-SE rule\n\n# Set up the decision tree classifier\ntree = DecisionTreeClassifier()\n\n# Set up the grid search parameters\nparam_grid = {'max_depth': range(1, 11)}\n\n# Run grid search cross-validation to find optimal tree depth\ngrid_search = GridSearchCV(tree, param_grid=param_grid, cv=10)\ngrid_search.fit(X_train, y_train)\n\n\n\n\nGridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n             param_grid={'max_depth': range(1, 11)})\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n             param_grid={'max_depth': range(1, 11)}) \n\n\nestimator: DecisionTreeClassifierDecisionTreeClassifier() \n\n DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier() \n\n\n\n\n# Calculate the mean and standard error of test scores for each tree depth\nmean_scores = grid_search.cv_results_['mean_test_score']\nstd_scores = grid_search.cv_results_['std_test_score'] / np.sqrt(10)\n\n# Find the optimal depth using the 1-SE rule\noptimal_depth = grid_search.best_params_['max_depth']\noptimal_score = mean_scores[optimal_depth - 1]\nse = std_scores[optimal_depth - 1]\nbest_depth = optimal_depth\nfor depth in range(optimal_depth - 1, -1, -1):\n    score = mean_scores[depth]\n    if score + se &lt; optimal_score:\n        break\n    else:\n        best_depth = depth + 1\n\n# Use the best estimator to fit and prune the tree\npruned_tree = DecisionTreeClassifier(max_depth=best_depth)\npruned_tree.fit(X_train, y_train)\n\n\n\n\nDecisionTreeClassifier(max_depth=3)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_depth=3) \n\n\n\nThe results are the same as not using 1-SE.\n\n\n\n\n\n\n\nFirst, use the R plot to determine what is the prediction of the first instance of MyCarseats.\nAnswer\n\nMyCarseats[1,]\n\nFollow Left, Left, Left =&gt; The predicted answer is “No”.\n\n\nR\nPython\n\n\n\nThe function predict can be used build the predictions of the test set (use option type=\"class\"). This is similar to the previously seen models: the predict function used on an object of class .rpart (created by the function rpart), in fact, calls the function predict.rpart which is adapted to the model.\n\npred &lt;- predict(carseats_tree_prune, newdata=df_te, type=\"class\")\ntable(Pred=pred, Obs=df_te$SaleHigh)\n\nNote that, like most categorical models, you may ask for the probabilities instead of the classes by setting type=\"prob\".\n\npredict(carseats_tree_prune, newdata=df_te, type=\"prob\")\n\n\n\nTo print a confusion matrix of the predicted labels versus the true labels, we can use the crosstab() function from the pandas library. We pass the predicted and true labels as arguments to crosstab(), and use the rownames and colnames parameters to label the rows and columns of the table, respectively.\nFinally, we use the predict_proba() method of the decision tree classifier to predict the class probabilities for the test data. The predicted probabilities are stored in the y_probs variable, which is a NumPy array with shape (n_samples, n_classes).\n\n# Predict the class labels for the test data with the python implementation\ny_pred = carseats_tree_prune.predict(X_test)\n\n# Print a confusion matrix of the predicted labels versus the true labels\nprint(pd.crosstab(index=y_pred, columns=y_test, rownames=['Pred'], colnames=['Obs']))\n\nThis tree is worse than the R implementation, probably due to differences in other default values that we did not tune for.\n\n# Predict the class probabilities for the test data\ny_probs = carseats_tree_prune.predict_proba(X_test)\nprint(pd.DataFrame(y_probs))\n\n\n\n\n\nBy looking at the tree, interpret the most important features for the Sales: the highest in the tree.\nNote that, for the level Good of the ShelveLoc variable, only the Price drives the Sales (according to the tree). Otherwise, it is a subtle mixture between Price and CompPrice.",
    "crumbs": [
      "Labs",
      "Decision Trees"
    ]
  },
  {
    "objectID": "labs/03_Models/032_Trees/Ex_ML_Tree.html#data-preparation",
    "href": "labs/03_Models/032_Trees/Ex_ML_Tree.html#data-preparation",
    "title": "Models: CART",
    "section": "",
    "text": "First, install the package ISLR in order to access the data set Carseats. Use ?Carseats to read its description. To apply a classification of the sales, we first create a categorical outcome SaleHigh which equals “Yes” if Sales &gt; 7.5 and “No” otherwise. Then we create a data frame MyCarseats containing SaleHigh and all the features of Carseats except Sales. Finally, split MyCarseats into a training and a test set (2/3 vs 1/3). Below we call them df_tr and df_te.\n\nlibrary(ISLR)\nlibrary(dplyr)\nMyCarseats &lt;- Carseats %&gt;% mutate(SaleHigh=ifelse(Sales &gt; 7.5, \"Yes\", \"No\"))\nMyCarseats &lt;- MyCarseats %&gt;% select(-Sales)\n\nset.seed(123) # for reproducibility \nindex_tr &lt;- sample(x=1:nrow(MyCarseats), size=2/3*nrow(MyCarseats), replace=FALSE)\ndf_tr &lt;- MyCarseats[index_tr,]\ndf_te &lt;- MyCarseats[-index_tr,]\n\nNote: this is not the point of this exercise but remember that in a real situation the first step of the data analysis would be an EDA.",
    "crumbs": [
      "Labs",
      "Decision Trees"
    ]
  },
  {
    "objectID": "labs/03_Models/032_Trees/Ex_ML_Tree.html#fit-plot",
    "href": "labs/03_Models/032_Trees/Ex_ML_Tree.html#fit-plot",
    "title": "Models: CART",
    "section": "",
    "text": "R\nPython\n\n\n\nThe rpart function in the package rpart can be used to fit a classification tree with the same type of formulas as naive_bayes. It can then be plotted using the function rpart.plot of the package rpart.plot.\n\nlibrary(rpart)\nlibrary(rpart.plot)\nset.seed(1234)\ncarseats_tree &lt;- rpart(SaleHigh ~ ., data=df_tr)\nrpart.plot(carseats_tree)\n\n\n\n\n# Load MLBA environment\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\nWe use the df_train and df_te created in R to carry out our CART training. For this, we will use the sklearn library. First, we copy our dataset from the R variable to a python variable. Then we use any encoder (e.g. OneHotEncoder, LabelEncoder, OrdinalEncoder) from sklearn.preprocessing to convert categorical data into a numeric form, which many sklearn machine learning algorithms require. In this case, we use the OneHoteEncoder which is similar to making dummy variables and turn each level of the category into a column. Also, we standardize the data in the case of python for faster computations using StandardScaler, which also helps to bring numerical stability and improve the results. After this, we divide the training and test sets into predictors (e.g., X_train) and the outcome (e.g., y_train) and initialize our classification tree and fit the model to the data.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n# We copy the training and testing datasets from R to Python.\ncarset_tr_py = r.df_tr.copy()\ncarset_te_py= r.df_te.copy()\n\n# We encode categorical variables as numeric, which is necessary for CART.\nle = OneHotEncoder()\ncat_vars = [\"ShelveLoc\", \"Urban\", \"US\", \"SaleHigh\"]\nfor var in cat_vars:\n    # Fit the LabelEncoder to the training set and transform the training and testing sets\n    # We need to use the same encoder for both sets to ensure consistency\n    carset_tr_py_encoded = le.fit_transform(carset_tr_py[[var]].values.reshape(-1, 1))\n    carset_te_py_encoded = le.transform(carset_te_py[[var]].values.reshape(-1, 1))\n    carset_tr_py[var] = carset_tr_py_encoded.toarray()\n    carset_te_py[var] = carset_te_py_encoded.toarray()\n\n# Split the data into training and testing sets\nX_train, y_train = carset_tr_py.drop(columns=[\"SaleHigh\"]), carset_tr_py[\"SaleHigh\"]\nX_test, y_test = carset_te_py.drop(columns=[\"SaleHigh\"]), carset_te_py[\"SaleHigh\"]\n\n# Standardize only the continuous variables\ncont_vars = [\"CompPrice\", \"Income\", \"Advertising\", \"Population\", \"Price\", \"Age\", \"Education\"]\nscaler = StandardScaler()\nX_train[cont_vars] = scaler.fit_transform(X_train[cont_vars])\nX_test[cont_vars] = scaler.transform(X_test[cont_vars])\n\n# To speed up the operation you can also transform the inputs\n# X_train = X_train.to_numpy()\n# y_train = y_train.to_numpy()\n\nWe can now train and plot our decision tree using DecisionTreeClassifier and plot_tree functions.\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nimport matplotlib.pyplot as plt\n\n# we clear any previous figures\nplt.clf()\n\nnp.random.seed(1234)\ncarseats_tree = DecisionTreeClassifier().fit(X_train, y_train)\nplt.figure(figsize=(20,10))\nplot_tree(carseats_tree,filled=True, feature_names=X_train.columns, label = 'root',fontsize=5)\nplt.savefig('tree_high_dpi', dpi=300)\n# for a better quality, save the image and load it again\n#plt.show()\n\nAs the image dimensions are not always great for matplotlib plots in Rstudio, we saved the image and now load it again below using an R chunk.\n\nknitr::include_graphics(\"tree_high_dpi.png\")",
    "crumbs": [
      "Labs",
      "Decision Trees"
    ]
  },
  {
    "objectID": "labs/03_Models/032_Trees/Ex_ML_Tree.html#pruning",
    "href": "labs/03_Models/032_Trees/Ex_ML_Tree.html#pruning",
    "title": "Models: CART",
    "section": "",
    "text": "R\nPython\n\n\n\nThe analysis of the tree complexity can be obtained using function plotcp.\n\nplotcp(carseats_tree)\n\nFrom the graph, we can identify that, according to the 1-SE rule, the tree with 8 nodes is equivalent to the tree with 12 nodes. This 8-nodes tree should be preferred.\nTo prune the tree (i.e., extract the tree with 8 nodes), we can use the function prune with argument cp. The cp of the tree can be read on the bottom x-axis of the plotcp. The argument in prune should be set to any value between the cp of 8-nodes tree (0.031) and the 11-node tree (0.019). Here 0.025 is OK.\n\ncarseats_tree_prune &lt;- prune(carseats_tree, cp=0.025)\nrpart.plot(carseats_tree_prune)\n\nImportant note: The CP evaluation relies on a cross-validation procedure, which uses random number generation. This is why we have set the seed to some value (1234). You may not find the same result with another seed or if you do not set it. In any case, just be coherent with your results and prune the tree accordingly.\nLet the computer do the work for you: Pruning using the 1-SE rule can be automatically obtained with the function autoprune in package adabag. Note that, because of the randomness involved in the CP evaluation, you may not find exactly the same result as the one obtained by hand. Use set.seed to make your result reproducible.\n\nlibrary(adabag)\nset.seed(123455)\nrpart.plot(autoprune(SaleHigh ~ ., data=df_tr))\n\n\n\nThe rpart.plot package in R provides a convenient plotcp() function to plot the complexity parameter table for a decision tree fit with rpart(). Unfortunately, scikit-learn doesn’t provide an equivalent function out of the box. However, you can still calculate the complexity parameter values for your decision tree and plot them using matplotlib.\n\n# Get the paths of the leaf nodes for the Car Seats decision tree using cost complexity pruning\npath = carseats_tree.cost_complexity_pruning_path(X_train, y_train)\n# Extract the effective alphas and total impurities of the leaf nodes from the path object\nccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n# Create a plot to visualize the relationship between effective alphas and total impurities\nfig, ax = plt.subplots()\nax.plot(ccp_alphas, impurities, marker='o', linestyle=\"-\")\nax.set_xlabel(\"Effective alpha\")\nax.set_ylabel(\"Total impurity of leaves\")\nax.set_title(\"Total Impurity vs Effective alpha for Car Seats dataset\")\nax.invert_xaxis()\nplt.show()\n\nThis plot only shows the training set results, which doesn’t tell us about over-fitting. A better approach is to compute accuracy (a different metric than rpart) on a second test set, called the validation set used solely for finding the ideal hyperparameters in machine learning. Once we choose the best hyperparameters, we re-train the model one final time with those parameters and compare everything on the test set (but we should no longer change our models based on the test set). We will learn more about this validation set during the upcoming lectures. Here we’ll use the two functions cross_val_score and KFold from the sklearn.module_selection sub-module. We will use ten folds to find the ideal alpha (equivalent to cp from rpart::rpart()). This is not using the 1-SE rule but proposes a good alternative.\n\nfrom sklearn.model_selection import cross_val_score, KFold\n\ndef plotcp(X_train, y_train, random_state=123):\n    # Create a decision tree classifier\n    clf = DecisionTreeClassifier(random_state=random_state)\n\n    # Calculate the cross-validation scores for different values of alpha\n    path = clf.cost_complexity_pruning_path(X_train, y_train)\n    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n\n    # Perform cross-validation for each alpha\n    kfold = KFold(n_splits=10, shuffle=True, random_state=random_state)\n    scores = []\n    for ccp_alpha in ccp_alphas:\n        clf = DecisionTreeClassifier(random_state=random_state, ccp_alpha=ccp_alpha)\n        score = cross_val_score(clf, X_train, y_train, cv=kfold, scoring='accuracy')\n        scores.append(np.mean(score))\n\n    # Plot the cross-validation scores vs alpha\n    fig, ax = plt.subplots()\n    ax.plot(ccp_alphas, scores, marker='o', linestyle=\"-\")\n    ax.set_xlabel(\"ccp_alpha\")\n    ax.set_ylabel(\"Cross-validation score (accuracy)\")\n    ax.set_title(\"Pruning Complexity Parameter (ccp) vs Cross-validation Score\")\n    ax.invert_xaxis()\n    plt.show()\n\nplotcp(X_train, y_train)\n\nWe can see that the best validation scores are obtained around a ccp_alpha of 0.008. Similar to cp from rpart, in scikit-learn, the parameter controls the complexity of a classification tree by setting a penalty on the number of leaf nodes. A higher value of results in a simpler tree with fewer splits and more nodes being pruned. More specifically, alpha is the regularization parameter used for controlling the cost complexity of the tree. The cost complexity is the sum of the misclassification cost and the complexity cost of the tree. The complexity cost is proportional to the number of terminal nodes (leaves) in the tree. A higher value of alpha thus means that the model is less likely to overfit the training data and more likely to generalize better to new, unseen data. However, setting alpha too high can result in underfitting and poor model performance on both the training and test data. The optimal alpha value depends on the specific dataset and the problem being solved and can be determined through cross-validation or other model selection techniques, as demonstrated above.\nOnce you find the ideal alpha, you can specify it with the ccp_alpha argument in DecisionTreeClassifier(). Here we will take ccp_alpha as 0.008 for simplicity.\n\n# Create a decision tree classifier with a ccp_alpha of 0.025\ncarseats_tree_prune = DecisionTreeClassifier(random_state=123, ccp_alpha=0.008)\n\n# Fit the model to the data\ncarseats_tree_prune.fit(X_train, y_train)\n\n\n\n\nDecisionTreeClassifier(ccp_alpha=0.008, random_state=123)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(ccp_alpha=0.008, random_state=123) \n\n\n\n# You can again plot the figure with\nplt.clf()\nplt.figure(figsize=(12,10))\nplot_tree(carseats_tree_prune,filled=True, feature_names=X_train.columns, label = 'root',fontsize=5)\nplt.savefig('tree_pruned_high_dpi', dpi=200)\nplt.close()\n\n\nknitr::include_graphics(\"tree_pruned_high_dpi.png\")\n\nThis model is a lot simpler compared to the first tree made using python (where ccp_alpha was 0 by default).\nFor automatically pruning the tree, unlike the adabag::autoprune() function in R’s adabag package, scikit-learn does not have a built-in function for automatic pruning of decision trees. Instead, you can use cross-validation to determine the optimal tree depth and use that to prune the tree.\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Set up the decision tree classifier\ntree = DecisionTreeClassifier()\n\n# Set up the grid search parameters\nparam_grid = {'max_depth': range(1, 11)}\n\n# Run grid search cross-validation to find optimal tree depth\ngrid_search = GridSearchCV(tree, param_grid=param_grid, cv=10)\ngrid_search.fit(X_train, y_train)\n\n\n\n\nGridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n             param_grid={'max_depth': range(1, 11)})\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n             param_grid={'max_depth': range(1, 11)}) \n\n\nestimator: DecisionTreeClassifierDecisionTreeClassifier() \n\n DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier() \n\n\n\n\n# Use the best estimator to fit and prune the tree\npruned_tree = grid_search.best_estimator_\n\nplt.clf()\nfig, ax = plt.subplots(figsize=(15, 10))\nplot_tree(pruned_tree, ax=ax, feature_names=X_train.columns)\nplt.show()\n# you can choose to re-train the model once again with this new parameter\n\n\n\n\n\n\n\nPython approach vs adabag::autoprune()\n\n\n\nThe technique above does not explicitly use the 1-SE rule for pruning the decision tree. Instead, it uses cross-validation to find the optimal tree depth based on the mean test score across all folds. According to the documentation of adabag::autoprune(), “The cross validation estimation of the error (xerror) has a random component. To avoid this randomness the 1-SE rule (or 1-SD rule) selects the simplest model with a xerror equal or less than the minimum xerror plus the standard deviation of the minimum xerror.” If you’re interested in the 1-SE, see the adapted python code for it below.\nImplementing GridSearchCV with 1-SE rule\n\n# Set up the decision tree classifier\ntree = DecisionTreeClassifier()\n\n# Set up the grid search parameters\nparam_grid = {'max_depth': range(1, 11)}\n\n# Run grid search cross-validation to find optimal tree depth\ngrid_search = GridSearchCV(tree, param_grid=param_grid, cv=10)\ngrid_search.fit(X_train, y_train)\n\n\n\n\nGridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n             param_grid={'max_depth': range(1, 11)})\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=10, estimator=DecisionTreeClassifier(),\n             param_grid={'max_depth': range(1, 11)}) \n\n\nestimator: DecisionTreeClassifierDecisionTreeClassifier() \n\n DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier() \n\n\n\n\n# Calculate the mean and standard error of test scores for each tree depth\nmean_scores = grid_search.cv_results_['mean_test_score']\nstd_scores = grid_search.cv_results_['std_test_score'] / np.sqrt(10)\n\n# Find the optimal depth using the 1-SE rule\noptimal_depth = grid_search.best_params_['max_depth']\noptimal_score = mean_scores[optimal_depth - 1]\nse = std_scores[optimal_depth - 1]\nbest_depth = optimal_depth\nfor depth in range(optimal_depth - 1, -1, -1):\n    score = mean_scores[depth]\n    if score + se &lt; optimal_score:\n        break\n    else:\n        best_depth = depth + 1\n\n# Use the best estimator to fit and prune the tree\npruned_tree = DecisionTreeClassifier(max_depth=best_depth)\npruned_tree.fit(X_train, y_train)\n\n\n\n\nDecisionTreeClassifier(max_depth=3)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_depth=3) \n\n\n\nThe results are the same as not using 1-SE.",
    "crumbs": [
      "Labs",
      "Decision Trees"
    ]
  },
  {
    "objectID": "labs/03_Models/032_Trees/Ex_ML_Tree.html#predictions",
    "href": "labs/03_Models/032_Trees/Ex_ML_Tree.html#predictions",
    "title": "Models: CART",
    "section": "",
    "text": "First, use the R plot to determine what is the prediction of the first instance of MyCarseats.\nAnswer\n\nMyCarseats[1,]\n\nFollow Left, Left, Left =&gt; The predicted answer is “No”.\n\n\nR\nPython\n\n\n\nThe function predict can be used build the predictions of the test set (use option type=\"class\"). This is similar to the previously seen models: the predict function used on an object of class .rpart (created by the function rpart), in fact, calls the function predict.rpart which is adapted to the model.\n\npred &lt;- predict(carseats_tree_prune, newdata=df_te, type=\"class\")\ntable(Pred=pred, Obs=df_te$SaleHigh)\n\nNote that, like most categorical models, you may ask for the probabilities instead of the classes by setting type=\"prob\".\n\npredict(carseats_tree_prune, newdata=df_te, type=\"prob\")\n\n\n\nTo print a confusion matrix of the predicted labels versus the true labels, we can use the crosstab() function from the pandas library. We pass the predicted and true labels as arguments to crosstab(), and use the rownames and colnames parameters to label the rows and columns of the table, respectively.\nFinally, we use the predict_proba() method of the decision tree classifier to predict the class probabilities for the test data. The predicted probabilities are stored in the y_probs variable, which is a NumPy array with shape (n_samples, n_classes).\n\n# Predict the class labels for the test data with the python implementation\ny_pred = carseats_tree_prune.predict(X_test)\n\n# Print a confusion matrix of the predicted labels versus the true labels\nprint(pd.crosstab(index=y_pred, columns=y_test, rownames=['Pred'], colnames=['Obs']))\n\nThis tree is worse than the R implementation, probably due to differences in other default values that we did not tune for.\n\n# Predict the class probabilities for the test data\ny_probs = carseats_tree_prune.predict_proba(X_test)\nprint(pd.DataFrame(y_probs))",
    "crumbs": [
      "Labs",
      "Decision Trees"
    ]
  },
  {
    "objectID": "labs/03_Models/032_Trees/Ex_ML_Tree.html#interpretation",
    "href": "labs/03_Models/032_Trees/Ex_ML_Tree.html#interpretation",
    "title": "Models: CART",
    "section": "",
    "text": "By looking at the tree, interpret the most important features for the Sales: the highest in the tree.\nNote that, for the level Good of the ShelveLoc variable, only the Price drives the Sales (according to the tree). Otherwise, it is a subtle mixture between Price and CompPrice.",
    "crumbs": [
      "Labs",
      "Decision Trees"
    ]
  },
  {
    "objectID": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html",
    "href": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html",
    "title": "Models: Support Vector Machine",
    "section": "",
    "text": "In this exercise, we apply the Support Vector Machines (SVM) to the classification problem of the data set Carseats from the package ISLR (already used with CART).\nSVM are often difficult to interpret. They are typically what we call “a black box” model, that is, a model which provides predictions without understanding what is behind (except if you have followed the course…) and how the features influences these predictions. Therefore, and because we do not want to get bored, we will also use the caret::train() function to make our first real machine learning application. Note that this application can be done also with the models that have been seen previously.\n\nThe lines below are just a repetition/reminder of the CART series to have the data ready. The only difference is the cast of SalesHigh into factors rather than characters because the SVM functions require it.\nTo proceed we first have to build the data (below is the) Install the package ISLR in order to access the data set Carseats. Use ?Carseats to read its description.\nTo apply a classification of the sales, we first create a categorical outcome SaleHigh which equals “Yes” if Sales &gt; 7.5 and “No” otherwise. Then we create a data frame MyCarseats containing SaleHigh and all the features of Carseats except Sales. Finally, split MyCarseats into a training and a test set (2/3 vs 1/3). Below we call them df_tr and df_te.\n\nlibrary(ISLR)\nlibrary(dplyr)\nMyCarseats &lt;- Carseats %&gt;% mutate(SaleHigh=ifelse(Sales &gt; 7.5, \"Yes\", \"No\"))\nMyCarseats &lt;- MyCarseats %&gt;% select(-Sales)\nMyCarseats$SaleHigh &lt;- as.factor(MyCarseats$SaleHigh)\n\nset.seed(123) # for reproducibility \nindex_tr &lt;- sample(x=1:nrow(MyCarseats), size=0.8*nrow(MyCarseats), replace=FALSE)\ndf_tr &lt;- MyCarseats[index_tr,]\ndf_te &lt;- MyCarseats[-index_tr,]\n\n\n\n\nR\nPython\n\n\n\nThe e1071::svm() function of the e1071 package allows to fit SVM to the data with several possible kernels. Below, it is the linear kernel. We fit a linear kernel and check the predictions on the test set.\n\nlibrary(e1071)\nset.seed(123)\ncarseats_svm &lt;- svm(SaleHigh ~ ., data=df_tr, kernel=\"linear\")\ncarseats_svm\ncarseats_svm_pred &lt;- predict(carseats_svm, newdata = df_te)\n\ntable(Pred=carseats_svm_pred, obs=df_te$SaleHigh)\n\nTo obtain a better insight about the prediction quality, we will use the accuracy measure. It is simply the proportion of correct predictions. This can be conveniently obtained (and much more) from the function caret::confusionMatrix() of the library caret. In the parameters, data are the predictions, and reference are the observations.\n\nlibrary(caret)\nconfusionMatrix(data=carseats_svm_pred, reference = df_te$SaleHigh )\n\nWe should only focus on the accuracy for now (the other measures will be studied later). In our run, it was ≈86\\%. That number is obtained using the default parameter, cost C=1.\n\n\n\n# The usual loading of our environment\nlibrary(reticulate)\nuse_condaenv(\"MLBA\", required = TRUE)\n\nSimilar to the CART exercises, we use the df_train and df_te created in R to carry out our SVM training. Once again, we continue using the sklearn library. First, we copy our dataset from the R variable to a python variable. Then we use LabelEncoder() from sklearn.preprocessing to convert categorical data into a numeric form, which many sklearn machine learning algorithms require. The encoding assigns a unique numerical value to each categorical value, which can sometimes help the performance. In the case of caret::train(), the function handles this transformation automatically. Also, we standardize the data in the case of python for faster computations using StandardScaler, because of the same numerical stability mentioned for CART. After this, we divide the training and test sets into predictors (e.g., X_train) and the outcome (e.g., y_train) and initialize our linear kernel SVM to fit the model to the data.\nWe will use classification_report and accuracy_score from sklearn.metrics to get more information on the performance. (you could also use confusion_matrix from the same module.)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import svm\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n\n# We copy the training and testing datasets from R to Python.\ncarset_tr_py = r.df_tr.copy()\ncarset_te_py= r.df_te.copy()\n\n# We encode categorical variables as numeric, which is necessary for SVM.\nle = LabelEncoder()\ncat_vars = [\"ShelveLoc\", \"Urban\", \"US\", \"SaleHigh\"]\nfor var in cat_vars:\n    # Fit the LabelEncoder to the training set and transform the training and testing sets\n    # We need to use the same encoder for both sets to ensure consistency\n    carset_tr_py[var] = le.fit_transform(carset_tr_py[var])\n    carset_te_py[var] = le.transform(carset_te_py[var])\n\n# Split the data into training and testing sets\nX_train, y_train = carset_tr_py.drop(columns=[\"SaleHigh\"]), carset_tr_py[\"SaleHigh\"]\nX_test, y_test = carset_te_py.drop(columns=[\"SaleHigh\"]), carset_te_py[\"SaleHigh\"]\n\n# Standardize only the continuous variables\ncont_vars = [\"CompPrice\", \"Income\", \"Advertising\", \"Population\", \"Price\", \"Age\", \"Education\"]\nscaler = StandardScaler()\nX_train[cont_vars] = scaler.fit_transform(X_train[cont_vars])\nX_test[cont_vars] = scaler.transform(X_test[cont_vars])\n\n# To speed up the operation you can also transform the inputs\n# X_train = X_train.to_numpy()\n# y_train = y_train.to_numpy()\n\n# Set random seed for reproducibility\nnp.random.seed(123)\n\n# Initialize a linear SVM model\ncarseats_svm_py = svm.SVC(kernel=\"linear\")\n\n# Fit the SVM model to the training data\ncarseats_svm_py.fit(X_train, y_train)\n\n\n\n\nSVC(kernel='linear')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  SVC?Documentation for SVCiFittedSVC(kernel='linear') \n\n\n# Print the model parameters\nprint(carseats_svm_py)\n# Make predictions on the test set\ncarseats_svm_pred_py = carseats_svm_py.predict(X_test)\n\n# Print a confusion matrix\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_py, rownames=['True'], colnames=['Predicted']))\n# Alternatively, print a confusin matrix using `sklearn.metrics`\n# print(confusion_matrix(y_test, carseats_svm_pred_py))\n\n# Compute metrics\nreport_linear_py = classification_report(y_test, carseats_svm_pred_py, target_names=['No', 'Yes'])\naccuracy_linear_py = accuracy_score(y_test, carseats_svm_pred_py)\n\nprint(report_linear_py)\nprint(\"Overall accuracy:\", accuracy_linear_py)\n\nThis performance is worse than our model in R and requires more tuning (due to differences in default values and implementations of the functions).\n\n\n\n\n\n\nR\nPython\n\n\n\nWe try now with a radial basis kernel (the default).\n\nset.seed(123)\ncarseats_rb &lt;- svm(SaleHigh ~ ., data=df_tr, kernel=\"radial\")\ncarseats_rb\ncarseats_rb_pred &lt;- predict(carseats_rb, newdata = df_te)\nconfusionMatrix(data=carseats_rb_pred, reference = df_te$SaleHigh )\n\nThe accuracy is now ≈81\\%. This shows how important that choice can be. In the same vein, we are now relying on the default parameters of the function. For the cost C it is 1, for the parameter gamma of the kernel, it is 1/(data dimension) (see ?svm). The train function allows us to make a better selection.\n\n\nThis is same as the linear kernel, and we just need to change the kernel parameter value from linear to rbf. Here’s the radial version:\n\nnp.random.seed(123) # once again, for reproducibility\n\n# Use a radial kernel in the SVM classifier\ncarseats_svm_rb_py = svm.SVC(kernel=\"rbf\")\ncarseats_svm_rb_py.fit(X_train, y_train)\n\n\n\n\nSVC()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  SVC?Documentation for SVCiFittedSVC() \n\n\nprint(carseats_svm_rb_py)\n# Predict the values with the radial approach\ncarseats_svm_pred_rb_py = carseats_svm_rb_py.predict(X_test)\n\n# Compute metrics\nreport_radial_py = classification_report(y_test, carseats_svm_pred_rb_py, target_names=['No', 'Yes'])\naccuracy_radial_py = accuracy_score(y_test, carseats_svm_pred_rb_py)\n\n# Print metrics\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_rb_py, rownames=['True'], colnames=['Predicted']))\nprint(report_radial_py)\nprint(\"Overall accuracy:\", accuracy_radial_py)\n\n\n\n\nIn both cases (R & python), with the default parameters, the linear kernel seems to do better than the radial one.\n\n\n\nR\nPython\n\n\n\nIn R, caret uses various libraries to run the svm models (check for yourself by searching for support vector machine here). For instance, calling svmLinear or svmRadial uses the library kernlab, and the kernlab::ksvm() function.\nThe C hyperparameter (from kernlab::ksvm()) accounts for the cost argument and controls the trade-off between allowing misclassifications in the training set and finding a decision boundary that generalizes well to new data. A larger cost value leads to a smaller margin and a more complex model that may overfit the data. On the other hand, a smaller cost value leads to a larger margin and a simpler model that may underfit the data.\nSimilarly to EX_ML_NN, to select the good hyperparameters, we build a search grid and fit the model with each possible value in the grid. Then, the best model is chosen among all the combinations of the hyperparameters.\nAs a reminder, the train function from caret. Has:\n\na formula.\na dataset.\na method (i.e. the model which in this case is SVM with linear kernel).\na training control procedure.\n\n\n\ntrctrl &lt;- trainControl(method = \"cv\", number=10)\nset.seed(143)\nsvm_Linear &lt;- train(SaleHigh ~., data = df_tr, method = \"svmLinear\",\n                    trControl=trctrl)\nsvm_Linear\n\nFor now, the validation accuracy is very high (≈89\\%). This is normal since this accuracy is computed on the training set.\nWe now supply a grid of values for the cost that we want to try and pass to the argugmenttuneGrid. Be patient, it may take time.\n\ngrid &lt;- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))\ngrid\nset.seed(143)\nsvm_Linear_Grid &lt;- train(SaleHigh ~., data = df_tr, method = \"svmLinear\",\n                           trControl=trctrl,\n                           tuneGrid = grid)\nsvm_Linear_Grid\nplot(svm_Linear_Grid)\nsvm_Linear_Grid$bestTune\n\nWe see that setting the cost to 1 provides the best model. The accuracy apparently reaches a plateau at this value. This is same as our cost parameter in section 1.2.\n\nThe sigma hyperparameter (also from kernlab::ksvm()) controls the width of the radial basis function kernel, which is used to transform the input data into a higher-dimensional feature space. A larger value of sigma corresponds to a narrower kernel and a more complex model, while a smaller value corresponds to a wider kernel and a simpler model.\nWe repeat the procedure for SVM with a radial basis kernel. Here, there are two parameters ( sigma and C) to tune. The grid choice is rather arbitrary (often the result of trials and errors), and very few general useful guidelines exist. The code below may take a few minutes to run.\n\ngrid_radial &lt;- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),\n                           C = c(1, 10, 100, 500, 1000))\ngrid_radial\nset.seed(143)\nsvm_Radial_Grid &lt;- train(SaleHigh ~., data = df_tr, method = \"svmRadial\",\n                           trControl=trctrl,\n                           tuneGrid = grid_radial)\nsvm_Radial_Grid\nplot(svm_Radial_Grid)\nsvm_Radial_Grid$bestTune\n\nThe optimal model from this search is with sigma = 0.01 and C=100.\n\n\n\nWe can use GridSearchCV from sklearn.model_selection to achieve the same goal in python and set the argument for cross-validation (cv to achieve the same results) and tune both types of kernels at once. sklearn.svm.SVC() does contain the two arguments for the C and sigma but the relationship is slightly different and we’ll explain below. Also, please note in this approach, the linear kernel by default ignores the sigma values.\nAs mentioned earlier, in sklearn.svm.SVC(), the equivalent parameter to cost is also C, which is straightforward. Still, the equivalent parameter to sigma is a bit trickier (called gamma in sklearn.svm.SVC()), which also controls the width of the radial basis function kernel. However, the relationship between gamma and sigma differs, and the two parameters cannot be directly compared. In particular, gamma is defined as the inverse of the width of the kernel, i.e., gamma = 1/(2 * sigma**2).\nFor simplicity’s sake, we’ll directly use the gamma with the same values as sigma; however, you can always run 'gamma': [1/(2*sigma**2) for sigma in [0.01, 0.02, 0.05, 0.1]] to get similar values (although you would want to round as this division results in non-terminating repeating decimal numbers).\nThe code will take a few minutes to run (longer than the R version).\n\nfrom sklearn.model_selection import GridSearchCV\n\nnp.random.seed(123) # for reproducibility\n\n# Define the grid of hyperparameters to search over\nparam_grid = {'C': [1, 10, 100, 500, 1000],\n              'gamma': [0.01, 0.02, 0.05, 0.1],\n              # 'gamma': [round(1/(2*sigma**2),2) for sigma in [0.01, 0.02, 0.05, 0.1]],\n              'kernel': ['linear', 'rbf']}\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(svm.SVC(), param_grid, cv=10, scoring='accuracy', n_jobs = 1) # you can also set n_jobs = -1 to use all the cores and obtain the results faster (but unfortunately atm it only works on Mac/Linux and not Windows OS with `reticulate`)\n# for more info, please see https://github.com/rstudio/reticulate/issues/1346\ngrid_search.fit(X_train, y_train)\n\n\n\n\nGridSearchCV(cv=10, estimator=SVC(), n_jobs=1,\n             param_grid={'C': [1, 10, 100, 500, 1000],\n                         'gamma': [0.01, 0.02, 0.05, 0.1],\n                         'kernel': ['linear', 'rbf']},\n             scoring='accuracy')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=10, estimator=SVC(), n_jobs=1,\n             param_grid={'C': [1, 10, 100, 500, 1000],\n                         'gamma': [0.01, 0.02, 0.05, 0.1],\n                         'kernel': ['linear', 'rbf']},\n             scoring='accuracy') \n\n\nestimator: SVCSVC() \n\n SVC?Documentation for SVCSVC() \n\n\n\n\nprint(\"Best hyperparameters: \", grid_search.best_params_)\nprint(\"Best score: \", grid_search.best_score_)\n\nNote that the accuracy of 85% is on the training set. The best parameters are C = 500, gamma = 0.01, kernel = rbf. We will use a new plotting library for seeing this evolution called searborn, which offers some great visualization tools.\nNote that the accuracy of is on the training set. The best parameters are C = 500, gamma = 0.01, kernel = rbf. We will use a new plotting library for seeing this evolution called searborn, which offers some great visualization tools.\nWe use the installed seaborn package from our Setup which allows for grouping our hyperparameters and displaying them with a heatmap.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming `results` is your DataFrame from `grid_search.cv_results_`\nresults = pd.DataFrame(grid_search.cv_results_)\n\n# Before performing the pivot or groupby operation, drop the 'params' column (it's a dictionary)\nresults = results.drop(columns=['params'])\n\n# Now, you can safely group by 'param_C', 'param_gamma', and 'param_kernel' to calculate mean test scores\ngrouped_results = results.pivot_table(index=['param_C', 'param_gamma'], columns='param_kernel', values='mean_test_score')\n\nplt.clf()\n\n# Create the heatmap\nax = sns.heatmap(grouped_results, annot=True, fmt='.3f', cmap='viridis', cbar=False)\nax.invert_yaxis()  # invert the y-axis to match your preference\n\nplt.xlabel('Kernel')\nplt.ylabel('C-Gamma')\nplt.show()\n\nWe can see that rbf kernel benefits from changes in C & gamma, however, for the linear, we’re always using the same gamma of 0.01 so for this kernel, the only changes are coming from C parameter.\n\n\n\n\n\n\nR\nPython\n\n\n\nAfter finding the best hyperparameters, it is often good practice to re-train the model with the best hyperparameters on the entire training set before evaluating everything on the test set. We do not need to re-train the model with the entire dataset for the linear SVM, as the best cost matched those used in section 1.2. We re-train the final model with the entire training set using optimal hyperparameters for the radial basis kernel.\n\ncarseats_rb_tuned &lt;- svm(SaleHigh ~ .,data = df_tr,\n                         kernel = \"radial\", gamma = svm_Radial_Grid$bestTune$sigma,\n                         cost = svm_Radial_Grid$bestTune$C)\ncarseats_rb_tuned_pred &lt;- predict(carseats_rb_tuned, newdata = df_te)\nconfusionMatrix(data=carseats_rb_tuned_pred, reference = df_te$SaleHigh)\n\nOverall, if we compare all the models, we see that the linear kernel SVM with cost of 1 looks like the best model. We already saw that it provides a 86\\% accuracy on the test set. This is what can be expected in the future from that model.\n\n\n\n# re-train the model with best hyperparameters\nsvm_best_py = svm.SVC(**grid_search.best_params_)\nsvm_best_py.fit(X_train, y_train)\n\n\n\n\nSVC(C=500, gamma=0.01)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  SVC?Documentation for SVCiFittedSVC(C=500, gamma=0.01) \n\n\n# predict on test dataset\ncarseats_svm_pred_py = svm_best_py.predict(X_test)\n\n# print confusion matrix and accuracy score\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_py, rownames=['True'], colnames=['Predicted']))\nprint(classification_report(y_test, carseats_svm_pred_py, target_names=['No', 'Yes']))\nprint('Accuracy Score:', accuracy_score(y_test, carseats_svm_pred_py))\n\nThe results are similar to the R outcome for the linear models (except differences in the confusion matrix).\n\n\n\n\nRepeat the analysis on the German credit data (german.csv). Since dataset is much larger than MyCarSeats, the tuning procedure may be longer. For this reason, just limit to a linear SVM model for the tuning with limited range for the grid search.",
    "crumbs": [
      "Labs",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#prepare-the-data",
    "href": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#prepare-the-data",
    "title": "Models: Support Vector Machine",
    "section": "",
    "text": "The lines below are just a repetition/reminder of the CART series to have the data ready. The only difference is the cast of SalesHigh into factors rather than characters because the SVM functions require it.\nTo proceed we first have to build the data (below is the) Install the package ISLR in order to access the data set Carseats. Use ?Carseats to read its description.\nTo apply a classification of the sales, we first create a categorical outcome SaleHigh which equals “Yes” if Sales &gt; 7.5 and “No” otherwise. Then we create a data frame MyCarseats containing SaleHigh and all the features of Carseats except Sales. Finally, split MyCarseats into a training and a test set (2/3 vs 1/3). Below we call them df_tr and df_te.\n\nlibrary(ISLR)\nlibrary(dplyr)\nMyCarseats &lt;- Carseats %&gt;% mutate(SaleHigh=ifelse(Sales &gt; 7.5, \"Yes\", \"No\"))\nMyCarseats &lt;- MyCarseats %&gt;% select(-Sales)\nMyCarseats$SaleHigh &lt;- as.factor(MyCarseats$SaleHigh)\n\nset.seed(123) # for reproducibility \nindex_tr &lt;- sample(x=1:nrow(MyCarseats), size=0.8*nrow(MyCarseats), replace=FALSE)\ndf_tr &lt;- MyCarseats[index_tr,]\ndf_te &lt;- MyCarseats[-index_tr,]",
    "crumbs": [
      "Labs",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#linear_svm",
    "href": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#linear_svm",
    "title": "Models: Support Vector Machine",
    "section": "",
    "text": "R\nPython\n\n\n\nThe e1071::svm() function of the e1071 package allows to fit SVM to the data with several possible kernels. Below, it is the linear kernel. We fit a linear kernel and check the predictions on the test set.\n\nlibrary(e1071)\nset.seed(123)\ncarseats_svm &lt;- svm(SaleHigh ~ ., data=df_tr, kernel=\"linear\")\ncarseats_svm\ncarseats_svm_pred &lt;- predict(carseats_svm, newdata = df_te)\n\ntable(Pred=carseats_svm_pred, obs=df_te$SaleHigh)\n\nTo obtain a better insight about the prediction quality, we will use the accuracy measure. It is simply the proportion of correct predictions. This can be conveniently obtained (and much more) from the function caret::confusionMatrix() of the library caret. In the parameters, data are the predictions, and reference are the observations.\n\nlibrary(caret)\nconfusionMatrix(data=carseats_svm_pred, reference = df_te$SaleHigh )\n\nWe should only focus on the accuracy for now (the other measures will be studied later). In our run, it was ≈86\\%. That number is obtained using the default parameter, cost C=1.\n\n\n\n# The usual loading of our environment\nlibrary(reticulate)\nuse_condaenv(\"MLBA\", required = TRUE)\n\nSimilar to the CART exercises, we use the df_train and df_te created in R to carry out our SVM training. Once again, we continue using the sklearn library. First, we copy our dataset from the R variable to a python variable. Then we use LabelEncoder() from sklearn.preprocessing to convert categorical data into a numeric form, which many sklearn machine learning algorithms require. The encoding assigns a unique numerical value to each categorical value, which can sometimes help the performance. In the case of caret::train(), the function handles this transformation automatically. Also, we standardize the data in the case of python for faster computations using StandardScaler, because of the same numerical stability mentioned for CART. After this, we divide the training and test sets into predictors (e.g., X_train) and the outcome (e.g., y_train) and initialize our linear kernel SVM to fit the model to the data.\nWe will use classification_report and accuracy_score from sklearn.metrics to get more information on the performance. (you could also use confusion_matrix from the same module.)\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import svm\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n\n# We copy the training and testing datasets from R to Python.\ncarset_tr_py = r.df_tr.copy()\ncarset_te_py= r.df_te.copy()\n\n# We encode categorical variables as numeric, which is necessary for SVM.\nle = LabelEncoder()\ncat_vars = [\"ShelveLoc\", \"Urban\", \"US\", \"SaleHigh\"]\nfor var in cat_vars:\n    # Fit the LabelEncoder to the training set and transform the training and testing sets\n    # We need to use the same encoder for both sets to ensure consistency\n    carset_tr_py[var] = le.fit_transform(carset_tr_py[var])\n    carset_te_py[var] = le.transform(carset_te_py[var])\n\n# Split the data into training and testing sets\nX_train, y_train = carset_tr_py.drop(columns=[\"SaleHigh\"]), carset_tr_py[\"SaleHigh\"]\nX_test, y_test = carset_te_py.drop(columns=[\"SaleHigh\"]), carset_te_py[\"SaleHigh\"]\n\n# Standardize only the continuous variables\ncont_vars = [\"CompPrice\", \"Income\", \"Advertising\", \"Population\", \"Price\", \"Age\", \"Education\"]\nscaler = StandardScaler()\nX_train[cont_vars] = scaler.fit_transform(X_train[cont_vars])\nX_test[cont_vars] = scaler.transform(X_test[cont_vars])\n\n# To speed up the operation you can also transform the inputs\n# X_train = X_train.to_numpy()\n# y_train = y_train.to_numpy()\n\n# Set random seed for reproducibility\nnp.random.seed(123)\n\n# Initialize a linear SVM model\ncarseats_svm_py = svm.SVC(kernel=\"linear\")\n\n# Fit the SVM model to the training data\ncarseats_svm_py.fit(X_train, y_train)\n\n\n\n\nSVC(kernel='linear')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  SVC?Documentation for SVCiFittedSVC(kernel='linear') \n\n\n# Print the model parameters\nprint(carseats_svm_py)\n# Make predictions on the test set\ncarseats_svm_pred_py = carseats_svm_py.predict(X_test)\n\n# Print a confusion matrix\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_py, rownames=['True'], colnames=['Predicted']))\n# Alternatively, print a confusin matrix using `sklearn.metrics`\n# print(confusion_matrix(y_test, carseats_svm_pred_py))\n\n# Compute metrics\nreport_linear_py = classification_report(y_test, carseats_svm_pred_py, target_names=['No', 'Yes'])\naccuracy_linear_py = accuracy_score(y_test, carseats_svm_pred_py)\n\nprint(report_linear_py)\nprint(\"Overall accuracy:\", accuracy_linear_py)\n\nThis performance is worse than our model in R and requires more tuning (due to differences in default values and implementations of the functions).",
    "crumbs": [
      "Labs",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#radial-basis-svm",
    "href": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#radial-basis-svm",
    "title": "Models: Support Vector Machine",
    "section": "",
    "text": "R\nPython\n\n\n\nWe try now with a radial basis kernel (the default).\n\nset.seed(123)\ncarseats_rb &lt;- svm(SaleHigh ~ ., data=df_tr, kernel=\"radial\")\ncarseats_rb\ncarseats_rb_pred &lt;- predict(carseats_rb, newdata = df_te)\nconfusionMatrix(data=carseats_rb_pred, reference = df_te$SaleHigh )\n\nThe accuracy is now ≈81\\%. This shows how important that choice can be. In the same vein, we are now relying on the default parameters of the function. For the cost C it is 1, for the parameter gamma of the kernel, it is 1/(data dimension) (see ?svm). The train function allows us to make a better selection.\n\n\nThis is same as the linear kernel, and we just need to change the kernel parameter value from linear to rbf. Here’s the radial version:\n\nnp.random.seed(123) # once again, for reproducibility\n\n# Use a radial kernel in the SVM classifier\ncarseats_svm_rb_py = svm.SVC(kernel=\"rbf\")\ncarseats_svm_rb_py.fit(X_train, y_train)\n\n\n\n\nSVC()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  SVC?Documentation for SVCiFittedSVC() \n\n\nprint(carseats_svm_rb_py)\n# Predict the values with the radial approach\ncarseats_svm_pred_rb_py = carseats_svm_rb_py.predict(X_test)\n\n# Compute metrics\nreport_radial_py = classification_report(y_test, carseats_svm_pred_rb_py, target_names=['No', 'Yes'])\naccuracy_radial_py = accuracy_score(y_test, carseats_svm_pred_rb_py)\n\n# Print metrics\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_rb_py, rownames=['True'], colnames=['Predicted']))\nprint(report_radial_py)\nprint(\"Overall accuracy:\", accuracy_radial_py)\n\n\n\n\nIn both cases (R & python), with the default parameters, the linear kernel seems to do better than the radial one.",
    "crumbs": [
      "Labs",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#tuning-the-hyperparameter",
    "href": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#tuning-the-hyperparameter",
    "title": "Models: Support Vector Machine",
    "section": "",
    "text": "R\nPython\n\n\n\nIn R, caret uses various libraries to run the svm models (check for yourself by searching for support vector machine here). For instance, calling svmLinear or svmRadial uses the library kernlab, and the kernlab::ksvm() function.\nThe C hyperparameter (from kernlab::ksvm()) accounts for the cost argument and controls the trade-off between allowing misclassifications in the training set and finding a decision boundary that generalizes well to new data. A larger cost value leads to a smaller margin and a more complex model that may overfit the data. On the other hand, a smaller cost value leads to a larger margin and a simpler model that may underfit the data.\nSimilarly to EX_ML_NN, to select the good hyperparameters, we build a search grid and fit the model with each possible value in the grid. Then, the best model is chosen among all the combinations of the hyperparameters.\nAs a reminder, the train function from caret. Has:\n\na formula.\na dataset.\na method (i.e. the model which in this case is SVM with linear kernel).\na training control procedure.\n\n\n\ntrctrl &lt;- trainControl(method = \"cv\", number=10)\nset.seed(143)\nsvm_Linear &lt;- train(SaleHigh ~., data = df_tr, method = \"svmLinear\",\n                    trControl=trctrl)\nsvm_Linear\n\nFor now, the validation accuracy is very high (≈89\\%). This is normal since this accuracy is computed on the training set.\nWe now supply a grid of values for the cost that we want to try and pass to the argugmenttuneGrid. Be patient, it may take time.\n\ngrid &lt;- expand.grid(C = c(0.01, 0.1, 1, 10, 100, 1000))\ngrid\nset.seed(143)\nsvm_Linear_Grid &lt;- train(SaleHigh ~., data = df_tr, method = \"svmLinear\",\n                           trControl=trctrl,\n                           tuneGrid = grid)\nsvm_Linear_Grid\nplot(svm_Linear_Grid)\nsvm_Linear_Grid$bestTune\n\nWe see that setting the cost to 1 provides the best model. The accuracy apparently reaches a plateau at this value. This is same as our cost parameter in section 1.2.\n\nThe sigma hyperparameter (also from kernlab::ksvm()) controls the width of the radial basis function kernel, which is used to transform the input data into a higher-dimensional feature space. A larger value of sigma corresponds to a narrower kernel and a more complex model, while a smaller value corresponds to a wider kernel and a simpler model.\nWe repeat the procedure for SVM with a radial basis kernel. Here, there are two parameters ( sigma and C) to tune. The grid choice is rather arbitrary (often the result of trials and errors), and very few general useful guidelines exist. The code below may take a few minutes to run.\n\ngrid_radial &lt;- expand.grid(sigma = c(0.01, 0.02, 0.05, 0.1),\n                           C = c(1, 10, 100, 500, 1000))\ngrid_radial\nset.seed(143)\nsvm_Radial_Grid &lt;- train(SaleHigh ~., data = df_tr, method = \"svmRadial\",\n                           trControl=trctrl,\n                           tuneGrid = grid_radial)\nsvm_Radial_Grid\nplot(svm_Radial_Grid)\nsvm_Radial_Grid$bestTune\n\nThe optimal model from this search is with sigma = 0.01 and C=100.\n\n\n\nWe can use GridSearchCV from sklearn.model_selection to achieve the same goal in python and set the argument for cross-validation (cv to achieve the same results) and tune both types of kernels at once. sklearn.svm.SVC() does contain the two arguments for the C and sigma but the relationship is slightly different and we’ll explain below. Also, please note in this approach, the linear kernel by default ignores the sigma values.\nAs mentioned earlier, in sklearn.svm.SVC(), the equivalent parameter to cost is also C, which is straightforward. Still, the equivalent parameter to sigma is a bit trickier (called gamma in sklearn.svm.SVC()), which also controls the width of the radial basis function kernel. However, the relationship between gamma and sigma differs, and the two parameters cannot be directly compared. In particular, gamma is defined as the inverse of the width of the kernel, i.e., gamma = 1/(2 * sigma**2).\nFor simplicity’s sake, we’ll directly use the gamma with the same values as sigma; however, you can always run 'gamma': [1/(2*sigma**2) for sigma in [0.01, 0.02, 0.05, 0.1]] to get similar values (although you would want to round as this division results in non-terminating repeating decimal numbers).\nThe code will take a few minutes to run (longer than the R version).\n\nfrom sklearn.model_selection import GridSearchCV\n\nnp.random.seed(123) # for reproducibility\n\n# Define the grid of hyperparameters to search over\nparam_grid = {'C': [1, 10, 100, 500, 1000],\n              'gamma': [0.01, 0.02, 0.05, 0.1],\n              # 'gamma': [round(1/(2*sigma**2),2) for sigma in [0.01, 0.02, 0.05, 0.1]],\n              'kernel': ['linear', 'rbf']}\n\n# Perform grid search with cross-validation\ngrid_search = GridSearchCV(svm.SVC(), param_grid, cv=10, scoring='accuracy', n_jobs = 1) # you can also set n_jobs = -1 to use all the cores and obtain the results faster (but unfortunately atm it only works on Mac/Linux and not Windows OS with `reticulate`)\n# for more info, please see https://github.com/rstudio/reticulate/issues/1346\ngrid_search.fit(X_train, y_train)\n\n\n\n\nGridSearchCV(cv=10, estimator=SVC(), n_jobs=1,\n             param_grid={'C': [1, 10, 100, 500, 1000],\n                         'gamma': [0.01, 0.02, 0.05, 0.1],\n                         'kernel': ['linear', 'rbf']},\n             scoring='accuracy')\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n\n  GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=10, estimator=SVC(), n_jobs=1,\n             param_grid={'C': [1, 10, 100, 500, 1000],\n                         'gamma': [0.01, 0.02, 0.05, 0.1],\n                         'kernel': ['linear', 'rbf']},\n             scoring='accuracy') \n\n\nestimator: SVCSVC() \n\n SVC?Documentation for SVCSVC() \n\n\n\n\nprint(\"Best hyperparameters: \", grid_search.best_params_)\nprint(\"Best score: \", grid_search.best_score_)\n\nNote that the accuracy of 85% is on the training set. The best parameters are C = 500, gamma = 0.01, kernel = rbf. We will use a new plotting library for seeing this evolution called searborn, which offers some great visualization tools.\nNote that the accuracy of is on the training set. The best parameters are C = 500, gamma = 0.01, kernel = rbf. We will use a new plotting library for seeing this evolution called searborn, which offers some great visualization tools.\nWe use the installed seaborn package from our Setup which allows for grouping our hyperparameters and displaying them with a heatmap.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Assuming `results` is your DataFrame from `grid_search.cv_results_`\nresults = pd.DataFrame(grid_search.cv_results_)\n\n# Before performing the pivot or groupby operation, drop the 'params' column (it's a dictionary)\nresults = results.drop(columns=['params'])\n\n# Now, you can safely group by 'param_C', 'param_gamma', and 'param_kernel' to calculate mean test scores\ngrouped_results = results.pivot_table(index=['param_C', 'param_gamma'], columns='param_kernel', values='mean_test_score')\n\nplt.clf()\n\n# Create the heatmap\nax = sns.heatmap(grouped_results, annot=True, fmt='.3f', cmap='viridis', cbar=False)\nax.invert_yaxis()  # invert the y-axis to match your preference\n\nplt.xlabel('Kernel')\nplt.ylabel('C-Gamma')\nplt.show()\n\nWe can see that rbf kernel benefits from changes in C & gamma, however, for the linear, we’re always using the same gamma of 0.01 so for this kernel, the only changes are coming from C parameter.",
    "crumbs": [
      "Labs",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#best-model",
    "href": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#best-model",
    "title": "Models: Support Vector Machine",
    "section": "",
    "text": "R\nPython\n\n\n\nAfter finding the best hyperparameters, it is often good practice to re-train the model with the best hyperparameters on the entire training set before evaluating everything on the test set. We do not need to re-train the model with the entire dataset for the linear SVM, as the best cost matched those used in section 1.2. We re-train the final model with the entire training set using optimal hyperparameters for the radial basis kernel.\n\ncarseats_rb_tuned &lt;- svm(SaleHigh ~ .,data = df_tr,\n                         kernel = \"radial\", gamma = svm_Radial_Grid$bestTune$sigma,\n                         cost = svm_Radial_Grid$bestTune$C)\ncarseats_rb_tuned_pred &lt;- predict(carseats_rb_tuned, newdata = df_te)\nconfusionMatrix(data=carseats_rb_tuned_pred, reference = df_te$SaleHigh)\n\nOverall, if we compare all the models, we see that the linear kernel SVM with cost of 1 looks like the best model. We already saw that it provides a 86\\% accuracy on the test set. This is what can be expected in the future from that model.\n\n\n\n# re-train the model with best hyperparameters\nsvm_best_py = svm.SVC(**grid_search.best_params_)\nsvm_best_py.fit(X_train, y_train)\n\n\n\n\nSVC(C=500, gamma=0.01)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  SVC?Documentation for SVCiFittedSVC(C=500, gamma=0.01) \n\n\n# predict on test dataset\ncarseats_svm_pred_py = svm_best_py.predict(X_test)\n\n# print confusion matrix and accuracy score\nprint(pd.crosstab(index=y_test, columns=carseats_svm_pred_py, rownames=['True'], colnames=['Predicted']))\nprint(classification_report(y_test, carseats_svm_pred_py, target_names=['No', 'Yes']))\nprint('Accuracy Score:', accuracy_score(y_test, carseats_svm_pred_py))\n\nThe results are similar to the R outcome for the linear models (except differences in the confusion matrix).",
    "crumbs": [
      "Labs",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#your-turn",
    "href": "labs/03_Models/034_SupportVectorMachine/Ex_ML_SVM.html#your-turn",
    "title": "Models: Support Vector Machine",
    "section": "",
    "text": "Repeat the analysis on the German credit data (german.csv). Since dataset is much larger than MyCarSeats, the tuning procedure may be longer. For this reason, just limit to a linear SVM model for the tuning with limited range for the grid search.",
    "crumbs": [
      "Labs",
      "Support Vector Machines"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html",
    "title": "Model scoring",
    "section": "",
    "text": "The data set is the one used in the series on linear regressions.\n\nlibrary(readr)\nreal_estate_data &lt;- read.csv(here::here(\"labs/data/real_estate_data.csv\"))\n\nThen we split the data in a training and a test set (0.8/0.2). For this, we use the createDataPartition function of the caret package.\n\nlibrary(caret)\nset.seed(234)\nindex_tr &lt;- createDataPartition(y = real_estate_data$Price, p= 0.8, list = FALSE)\ndf_tr &lt;- real_estate_data[index_tr,]\ndf_te &lt;- real_estate_data[-index_tr,]\n\n\nWe will compare a linear regression, a regression tree and a 3-NN (KNN).\n\n\nR\nPython\n\n\n\n\nlibrary(rpart)\nest_lm &lt;- lm(Price~TransDate+HouseAge+Dist+\n               NumStores+Lat+Long, data=df_tr)\nest_rt &lt;- rpart(Price~TransDate+HouseAge+Dist+\n                      NumStores+Lat+Long, data=df_tr)\nest_knn &lt;- knnreg(Price~TransDate+HouseAge+Dist+\n                      NumStores+Lat+Long, data=df_tr, k = 3)\n\n\n\n\n# Load the course python environment as usual with a r code chunks.\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\n\n# Fit the models: linear regression, regression tree, and KNN\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Define predictors and target variable\npredictors = ['TransDate', 'HouseAge', 'Dist', 'NumStores', 'Lat', 'Long']\ntarget = 'Price'\n\n# Fit models\nest_lm = LinearRegression().fit(r.df_tr[predictors], r.df_tr[target])\nest_rt = DecisionTreeRegressor(random_state=234).fit(r.df_tr[predictors], r.df_tr[target])\nest_knn = KNeighborsRegressor(n_neighbors=3).fit(r.df_tr[predictors], r.df_tr[target])\n\n\n\n\n\n\n\nR\nPython\n\n\n\nWe now compute the R2 for each model using the R2 function (in caret).\n\nR2(predict(est_lm, newdata = df_te), df_te$Price)\nR2(predict(est_rt, newdata = df_te), df_te$Price)\nR2(predict(est_knn, newdata = df_te), df_te$Price)\n\nJust for the exercise, we can compute it by hand (square of the correlation)\n\ncor(predict(est_lm, newdata = df_te), df_te$Price)^2\n\n\n\n\n# Same thing as the R code\nfrom sklearn.metrics import r2_score\nimport numpy as np\n\n# Only to demonostrate which argument goes where (different from `caret::R2`)\nprint(r2_score(y_true = r.df_te[target], y_pred = est_lm.predict(r.df_te[predictors])))\nprint(r2_score(r.df_te[target], est_rt.predict(r.df_te[predictors])))\nprint(r2_score(r.df_te[target], est_knn.predict(r.df_te[predictors])))\n# Computing it by hand gives us the same result as R\nnp.corrcoef(est_lm.predict(r.df_te[predictors]), r.df_te[target])[0][1]**2\n\nTo understand why the results are different in caret::R2() vs. sklearn.metrics.r2_score(), see this post on stackoverflow. If you want to get the same results in both, you can set the argument form = 'corr' to form = \"traditional\" in caret::R2() so that for instance R2(predict(est_lm, newdata = df_te), df_te$Price, form = \"traditional\"), produces the same result of 0.642401.\nAdditionally, please note that the performance of the tree is highly dependent on the seed, so setting a different seed can lead to different results.\n\n\n\n\nNow, we compute the RMSE.\n\n\nR\nPython\n\n\n\n\nRMSE(predict(est_lm, newdata = df_te), df_te$Price)\nRMSE(predict(est_rt, newdata = df_te), df_te$Price)\nRMSE(predict(est_knn, newdata = df_te), df_te$Price)\n\nThe formula would be:\n\nsqrt(mean((predict(est_lm, newdata = df_te)-df_te$Price)^2))\n\n\n\n\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\nprint(mean_squared_error(r.df_te[target], est_lm.predict(r.df_te[predictors]), squared = False))\n# alternatively in the older version of `sklearn`, you had to run the code below\n# print(np.sqrt(mean_squared_error(r.df_te[target], est_lm.predict(r.df_te[predictors]))))\nprint(mean_squared_error(r.df_te[target], est_rt.predict(r.df_te[predictors]), squared = False))\nprint(mean_squared_error(r.df_te[target], est_knn.predict(r.df_te[predictors]), squared = False))\n\n\n\n\n\nNow, we compute the MAE.\n\n\nR\nPython\n\n\n\n\nMAE(predict(est_lm, newdata = df_te), df_te$Price)\nMAE(predict(est_rt, newdata = df_te), df_te$Price)\nMAE(predict(est_knn, newdata = df_te), df_te$Price)\n\nThe formula would be:\n\nmean(abs(predict(est_lm, newdata = df_te)-df_te$Price))\n\n\n\n\n# Compute MAE for each model\nfrom sklearn.metrics import mean_absolute_error\n\nprint(mean_absolute_error(r.df_te[target], est_lm.predict(r.df_te[predictors])))\nprint(mean_absolute_error(r.df_te[target], est_rt.predict(r.df_te[predictors])))\nprint(mean_absolute_error(r.df_te[target], est_knn.predict(r.df_te[predictors])))\n\n\n\n\n\nThese three measures agree on the fact that the regression tree is the best model. To inspect further the predictions, we use scatterplots:\n\n\nR\nPython\n\n\n\n\npar(mfrow=c(2,2))\nplot(df_te$Price ~ predict(est_lm, newdata = df_te), xlab=\"Prediction\", \n     ylab=\"Observed prices\", main=\"Lin. Reg.\")\nabline(0,1)\nplot(df_te$Price ~ predict(est_rt, newdata = df_te), xlab=\"Prediction\", \n     ylab=\"Observed prices\", main=\"Lin. Reg.\")\nabline(0,1)\nplot(df_te$Price ~ predict(est_knn, newdata = df_te), xlab=\"Prediction\", \n     ylab=\"Observed prices\", main=\"Lin. Reg.\")\nabline(0,1)\npar(mfrow=c(1,1))\n\n\n\n\n# visualize also in Python\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\n\nplt.subplot(221)\nplt.scatter(est_lm.predict(r.df_te[predictors]), r.df_te[target], alpha=0.5)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Observed prices\")\nplt.title(\"Lin. Reg.\")\nplt.plot(r.df_te[target], r.df_te[target], color='red')\n\nplt.subplot(222)\nplt.scatter(est_rt.predict(r.df_te[predictors]), r.df_te[target], alpha=0.5)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Observed prices\")\nplt.title(\"Regression Tree\")\nplt.plot(r.df_te[target], r.df_te[target], color='red')\n\nplt.subplot(223)\nplt.scatter(est_knn.predict(r.df_te[predictors]), r.df_te[target], alpha=0.5)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Observed prices\")\nplt.title(\"KNN\")\nplt.plot(r.df_te[target], r.df_te[target], color='red')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe scatterplots are in line with the conclusion that KNN is the best, even though it is not easy to declare from a plot. We can in addition see that the regression tree (RT) has made more error on the larger prices.",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#data",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#data",
    "title": "Model scoring",
    "section": "",
    "text": "The data set is the one used in the series on linear regressions.\n\nlibrary(readr)\nreal_estate_data &lt;- read.csv(here::here(\"labs/data/real_estate_data.csv\"))\n\nThen we split the data in a training and a test set (0.8/0.2). For this, we use the createDataPartition function of the caret package.\n\nlibrary(caret)\nset.seed(234)\nindex_tr &lt;- createDataPartition(y = real_estate_data$Price, p= 0.8, list = FALSE)\ndf_tr &lt;- real_estate_data[index_tr,]\ndf_te &lt;- real_estate_data[-index_tr,]",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#models",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#models",
    "title": "Model scoring",
    "section": "",
    "text": "We will compare a linear regression, a regression tree and a 3-NN (KNN).\n\n\nR\nPython\n\n\n\n\nlibrary(rpart)\nest_lm &lt;- lm(Price~TransDate+HouseAge+Dist+\n               NumStores+Lat+Long, data=df_tr)\nest_rt &lt;- rpart(Price~TransDate+HouseAge+Dist+\n                      NumStores+Lat+Long, data=df_tr)\nest_knn &lt;- knnreg(Price~TransDate+HouseAge+Dist+\n                      NumStores+Lat+Long, data=df_tr, k = 3)\n\n\n\n\n# Load the course python environment as usual with a r code chunks.\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\n\n# Fit the models: linear regression, regression tree, and KNN\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Define predictors and target variable\npredictors = ['TransDate', 'HouseAge', 'Dist', 'NumStores', 'Lat', 'Long']\ntarget = 'Price'\n\n# Fit models\nest_lm = LinearRegression().fit(r.df_tr[predictors], r.df_tr[target])\nest_rt = DecisionTreeRegressor(random_state=234).fit(r.df_tr[predictors], r.df_tr[target])\nest_knn = KNeighborsRegressor(n_neighbors=3).fit(r.df_tr[predictors], r.df_tr[target])",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#r-squared",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#r-squared",
    "title": "Model scoring",
    "section": "",
    "text": "R\nPython\n\n\n\nWe now compute the R2 for each model using the R2 function (in caret).\n\nR2(predict(est_lm, newdata = df_te), df_te$Price)\nR2(predict(est_rt, newdata = df_te), df_te$Price)\nR2(predict(est_knn, newdata = df_te), df_te$Price)\n\nJust for the exercise, we can compute it by hand (square of the correlation)\n\ncor(predict(est_lm, newdata = df_te), df_te$Price)^2\n\n\n\n\n# Same thing as the R code\nfrom sklearn.metrics import r2_score\nimport numpy as np\n\n# Only to demonostrate which argument goes where (different from `caret::R2`)\nprint(r2_score(y_true = r.df_te[target], y_pred = est_lm.predict(r.df_te[predictors])))\nprint(r2_score(r.df_te[target], est_rt.predict(r.df_te[predictors])))\nprint(r2_score(r.df_te[target], est_knn.predict(r.df_te[predictors])))\n# Computing it by hand gives us the same result as R\nnp.corrcoef(est_lm.predict(r.df_te[predictors]), r.df_te[target])[0][1]**2\n\nTo understand why the results are different in caret::R2() vs. sklearn.metrics.r2_score(), see this post on stackoverflow. If you want to get the same results in both, you can set the argument form = 'corr' to form = \"traditional\" in caret::R2() so that for instance R2(predict(est_lm, newdata = df_te), df_te$Price, form = \"traditional\"), produces the same result of 0.642401.\nAdditionally, please note that the performance of the tree is highly dependent on the seed, so setting a different seed can lead to different results.",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#rmse",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#rmse",
    "title": "Model scoring",
    "section": "",
    "text": "Now, we compute the RMSE.\n\n\nR\nPython\n\n\n\n\nRMSE(predict(est_lm, newdata = df_te), df_te$Price)\nRMSE(predict(est_rt, newdata = df_te), df_te$Price)\nRMSE(predict(est_knn, newdata = df_te), df_te$Price)\n\nThe formula would be:\n\nsqrt(mean((predict(est_lm, newdata = df_te)-df_te$Price)^2))\n\n\n\n\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\nprint(mean_squared_error(r.df_te[target], est_lm.predict(r.df_te[predictors]), squared = False))\n# alternatively in the older version of `sklearn`, you had to run the code below\n# print(np.sqrt(mean_squared_error(r.df_te[target], est_lm.predict(r.df_te[predictors]))))\nprint(mean_squared_error(r.df_te[target], est_rt.predict(r.df_te[predictors]), squared = False))\nprint(mean_squared_error(r.df_te[target], est_knn.predict(r.df_te[predictors]), squared = False))",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#mae",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#mae",
    "title": "Model scoring",
    "section": "",
    "text": "Now, we compute the MAE.\n\n\nR\nPython\n\n\n\n\nMAE(predict(est_lm, newdata = df_te), df_te$Price)\nMAE(predict(est_rt, newdata = df_te), df_te$Price)\nMAE(predict(est_knn, newdata = df_te), df_te$Price)\n\nThe formula would be:\n\nmean(abs(predict(est_lm, newdata = df_te)-df_te$Price))\n\n\n\n\n# Compute MAE for each model\nfrom sklearn.metrics import mean_absolute_error\n\nprint(mean_absolute_error(r.df_te[target], est_lm.predict(r.df_te[predictors])))\nprint(mean_absolute_error(r.df_te[target], est_rt.predict(r.df_te[predictors])))\nprint(mean_absolute_error(r.df_te[target], est_knn.predict(r.df_te[predictors])))",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#best-model",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#best-model",
    "title": "Model scoring",
    "section": "",
    "text": "These three measures agree on the fact that the regression tree is the best model. To inspect further the predictions, we use scatterplots:\n\n\nR\nPython\n\n\n\n\npar(mfrow=c(2,2))\nplot(df_te$Price ~ predict(est_lm, newdata = df_te), xlab=\"Prediction\", \n     ylab=\"Observed prices\", main=\"Lin. Reg.\")\nabline(0,1)\nplot(df_te$Price ~ predict(est_rt, newdata = df_te), xlab=\"Prediction\", \n     ylab=\"Observed prices\", main=\"Lin. Reg.\")\nabline(0,1)\nplot(df_te$Price ~ predict(est_knn, newdata = df_te), xlab=\"Prediction\", \n     ylab=\"Observed prices\", main=\"Lin. Reg.\")\nabline(0,1)\npar(mfrow=c(1,1))\n\n\n\n\n# visualize also in Python\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\n\nplt.subplot(221)\nplt.scatter(est_lm.predict(r.df_te[predictors]), r.df_te[target], alpha=0.5)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Observed prices\")\nplt.title(\"Lin. Reg.\")\nplt.plot(r.df_te[target], r.df_te[target], color='red')\n\nplt.subplot(222)\nplt.scatter(est_rt.predict(r.df_te[predictors]), r.df_te[target], alpha=0.5)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Observed prices\")\nplt.title(\"Regression Tree\")\nplt.plot(r.df_te[target], r.df_te[target], color='red')\n\nplt.subplot(223)\nplt.scatter(est_knn.predict(r.df_te[predictors]), r.df_te[target], alpha=0.5)\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Observed prices\")\nplt.title(\"KNN\")\nplt.plot(r.df_te[target], r.df_te[target], color='red')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nThe scatterplots are in line with the conclusion that KNN is the best, even though it is not easy to declare from a plot. We can in addition see that the regression tree (RT) has made more error on the larger prices.",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#data-1",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#data-1",
    "title": "Model scoring",
    "section": "Data",
    "text": "Data\nThe data set is the visit data (already used in previous exercises). For simplicity, we turn the outcome (visits) into factor. Like before, that are also split into a training and a test set.\n\nDocVis &lt;- read.csv(here::here(\"labs/data/DocVis.csv\"))\nDocVis$visits &lt;- as.factor(DocVis$visits)\n\nlibrary(caret)\nset.seed(346)\nindex_tr &lt;- createDataPartition(y = DocVis$visits, p= 0.8, list = FALSE)\ndf_tr &lt;- DocVis[index_tr,]\ndf_te &lt;- DocVis[-index_tr,]",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#models-1",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#models-1",
    "title": "Model scoring",
    "section": "Models",
    "text": "Models\nWe will compare a logistic regression, a classification tree (pruned) and a SVM with radial basis (cost and gamma tuned).\n\n\nR\nPython\n\n\n\nNote that the code for tuning the SVM is provided below in comments because of the time it takes to run. The final parameters have been selected accordingly. Also, the SVM fit includes the argument probability=TRUE to allow the calculations of predicted probabilities later.\n\nlibrary(e1071)\nlibrary(adabag)\n\n## Logistic regression\nDoc_lr &lt;- glm(visits~., data=df_tr, family=\"binomial\")\nDoc_lr &lt;- step(Doc_lr)\n\n## Classification tree \nDoc_ct &lt;- autoprune(visits~., data=df_tr)\n\n## SVM radial basis\n# grid_radial &lt;- expand.grid(sigma = c(0.0001, 0.001, 0.01, 0.1),\n#                           C = c(0.1, 1, 10, 100, 1000))\n# trctrl &lt;- trainControl(method = \"cv\", number=10)\n# set.seed(143)\n# Doc_svm &lt;- train(visits ~., data = df_tr, method = \"svmRadial\",\n#                          trControl=trctrl,\n#                          tuneGrid = grid_radial)\nDoc_svm &lt;- svm(visits~., data=df_tr, gamma=0.001, cost=1000, probability=TRUE)\n\n\n\n\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\n# We first put the data in a nice format by one-hot encoding the categorical variables\nX_train = pd.get_dummies(r.df_tr.drop('visits', axis=1))\ny_train = r.df_tr['visits']\nX_test = pd.get_dummies(r.df_te.drop('visits', axis=1))\ny_test = r.df_te['visits']\n\n## Logistic regression\ndoc_lr = LogisticRegression()\ndoc_lr.fit(X_train, y_train)\n\n\n\n\nLogisticRegression()\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n## Classification tree\ndoc_ct = DecisionTreeClassifier(random_state=123)\ndoc_ct.fit(X_train, y_train)\n\n\n\n\nDecisionTreeClassifier(random_state=123)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(random_state=123) \n\n\n## SVM radial basis\ndoc_svm = SVC(kernel='rbf', gamma=0.001, C=1000, probability=True, random_state=123)\ndoc_svm.fit(X_train, y_train)\n\n\n\n\nSVC(C=1000, gamma=0.001, probability=True, random_state=123)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  SVC?Documentation for SVCiFittedSVC(C=1000, gamma=0.001, probability=True, random_state=123)",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#predictions",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#predictions",
    "title": "Model scoring",
    "section": "Predictions",
    "text": "Predictions\nWe now compute the predicted probabilities and the predictions of all the models.\n\n\nR\nPython\n\n\n\nNote that, for SVM, we need to extract the attribute “probabilities” from the predicted object. This can be done with the attr function.\n\n## Logistic regression\nDoc_lr_prob &lt;- predict(Doc_lr, newdata=df_te, type=\"response\")\nDoc_lr_pred &lt;- ifelse(Doc_lr_prob&gt;0.5,\"Yes\",\"No\")\n\n## Classification tree \nDoc_ct_prob &lt;- predict(Doc_ct, newdata=df_te, type=\"prob\")\nDoc_ct_pred &lt;- predict(Doc_ct, newdata=df_te, type=\"class\")\n\n## SVM radial basis\nlibrary(dplyr)\nDoc_svm_prob &lt;- predict(Doc_svm, newdata=df_te, probability=TRUE) %&gt;% attr(\"probabilities\")\nDoc_svm_pred &lt;- predict(Doc_svm, newdata=df_te, type=\"class\")\n\n\n\n\n## Logistic regression\n## the second column represents the `no` values, to make sure of that, you can run `doc_lr.classes_`\ndoc_lr_prob = doc_lr.predict_proba(X_test)[:,1]\ndoc_lr_pred = np.where(doc_lr_prob&gt;0.5, \"Yes\", \"No\")\n\n## Classification tree\ndoc_ct_prob = doc_ct.predict_proba(X_test)[:,1]\ndoc_ct_pred = doc_ct.predict(X_test)\n\n## SVM radial basis\ndoc_svm_prob = doc_svm.predict_proba(X_test)[:,1]\ndoc_svm_pred = doc_svm.predict(X_test)",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#confusion-matrices-prediction-based-measures",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#confusion-matrices-prediction-based-measures",
    "title": "Model scoring",
    "section": "Confusion matrices & prediction-based measures",
    "text": "Confusion matrices & prediction-based measures\n\n\nR\nPython\n\n\n\nThe confusionMatrix function provides all the accuracy measures that we want.\n\nconfusionMatrix(data=as.factor(Doc_lr_pred), reference = df_te$visits)\nconfusionMatrix(data=as.factor(Doc_ct_pred), reference = df_te$visits)\nconfusionMatrix(data=as.factor(Doc_svm_pred), reference = df_te$visits)\n\n\n\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, balanced_accuracy_score, cohen_kappa_score\n\n## Logistic regression\nprint(confusion_matrix(y_test, doc_lr_pred))\nprint(f\"Accuracy: {accuracy_score(y_test, doc_lr_pred):.3f}\")\nprint(f\"Kappa: {cohen_kappa_score(y_test, doc_lr_pred):.3f}\")\nprint(f\"Balanced accuracy: {balanced_accuracy_score(y_test, doc_lr_pred):.3f}\")\n## Classification tree\nprint(confusion_matrix(y_test, doc_ct_pred))\nprint(f\"Accuracy: {accuracy_score(y_test, doc_ct_pred):.3f}\")\nprint(f\"Kappa: {cohen_kappa_score(y_test, doc_ct_pred):.3f}\")\nprint(f\"Balanced accuracy: {balanced_accuracy_score(y_test, doc_ct_pred):.3f}\")\n## SVM radial basis\nprint(confusion_matrix(y_test, doc_svm_pred))\nprint(f\"Accuracy: {accuracy_score(y_test, doc_svm_pred):.3f}\")\nprint(f\"Kappa: {cohen_kappa_score(y_test, doc_svm_pred):.3f}\")\nprint(f\"Balanced accuracy: {balanced_accuracy_score(y_test, doc_svm_pred):.3f}\")\n\nDifferent results for the tree and CSV due to randomness, but even with that, SVM remains the best model in terms of accuracy.\n\n\n\nThe conclusion may be different from one measure to another\n\nAccuracy: the SVM reaches the highest accuracy\nKappa: the CT is the highest.\nBalanced accuracy: the CT is the highest.\netc.\n\nLooking at the confusion matrix, we see that the data is highly unbalanced (many more “No” than “Yes”). Therefore, measures like balanced accuracy and kappa are interesting because they take this characteristics into account. This shows that the CT is probably better than the SVM because it reaches a better balance between predicting “Yes” and “No”.\nBy looking at the sensitivity and specificity (!! here the positive class is “No”), we see that the best model to recover the “No” is the logistic regression (largest sensitivity) and the best model to recover the “Yes” is the classification tree (largest specificity).",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#probability-based-measures",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#probability-based-measures",
    "title": "Model scoring",
    "section": "Probability-based measures",
    "text": "Probability-based measures\n\n\nR\nPython\n\n\n\nTo compute the AUC (area under the ROC curve) we can use the caret::twoClassSummary function. The use of this function can be tricky. Its argument should be a data frame with columns (names are fixed):\n\n“obs”: the observed classes\n“pred”: the predicted classes\ntwo columns with names being the levels of the classes, here “Yes” and “No”, containing the predicted probabilities.\n\n\ndf_pred_lr &lt;- data.frame(obs=df_te$visits,\n                         Yes=Doc_lr_prob,\n                         No=1-Doc_lr_prob,\n                         pred=as.factor(Doc_lr_pred))\nhead(df_pred_lr)\n\ndf_pred_ct &lt;- data.frame(obs=df_te$visits,\n                         Doc_ct_prob,\n                         pred=as.factor(Doc_ct_pred))\nhead(df_pred_ct)\ndf_pred_svm &lt;- data.frame(obs=df_te$visits,\n                          Doc_svm_prob,\n                          pred=as.factor(Doc_svm_pred))\nhead(df_pred_svm)\n\nThen we pass these objects to the function, and levels of the classes to be predicted (for the function to be able to recover them in the data frame). The function compute the AUC by default (under the name ROC_.. not very wise) as well as sensitivity and specificity (that we already have).\n\ntwoClassSummary(df_pred_lr, lev = levels(df_pred_lr$obs))\ntwoClassSummary(df_pred_ct, lev = levels(df_pred_lr$obs))\ntwoClassSummary(df_pred_svm, lev = levels(df_pred_lr$obs))\n\nThis brings us another view: the logistic regression has the highest AUC. This shows that varying the prediction threshold provides a good potential of improving the specificity and the sensitivity (in fine, the balanced accuracy).\nNow we compute the entropy using the mnLogLoss function (entropy is also called log-loss).\n\nmnLogLoss(df_pred_lr, lev = levels(df_pred_lr$obs))\nmnLogLoss(df_pred_ct, lev = levels(df_pred_lr$obs))\nmnLogLoss(df_pred_svm, lev = levels(df_pred_lr$obs))\n\nHere again, the entropy selects the logistic regression as the best model, though close to classification tree and SVM.\n\n\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\n## Logistic regression\nprint(f\"AUC: {roc_auc_score(y_test, doc_lr_prob):.3f}\")\n## Classification tree\nprint(f\"AUC: {roc_auc_score(y_test, doc_ct_prob):.3f}\")\n## SVM radial basis\nprint(f\"AUC: {roc_auc_score(y_test, doc_svm_prob):.3f}\")\n# Now we compute the entropy using the `log_loss` function (entropy is also called *log-loss*).\n\nfrom sklearn.metrics import log_loss\n\n## Logistic regression\nprint(f\"Log-loss: {log_loss(y_test, doc_lr_prob):.3f}\")\n## Classification tree\nprint(f\"Log-loss: {log_loss(y_test, doc_ct_prob):.3f}\")\n## SVM radial basis\nprint(f\"Log-loss: {log_loss(y_test, doc_svm_prob):.3f}\")",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#roc-curve-prob-threshold-tuning",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#roc-curve-prob-threshold-tuning",
    "title": "Model scoring",
    "section": "ROC curve & prob threshold tuning",
    "text": "ROC curve & prob threshold tuning\n\n\nR\nPython\n\n\n\nTo go deeper in the analysis, we now produce the ROC curve of each model using the roc function of the proc package.\n\nlibrary(pROC)\nROC_lr &lt;- roc(obs ~ Yes, data=df_pred_lr)\nROC_ct &lt;- roc(obs ~ Yes, data=df_pred_ct)\nROC_svm &lt;- roc(obs ~ Yes, data=df_pred_svm)\n\nplot(ROC_lr, print.thres=\"best\")\nplot(ROC_ct, print.thres=\"best\", add=TRUE)\nplot(ROC_svm, print.thres=\"best\", add=TRUE)\n\nThe plotting function provides an “optimal” threshold that reaches the best trade-off between sensitivity and specificity (according to some criterion). We see that there is room to improve this trade-off.\nNow, to tune this threshold, we need to do it on the training set to avoid overfitting. To do this, we just repeat the previous calculations (predictions) on the training set. To simplify, we only do this on the logistic regression (note that you can try on the other models; you may find that logistic regression is the best one).\n\nDoc_lr_prob_tr &lt;- predict(Doc_lr, newdata=df_tr, type=\"response\")\ndf_pred_lr_tr &lt;- data.frame(obs=df_tr$visits,\n                            Yes=Doc_lr_prob_tr)\nROC_lr_tr &lt;- roc(obs ~ Yes, data=df_pred_lr_tr)\nplot(ROC_lr_tr, print.thres=\"best\")\n\nThe best threshold is 0.193. Now let us compute the confusion table with this threshold.\n\nDoc_lr_pred_opt &lt;- ifelse(Doc_lr_prob&gt;0.193,\"Yes\",\"No\")\nconfusionMatrix(data=as.factor(Doc_lr_pred_opt), reference = df_te$visits)\n\nWe now have a model with an accuracy of circa 70\\% but with a balanced accuracy of 67\\%. Far from perfect, this is still an interesting improvement compare to the CT 62\\%. The specificity and sensitivity are now respectively 62\\% and 72\\%. The specificity in particular made a huge improvement (from around 29\\% at best - by CT - to 62\\% - by log. reg).\nIf the aim is to predict both “Yes” and “No”, this last model (log. reg. with tuned threshold) is the best one to use.\n\n\n\n## Logistic regression\n## We need to turn back our results into binary values to be plotted\ndoc_lr_prob_dict = {'Yes': 1, 'No': 0}\ny_test_binary = np.array([doc_lr_prob_dict[x] for x in y_test])\nfpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test_binary, doc_lr_prob)\nplt.plot(fpr_lr, tpr_lr, label=\"Logistic Regression\")\n\n## Classification tree\ndoc_ct_prob_dict = {'Yes': 1, 'No': 0}\ny_test_binary = np.array([doc_ct_prob_dict[x] for x in y_test])\nfpr_ct, tpr_ct, thresholds_ct = roc_curve(y_test_binary, doc_ct_prob)\nplt.plot(fpr_ct, tpr_ct, label=\"Classification Tree\")\n\n## SVM radial basis\ndoc_svm_prob_dict = {'Yes': 1, 'No': 0}\ny_test_binary = np.array([doc_svm_prob_dict[x] for x in y_test])\nfpr_svm, tpr_svm, thresholds_svm = roc_curve(y_test_binary, doc_svm_prob)\n\n# Clear the last plot (if any)\n# plt.clf()\n\nplt.plot(fpr_svm, tpr_svm, label=\"SVM Radial Basis\")\n# Plot the ROC curve\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Classifier\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()\n\nWe can then plot the results in the similar way to R:\n\ndoc_lr_prob_tr = doc_lr.predict_proba(X_train)[:,1]\ndoc_lr_prob_tr_dict = {'Yes': 1, 'No': 0}\ny_train_binary = np.array([doc_lr_prob_tr_dict[x] for x in y_train])\nfpr_lr_tr, tpr_lr_tr, thresholds_lr_tr = roc_curve(y_train_binary, doc_lr_prob_tr)\noptimal_idx = np.argmax(tpr_lr_tr - fpr_lr_tr)\noptimal_threshold = thresholds_lr_tr[optimal_idx]\nprint(f\"Optimal threshold: {optimal_threshold:.3f}\")\n\nFinally, we print the confusion matrix again:\n\ndoc_lr_pred_opt = np.where(doc_lr_prob &gt; optimal_threshold, \"Yes\", \"No\")\nprint(confusion_matrix(y_test, doc_lr_pred_opt))\n\nThe logistic regression produced with R was better.",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#classification",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#classification",
    "title": "Model scoring",
    "section": "Classification",
    "text": "Classification\nRepeat the analysis on the German credit data. Put several models in competition. Tune them and try to optimize their threshold. Select the best one and analyze its performance.",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/04_Metrics/Ex_ML_Scoring.html#regression",
    "href": "labs/04_Metrics/Ex_ML_Scoring.html#regression",
    "title": "Model scoring",
    "section": "Regression",
    "text": "Regression\nRepeat the analysis on the nursing cost data. Put several models in competition. Tune them and select the best one. Analyze its performance using a scatterplot.",
    "crumbs": [
      "Labs",
      "Metrics"
    ]
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html",
    "title": "Ensemble Methods",
    "section": "",
    "text": "In this part of the lab, we will look at how the randomForest library (alternative to ranger) can be applied for classification and regression tasks. At the very end, please feel free to apply these techniques to one of your favorite datasets seen in class (classification or regression).\n\n\n\n\n\n\nHyperparameters of RF\n\n\n\nR (using the randomForest library):\n\n\nntree: The number of trees in the forest (equivalent to n_estimators in python).\n\nmtry: The number of features to consider when looking for the best split. (similar to max_features in python)\n\nmax.depth: The maximum depth of each tree.\n\nnodesize: The minimum number of samples required to split an internal node (equivalent to min_samples_split in python).\n\nPython (using the sklearn library):\n\n\nn_estimators: The number of trees in the forest.\n\nmax_features: The number of features to consider when looking for the best split.\n\nmax_depth: The maximum depth of each tree.\n\nmin_samples_split: The minimum number of samples required to split an internal node.\n\nmin_samples_leaf: The minimum number of samples required to be at a leaf node.\n\n\n\n\n\n\n\n\n\nFew tips on RF hyperparameters\n\n\n\nA few (among many) tips for finding the ideal hyperparameters for RF:\n\n\nntree (R) / n_estimators (python): Use a large number of trees in the forest to improve model performance, but be aware of the increased computation time. The default value is usually a good starting point.\n\nmtry (R) / max_features (python): Experiment with different values, usually starting with the default (square root of the number of features for classification or one-third of the number of features for regression). Increasing this value may improve model performance but can also increase computation time.\n\nmax.depth: Control the depth of each tree to manage overfitting. Deeper trees capture more complex patterns but can lead to overfitting. Experiment with different values, keeping in mind that a shallower tree can be more interpretable and less prone to overfitting.\n\nnodesize (R) / min_samples_split (python): Increasing this value can help reduce overfitting, but setting it too high might lead to underfitting. Experiment with different values to find the optimal balance.\n\n\n\n\n\nLoad the library randomForest in R. Then, load the wine data set. This dataset is about white wine quality (in fact Portuguese vinho verde). The data contains 11 numerical features and 1 factor variable:\n\nfixed.acidity\nvolatile.acidity\ncitric.acid\nresidual.sugar\nchlorides\nfree.sulfur.dioxide\ntotal.sulfur.dioxide\ndensity\npH\nsulphates\nalcohol\nquality: Good/Bad\n\nAll the numerical features have units. The data source can be found here. For simplicity, only an extraction of 200 wines are used in this exercise. Note that in the original data set, the quality is a score (0 to 10) that was turned as factor here for the exercise (Bad: 0 to 5, Good: 6 to 10). Also, note that in the data source, the objective is to predict the quality from the other features (supervised learning).\nAs mentioned, the outcome variable used for this dataset is the wine quality. We should first coerce the classes as factors. Then, we make the training/test set random split with a 75/25 scheme.\n\n\nR\nPython\n\n\n\n\nlibrary(randomForest)\nwine &lt;- read.csv(here::here(\"data/Wine.csv\"))\nwine$quality &lt;- as.factor(wine$quality)\n\n# define a function to get the splitting index (training and testing) of a given dataset\nget_split_index &lt;- function(dataset, train_proportion = 0.75) {\n  set.seed(123)\n  index &lt;-\n    sample(\n      x = 1:2,\n      size = nrow(dataset),\n      replace = TRUE,\n      prob = c(train_proportion, 1 - train_proportion)\n    )\n  return(index)\n}\n\nwine_index &lt;- get_split_index(wine)\nwine_tr &lt;- wine[wine_index == 1, ]\nwine_te &lt;- wine[wine_index == 2, ]\n\n\n\n\n# Load the course python environment as usual with a r code chunks.\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\nSimilar to the previous labs, in python, we can use the usual sklearn library to do all our modelling. Please note that we will load the data again in python to make the demo easier. Additionally, we’ll load all the necessary libraries for this lab in this code chunk.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# we first move up one directory to achieve relative paths\nwine = pd.read_csv('../data/Wine.csv')\n\nwine['quality'] = wine['quality'].astype('category')\n\n# Split wine dataset into train and test\ntrain_wine, test_wine = train_test_split(wine, test_size=0.25, random_state=123)\n\n\n\n\nNote: Here, we have written a function to make the split as we will also need to apply it also for another dataset in the regression part.\n\nFit a random forest on the train set. The target is the taste variable that we want to predict. Specify for the number of trees ntree=1000 (by default, the function selects 500 trees). Remember to exclude quality in the predictors of the formula. Also, use the option importance=TRUE, we will need it afterward. Then test the model by computing the accuracy on the test set. You may use confusionMatrix from caret.\n\n\nR\nPython\n\n\n\n\nwine_rf &lt;- randomForest(quality~., data=wine_tr, ntree=1000, importance=TRUE)\nwine.pred_rf &lt;- predict(wine_rf, newdata=wine_te)\n\nlibrary(caret)\nconfusionMatrix(data=wine.pred_rf, reference = wine_te$quality)\n\n\n\n\n# Fit a Random Forest classifier on the train set\nwine_rf = RandomForestClassifier(n_estimators=1000, random_state=123)\nwine_rf.fit(train_wine.drop('quality', axis=1), train_wine['quality'])\n\n# Test the model and compute the accuracy\nwine_pred_rf = wine_rf.predict(test_wine.drop('quality', axis=1))\nprint(confusion_matrix(test_wine['quality'], wine_pred_rf))\nprint(\"Accuracy:\", accuracy_score(test_wine['quality'], wine_pred_rf))\n\nThis model is worse than the R version mostly because of the different defaults.\n\n\n\n\nExtract the model-specific variable importance using the functions varImpPlot (plots) and importance (values) on the model. Observe well that the mean decrease in accuracy of each variable is also computed for each specific class. In particular, what makes density special for predicting Good compare to another variable (like for example citric.acid)?\n\n\nR\nPython\n\n\n\n\nvarImpPlot(wine_rf)\nimportance(wine_rf)\n\n\n\n\n# Variable importance\nwine_importances = pd.Series(wine_rf.feature_importances_, index=train_wine.drop('quality', axis=1).columns)\nwine_importances.sort_values(ascending=False).plot(kind='bar')\n\n\n\n\ndensity is important for predicting the Good since their predictions is much less accurate if we do not use it. citric.acid is both overall less important than density but especially for prediction of Good.\n\nIn this part, we will be using the real_estate_data.csv once again. After reading the data, apply a random forest to predict price using all the other variables except No, Month and Year. Compute the RMSE and inspect the prediction quality with a graph. Note that the importance is not specific to any class here.\n\n\nR\nPython\n\n\n\n\nlibrary(dplyr)\nlibrary(magrittr)\nreal_estate_data &lt;- read.csv(here::here(\"labs/data/real_estate_data.csv\"))\n\n# select the columns of interest\nreal_estate_data &lt;- \n  real_estate_data %&gt;% \n  select(-c(No, Month, Year))\n\n# once again, divide the data into training and testing sets using the function created earlier\nrestate_index &lt;- get_split_index(real_estate_data)\nrestate_tr &lt;- real_estate_data[restate_index == 1, ]\nrestate_te &lt;- real_estate_data[restate_index == 2, ]\n\n# apply the RF model as a regression\nrestate_rf &lt;- randomForest(Price~., data=restate_tr, ntree=1000, importance=TRUE)\nrestate.pred_rf&lt;-predict(restate_rf, newdata=restate_te)\n\n# compute rmse and plot the results as well the VarImp\n(rmse &lt;- sqrt(mean((restate_te$Price - restate.pred_rf)^2)))\nplot(restate_te$Price ~ restate.pred_rf)\nabline(0,1)\nvarImpPlot(restate_rf)\nimportance(restate_rf)\n\n\n\n\n# Load real estate dataset\nreal_estate_data = pd.read_csv(\"../data/real_estate_data.csv\")\n\nreal_estate_data = real_estate_data.drop(['No', 'Month', 'Year'], axis=1)\n\n# Split real estate dataset into train and test\ntrain_restate, test_restate = train_test_split(real_estate_data, test_size=0.25, random_state=123)\n\n# Fit a Random Forest regressor on the train set\nrestate_rf = RandomForestRegressor(n_estimators=1000, random_state=123)\nrestate_rf.fit(train_restate.drop('Price', axis=1), train_restate['Price'])\n\n# Test the model and compute the RMSE\nrestate_pred_rf = restate_rf.predict(test_restate.drop('Price', axis=1))\nrmse = np.sqrt(mean_squared_error(test_restate['Price'], restate_pred_rf))\nprint(\"RMSE:\", rmse)\n\n# Plot the prediction quality\nplt.scatter(test_restate['Price'], restate_pred_rf)\nplt.xlabel('Actual Price')\nplt.ylabel('Predicted Price')\nplt.plot([min(test_restate['Price']), max(test_restate['Price'])], [min(test_restate['Price']), max(test_restate['Price'])], color='red')\nplt.show()\n\n# Variable importance\nrestate_importances = pd.Series(restate_rf.feature_importances_, index=train_restate.drop('Price', axis=1).columns)\nrestate_importances.sort_values(ascending=False).plot(kind='bar')\n\n\n\n\nCompare this model with the one you came up with in Ex_ML_LinLogReg . Which one would you go for?",
    "crumbs": [
      "Labs",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html#classification",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html#classification",
    "title": "Ensemble Methods",
    "section": "",
    "text": "Load the library randomForest in R. Then, load the wine data set. This dataset is about white wine quality (in fact Portuguese vinho verde). The data contains 11 numerical features and 1 factor variable:\n\nfixed.acidity\nvolatile.acidity\ncitric.acid\nresidual.sugar\nchlorides\nfree.sulfur.dioxide\ntotal.sulfur.dioxide\ndensity\npH\nsulphates\nalcohol\nquality: Good/Bad\n\nAll the numerical features have units. The data source can be found here. For simplicity, only an extraction of 200 wines are used in this exercise. Note that in the original data set, the quality is a score (0 to 10) that was turned as factor here for the exercise (Bad: 0 to 5, Good: 6 to 10). Also, note that in the data source, the objective is to predict the quality from the other features (supervised learning).\nAs mentioned, the outcome variable used for this dataset is the wine quality. We should first coerce the classes as factors. Then, we make the training/test set random split with a 75/25 scheme.\n\n\nR\nPython\n\n\n\n\nlibrary(randomForest)\nwine &lt;- read.csv(here::here(\"data/Wine.csv\"))\nwine$quality &lt;- as.factor(wine$quality)\n\n# define a function to get the splitting index (training and testing) of a given dataset\nget_split_index &lt;- function(dataset, train_proportion = 0.75) {\n  set.seed(123)\n  index &lt;-\n    sample(\n      x = 1:2,\n      size = nrow(dataset),\n      replace = TRUE,\n      prob = c(train_proportion, 1 - train_proportion)\n    )\n  return(index)\n}\n\nwine_index &lt;- get_split_index(wine)\nwine_tr &lt;- wine[wine_index == 1, ]\nwine_te &lt;- wine[wine_index == 2, ]\n\n\n\n\n# Load the course python environment as usual with a r code chunks.\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\nSimilar to the previous labs, in python, we can use the usual sklearn library to do all our modelling. Please note that we will load the data again in python to make the demo easier. Additionally, we’ll load all the necessary libraries for this lab in this code chunk.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# we first move up one directory to achieve relative paths\nwine = pd.read_csv('../data/Wine.csv')\n\nwine['quality'] = wine['quality'].astype('category')\n\n# Split wine dataset into train and test\ntrain_wine, test_wine = train_test_split(wine, test_size=0.25, random_state=123)\n\n\n\n\nNote: Here, we have written a function to make the split as we will also need to apply it also for another dataset in the regression part.\n\nFit a random forest on the train set. The target is the taste variable that we want to predict. Specify for the number of trees ntree=1000 (by default, the function selects 500 trees). Remember to exclude quality in the predictors of the formula. Also, use the option importance=TRUE, we will need it afterward. Then test the model by computing the accuracy on the test set. You may use confusionMatrix from caret.\n\n\nR\nPython\n\n\n\n\nwine_rf &lt;- randomForest(quality~., data=wine_tr, ntree=1000, importance=TRUE)\nwine.pred_rf &lt;- predict(wine_rf, newdata=wine_te)\n\nlibrary(caret)\nconfusionMatrix(data=wine.pred_rf, reference = wine_te$quality)\n\n\n\n\n# Fit a Random Forest classifier on the train set\nwine_rf = RandomForestClassifier(n_estimators=1000, random_state=123)\nwine_rf.fit(train_wine.drop('quality', axis=1), train_wine['quality'])\n\n# Test the model and compute the accuracy\nwine_pred_rf = wine_rf.predict(test_wine.drop('quality', axis=1))\nprint(confusion_matrix(test_wine['quality'], wine_pred_rf))\nprint(\"Accuracy:\", accuracy_score(test_wine['quality'], wine_pred_rf))\n\nThis model is worse than the R version mostly because of the different defaults.\n\n\n\n\nExtract the model-specific variable importance using the functions varImpPlot (plots) and importance (values) on the model. Observe well that the mean decrease in accuracy of each variable is also computed for each specific class. In particular, what makes density special for predicting Good compare to another variable (like for example citric.acid)?\n\n\nR\nPython\n\n\n\n\nvarImpPlot(wine_rf)\nimportance(wine_rf)\n\n\n\n\n# Variable importance\nwine_importances = pd.Series(wine_rf.feature_importances_, index=train_wine.drop('quality', axis=1).columns)\nwine_importances.sort_values(ascending=False).plot(kind='bar')\n\n\n\n\ndensity is important for predicting the Good since their predictions is much less accurate if we do not use it. citric.acid is both overall less important than density but especially for prediction of Good.",
    "crumbs": [
      "Labs",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html#regression",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html#regression",
    "title": "Ensemble Methods",
    "section": "",
    "text": "In this part, we will be using the real_estate_data.csv once again. After reading the data, apply a random forest to predict price using all the other variables except No, Month and Year. Compute the RMSE and inspect the prediction quality with a graph. Note that the importance is not specific to any class here.\n\n\nR\nPython\n\n\n\n\nlibrary(dplyr)\nlibrary(magrittr)\nreal_estate_data &lt;- read.csv(here::here(\"labs/data/real_estate_data.csv\"))\n\n# select the columns of interest\nreal_estate_data &lt;- \n  real_estate_data %&gt;% \n  select(-c(No, Month, Year))\n\n# once again, divide the data into training and testing sets using the function created earlier\nrestate_index &lt;- get_split_index(real_estate_data)\nrestate_tr &lt;- real_estate_data[restate_index == 1, ]\nrestate_te &lt;- real_estate_data[restate_index == 2, ]\n\n# apply the RF model as a regression\nrestate_rf &lt;- randomForest(Price~., data=restate_tr, ntree=1000, importance=TRUE)\nrestate.pred_rf&lt;-predict(restate_rf, newdata=restate_te)\n\n# compute rmse and plot the results as well the VarImp\n(rmse &lt;- sqrt(mean((restate_te$Price - restate.pred_rf)^2)))\nplot(restate_te$Price ~ restate.pred_rf)\nabline(0,1)\nvarImpPlot(restate_rf)\nimportance(restate_rf)\n\n\n\n\n# Load real estate dataset\nreal_estate_data = pd.read_csv(\"../data/real_estate_data.csv\")\n\nreal_estate_data = real_estate_data.drop(['No', 'Month', 'Year'], axis=1)\n\n# Split real estate dataset into train and test\ntrain_restate, test_restate = train_test_split(real_estate_data, test_size=0.25, random_state=123)\n\n# Fit a Random Forest regressor on the train set\nrestate_rf = RandomForestRegressor(n_estimators=1000, random_state=123)\nrestate_rf.fit(train_restate.drop('Price', axis=1), train_restate['Price'])\n\n# Test the model and compute the RMSE\nrestate_pred_rf = restate_rf.predict(test_restate.drop('Price', axis=1))\nrmse = np.sqrt(mean_squared_error(test_restate['Price'], restate_pred_rf))\nprint(\"RMSE:\", rmse)\n\n# Plot the prediction quality\nplt.scatter(test_restate['Price'], restate_pred_rf)\nplt.xlabel('Actual Price')\nplt.ylabel('Predicted Price')\nplt.plot([min(test_restate['Price']), max(test_restate['Price'])], [min(test_restate['Price']), max(test_restate['Price'])], color='red')\nplt.show()\n\n# Variable importance\nrestate_importances = pd.Series(restate_rf.feature_importances_, index=train_restate.drop('Price', axis=1).columns)\nrestate_importances.sort_values(ascending=False).plot(kind='bar')\n\n\n\n\nCompare this model with the one you came up with in Ex_ML_LinLogReg . Which one would you go for?",
    "crumbs": [
      "Labs",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html#classification-1",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html#classification-1",
    "title": "Ensemble Methods",
    "section": "Classification",
    "text": "Classification\nTraining and testing the model\nWe now fit a GBM model on the wine training set and apply it to the same target variable quality. We can train the model and add\n\n\nR\nPython\n\n\n\n\nlibrary(gbm)\nset.seed(123)\nwine_gbm &lt;- gbm(quality~., data=wine_tr, distribution=\"multinomial\", n.trees=1000, interaction.depth=4, shrinkage=0.01)\nwine.pred_gbm &lt;- predict(wine_gbm, newdata=wine_te, n.trees=1000, type=\"response\")\nwine.pred_gbm_class &lt;- apply(wine.pred_gbm, 1, which.max)\nlevels(wine_te$quality) &lt;- 1:length(levels(wine_te$quality))\nconfusionMatrix(factor(wine.pred_gbm_class), wine_te$quality)\n\n\n\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Fit a Gradient Boosting classifier on the train set\nwine_gbm = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.01, max_depth=4, random_state=123)\nwine_gbm.fit(train_wine.drop('quality', axis=1), train_wine['quality'])\n\n# Test the model and compute the accuracy\nwine_pred_gbm = wine_gbm.predict(test_wine.drop('quality', axis=1))\nprint(confusion_matrix(test_wine['quality'], wine_pred_gbm))\nprint(\"Accuracy:\", accuracy_score(test_wine['quality'], wine_pred_gbm))",
    "crumbs": [
      "Labs",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html#regression-1",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html#regression-1",
    "title": "Ensemble Methods",
    "section": "Regression",
    "text": "Regression\nIn this part, we will continue using the real_estate_data.csv. Fit a GBM model on the real estate training set to predict price using all the other variables except No, Month, and Year. Then compute the metrics and plot the predictions.\n\n\nR\nPython\n\n\n\n\nset.seed(123)\nrestate_gbm &lt;- gbm(Price~., data=restate_tr, distribution=\"gaussian\", n.trees=1000, interaction.depth=4, shrinkage=0.01)\nrestate.pred_gbm&lt;-predict(restate_gbm, newdata=restate_te, n.trees=1000)\n\n# compute rmse and plot the results\n(rmse_gbm &lt;- sqrt(mean((restate_te$Price - restate.pred_gbm)^2)))\nplot(restate_te$Price ~ restate.pred_gbm)\nabline(0,1)\n\n\n\n\n# run the code below if you have not cleared the plot yet\nplt.clf()\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n# Fit a Gradient Boosting regressor on the train set\nrestate_gbm = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01, max_depth=4, random_state=123)\nrestate_gbm.fit(train_restate.drop('Price', axis=1), train_restate['Price'])\n# Test the model and compute the RMSE\n\nrestate_pred_gbm = restate_gbm.predict(test_restate.drop('Price', axis=1))\nrmse_gbm = np.sqrt(mean_squared_error(test_restate['Price'], restate_pred_gbm))\nprint(\"RMSE:\", rmse_gbm)\n# Plot the prediction quality\n\nplt.scatter(test_restate['Price'], restate_pred_gbm)\nplt.xlabel('Actual Price')\nplt.ylabel('Predicted Price')\nplt.plot([min(test_restate['Price']), max(test_restate['Price'])], [min(test_restate['Price']), max(test_restate['Price'])], color='red')\nplt.show()\n\n\n\n\nCompare the GBM model with the Random Forest model you came up with earlier. Which one would you go for?",
    "crumbs": [
      "Labs",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html#what-is-xgboost",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html#what-is-xgboost",
    "title": "Ensemble Methods",
    "section": "What is XGBoost?",
    "text": "What is XGBoost?\nXGBoost (Extreme Gradient Boosting) is an optimized implementation of the gradient boosting algorithm. It is designed for high performance and efficient memory usage. XGBoost improves upon the base Gradient Boosting Machine (GBM) by incorporating regularization to prevent overfitting and implementing parallel processing techniques for faster training. The algorithm also offers built-in cross-validation and early stopping to save time and resources during model training.",
    "crumbs": [
      "Labs",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "labs/06_Ensembles/Ex_ML_Ensemble.html#modelling-with-xgboost",
    "href": "labs/06_Ensembles/Ex_ML_Ensemble.html#modelling-with-xgboost",
    "title": "Ensemble Methods",
    "section": "Modelling with XGBoost",
    "text": "Modelling with XGBoost\nWe’ll use the xgboost library in both R and python. You can see some of the hyperparameters below:\n\n\n\n\n\n\nHyperparameters of XGBoost\n\n\n\n\n\neta: Controls the learning rate, which determines the step size at each iteration while updating the model weights. Smaller values make the model more robust to overfitting but require more iterations to converge. Typical values range from 0.01 to 0.3.\n\nmax_depth: Controls the maximum depth of each tree. Deeper trees can model more complex relationships but are more prone to overfitting. Experiment with different values, keeping in mind that a shallower tree can be more interpretable and less prone to overfitting.\n\nmin_child_weight: Controls the minimum sum of instance weights needed in a child node. Increasing this value helps to prevent overfitting by making the model more conservative.\n\n\n\nYou can read more about the package in its documentation.\n\n\nR\nPython\n\n\n\n\n# Install and load the package\n# install.packages(\"xgboost\")\nlibrary(xgboost)\n\n# Prepare data for XGBoost\ndtrain &lt;- xgb.DMatrix(data = as.matrix(restate_tr[, -ncol(restate_tr)]), label = restate_tr$Price)\ndtest &lt;- xgb.DMatrix(data = as.matrix(restate_te[, -ncol(restate_te)]), label = restate_te$Price)\n\n# Set hyperparameters\nparams &lt;- list(\n  objective = \"reg:squarederror\",\n  eta = 0.1,\n  max_depth = 5,\n  min_child_weight = 1,\n  subsample = 1,\n  colsample_bytree = 1\n)\n\n# Train the model\nxgb_model &lt;- xgb.train(params, dtrain, nrounds = 1000)\n\n# Test the model and compute the RMSE\nrestate_pred_xgb &lt;- predict(xgb_model, dtest)\nrmse_xgb &lt;- sqrt(mean((restate_te$Price - restate_pred_xgb)^2))\nprint(paste(\"RMSE:\", rmse_xgb))\n\n\n\nWe will first install the xgboost with a R code chunk:\n\nreticulate::py_install(\"xgboost\", pip=TRUE)\n\nThen we can run the XGBoost model with xgboost.\n\n# Install and load the package\nimport xgboost as xgb\n\n# Prepare data for XGBoost\ndtrain = xgb.DMatrix(train_restate.drop(\"Price\", axis=1), label=train_restate[\"Price\"])\ndtest = xgb.DMatrix(test_restate.drop(\"Price\", axis=1), label=test_restate[\"Price\"])\n\n# Set hyperparameters\nparams = {\n    \"objective\": \"reg:squarederror\",\n    \"eta\": 0.1,\n    \"max_depth\": 3,\n    \"min_child_weight\": 1,\n    \"subsample\": 1,\n    \"colsample_bytree\": 1,\n}\n\n# Train the model\nxgb_model = xgb.train(params, dtrain, num_boost_round=1000)\n\n# Test the model and compute the RMSE\nrestate_pred_xgb = xgb_model.predict(dtest)\nrmse_xgb = np.sqrt(mean_squared_error(test_restate[\"Price\"], restate_pred_xgb))\nprint(\"RMSE:\", rmse_xgb)\n\nAlthough initially our GBM suffered compared to the RF, we can see that XGBoost can help improve the result (the case for the python implementation). However, random forest still outperforms all the other models.\n\n\n\nFeel free to apply XGBoost to the dataset of your choice.",
    "crumbs": [
      "Labs",
      "Ensemble Methods"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html",
    "href": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "In this exercise, we’ll use the same wine data introduced in the ensemble exercises. As noted before, all the numerical features have units. Additionally, the original objective of this dataset was to predict the wine quality from the other features (supervised learning). The data will only be used for unsupervised task here.\nFirst, load the data and scale the numerical features:\n\n\nR\nPython\n\n\n\n\nwine &lt;- read.csv(here::here(\"labs/data/Wine.csv\"))\nrow.names(wine) &lt;- paste(\"W\", c(1:nrow(wine)), sep=\"\") # row names are used after\nhead(wine)\nsummary(wine)\nwine[,-12] &lt;- scale(wine[,-12]) ## scale all the features except \"quality\"\n\n\n\n\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nwine = pd.read_csv('../../data/Wine.csv')\nwine.index = [\"W\" + str(i) for i in range(1, len(wine)+1)]\nscaler = StandardScaler()\nwine.iloc[:, :-1] = scaler.fit_transform(wine.iloc[:, :-1])\nwine.head()\nwine.describe()\n\n\n\n\n\n\n\nWarning\n\n\n\nPlease note that all the interpretations will be based on the R outputs (like PCA and most other exercises). As usual, the python outputs may be slightly different due to differences in the implementations of the algorithms.",
    "crumbs": [
      "Labs",
      "Clustering"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#distances",
    "href": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#distances",
    "title": "Clustering",
    "section": "Distances",
    "text": "Distances\nWe apply here an agglomerative hierarchical clustering (AGNES). Only the numerical features are used here. First, we compute the distances and plot them. We use Manhattan distance below.\n\n\nR\nPython\n\n\n\n\nlibrary(reshape2) # contains the melt function\nlibrary(ggplot2)\nwine_d &lt;- dist(wine[,-12], method = \"manhattan\") # matrix of Manhattan distances \n\nwine_melt &lt;- melt(as.matrix(wine_d)) # create a data frame of the distances in long format\nhead(wine_melt)\n\nggplot(data = wine_melt, aes(x=Var1, y=Var2, fill=value)) + \n  geom_tile() \n\n\n\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# wine_d = pd.DataFrame(np.abs(wine.iloc[:, :-1].values[:, None] - wine.iloc[:, :-1].values), columns=wine.index, index=wine.index)\n# from scipy.spatial.distance import pdist, squareform\n\n# wine_d = pdist(wine.iloc[:, :-1], metric='cityblock')\n# wine_d = squareform(wine_d)\n# wine_d_df = pd.DataFrame(wine_d, index=wine.index, columns=wine.index)\nfrom scipy.spatial.distance import cdist\n\nwine_d = pd.DataFrame(cdist(wine.iloc[:, :-1], wine.iloc[:, :-1], metric='cityblock'), columns=wine.index, index=wine.index)\n\nwine_melt = wine_d.reset_index().melt(id_vars='index', var_name='Var1', value_name='value')\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(wine_d, cmap=\"coolwarm\", center=0)\nplt.show()\n\n\n\n\nWe can see that some wines are closer than others (darker color). However, it is not really possible to extract any information from such a graph.",
    "crumbs": [
      "Labs",
      "Clustering"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#dendrogram",
    "href": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#dendrogram",
    "title": "Clustering",
    "section": "Dendrogram",
    "text": "Dendrogram\nNow, we build a dendrogram using a complete linkage.\n\n\nR\nPython\n\n\n\n\nwine_hc &lt;- hclust(wine_d, method = \"complete\")\nplot(wine_hc, hang=-1)\n\n\n\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\nwine_linkage = linkage(wine.iloc[:, :-1], method='complete', metric='cityblock')\nplt.figure(figsize=(10, 8))\ndendrogram(wine_linkage, labels=wine.index, orientation='top', color_threshold=0, leaf_font_size=10)\nplt.show()\n\n\n\n\nWe cut the tree to 4 clusters, and represent the result. We also extract the cluster assignment of each wine.\n\n\nR\nPython\n\n\n\n\nplot(wine_hc, hang=-1)\nrect.hclust(wine_hc, k=4)\nwine_clust &lt;- cutree(wine_hc, k=4)\nwine_clust\n\n\n\n\nfrom scipy.cluster.hierarchy import fcluster\n\nplt.figure(figsize=(10, 8))\ndendrogram(wine_linkage, labels=wine.index, orientation='top', color_threshold=80, leaf_font_size=10)\nplt.axhline(y=80, color='black', linestyle='--')\nplt.show()\nwine_clust = fcluster(wine_linkage, 4, criterion='maxclust')\nwine_clust",
    "crumbs": [
      "Labs",
      "Clustering"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#interpretation-of-the-clusters",
    "href": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#interpretation-of-the-clusters",
    "title": "Clustering",
    "section": "Interpretation of the clusters",
    "text": "Interpretation of the clusters\nNow we analyze the clusters by looking at the distribution of the features within each cluster.\n\n\nR\nPython\n\n\n\n\nwine_comp &lt;- data.frame(wine[,-12], Clust=factor(wine_clust), Id=row.names(wine))\nwine_df &lt;- melt(wine_comp, id=c(\"Id\", \"Clust\"))\nhead(wine_df)\n\nggplot(wine_df, aes(y=value, group=Clust, fill=Clust)) +\n  geom_boxplot() +\n  facet_wrap(~variable, ncol=4, nrow=3)\n\n\n\n\nwine_comp = wine.iloc[:, :-1].copy()\nwine_comp['Clust'] = wine_clust\nwine_comp['Id'] = wine.index\nwine_melt = wine_comp.melt(id_vars=['Id', 'Clust'])\n\nplt.figure(figsize=(14, 10))\nsns.boxplot(x='variable', y='value', hue='Clust', data=wine_melt)\nplt.xticks(rotation=90)\nplt.legend(title='Cluster')\nplt.show()\n\n\n\n\nWe can see for example that - Cluster 4 (the smallest): small pH, large fixed.acidity and citric.acid, a large density, and a small alcohol. Also a large free.sulfur.dioxide. - Cluster 2: also has large fixed.acidity but not citric.acid. It looks like less acid than cluster 3. - Cluster 3: apparently has a large alcohol. - Etc.",
    "crumbs": [
      "Labs",
      "Clustering"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#choice-of-the-number-of-clusters",
    "href": "labs/08_UnsupervisedLearning/081_Clustering/Ex_ML_Clustering.html#choice-of-the-number-of-clusters",
    "title": "Clustering",
    "section": "Choice of the number of clusters",
    "text": "Choice of the number of clusters\nTo choose the number of cluster, we can inspect the dendrogram (judgmental approach), or we can rely on a statistics. Below, we use the within sum-of-squares, the GAP statistics, and the silhouette. It is obtained by the function fviz_nbclust in package factoextra. It uses the dendrogram with complete linkage on Manhattan distance is obtained using the function hcut with hc_method=\"complete\" and hc_metric=\"manhattan\".\n\n\nR\nPython\n\n\n\n\nlibrary(factoextra)\nfviz_nbclust(wine[,-12],\n             hcut, hc_method=\"complete\",\n             hc_metric=\"manhattan\",\n             method = \"wss\", \n             k.max = 25, verbose = FALSE)\nfviz_nbclust(wine[,-12],\n             hcut, hc_method=\"complete\",\n             hc_metric=\"manhattan\",\n             method = \"silhouette\", \n             k.max = 25, verbose = FALSE)\nfviz_nbclust(wine[,-12],\n             hcut, hc_method=\"complete\",\n             hc_metric=\"manhattan\",\n             method = \"gap\", \n             k.max = 25, verbose = FALSE)\n\n\n\nWe need to install the gap_statistic for obtaining a suggested number of clusters. For more information, see https://github.com/milesgranger/gap_statistic\n\npy_install(c(\"gap-stat\",\"scikit-learn-extra\"), envname = \"MLBA\", pip=TRUE)\n\n\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score\nfrom gap_statistic import OptimalK\nimport numpy as np\n\nX = wine.iloc[:, :-1].values\n\n# Clear previous plot (if any)\nplt.clf()\n\n# Within sum-of-squares\nwss = []\nfor k in range(1, 26):\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n    wss.append(sum(((X - centroids[labels]) ** 2).sum(axis=1)))\n\n\n\n\nAgglomerativeClustering(linkage='complete', metric='manhattan', n_clusters=25)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  AgglomerativeClustering?Documentation for AgglomerativeClusteringiFittedAgglomerativeClustering(linkage='complete', metric='manhattan', n_clusters=25) \n\n\nplt.plot(range(1, 26), wss, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Within sum-of-squares')\nplt.show()\n# Clear previous plot (if any)\nplt.clf()\n\n# Silhouette\nsilhouette_scores = []\nfor k in range(2, 26):\n    model = AgglomerativeClustering(n_clusters=k, metric='manhattan', linkage='complete')\n    model.fit(X)\n    labels = model.labels_\n    silhouette_scores.append(silhouette_score(X, labels))\n\n\n\n\nAgglomerativeClustering(linkage='complete', metric='manhattan', n_clusters=25)\nIn a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n\n\n  AgglomerativeClustering?Documentation for AgglomerativeClusteringiFittedAgglomerativeClustering(linkage='complete', metric='manhattan', n_clusters=25) \n\n\nplt.plot(range(2, 26), silhouette_scores, marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette score')\nplt.show()\n# Clear previous plot (if any)\nplt.clf()\n\n# Gap statistic\ngs_obj = OptimalK(n_jobs=1)\nn_clusters = gs_obj(X, cluster_array=np.arange(1, 26))\ngap_stats = gs_obj.gap_df\n\nplt.plot(gap_stats['n_clusters'], gap_stats['gap_value'], marker='o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Gap statistic')\nplt.show()\n\n\n\n\nLike often, these methods are not easy to interpret. Globally, they choose k=2 or k=3.",
    "crumbs": [
      "Labs",
      "Clustering"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html",
    "href": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html",
    "title": "Autoencoders",
    "section": "",
    "text": "In this series of exercises, we illustrate autoencoders on the wine data already used for clustering & PCA. We first load the data.\n\n\nR\nPython\n\n\n\n\nwine &lt;- read.csv(here::here(\"labs/data/Wine.csv\"))\nrow.names(wine) &lt;- paste(\"W\", c(1:nrow(wine)), sep=\"\") \nwine[,-12] &lt;- scale(wine[,-12])\n\n\n\n\nlibrary(reticulate)\nuse_condaenv(\"MLBA\")\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\nwine = pd.read_csv('../../data/Wine.csv')\nwine.index = [\"W\" + str(i) for i in range(1, len(wine)+1)]\nscaler = StandardScaler()\nwine.iloc[:, :-1] = scaler.fit_transform(wine.iloc[:, :-1])\n\n\n\n\nNote that here the scaling of the variables is optional. Scaling the features can sometimes help with the autoencoder, especially when creating lower-dimensional representation of the data.",
    "crumbs": [
      "Labs",
      "Autoencoders"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#simulate-missing-values",
    "href": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#simulate-missing-values",
    "title": "Autoencoders",
    "section": "Simulate missing values",
    "text": "Simulate missing values\nThis step is done only for demonstration since the wine data does not contain missing values, and we must artificially create them and place them in a dataframe called wine_missing. Applying this technique doesn’t require this step since your dataset already should already contain the missing values.\n\n\nR\nPython\n\n\n\n\nset.seed(123)\nwine_missing &lt;- wine\nwine_missing[sample(1:nrow(wine_missing), size = nrow(wine_missing)*0.2), sample(1:ncol(wine_missing), size = ncol(wine_missing)*0.2)] &lt;- NA\n\n\n\n\nimport numpy as np\nimport random\nnp.random.seed(123)\n# Create a copy of the wine dataset and randomly remove 20% of the data\nwine_missing = wine.copy()\nix = [(row, col) for row in range(wine_missing.shape[0]) for col in range(wine_missing.shape[1])]\nfor row, col in random.sample(ix, int(round(.2*len(ix)))):\n    wine_missing.iat[row, col] = np.nan",
    "crumbs": [
      "Labs",
      "Autoencoders"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#train-with-complete-data",
    "href": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#train-with-complete-data",
    "title": "Autoencoders",
    "section": "Train with complete data",
    "text": "Train with complete data\nNow, we can use an autoencoder to infer these missing values. To do so, we can train the autoencoder with the complete data and then use it to infer the missing values.\n\n\nR\nPython\n\n\n\n\n# Re-define the autoencoder model\ninput_layer &lt;- layer_input(shape = input_dim)\nencoder_layer &lt;-\n  layer_dense(units = encoding_dim, activation = 'relu')(input_layer)\ndecoder_layer &lt;-\n  layer_dense(units = input_dim, activation = 'linear')(encoder_layer)\nautoencoder &lt;- keras_model(input_layer, decoder_layer)\nsummary(autoencoder)\nautoencoder %&gt;% compile(optimizer = \"adam\", loss = \"mse\")\n\n# Train the autoencoder with the complete data\nhist &lt;- \n  autoencoder %&gt;% fit(\n  x = as.matrix(wine[, -12]),\n  y = as.matrix(wine[, -12]),\n  epochs = 300,\n  batch_size = 32,\n  shuffle = TRUE,\n  verbose = 0, #set it as `1` if you want to see the training messages\n  validation_split = 0.2,\n  callbacks = callback_early_stopping(\n    monitor = \"val_loss\",\n    patience = 5,\n    restore_best_weights = TRUE\n  ),\n)\n\n## uncomment if you want to see the training plot\n# plot(hist)\n\n# Replace missing values with the inferred values\nwine_missing_original &lt;- wine_missing\n\n# replace the missing values with something before being able to make a prediction on it (model cannot re-construct NA directly)\nwine_missing[is.na(wine_missing)] &lt;- 0  # Mask the missing values as 0\n\n# Use the autoencoder to infer the missing values\npredicted_wine &lt;- autoencoder %&gt;% predict(as.matrix(wine_missing[, -12]))\n\n\n\n\n# to introduce early stopping\nfrom keras.callbacks import EarlyStopping\n\n# Re-define the autoencoder model\ninput_layer = Input(shape=(input_dim,))\nencoder_layer = Dense(encoding_dim, activation='relu')(input_layer)\ndecoder_layer = Dense(input_dim, activation='linear')(encoder_layer)\nautoencoder = Model(input_layer, decoder_layer)\nautoencoder.compile(optimizer='adam', loss='mse')\n\n# Train the autoencoder with the complete data\nhist = autoencoder.fit(wine.iloc[:, :-1], \n  wine.iloc[:, :-1], \n  epochs=300, \n  batch_size=32, \n  shuffle=True, \n  verbose=0, #set it as `1` if you want to see the training messages\n  validation_split=0.2, \n  callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)])\n\n# # uncomment if you want to see the training plot\n# plt.clf()\n# plt.plot(hist.history['loss'])\n# plt.plot(hist.history['val_loss'])\n# plt.title('Model loss')\n# plt.ylabel('Loss')\n# plt.xlabel('Epoch')\n# plt.legend(['Train', 'Val'], loc='upper right')\n# plt.show()\n\n# Replace missing values with the inferred values\nwine_missing_original = wine_missing.copy()\n\n# replace the missing values with something before being able to make a prediction on it (model cannot re-construct NA directly)\nwine_missing.fillna(0, inplace=True)  # Mask the missing values as 0\n\n# Use the autoencoder to infer the missing values\npredicted_wine = autoencoder.predict(wine_missing.iloc[:, :-1].values)",
    "crumbs": [
      "Labs",
      "Autoencoders"
    ]
  },
  {
    "objectID": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#predict-missing-values",
    "href": "labs/08_UnsupervisedLearning/083_AutoEncoders/Ex_ML_Autoencoder.html#predict-missing-values",
    "title": "Autoencoders",
    "section": "Predict missing values",
    "text": "Predict missing values\nFinally, we replace the missing values in the wine dataset with the inferred values from the autoencoder.\n\n\nR\nPython\n\n\n\n\nwine_missing[is.na(wine_missing_original)] &lt;- predicted_wine[is.na(wine_missing_original)]\n\nlibrary(dplyr)\n# see the old vs new values by filtering for the rows that contained an NA\nmissing_rows &lt;- wine_missing_original %&gt;% \n  mutate(row_index = row_number()) %&gt;%\n  dplyr::filter_at(vars(dplyr::everything()),any_vars(is.na(.)))\n\n# you can see the new values that were re-constructed\nwine_missing_original[missing_rows$row_index,]\nwine_missing[missing_rows$row_index,]\n\n\n\n\n# Identify the missing values in the original dataframe\nmissing_mask = wine_missing_original.iloc[:, :-1].isna()\n\n# Replace the missing values with the predicted ones\nfor i in range(wine_missing.shape[1] - 1):  # iterate over all columns except the last one\n    wine_missing.loc[missing_mask.iloc[:, i], wine_missing.columns[i]] = predicted_wine[missing_mask.iloc[:, i], i]\n\n# see the old vs new values by filtering for the rows that contained an NA\nmissing_rows = wine_missing_original[wine_missing_original.iloc[:, :-1].isna().any(axis=1)]\n\n# you can see the new values that were re-constructed\nprint(wine_missing_original.loc[missing_rows.index, :])\nprint(wine_missing.loc[missing_rows.index, :])\n\n\n\n\nNote how the dataset wine_missing doesn’t contain any missing values. The missing values have been inferred by the autoencoder. This method can be a powerful tool to deal with missing data in machine learning projects.\n\n\n\n\n\n\nImportant\n\n\n\nThere are two essential things to note about using autoencoders for imputation:\n\nAlternative to our approach, you can train with incomplete data and mask the “NA” in the training data with 0s (as shown for inference). In theory, you can put anything for the mask value, for example, the mean or median value for the missing inputs, and then try to recover those (although if you scale your data, 0 may work better in practice). Once the model is trained, you can recover the value for NA while masking the NAs again with 0s (as already implemented by us). In that case, you’re pushing your model to predict 0 for the missing instances, which can sometimes be inappropriate.\nPlease note for auto-encoders to work well, you’ll need a lot of observations. Additionally, you should always compare the performance of this technique against simply using mean and median values for the missing data. Another library in R commonly used for dealing with missing data is called missForest, which uses a random forest for imputation. If you need techniques to deal with missing data, feel free to check it out and make sure you understand how it works (it falls beyond the scope of this course).",
    "crumbs": [
      "Labs",
      "Autoencoders"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Book references\n\n\nLicense\nThis course is provided under  which is the Creative Commons Attribution-ShareAlike 4.0 International license.\n\n\nAcknowledgements\nThe MLBA course website design was adapted and inspired from the following sources:\n\nDr. Mine Çetinkaya-Rundel STA 101 website\nDr. Tomas Masák, Dr. Linda Mhalla and Charles Dufour MATH-517 website",
    "crumbs": [
      "Course information",
      "References"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Important dates (more info during the class)\n\n\n\n\nMonday the 29th of April: Written exam (in-class)\n\nSunday the 19th of May: Project report deadline\n\nMonday the 27th of May: Presentations of the projects",
    "crumbs": [
      "Course information",
      "Schedule"
    ]
  }
]